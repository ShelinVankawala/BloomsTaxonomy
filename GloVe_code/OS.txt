microsoft windows overview history the story of windows begins with a very different os developed by microsoft for the first ibm personal computer and referred to as msdos the initial version msdos was released in august it consisted of lines of assembly language source code and ran in kbytes of memory using the intel microprocessor the ibm pc was an important stage in a continuing revolution in computing that has expanded computing from the data center of the s to the departmental minicomputer of the s and to the desktop in the s the revolution has continued with computing moving into the briefcase in the s and into our pockets during the most recent decade microsofts initial os ran a single application at a time using a command line interface to control the system it took a long time for microsoft to develop a true gui interface for the pc on their third try they succeeded the bit windows shipped in and instantly became successful selling a million copies in six months windows was implemented as a layer on top of msdos and suffered from the limitations of that primitive system five years later microsoft shipped a bit version windows which was also very successful and led to the development of additional versions windows and windows me meanwhile it had become clear to microsoft that the msdos platform could not sustain a truly modern os in microsoft hired dave cutler who had developed the very successful rsxm and vaxvms operating systems at digital equipment corporation cutlers charter was to develop a modern os which was portable to architectures other than the intel x family and yet compatible with the os system that microsoft was jointly developing with ibm as well as the portable unix standard posix this system was christened nt new technology the first version of windows nt was released in with the same gui as windows the followon to windows however nt was a new bit os with the ability to support older dos and windows applications as well as provide os support several versions of nt x followed with support for additional hardware platforms in microsoft released nt with the same user interface as windows in microsoft introduced the next major upgrade of the nt os windows the underlying executive and kernel architecture is fundamentally the same as in nt but new features have been added the emphasis in windows was the addition of services and functions to support distributed processing the central element of windows s new features was active directory which is a distributed directory service able to map names of arbitrary objects to any kind of information about those objects windows also added the plugandplay and powermanagement facilities that were already in windows the successor to windows these features are particularly important for laptop computers in a new desktop version of nt was released known as windows xp the goal of windows xp was to finally replace the versions of windows based on msdos with an os based on nt in microsoft shipped windows vista for the desktop and a short time later windows server in they shipped windows and windows server r despite the difference in naming the client and server versions of these systems use many of the same files but with additional features and capabilities enabled for servers over the years nt has attempted to support multiple processor architectures the intel i was the original target for nt as well as the x subsequently nt added support for the digital alpha architecture the powerpc and the mips later came the intel ia itanium and the bit version of the x based on the amd processor architecture windows supports only x and amd windows server r supports only amd and ia but microsoft has announced that it will end support for ia in future releases all the other processor architectures have failed in the market and today only the x amd and arm architectures are viable microsofts support for arm is limited to their windows ce os which runs on phones and handheld devices windows ce has little relationship to the ntbased windows that runs on slates netbookslaptops desktops and servers microsoft has announced that it is developing a version of nt that targets cloud computing windows azure azure includes a number of features that are specific to the requirements of public and private clouds though it is closely related to windows server it does not share files in the same way that the windows client and server versions do the modern os modern operating systems such as todays windows and unix with all its flavors like solaris linux and macos x must exploit the capabilities of all the billions of transistors on each silicon chip they must work with multiple bit and bit cpus with adjunct gpus dsps and fixed function units they must provide support for sophisticated inputoutput multiple touchsensitive displays cameras microphones biometric and other sensors and handle a variety of data challenges streaming media photos scientific number crunching search queries all while giving a human being a responsive realtime experience with the computing system to handle these requirements the computer can not be doing only one thing at a time unlike the early days of the pc when the os ran a single application at a time hundreds of activities are taking place to provide the modern computing experience the os can no longer just switch to the application and step away until it is needed it must aggressively manage the system and coordinate between all the competing computations that are taking place often simultaneously on the multiple cpus gpus and dsps that may be present in a modern computing environment thus all modern operating systems have multitasking capability even though they may be acting on behalf of only a single human being called the user windows is a sophisticated multitasking os designed to manage the complexity of the modern computing environment provide a rich platform for application developers and support a rich set of experiences for users like solaris windows is designed to have the features that enterprises need while at the same time windows like macos provides the simplicity and easeofuse that consumers require in the following sections we will present an overview of the fundamental structure and capabilities of windows architecture figure illustrates the overall structure of windows all releases of windows based on nt have essentially the same structure at this level of detail as with virtually all operating systems windows separates applicationoriented software from the core os software the latter which includes the executive the kernel device drivers and the hardware abstraction layer runs in kernel mode kernel mode software has access to system data and to the hardware the remaining software running in user mode has limited access to system data operating system organization windows has a highly modular architecture each system function is managed by just one component of the os the rest of the os and all applications access that function through the responsible component using standard interfaces key system data can only be accessed through the appropriate system support service processes applications processes service control svchostexe manager task manager environment lsass winmgmtexe subsystems windows winlogon spooler explorer posix session servicesexe user manager application subsytem dlls win ntdlldll system user mode threads kernel mode system service dispatcher kernelmode callable interfaces win user io manager file system cache security reference manager registry gdi device object manager manager plugandplay power manager monitor virtual memory threads processes and configuration call local procedure and file graphics system drivers drivers kernel hardware abstraction layer hal lsass local security authentication server colored area indicates executive posix portable operating system interface gdi graphics device interface dll dynamic link libraries figure windows and windows vista architecture russ function in principle any module can be removed upgraded or replaced without rewriting the entire system or its standard application program interfaces apis the kernelmode components of windows are the following executive contains the core os services such as memory management process and thread management security io and interprocess communication kernel controls execution of the processors the kernel manages thread scheduling process switching exception and interrupt handling and multiprocessor synchronization unlike the rest of the executive and the user level the kernels own code does not run in threads hardware abstraction layer hal maps between generic hardware commands and responses and those unique to a specific platform it isolates the os from platformspecific hardware differences the hal makes each computers system bus direct memory access dma controller interrupt controller system timers and memory controller look the same to the executive and kernel components it also delivers the support needed for smp explained subsequently device drivers dynamic libraries that extend the functionality of the executive these include hardware device drivers that translate user io function calls into specific hardware device io requests and software components for implementing file systems network protocols and any other system extensions that need to run in kernel mode windowing and graphics system implements the gui functions such as dealing with windows user interface controls and drawing the windows executive includes components for specific system functions and provides an api for usermode software following is a brief description of each of the executive modules io manager provides a framework through which io devices are accessible to applications and is responsible for dispatching to the appropriate device drivers for further processing the io manager implements all the windows io apis and enforces security and naming for devices network protocols and file systems using the object manager windows io is discussed in chapter cache manager improves the performance of filebased io by causing recently referenced file data to reside in main memory for quick access and deferring disk writes by holding the updates in memory for a short time before sending them to the disk in more efficient batches object manager creates manages and deletes windows executive objects that are used to represent resources such as processes threads and synchronization objects it enforces uniform rules for retaining naming and setting the security of objects the object manager also creates the entries in each processes handle table which consist of access control information and a pointer to the object windows objects are discussed later in this section plugandplay manager determines which drivers are required to support a particular device and loads those drivers power manager coordinates power management among various devices and can be configured to reduce power consumption by shutting down idle devices putting the processor to sleep and even writing all of memory to disk and shutting off power to the entire system security reference monitor enforces accessvalidation and auditgeneration rules the windows objectoriented model allows for a consistent and uniform view of security right down to the fundamental entities that make up the executive thus windows uses the same routines for access validation and for audit checks for all protected objects including files processes address spaces and io devices windows security is discussed in chapter virtual memory manager manages virtual addresses physical memory and the paging files on disk controls the memory management hardware and data structures which map virtual addresses in the processs address space to physical pages in the computers memory windows virtual memory management is described in chapter processthread manager creates manages and deletes process and thread objects windows process and thread management are described in chapter configuration manager responsible for implementing and managing the system registry which is the repository for both systemwide and peruser settings of various parameters advanced local procedure call alpc facility implements an efficient crossprocess procedure call mechanism for communication between local processes implementing services and subsystems similar to the remote procedure call rpc facility used for distributed processing usermode processes four basic types of usermode processes are supported by windows special system processes usermode services needed to manage the system such as the session manager the authentication subsystem the service manager and the logon process service processes the printer spooler the event logger usermode components that cooperate with device drivers various network services and many many others services are used by both microsoft and external software developers to extend system functionality as they are the only way to run background usermode activity on a windows system environment subsystems provide different os personalities environments the supported subsystems are win and posix each environment subsystem includes a subsystem process shared among all applications using the subsystem and dynamic link libraries dlls that convert the user application calls to alpc calls on the subsystem process andor native windows calls user applications executables exes and dlls that provide the functionality users run to make use of the system exes and dlls are generally targeted at a specific environment subsystem although some of the programs that are provided as part of the os use the native system interfaces nt api there is also support for running bit programs on bit systems windows is structured to support applications written for multiple os personalities windows provides this support using a common set of kernel mode components that underlie the os environment subsystems the implementation of each environment subsystem includes a separate process which contains the shared data structures privileges and executive object handles needed to implement a particular personality the process is started by the windows session manager when the first application of that type is started the subsystem process runs as a system user so the executive will protect its address space from processes run by ordinary users an environment subsystem provides a graphical or commandline user interface that defines the look and feel of the os for a user in addition each subsystem provides the api for that particular environment this means that applications created for a particular operating environment need only be recompiled to run on windows because the os interface that they see is the same as that for which they were written the source code does not need to be modified clientserver model the windows os services the environment subsystems and the applications are structured using the clientserver computing model which is a common model for distributed computing and which is discussed in part six this same architecture can be adopted for use internally to a single system as is the case with windows the native nt api is a set of kernelbased services which provide the core abstractions used by the system such as processes threads virtual memory io and communication windows provides a far richer set of services by using the clientserver model to implement functionality in usermode processes both the environment subsystems and the windows usermode services are implemented as processes that communicate with clients via rpc each server process waits for a request from a client for one of its services eg memory services process creation services or networking services a client which can be an application program or another server program requests a service by sending a message the message is routed through the executive to the appropriate server the server performs the requested operation and returns the results or status information by means of another message which is routed through the executive back to the client advantages of a clientserver architecture include the following it simplifies the executive it is possible to construct a variety of apis implemented in usermode servers without any conflicts or duplications in the executive new apis can be added easily it improves reliability each new server runs outside of the kernel with its own partition of memory protected from other servers a single server can fail without crashing or corrupting the rest of the os it provides a uniform means for applications to communicate with services via rpcs without restricting flexibility the messagepassing process is hidden from the client applications by function stubs which are small pieces of code which wrap the rpc call when an application makes an api call to an environment subsystem or a service the stub in the client application packages the parameters for the call and sends them as a message to the server process that implements the call it provides a suitable base for distributed computing typically distributed computing makes use of a clientserver model with remote procedure calls implemented using distributed client and server modules and the exchange of messages between clients and servers with windows a local server can pass a message on to a remote server for processing on behalf of local client applications clients need not know whether a request is being serviced locally or remotely indeed whether a request is serviced locally or remotely can change dynamically based on current load conditions and on dynamic configuration changes threads and smp two important characteristics of windows are its support for threads and for symmetric multiprocessing smp both of which were introduced in section russ lists the following features of windows that support threads and smp os routines can run on any available processor and different routines can execute simultaneously on different processors windows supports the use of multiple threads of execution within a single process multiple threads within the same process may execute on different processors simultaneously server processes may use multiple threads to process requests from more than one client simultaneously windows provides mechanisms for sharing data and resources between processes and flexible interprocess communication capabilities windows objects though the core of windows is written in c the design principles followed draw heavily on the concepts of objectoriented design this approach facilitates the sharing of resources and data among processes and the protection of resources from unauthorized access among the key objectoriented concepts used by windows are the following encapsulation an object consists of one or more items of data called attributes and one or more procedures that may be performed on those data called services the only way to access the data in an object is by invoking one of the objects services thus the data in the object can easily be protected from unauthorized use and from incorrect use eg trying to execute a nonexecutable piece of data object class and instance an object class is a template that lists the attributes and services of an object and defines certain object characteristics the os can create specific instances of an object class as needed for example there is a single process object class and one process object for every currently active process this approach simplifies object creation and management inheritance although the implementation is hand coded the executive uses inheritance to extend object classes by adding new features every executive class is based on a base class which specifies virtual methods that support creating naming securing and deleting objects dispatcher objects are executive objects that inherit the properties of an event object so they can use common synchronization methods other specific object types such as the device class allow classes for specific devices to inherit from the base class and add additional data and methods polymorphism internally windows uses a common set of api functions to manipulate objects of any type this is a feature of polymorphism as defined in appendix d however windows is not completely polymorphic because there are many apis that are specific to a single object type the reader unfamiliar with objectoriented concepts should review appendix d not all entities in windows are objects objects are used in cases where data are intended for user mode access or when data access is shared or restricted among the entities represented by objects are files processes threads semaphores timers and graphical windows windows creates and manages all types of objects in a uniform way via the object manager the object manager is responsible for creating and destroying objects on behalf of applications and for granting access to an objects services and data each object within the executive sometimes referred to as a kernel object to distinguish from userlevel objects not of concern to the executive exists as a memory block allocated by the kernel and is directly accessible only by kernel mode components some elements of the data structure eg object name security parameters usage count are common to all object types while other elements are specific to a particular object type eg a thread objects priority because these object data structures are in the part of each processs address space accessible only by the kernel it is impossible for an application to reference these data structures and read or write them directly instead applications manipulate objects indirectly through the set of object manipulation functions supported by the executive when an object is created the application that requested the creation receives back a handle for the object in essence a handle is an index into a perprocess executive table containing a pointer to the referenced object this handle can then be used by any thread within the same process to invoke win functions that work with objects or can be duplicated into other processes objects may have security information associated with them in the form of a security descriptor sd this security information can be used to restrict access to the object based on contents of a token object which describes a particular user for example a process may create a named semaphore object with the intent that only certain users should be able to open and use that semaphore the sd for the semaphore object can list those users that are allowed or denied access to the semaphore object along with the sort of access permitted read write change etc in windows objects may be either named or unnamed when a process creates an unnamed object the object manager returns a handle to that object and the handle is the only way to refer to it handles can be inherited by child processes or duplicated between processes named objects are also given a name that other unrelated processes can use to obtain a handle to the object for example if proctable windows kernel control objects asynchronous procedure call used to break into the execution of a specified thread and to cause a procedure to be called in a specified processor mode deferred procedure call used to postpone interrupt processing to avoid delaying hardware interrupts also used to implement timers and interprocessor communication interrupt used to connect an interrupt source to an interrupt service routine by means of an entry in an interrupt dispatch table idt each processor has an idt that is used to dispatch interrupts that occur on that processor process represents the virtual address space and control information necessary for the execution of a set of thread objects a process contains a pointer to an address map a list of ready threads containing thread objects a list of threads belonging to the process the total accumulated time for all threads executing within the process and a base priority thread represents thread objects including scheduling priority and quantum and which processors the thread may run on profile used to measure the distribution of run time within a block of code both user and system code can be profiled ess a wishes to synchronize with process b it could create a named event object and pass the name of the event to b process b could then open and use that event object however if a simply wished to use the event to synchronize two threads within itself it would create an unnamed event object because there is no need for other processes to be able to use that event there are two categories of objects used by windows for synchronizing the use of the processor dispatcher objects the subset of executive objects which threads can wait on to control the dispatching and synchronization of threadbased system operations these are described in chapter control objects used by the kernel component to manage the operation of the processor in areas not managed by normal thread scheduling table lists the kernel control objects windows is not a fullblown objectoriented os it is not implemented in an objectoriented language data structures that reside completely within one executive component are not represented as objects nevertheless windows illustrates the power of objectoriented technology and represents the increasing trend toward the use of this technology in os design what is new in windows the core architecture of windows has been very stable however at each release there are new features and improvements made even at the lower levels of the system many of the changes in windows are not visible in the features themselves but in the performance and stability of the system these are due to changes in the engineering behind windows other improvements are due to new features or improvements to existing features engineering improvements the performance of hundreds of key scenarios such as opening a file from the gui are tracked and continuously characterized to identify and fix problems the system is now built in layers which can be separately tested improving modularity and reducing complexity performance improvements the amount of memory required has been reduced both for clients and servers the vmm is more aggressive about limiting the memory use of runaway processes see section background processes can arrange to start upon an event trigger such as a plugging in a camera rather than running continuously reliability improvements the usermode heap is more tolerant of memory allocation errors by cc programmers such as continuing to use memory after it is freed programs that make such errors are detected and the heap allocation policies are modified for that program to defer freeing memory and avoid corruption of the programs data energy efficiency many improvements have been made to the energy efficiency of windows on servers unused processors can be parked reducing their energy use all windows systems are more efficient in how the timers work avoiding timer interrupts and the associated background activity allows the processors to remain idle longer which allows modern processors to consume less energy windows accomplishes this by coalescing timer interrupts into batches security windows builds on the security features in windows vista which added integrity levels to the security model provided bitlocker volume encryption see section and limited privileged actions by ordinary users bitlocker is now easier to set up and use and privileged actions result in many fewer annoying gui popups thread improvements the most interesting windows changes were in the kernel the number of logical cpus available on each system is growing dramatically previous versions of windows limited the number of cpus to because of the bitmasks used to represent values like processor affinity see section windows can support hundreds of cpus to ensure that the performance of the system scaled with the number of cpus major improvements were made to the kernelscheduling code to break apart locks and reduce contention as the number of available cpus increase new programming environments are being developed to support the finergrain parallelism than is available with threads windows supports a form of usermode scheduling which separates the usermode and kernelmode portions of threads allowing the usermode portions to yield the cpu without entering the kernel scheduler finally windows server r introduced dynamic fair share scheduling dfss to allow multiuser servers to limit how much one user can interfere with another dfss keeps a user with running threads from getting twice as much processor time as a user with only running threads traditional unix systems history the history of unix is an ofttold tale and will not be repeated in great detail here instead we provide a brief summary unix was initially developed at bell labs and became operational on a pdp in some of the people involved at bell labs had also participated in the timesharing work being done at mits project mac that project led to the development of first ctss and then multics although it is common to say that the original unix was a scaleddown version of multics the developers of unix actually claimed to be more influenced by ctss ritc nevertheless unix incorporated many ideas from multics work on unix at bell labs and later elsewhere produced a series of versions of unix the first notable milestone was porting the unix system from the pdp to the pdp this was the first hint that unix would be an os for all computers the next important milestone was the rewriting of unix in the programming language c this was an unheardof strategy at the time it was generally felt that something as complex as an os which must deal with timecritical events had to be written exclusively in assembly language reasons for this attitude include the following memory both ram and secondary store was small and expensive by todays standards so effective use was important this included various techniques for overlaying memory with different code and data segments and selfmodifying code even though compilers had been available since the s the computer industry was generally skeptical of the quality of automatically generated code with resource capacity small efficient code both in terms of time and space was essential processor and bus speeds were relatively slow so saving clock cycles could make a substantial difference in execution time the c implementation demonstrated the advantages of using a highlevel language for most if not all of the system code today virtually all unix implementations are written in c these early versions of unix were popular within bell labs in the unix system was described in a technical journal for the first time ritc this spurred great interest in the system licenses for unix were provided to commercial institutions as well as universities the first widely available version outside bell labs was version in the followon version released in is the ancestor of most modern unix systems the most important of the nonatt systems to be developed was done at the university of california at berkeley called unix bsd berkeley software distribution running first on pdp and then vax computers att continued to develop and refine the system by bell labs had combined several att variants of unix into a single system marketed commercially as unix system iii a number of features was later added to the os to produce unix system v description figure provides a general description of the classic unix architecture the underlying hardware is surrounded by the os software the os is often called the system kernel or simply the kernel to emphasize its isolation from the user and applications it is the unix kernel that we will be concerned with in our use of unix as an example in this book unix also comes equipped with a number of user services and interfaces that are considered part of the system these can be grouped into the shell other interface software and the components of the c compiler compiler assembler loader the layer outside of this consists of user applications and the user interface to the c compiler a closer look at the kernel is provided in figure user programs can invoke os services either directly or through library programs the system call interface is the boundary with the user and allows higherlevel software to gain access to specific kernel functions at the other end the os contains primitive routines that interact directly with the hardware between these two interfaces the system is divided into two main parts one concerned with process control and the other concerned with file management and io the process control subsystem is responsible for memory management the scheduling and dispatching of processes and the synchronization and interprocess communication of processes the file system exchanges data between memory and external devices either as a stream of characters or in blocks to achieve this a variety of device drivers are used for blockoriented transfers a disk cache approach is used a system buffer in main memory is interposed between the user address space and the external device the description in this subsection has dealt with what might be termed traditional unix systems vaha uses this term to refer to system v release svr bsd and earlier versions the following general statements may be unix commands and libraries system call interface kernel hardware userwritten applications figure general unix architecture user programs trap libraries user level kernel level system call interface interprocess communication file subsystem process control scheduler subsystem buffer cache memory management character block device drivers hardware control kernel level hardware level hardware figure traditional unix kernel made about a traditional unix system it is designed to run on a single processor and lacks the ability to protect its data structures from concurrent access by multiple processors its kernel is not very versatile supporting a single type of file system process scheduling policy and executable file format the traditional unix kernel is not designed to be extensible and has few facilities for code reuse the result is that as new features were added to the various unix versions much new code had to be added yielding a bloated and unmodular kernel modern unix systems as unix evolved the number of different implementations proliferated each providing some useful features there was a need to produce a new implementation that unified many of the important innovations added other modern os design features and produced a more modular architecture typical of the modern unix kernel is the architecture depicted in figure there is a small core of facilities written in coff aout elf exec switch file mappings nfs ffs device virtual vnodevfs mappings memory interface framework sfs anonymous mappings rfs common facilities disk driver timesharing block scheduler processes device framework switch tape driver system streams processes network tty driver driver figure modern unix kernel a modular fashion that provide functions and services needed by a number of os processes each of the outer circles represents functions and an interface that may be implemented in a variety of ways we now turn to some examples of modern unix systems system v release svr svr developed jointly by att and sun microsystems combines features from svr bsd microsoft xenix system v and sunos it was almost a total rewrite of the system v kernel and produced a clean if complex implementation new features in the release include realtime processing support process scheduling classes dynamically allocated data structures virtual memory management virtual file system and a preemptive kernel svr draws on the efforts of both commercial and academic designers and was developed to provide a uniform platform for commercial unix deployment it has succeeded in this objective and is perhaps the most important unix variant it incorporates most of the important features ever developed on any unix system and does so in an integrated commercially viable fashion svr runs on processors ranging from bit microprocessors up to supercomputers bsd the berkeley software distribution bsd series of unix releases have played a key role in the development of os design theory xbsd is widely used in academic installations and has served as the basis of a number of commercial unix products it is probably safe to say that bsd is responsible for much of the popularity of unix and that most enhancements to unix first appeared in bsd versions bsd was the final version of bsd to be released by berkeley with the design and implementation organization subsequently dissolved it is a major upgrade to bsd and includes a new virtual memory system changes in the kernel structure and a long list of other feature enhancements one of the most widely used and best documented versions of bsd is freebsd freebsd is popular for internetbased servers and firewalls and is used in a number of embedded systems the latest version of the macintosh os mac os x is based on freebsd and the mach microkernel solaris solaris is suns svrbased unix release with the latest version being solaris provides all of the features of svr plus a number of more advanced features such as a fully preemptable multithreaded kernel full support for smp and an objectoriented interface to file systems solaris is the most widely used and most successful commercial unix implementation linux history linux started out as a unix variant for the ibm pc intel architecture linus torvalds a finnish student of computer science wrote the initial version torvalds posted an early version of linux on the internet in since then a number of people collaborating over the internet have contributed to the development of linux all under the control of torvalds because linux is free and the source code is available it became an early alternative to other unix workstations such as those offered by sun microsystems and ibm today linux is a fullfeatured unix system that runs on all of these platforms and more including intel pentium and itanium and the motorolaibm powerpc key to the success of linux has been the availability of free software packages under the auspices of the free software foundation fsf fsfs goal is stable platformindependent software that is free high quality and embraced by the user community fsfs gnu project provides tools for software developers and the gnu is a recursive acronym for gnus not unix the gnu project is a free software set of packages and tools for developing a unixlike operating system it is often used with the linux kernel gnu public license gpl is the fsf seal of approval torvalds used gnu tools in developing his kernel which he then released under the gpl thus the linux distributions that you see today are the product of fsfs gnu project torvalds individual effort and the efforts of many collaborators all over the world in addition to its use by many individual programmers linux has now made significant penetration into the corporate world this is not only because of the free software but also because of the quality of the linux kernel many talented programmers have contributed to the current version resulting in a technically impressive product moreover linux is highly modular and easily configured this makes it easy to squeeze optimal performance from a variety of hardware platforms plus with the source code available vendors can tweak applications and utilities to meet specific requirements throughout this book we will provide details of linux kernel internals based on the most recent version linux modular structure most unix kernels are monolithic recall from earlier in this chapter that a monolithic kernel is one that includes virtually all of the os functionality in one large block of code that runs as a single process with a single address space all the functional components of the kernel have access to all of its internal data structures and routines if changes are made to any portion of a typical monolithic os all the modules and routines must be relinked and reinstalled and the system rebooted before the changes can take effect as a result any modification such as adding a new device driver or file system function is difficult this problem is especially acute for linux for which development is global and done by a loosely associated group of independent programmers although linux does not use a microkernel approach it achieves many of the potential advantages of this approach by means of its particular modular architecture linux is structured as a collection of modules a number of which can be automatically loaded and unloaded on demand these relatively independent blocks are referred to as loadable modules goye in essence a module is an object file whose code can be linked to and unlinked from the kernel at runtime typically a module implements some specific function such as a file system a device driver or some other feature of the kernels upper layer a module does not execute as its own process or thread although it can create kernel threads for various purposes as necessary rather a module is executed in kernel mode on behalf of the current process thus although linux may be considered monolithic its modular structure overcomes some of the difficulties in developing and evolving the kernel the linux loadable modules have two important characteristics dynamic linking a kernel module can be loaded and linked into the kernel while the kernel is already in memory and executing a module can also be unlinked and removed from memory at any time stackable modules the modules are arranged in a hierarchy individual modules serve as libraries when they are referenced by client modules higher up in the hierarchy and as clients when they reference modules further down dynamic linking fran facilitates configuration and saves kernel memory in linux a user program or user can explicitly load and unload kernel modules using the insmod and rmmod commands the kernel itself monitors the need for particular functions and can load and unload modules as needed with stackable modules dependencies between modules can be defined this has two benefits code common to a set of similar modules eg drivers for similar hardware can be moved into a single module reducing replication the kernel can make sure that needed modules are present refraining from unloading a module on which other running modules depend and loading any additional required modules when a new module is loaded figure is an example that illustrates the structures used by linux to manage modules the figure shows the list of kernel modules after only two modules have been loaded fat and vfat each module is defined by two tables the module table and the symbol table the module table includes the following elements next pointer to the following module all modules are organized into a linked list the list begins with a pseudomodule not shown in figure name pointer to module name size module size in memory pages usecount module usage counter the counter is incremented when an operation involving the modules functions is started and decremented when the operation terminates module module next next name name size size usecount usecount flags flags nysms nysms ndeps ndeps syms fat syms vfat deps deps refs refs symboltable symboltable value value name name value value name name value value name name figure example list of linux kernel modules flags module flags nsyms number of exported symbols ndeps number of referenced modules syms pointer to this modules symbol table deps pointer to list of modules that are referenced by this module refs pointer to list of modules that use this module the symbol table defines those symbols controlled by this module that are used elsewhere figure shows that the vfat module was loaded after the fat module and that the vfat module is dependent on the fat module kernel components figure taken from mosb shows the main components of the linux kernel as implemented on an ia architecture eg intel itanium the figure shows several processes running on top of the kernel each box indicates a separate process while each squiggly line with an arrowhead represents a thread of execution the kernel itself consists of an interacting collection of components with arrows processes user level signals system calls processes scheduler file network virtual systems protocols kernel memory char device block device network drivers drivers device drivers traps physical interrupts faults memory cpu system terminal disk network interface hardware memory controller figure linux kernel components in linux there is no distinction between the concepts of processes and threads however multiple threads in linux can be grouped together in such a way that effectively you can have a single process comprising multiple threads these matters are discussed in chapter table some linux signals sighup terminal hangup sigcont continue sigquit keyboard quit sigtstp keyboard stop sigtrap trace trap sigttou terminal write sigbus bus error sigxcpu cpu limit exceeded sigkill kill signal sigvtalrm virtual alarm clock sigsegv segmentation violation sigwinch window size unchanged sigpipt broken pipe sigpwr power failure sigterm termination sigrtmin first realtime signal sigchld child status unchanged sigrtmax last realtime signal indicating the main interactions the underlying hardware is also depicted as a set of components with arrows indicating which kernel components use or control which hardware components all of the kernel components of course execute on the processor but for simplicity these relationships are not shown briefly the principal kernel components are the following signals the kernel uses signals to call into a process for example signals are used to notify a process of certain faults such as division by zero table gives a few examples of signals system calls the system call is the means by which a process requests a specific kernel service there are several hundred system calls which can be roughly grouped into six categories file system process scheduling interprocess communication socket networking and miscellaneous table defines a few examples in each category processes and scheduler creates manages and schedules processes virtual memory allocates and manages virtual memory for processes table some linux system calls file system related close close a file descriptor link make a new name for a file open open and possibly create a file or device read read from file descriptor write write to file descriptor process related execve execute program exit terminate the calling process getpid get process identification setuid set user identity of the current process prtrace provides a means by which a parent process may observe and control the execution of another process and examine and change its core image and registers table continued scheduling related schedgetparam set the scheduling parameters associated with the scheduling policy for the process identified by pid schedgetprioritymax return the maximum priority value that can be used with the scheduling algorithm identified by policy schedsetscheduler set both the scheduling policy eg fifo and the associated parameters for the process pid schedrrgetinterval write into the timespec structure pointed to by the parameter tp the roundrobin time quantum for the process pid schedyield a process can relinquish the processor voluntarily without blocking via this system call the process will then be moved to the end of the queue for its static priority and a new process gets to run interprocess communication ipc related msgrcv a message buffer structure is allocated to receive a message the system call then reads a message from the message queue specified by msqid into the newly created message buffer semctl perform the control operation specified by cmd on the semaphore set semid semop perform operations on selected members of the semaphore set semid shmat attach the shared memory segment identified by semid to the data segment of the calling process shmctl allow the user to receive information on a shared memory segment set the owner group and permissions of a shared memory segment or destroy a segment socket networking related bind assigns the local ip address and port for a socket returns for success and for error connect establish a connection between the given socket and the remote socket associated with sockaddr gethostname return local host name send send the bytes contained in buffer pointed to by msg over the given socket setsockopt set the options on a socket miscellaneous createmodule attempt to create a loadable module entry and reserve the kernel memory that will be needed to hold the module fsync copy all incore parts of a file to disk and waits until the device reports that all parts are on stable storage querymodule request information related to loadable modules from the kernel time return the time in seconds since january vhangup simulate a hangup on the current terminal this call arranges for other users to have a clean tty at login time file systems provides a global hierarchical namespace for files directories and other file related objects and provides file system functions network protocols supports the sockets interface to users for the tcpip protocol suite character device drivers manages devices that require the kernel to send or receive data one byte at a time such as terminals modems and printers block device drivers manages devices that read and write data in blocks such as various forms of secondary memory magnetic disks cdroms etc network device drivers manages network interface cards and communications ports that connect to network devices such as bridges and routers traps and faults handles traps and faults generated by the processor such as a memory fault physical memory manages the pool of page frames in real memory and allocates pages for virtual memory interrupts handles interrupts from peripheral devices linux vserver virtual machine architecture linux vserver is an opensource fast lightweight approach to implementing virtual machines on a linux server solt lign only a single copy of the linux kernel is involved vserver consists of a relatively modest modification to the kernel plus a small set of os userland tools the vserver linux kernel supports a number of separate virtual servers the kernel manages all system resources and tasks including process scheduling memory disk space and processor time this is closer in concept to the process vm rather than the system vm of figure each virtual server is isolated from the others using linux kernel capabilities this provides security and makes it easy to set up multiple virtual machines on a single platform the isolation involves four elements chroot chcontext chbind and capabilities the chroot command is a unix or linux command to make the root directory become something other than its default for the lifetime of the current process it can only be run by privileged users and is used to give a process commonly a network server such as ftp or http access to a restricted portion of the file system this command provides file system isolation all commands executed by the virtual server can only affect files that start with the defined root for that server the chcontext linux utility allocates a new security context and executes commands in that context the usual or hosted security context is the context this context has the same privileges as the root user uid this context can see and kill other tasks in the other contexts context number is used to view the term userland refers to all application software that runs in user space rather than kernel space os userland usually refers to the various programs and libraries that the operating system uses to interact with the kernel software that performs inputoutput manipulates file system objects etc server server applications applications vmhost vm vmn virtual platform hosting platform vm admin remote admin core services dev usr home proc dev usr home proc dev usr home proc standard os image figure linux vserver architecture other contexts but can not affect them all other contexts provide complete isolation processes from one context can neither see nor interact with processes from another context this provides the ability to run similar contexts on the same computer without any interaction possible at the application level thus each virtual server has its own execution context that provides process isolation the chbind utility executes a command and locks the resulting process and its children into using a specific ip address once called all packets sent out by this virtual server through the systems network interface are assigned the sending ip address derived from the argument given to chbind this system call provides network isolation each virtual server uses a separate and distinct ip address incoming traffic intended for one virtual server can not be accessed by other virtual servers finally each virtual server is assigned a set of capabilities the concept of capabilities as used in linux refers to a partitioning of the privileges available to a root user such as the ability to read files or to trace processes owned by another user thus each virtual server can be assigned a limited subset of the root users privileges this provides root isolation vserver can also set resource limits such as limits to the amount of virtual memory a process may use figure based on solt shows the general architecture of linux vserver vserver provides a shared virtualized os image consisting of a root file system and a shared set of system libraries and kernel services each vm can be booted shut down and rebooted independently figure shows three groupings of software running on the computer system the hosting platform includes the shared os image and a privileged host vm whose function is to monitor and manage the other vms the virtual platform creates virtual machines and is the view of the system seen by the applications running on the individual vms recommended reading and web sites brin is an excellent collection of papers covering major advances in os design over the years swai is a provocative and interesting short article on the future of operating systems mukh provides a good discussion of os design issues for smps chap contains five articles on recent design directions for multiprocessor operating systems worthwhile discussions of the principles of microkernel design are contained in lied and lied the latter focuses on performance issues li and smit provide good treatments of virtual machines an excellent treatment of unix internals which provides a comparative analysis of a number of variants is vaha for unix svr good provides a definitive treatment with ample technical detail for the popular opensource freebsd mcku is highly recommended mcdo provides a good treatment of solaris internals good treatments of linux internals are love and maue although there are countless books on various versions of windows there is remarkably little material available on windows internals the book to read is russ brin brinch hansen p classic operating systems from batch processing to distributed systems new york springerverlag chap chapin s and maccabe a eds multiprocessor operating systems harnessing the power special issue of ieee concurrency apriljune good goodheart b and cox j the magic garden explained the internals of unix system v release englewood cliffs nj prentice hall love love r linux kernel development upper saddle river nj addisonwesley li li y li w and jiang c a survey of virtual machine systems current technology and future trends proceedings third international symposium on electronic commerce and security lied liedtke j on kernel construction proceedings of the fifteenth acm symposium on operating systems principles december lied liedtke j toward real microkernels communications of the acm september maue mauerer w professional linux kernel architecture new york wiley mcdo mcdougall r and mauro j solaris internals solaris and opensolaris kernel architecture palo alto ca sun microsystems press mcku mckusick m and nevilleneil j the design and implementation of the freebsd operating system reading ma addisonwesley mukh mukherjee b and karsten s operating systems for parallel machines in parallel computers theory and practice edited by t casavant p tvrkik and f plasil los alamitos ca ieee computer society press russ russinovich m solomon d and ionescu a windows internals covering windows and windows server r redmond wa microsoft press smit smith j and nair r the architecture of virtual machines computer may swai swaine m wither operating systems dr dobbs journal march vaha vahalia u unix internals the new frontiers upper saddle river nj prentice hall recommended web sites the operating system resource center a useful collection of documents and papers on a wide range of os topics operating system technical comparison includes a substantial amount of information on a variety of operating systems acm special interest group on operating systems information on sigops publications and conferences ieee technical committee on operating systems and application environments includes an online newsletter and links to other sites the composresearch faq lengthy and worthwhile faq covering os design issues unix guru universe excellent source of unix information linux documentation project the name describes the site ibms linux website provides a wide range of technical and user information on linux much of it is devoted to ibm products but there is a lot of useful general technical information windows development good source of information on windows internals key terms review questions and problems key terms batch processing multiprogramming round robin batch system multitasking scheduling execution context multithreading serial processing interrupt nucleus symmetric multiprocessing job operating system task job control language physical address thread kernel privileged instruction time sharing memory management process timesharing system microkernel process state uniprogramming monitor real address virtual address monolithic kernel resident monitor virtual machine multiprogrammed batch system review questions what are three objectives of an os design what is the kernel of an os what is multiprogramming what is a process how is the execution context of a process used by the os list and briefly explain five storage management responsibilities of a typical os explain the distinction between a real address and a virtual address describe the roundrobin scheduling technique explain the difference between a monolithic kernel and a microkernel what is multithreading list the key design issues for an smp operating system problems suppose that we have a multiprogrammed computer in which each job has identical characteristics in one computation period t for a job half the time is spent in io and the other half in processor activity each job runs for a total of n periods assume that a simple roundrobin scheduling is used and that io operations can overlap with processor operation define the following quantities turnaround time actual time to complete a job throughput average number of jobs completed per time period t processor utilization percentage of time that the processor is active not waiting compute these quantities for one two and four simultaneous jobs assuming that the period t is distributed in each of the following ways a io first half processor second half b io first and fourth quarters processor second and third quarter an iobound program is one that if run alone would spend more time waiting for io than using the processor a processorbound program is the opposite suppose a shortterm scheduling algorithm favors those programs that have used little processor time in the recent past explain why this algorithm favors iobound programs and yet does not permanently deny processor time to processorbound programs contrast the scheduling policies you might use when trying to optimize a timesharing system with those you would use to optimize a multiprogrammed batch system what is the purpose of system calls and how do system calls relate to the os and to the concept of dualmode kernelmode and usermode operation in ibms mainframe os os one of the major modules in the kernel is the system resource manager this module is responsible for the allocation of resources among address spaces processes the srm gives os a degree of sophistication unique among operating systems no other mainframe os and certainly no other type of os can match the functions performed by srm the concept of resource includes processor real memory and io channels srm accumulates statistics pertaining to utilization of processor channel and various key data structures its purpose is to provide optimum performance based on performance monitoring and analysis the installation sets forth various performance objectives and these serve as guidance to the srm which dynamically modifies installation and job performance characteristics based on system utilization in turn the srm provides reports that enable the trained operator to refine the configuration and parameter settings to improve user service this problem concerns one example of srm activity real memory is divided into equalsized blocks called frames of which there may be many thousands each frame can hold a block of virtual memory referred to as a page srm receives control approximately times per second and inspects each and every page frame if the page has not been referenced or changed a counter is incremented by over time srm averages these numbers to determine the average number of seconds that a page frame in the system goes untouched what might be the purpose of this and what action might srm take a multiprocessor with eight processors has attached tape drives there is a large number of jobs submitted to the system that each require a maximum of four tape drives to complete execution assume that each job starts running with only three tape drives for a long period before requiring the fourth tape drive for a short period toward the end of its operation also assume an endless supply of such jobs a assume the scheduler in the os will not start a job unless there are four tape drives available when a job is started four drives are assigned immediately and are not released until the job finishes what is the maximum number of jobs that can be in progress at once what are the maximum and minimum number of tape drives that may be left idle as a result of this policy b suggest an alternative policy to improve tape drive utilization and at the same time avoid system deadlock what is the maximum number of jobs that can be in progress at once what are the bounds on the number of idling tape drives chapter process description and control what is a process background processes and process control blocks process states a twostate process model the creation and termination of processes a fivestate model suspended processes process description operating system control structures process control structures process control modes of execution process creation process switching execution of the operating system nonprocess kernel execution within user processes processbased operating system security issues system access threats countermeasures unix svr process management process states process description process control summary recommended reading key terms review questions and problems the concept of process is fundamental to the structure of modern computer operating systems its evolution in analyzing problems of synchronization deadlock and scheduling in operating systems has been a major intellectual contribution of computer science what can be automated the computer science and engineering research study mit press learning objectives after studying this chapter you should be able to define the term process and explain the relationship between processes and process control blocks explain the concept of a process state and discuss the state transitions the processes undergo list and describe the purpose of the data structures and data structure elements used by an os to manage processes assess the requirements for process control by the os understand the issues involved in the execution of os code assess the key security issues that relate to operating systems describe the process management scheme for unix svr all multiprogramming operating systems from singleuser systems such as windows for end users to mainframe systems such as ibms mainframe operating system zos which can support thousands of users are built around the concept of the process most requirements that the os must meet can be expressed with reference to processes the os must interleave the execution of multiple processes to maximize processor utilization while providing reasonable response time the os must allocate resources to processes in conformance with a specific policy eg certain functions or applications are of higher priority while at the same time avoiding deadlock the os may be required to support interprocess communication and user creation of processes both of which may aid in the structuring of applications we begin with an examination of the way in which the os represents and controls processes then the chapter discusses process states which characterize the behavior of processes then we look at the data structures that the os uses to manage processes these include data structures to represent the state of each deadlock is examined in chapter as a simple example deadlock occurs if two processes need the same two resources to continue and each has ownership of one unless some action is taken each process will wait indefinitely for the missing resource process and data structures that record other characteristics of processes that the os needs to achieve its objectives next we look at the ways in which the os uses these data structures to control process execution finally we discuss process management in unix svr chapter provides more modern examples of process management this chapter occasionally refers to virtual memory much of the time we can ignore this concept in dealing with processes but at certain points in the discussion virtual memory considerations are pertinent virtual memory is previewed in chapter and discussed in detail in chapter a set of animations that illustrate concepts in this chapter is available online click on the rotating globe at this books web site at williamstallingscomososehtml for access what is a process background before defining the term process it is useful to summarize some of the concepts introduced in chapters and a computer platform consists of a collection of hardware resources such as the processor main memory io modules timers disk drives and so on computer applications are developed to perform some task typically they accept input from the outside world perform some processing and generate output it is inefficient for applications to be written directly for a given hardware platform the principal reasons for this are as follows a numerous applications can be developed for the same platform thus it makes sense to develop common routines for accessing the computers resources b the processor itself provides only limited support for multiprogramming software is needed to manage the sharing of the processor and other resources by multiple applications at the same time c when multiple applications are active at the same time it is necessary to protect the data io use and other resource use of each application from the others the os was developed to provide a convenient featurerich secure and consistent interface for applications to use the os is a layer of software between the applications and the computer hardware figure that supports applications and utilities we can think of the os as providing a uniform abstract representation of resources that can be requested and accessed by applications resources include main memory network interfaces file systems and so on once the os has created these resource abstractions for applications to use it must also manage their use for example an os may permit resource sharing and resource protection now that we have the concepts of applications system software and resources we are in a position to discuss how the os can in an orderly fashion manage the execution of applications so that resources are made available to multiple applications the physical processor is switched among multiple applications so all will appear to be progressing the processor and io devices can be used efficiently the approach taken by all modern operating systems is to rely on a model in which the execution of an application corresponds to the existence of one or more processes processes and process control blocks recall from chapter that we suggested several definitions of the term process including a program in execution an instance of a program running on a computer the entity that can be assigned to and executed on a processor a unit of activity characterized by the execution of a sequence of instructions a current state and an associated set of system resources we can also think of a process as an entity that consists of a number of elements two essential elements of a process are program code which may be shared with other processes that are executing the same program and a set of data associated with that code let us suppose that the processor begins to execute this program code and we refer to this executing entity as a process at any given point in time while the program is executing this process can be uniquely characterized by a number of elements including the following identifier a unique identifier associated with this process to distinguish it from all other processes state if the process is currently executing it is in the running state priority priority level relative to other processes program counter the address of the next instruction in the program to be executed memory pointers includes pointers to the program code and data associated with this process plus any memory blocks shared with other processes context data these are data that are present in registers in the processor while the process is executing io status information includes outstanding io requests io devices eg disk drives assigned to this process a list of files in use by the process and so on accounting information may include the amount of processor time and clock time used time limits account numbers and so on identifier state priority program counter memory pointers context data io status information accounting information figure simplified process control block the information in the preceding list is stored in a data structure typically called a process control block figure that is created and managed by the os the significant point about the process control block is that it contains sufficient information so that it is possible to interrupt a running process and later resume execution as if the interruption had not occurred the process control block is the key tool that enables the os to support multiple processes and to provide for multiprocessing when a process is interrupted the current values of the program counter and the processor registers context data are saved in the appropriate fields of the corresponding process control block and the state of the process is changed to some other value such as blocked or ready described subsequently the os is now free to put some other process in the running state the program counter and context data for this process are loaded into the processor registers and this process now begins to execute thus we can say that a process consists of program code and associated data plus a process control block for a singleprocessor computer at any given time at most one process is executing and that process is in the running state process states as just discussed for a program to be executed a process or task is created for that program from the processors point of view it executes instructions from its repertoire in some sequence dictated by the changing values in the program counter address main memory program counter dispatcher process a process b process c figure snapshot of example execution figure at instruction cycle register over time the program counter may refer to code in different programs that are part of different processes from the point of view of an individual program its execution involves a sequence of instructions within that program we can characterize the behavior of an individual process by listing the sequence of instructions that execute for that process such a listing is referred to as a trace of the process we can characterize behavior of the processor by showing how the traces of the various processes are interleaved let us consider a very simple example figure shows a memory layout of three processes to simplify the discussion we assume no use of virtual memory thus all three processes are represented by programs that are fully loaded in main memory in addition there is a small dispatcher program that switches the processor from one process to another figure shows the traces of each of the processes during the early part of their execution the first instructions executed in processes a and c are shown process b executes four instructions and we assume that the fourth instruction invokes an io operation for which the process must wait now let us view these traces from the processors point of view figure shows the interleaved traces resulting from the first instruction cycles for convenience the instruction cycles are numbered in this figure the shaded areas represent code executed by the dispatcher the same sequence of instructions is executed by the dispatcher in each instance because the same functionality of the dispatcher is being executed we assume that the os only allows a process to continue execution for a maximum of six instruction cycles after which it is interrupted a trace of process a b trace of process b c trace of process c starting address of program of process a starting address of program of process b starting address of program of process c figure traces of processes of figure this prevents any single process from monopolizing processor time as figure shows the first six instructions of process a are executed followed by a timeout and the execution of some code in the dispatcher which executes six instructions before turning control to process b after four instructions are executed process b requests an io action for which it must wait therefore the processor stops executing process b and moves on via the dispatcher to process c after a timeout the processor moves back to process a when this process times out process b is still waiting for the io operation to complete so the dispatcher moves on to process c again a twostate process model the operating systems principal responsibility is controlling the execution of processes this includes determining the interleaving pattern for execution and allocating resources to processes the first step in designing an os to control processes is to describe the behavior that we would like the processes to exhibit we can construct the simplest possible model by observing that at any time a process is either being executed by a processor or not in this model a process may be in one of two states running or not running as shown in figure a when the os creates a new process it creates a process control block for the process and enters that process into the system in the not running state the process exists is known to the os and is waiting for an opportunity to execute from time to time the currently running process will be interrupted and the dispatcher portion of the os will select some other process to run the former process moves from the the small number of instructions executed for the processes and the dispatcher are unrealistically low they are used in this simplified example to clarify the discussion timeout timeout timeout io request timeout starting address of dispatcher program shaded areas indicate execution of dispatcher process first and third columns count instruction cycles second and fourth columns show address of instruction being executed figure combined trace of processes of figure running state to the not running state and one of the other processes moves to the running state from this simple model we can already begin to appreciate some of the design elements of the os each process must be represented in some way so that the os can keep track of it that is there must be some information relating to each process including current state and location in memory this is the process control block processes that are not running must be kept in some sort of queue waiting their turn to execute figure b suggests a structure there is a single queue in which each entry is a pointer to the process control block of a particular process alternatively dispatch enter not running exit running pause a state transition diagram queue enter dispatch exit processor pause b queueing diagram figure twostate process model the queue may consist of a linked list of data blocks in which each block represents one process we will explore this latter implementation subsequently we can describe the behavior of the dispatcher in terms of this queueing diagram a process that is interrupted is transferred to the queue of waiting processes alternatively if the process has completed or aborted it is discarded exits the system in either case the dispatcher takes another process from the queue to execute the creation and termination of processes before refining our simple twostate model it will be useful to discuss the creation and termination of processes ultimately and regardless of the model of process behavior that is used the life of a process is bounded by its creation and termination process creation when a new process is to be added to those currently being managed the os builds the data structures that are used to manage the process and allocates address space in main memory to the process we describe these data structures in section these actions constitute the creation of a new process four common events lead to the creation of a process as indicated in table in a batch environment a process is created in response to the submission of a job in an interactive environment a process is created when a new user attempts to log on in both cases the os is responsible for the creation of the new process an os may also create a process on behalf of an application for example if a user requests that a file be printed the os can create a process that will manage the printing the requesting process can thus proceed independently of the time required to complete the printing task table reasons for process creation new batch job the os is provided with a batch job control stream usually on tape or disk when the os is prepared to take on new work it will read the next sequence of job control commands interactive logon a user at a terminal logs on to the system created by os to provide a service the os can create a process to perform a function on behalf of a user program without the user having to wait eg a process to control printing spawned by existing process for purposes of modularity or to exploit parallelism a user program can dictate the creation of a number of processes traditionally the os created all processes in a way that was transparent to the user or application program and this is still commonly found with many contemporary operating systems however it can be useful to allow one process to cause the creation of another for example an application process may generate another process to receive data that the application is generating and to organize those data into a form suitable for later analysis the new process runs in parallel to the original process and is activated from time to time when new data are available this arrangement can be very useful in structuring the application as another example a server process eg print server file server may generate a new process for each request that it handles when the os creates a process at the explicit request of another process the action is referred to as process spawning when one process spawns another the former is referred to as the parent process and the spawned process is referred to as the child process typically the related processes need to communicate and cooperate with each other achieving this cooperation is a difficult task for the programmer this topic is discussed in chapter process termination table summarizes typical reasons for process termination any computer system must provide a means for a process to indicate its completion a batch job should include a halt instruction or an explicit os service call for termination in the former case the halt instruction will generate an interrupt to alert the os that a process has completed for an interactive application the action of the user will indicate when the process is completed for example in a timesharing system the process for a particular user is to be terminated when the user logs off or turns off his or her terminal on a personal computer or workstation a user may quit an application eg word processing or spreadsheet all of these actions ultimately result in a service request to the os to terminate the requesting process additionally a number of error and fault conditions can lead to the termination of a process table lists some of the more commonly recognized conditions finally in some operating systems a process may be terminated by the process that created it or when the parent process is itself terminated a forgiving operating system might in some cases allow the user to recover from a fault without terminating the process for example if a user requests access to a file and that access is denied the operating system might simply inform the user that access is denied and allow the process to proceed table reasons for process termination normal completion the process executes an os service call to indicate that it has completed running time limit exceeded the process has run longer than the specified total time limit there are a number of possibilities for the type of time that is measured these include total elapsed time wall clock time amount of time spent executing and in the case of an interactive process the amount of time since the user last provided any input memory unavailable the process requires more memory than the system can provide bounds violation the process tries to access a memory location that it is not allowed to access protection error the process attempts to use a resource such as a file that it is not allowed to use or it tries to use it in an improper fashion such as writing to a readonly file arithmetic error the process tries a prohibited computation such as division by zero or tries to store numbers larger than the hardware can accommodate time overrun the process has waited longer than a specified maximum for a certain event to occur io failure an error occurs during input or output such as inability to find a file failure to read or write after a specified maximum number of tries when for example a defective area is encountered on a tape or invalid operation such as reading from the line printer invalid instruction the process attempts to execute a nonexistent instruction often a result of branching into a data area and attempting to execute the data privileged instruction the process attempts to use an instruction reserved for the operating system data misuse a piece of data is of the wrong type or is not initialized operator or os intervention for some reason the operator or the operating system has terminated the process eg if a deadlock exists parent termination when a parent terminates the operating system may automatically terminate all of the offspring of that parent parent request a parent process typically has the authority to terminate any of its offspring a fivestate model if all processes were always ready to execute then the queueing discipline suggested by figure b would be effective the queue is a firstinfirstout list and the processor operates in roundrobin fashion on the available processes each process in the queue is given a certain amount of time in turn to execute and then returned to the queue unless blocked however even with the simple example that we have described this implementation is inadequate some processes in the not running state are ready to execute while others are blocked waiting for an io operation to complete thus using a single queue the dispatcher could not just select the process at the oldest end of the queue rather the dispatcher would have to scan the list looking for the process that is not blocked and that has been in the queue the longest a more natural way to handle this situation is to split the not running state into two states ready and blocked this is shown in figure for good measure dispatch new admit ready running release exit timeout event event occurs wait blocked figure fivestate process model we have added two additional states that will prove useful the five states in this new diagram are running the process that is currently being executed for this chapter we will assume a computer with a single processor so at most one process at a time can be in this state ready a process that is prepared to execute when given the opportunity blockedwaiting a process that can not execute until some event occurs such as the completion of an io operation new a process that has just been created but has not yet been admitted to the pool of executable processes by the os typically a new process has not yet been loaded into main memory although its process control block has been created exit a process that has been released from the pool of executable processes by the os either because it halted or because it aborted for some reason the new and exit states are useful constructs for process management the new state corresponds to a process that has just been defined for example if a new user attempts to log on to a timesharing system or a new batch job is submitted for execution the os can define a new process in two stages first the os performs the necessary housekeeping chores an identifier is associated with the process any tables that will be needed to manage the process are allocated and built at this point the process is in the new state this means that the os has performed the necessary actions to create the process but has not committed itself to the execution of the process for example the os may limit the number of processes that may be in the system for reasons of performance or main memory limitation while a process is in the new state information concerning the process that is needed by the os is maintained in control tables in main memory however the process itself is waiting is a frequently used alternative term for blocked as a process state generally we will use blocked but the terms are interchangeable not in main memory that is the code of the program to be executed is not in main memory and no space has been allocated for the data associated with that program while the process is in the new state the program remains in secondary storage typically disk storage similarly a process exits a system in two stages first a process is terminated when it reaches a natural completion point when it aborts due to an unrecoverable error or when another process with the appropriate authority causes the process to abort termination moves the process to the exit state at this point the process is no longer eligible for execution the tables and other information associated with the job are temporarily preserved by the os which provides time for auxiliary or support programs to extract any needed information for example an accounting program may need to record the processor time and other resources utilized by the process for billing purposes a utility program may need to extract information about the history of the process for purposes related to performance or utilization analysis once these programs have extracted the needed information the os no longer needs to maintain any data relating to the process and the process is deleted from the system figure indicates the types of events that lead to each state transition for a process the possible transitions are as follows null new a new process is created to execute a program this event occurs for any of the reasons listed in table new ready the os will move a process from the new state to the ready state when it is prepared to take on an additional process most systems set some limit based on the number of existing processes or the amount of virtual memory committed to existing processes this limit assures that there are not so many active processes as to degrade performance ready running when it is time to select a process to run the os chooses one of the processes in the ready state this is the job of the scheduler or dispatcher scheduling is explored in part four running exit the currently running process is terminated by the os if the process indicates that it has completed or if it aborts see table running ready the most common reason for this transition is that the running process has reached the maximum allowable time for uninterrupted execution virtually all multiprogramming operating systems impose this type of time discipline there are several other alternative causes for this transition which are not implemented in all operating systems of particular importance is the case in which the os assigns different levels of priority to different processes suppose for example that process a is running at a given priority level and process b at a higher priority level is blocked if the os learns that the event upon which process b has been waiting has occurred moving b to a ready state then it can interrupt process a and dispatch process b we in the discussion in this paragraph we ignore the concept of virtual memory in systems that support virtual memory when a process moves from new to ready its program code and data are loaded into virtual memory virtual memory was briefly discussed in chapter and is examined in detail in chapter say that the os has preempted process a finally a process may voluntarily release control of the processor an example is a background process that performs some accounting or maintenance function periodically running blocked a process is put in the blocked state if it requests something for which it must wait a request to the os is usually in the form of a system service call that is a call from the running program to a procedure that is part of the operating system code for example a process may request a service from the os that the os is not prepared to perform immediately it can request a resource such as a file or a shared section of virtual memory that is not immediately available or the process may initiate an action such as an io operation that must be completed before the process can continue when processes communicate with each other a process may be blocked when it is waiting for another process to provide data or waiting for a message from another process blocked ready a process in the blocked state is moved to the ready state when the event for which it has been waiting occurs ready exit for clarity this transition is not shown on the state diagram in some systems a parent may terminate a child process at any time also if a parent terminates all child processes associated with that parent may be terminated blocked exit the comments under the preceding item apply returning to our simple example figure shows the transition of each process among the states figure a suggests the way in which a queueing discipline might be implemented with two queues a ready queue and a blocked queue as each process is admitted to the system it is placed in the ready queue when it is time for the os to choose another process to run it selects one from the ready process a process b process c dispatcher running ready blocked figure process states for the trace of figure in general the term preemption is defined to be the reclaiming of a resource from a process before the process has finished using it in this case the resource is the processor itself the process is executing and could continue to execute but is preempted so that another process can be executed ready queue release admit dispatch processor timeout blocked queue event event wait occurs a single blocked queue ready queue release admit dispatch processor timeout event queue event event wait occurs event queue event event wait occurs event n queue event n event n wait occurs b multiple blocked queues figure queueing model for figure queue in the absence of any priority scheme this can be a simple firstinfirstout queue when a running process is removed from execution it is either terminated or placed in the ready or blocked queue depending on the circumstances finally when an event occurs any process in the blocked queue that has been waiting on that event only is moved to the ready queue this latter arrangement means that when an event occurs the os must scan the entire blocked queue searching for those processes waiting on that event in a large os there could be hundreds or even thousands of processes in that queue therefore it would be more efficient to have a number of queues one for each event then when the event occurs the entire list of processes in the appropriate queue can be moved to the ready state figure b one final refinement if the dispatching of processes is dictated by a priority scheme then it would be convenient to have a number of ready queues one for each priority level the os could then readily determine which is the highestpriority ready process that has been waiting the longest suspended processes the need for swapping the three principal states just described ready running blocked provide a systematic way of modeling the behavior of processes and guide the implementation of the os some operating systems are constructed using just these three states however there is good justification for adding other states to the model to see the benefit of these new states consider a system that does not employ virtual memory each process to be executed must be loaded fully into main memory thus in figure b all of the processes in all of the queues must be resident in main memory recall that the reason for all of this elaborate machinery is that io activities are much slower than computation and therefore the processor in a uniprogramming system is idle most of the time but the arrangement of figure b does not entirely solve the problem it is true that in this case memory holds multiple processes and that the processor can move to another process when one process is blocked but the processor is so much faster than io that it will be common for all of the processes in memory to be waiting for io thus even with multiprogramming a processor could be idle most of the time what to do main memory could be expanded to accommodate more processes but there are two flaws in this approach first there is a cost associated with main memory which though small on a perbyte basis begins to add up as we get into the gigabytes of storage second the appetite of programs for memory has grown as fast as the cost of memory has dropped so larger memory results in larger processes not more processes another solution is swapping which involves moving part or all of a process from main memory to disk when none of the processes in main memory is in the ready state the os swaps one of the blocked processes out on to disk into a suspend queue this is a queue of existing processes that have been temporarily kicked out of main memory or suspended the os then brings in another process from the suspend queue or it honors a newprocess request execution then continues with the newly arrived process swapping however is an io operation and therefore there is the potential for making the problem worse not better but because disk io is generally the fastest io on a system eg compared to tape or printer io swapping will usually enhance performance with the use of swapping as just described one other state must be added to our process behavior model figure a the suspend state when all of the processes in main memory are in the blocked state the os can suspend one process by putting it in the suspend state and transferring it to disk the space that is freed in main memory can then be used to bring in another process when the os has performed a swappingout operation it has two choices for selecting a process to bring into main memory it can admit a newly created process or it can bring in a previously suspended process it would appear that the preference should be to bring in a previously suspended process to provide it with service rather than increasing the total load on the system admit dispatch release new ready running exit timeout activate event occurs event wait suspend suspend blocked a with one suspend state new admit admit suspend activate dispatch release ready ready running exit suspend suspend timeout event occurs event occurs event wait blocked activate suspend blocked suspend b with two suspend states figure process state transition diagram with suspend states but this line of reasoning presents a difficulty all of the processes that have been suspended were in the blocked state at the time of suspension it clearly would not do any good to bring a blocked process back into main memory because it is still not ready for execution recognize however that each process in the suspend state was originally blocked on a particular event when that event occurs the process is not blocked and is potentially available for execution therefore we need to rethink this aspect of the design there are two independent concepts here whether a process is waiting on an event blocked or not and whether a process has been swapped out of main memory suspended or not to accommodate this combination we need four states ready the process is in main memory and available for execution blocked the process is in main memory and awaiting an event blockedsuspend the process is in secondary memory and awaiting an event readysuspend the process is in secondary memory but is available for execution as soon as it is loaded into main memory before looking at a state transition diagram that encompasses the two new suspend states one other point should be mentioned the discussion so far has assumed that virtual memory is not in use and that a process is either all in main memory or all out of main memory with a virtual memory scheme it is possible to execute a process that is only partially in main memory if reference is made to a process address that is not in main memory then the appropriate portion of the process can be brought in the use of virtual memory would appear to eliminate the need for explicit swapping because any desired address in any desired process can be moved in or out of main memory by the memory management hardware of the processor however as we shall see in chapter the performance of a virtual memory system can collapse if there is a sufficiently large number of active processes all of which are partially in main memory therefore even in a virtual memory system the os will need to swap out processes explicitly and completely from time to time in the interests of performance let us look now in figure b at the state transition model that we have developed the dashed lines in the figure indicate possible but not necessary transitions important new transitions are the following blocked blockedsuspend if there are no ready processes then at least one blocked process is swapped out to make room for another process that is not blocked this transition can be made even if there are ready processes available if the os determines that the currently running process or a ready process that it would like to dispatch requires more main memory to maintain adequate performance blockedsuspend readysuspend a process in the blockedsuspend state is moved to the readysuspend state when the event for which it has been waiting occurs note that this requires that the state information concerning suspended processes must be accessible to the os readysuspend ready when there are no ready processes in main memory the os will need to bring one in to continue execution in addition it might be the case that a process in the readysuspend state has higher priority than any of the processes in the ready state in that case the os designer may dictate that it is more important to get at the higherpriority process than to minimize swapping ready readysuspend normally the os would prefer to suspend a blocked process rather than a ready one because the ready process can now be executed whereas the blocked process is taking up main memory space and can not be executed however it may be necessary to suspend a ready process if that is the only way to free up a sufficiently large block of main memory also the os may choose to suspend a lowerpriority ready process rather than a higherpriority blocked process if it believes that the blocked process will be ready soon several other transitions that are worth considering are the following new readysuspend and new ready when a new process is created it can either be added to the ready queue or the readysuspend queue in either case the os must create a process control block and allocate an address space to the process it might be preferable for the os to perform these housekeeping duties at an early time so that it can maintain a large pool of processes that are not blocked with this strategy there would often be insufficient room in main memory for a new process hence the use of the new readysuspend transition on the other hand we could argue that a justintime philosophy of creating processes as late as possible reduces os overhead and allows that os to perform the processcreation duties at a time when the system is clogged with blocked processes anyway blockedsuspend blocked inclusion of this transition may seem to be poor design after all if a process is not ready to execute and is not already in main memory what is the point of bringing it in but consider the following scenario a process terminates freeing up some main memory there is a process in the blockedsuspend queue with a higher priority than any of the processes in the readysuspend queue and the os has reason to believe that the blocking event for that process will occur soon under these circumstances it would seem reasonable to bring a blocked process into main memory in preference to a ready process running readysuspend normally a running process is moved to the ready state when its time allocation expires if however the os is preempting the process because a higherpriority process on the blockedsuspend queue has just become unblocked the os could move the running process directly to the readysuspend queue and free some main memory any state exit typically a process terminates while it is running either because it has completed or because of some fatal fault condition however in some operating systems a process may be terminated by the process that created it or when the parent process is itself terminated if this is allowed then a process in any state can be moved to the exit state other uses of suspension so far we have equated the concept of a suspended process with that of a process that is not in main memory a process that is not in main memory is not immediately available for execution whether or not it is awaiting an event we can generalize the concept of a suspended process let us define a suspended process as having the following characteristics the process is not immediately available for execution the process may or may not be waiting on an event if it is this blocked condition is independent of the suspend condition and occurrence of the blocking event does not enable the process to be executed immediately table reasons for process suspension swapping the os needs to release sufficient main memory to bring in a process that is ready to execute other os reason the os may suspend a background or utility process or a process that is suspected of causing a problem interactive user request a user may wish to suspend execution of a program for purposes of debugging or in connection with the use of a resource timing a process may be executed periodically eg an accounting or system monitoring process and may be suspended while waiting for the next time interval parent process request a parent process may wish to suspend execution of a descendent to examine or modify the suspended process or to coordinate the activity of various descendants the process was placed in a suspended state by an agent either itself a parent process or the os for the purpose of preventing its execution the process may not be removed from this state until the agent explicitly orders the removal table lists some reasons for the suspension of a process one reason that we have discussed is to provide memory space either to bring in a readysuspended process or to increase the memory allocated to other ready processes the os may have other motivations for suspending a process for example an auditing or tracing process may be employed to monitor activity on the system the process may be used to record the level of utilization of various resources processor memory channels and the rate of progress of the user processes in the system the os under operator control may turn this process on and off from time to time if the os detects or suspects a problem it may suspend a process one example of this is deadlock which is discussed in chapter as another example a problem is detected on a communications line and the operator has the os suspend the process that is using the line while some tests are run another set of reasons concerns the actions of an interactive user for example if a user suspects a bug in the program he or she may debug the program by suspending its execution examining and modifying the program or data and resuming execution or there may be a background process that is collecting trace or accounting statistics which the user may wish to be able to turn on and off timing considerations may also lead to a swapping decision for example if a process is to be activated periodically but is idle most of the time then it should be swapped out between uses a program that monitors utilization or user activity is an example finally a parent process may wish to suspend a descendent process for example process a may spawn process b to perform a file read subsequently process b encounters an error in the file read procedure and reports this to process a process a suspends process b to investigate the cause in all of these cases the activation of a suspended process is requested by the agent that initially requested the suspension process description the os controls events within the computer system it schedules and dispatches processes for execution by the processor allocates resources to processes and responds to requests by user processes for basic services fundamentally we can think of the os as that entity that manages the use of system resources by processes this concept is illustrated in figure in a multiprogramming environment there are a number of processes p pn that have been created and exist in virtual memory each process during the course of its execution needs access to certain system resources including the processor io devices and main memory in the figure process p is running at least part of the process is in main memory and it has control of two io devices process p is also in main memory but is blocked waiting for an io device allocated to p process pn has been swapped out and is therefore suspended we explore the details of the management of these resources by the os on behalf of the processes in later chapters here we are concerned with a more fundamental question what information does the os need to control processes and manage resources for them operating system control structures if the os is to manage processes and resources it must have information about the current status of each process and resource the universal approach to providing this information is straightforward the os constructs and maintains tables of information about each entity that it is managing a general idea of the scope of this effort is indicated in figure which shows four different types of tables maintained by the os memory io file and process although the details will differ from one os to another fundamentally all operating systems maintain information in these four categories memory tables are used to keep track of both main real and secondary virtual memory some of main memory is reserved for use by the os the remainder is available for use by processes processes are maintained on secondary memory using some sort of virtual memory or simple swapping mechanism the memory tables must include the following information the allocation of main memory to processes the allocation of secondary memory to processes p p pn virtual memory computer resources processor io io io main memory figure processes and resources resource allocation at one snapshot in time process memory tables image process memory devices io tables files processes file tables primary process table process process process process image process n process n figure general structure of operating system control tables any protection attributes of blocks of main or virtual memory such as which processes may access certain shared memory regions any information needed to manage virtual memory we examine the information structures for memory management in detail in part three io tables are used by the os to manage the io devices and channels of the computer system at any given time an io device may be available or assigned to a particular process if an io operation is in progress the os needs to know the status of the io operation and the location in main memory being used as the source or destination of the io transfer io management is examined in chapter the os may also maintain file tables these tables provide information about the existence of files their location on secondary memory their current status and other attributes much if not all of this information may be maintained and used by a file management system in which case the os has little or no knowledge of files in other operating systems much of the detail of file management is managed by the os itself this topic is explored in chapter finally the os must maintain process tables to manage processes the remainder of this section is devoted to an examination of the required process tables before proceeding to this discussion two additional points should be made first although figure shows four distinct sets of tables it should be clear that these tables must be linked or crossreferenced in some fashion memory io and files are managed on behalf of processes so there must be some reference to these resources directly or indirectly in the process tables the files referred to in the file tables are accessible via an io device and will at some times be in main or virtual memory the tables themselves must be accessible by the os and therefore are subject to memory management second how does the os know to create the tables in the first place clearly the os must have some knowledge of the basic environment such as how much main memory exists what are the io devices and what are their identifiers and so on this is an issue of configuration that is when the os is initialized it must have access to some configuration data that define the basic environment and these data must be created outside the os with human assistance or by some autoconfiguration software process control structures consider what the os must know if it is to manage and control a process first it must know where the process is located second it must know the attributes of the process that are necessary for its management eg process id and process state process location before we can deal with the questions of where a process is located or what its attributes are we need to address an even more fundamental question what is the physical manifestation of a process at a minimum a process must include a program or set of programs to be executed associated with these programs is a set of data locations for local and global variables and any defined constants thus a process will consist of at least sufficient memory to hold the programs and data of that process in addition the execution of a program typically involves a stack see appendix p that is used to keep track of procedure calls and parameter passing between procedures finally each process has associated with it a number of attributes that are used by the os for process control typically the collection of attributes is referred to as a process control block we can refer to this collection of program data stack and attributes as the process image table the location of a process image will depend on the memory management scheme being used in the simplest case the process image is maintained as a table typical elements of a process image user data the modifiable part of the user space may include program data a user stack area and programs that may be modified user program the program to be executed stack each process has one or more lastinfirstout lifo stacks associated with it a stack is used to store parameters and calling addresses for procedure and system calls process control block data needed by the os to control the process see table other commonly used names for this data structure are task control block process descriptor and task descriptor contiguous or continuous block of memory this block is maintained in secondary memory usually disk so that the os can manage the process at least a small portion of its image must be maintained in main memory to execute the process the entire process image must be loaded into main memory or at least virtual memory thus the os needs to know the location of each process on disk and for each such process that is in main memory the location of that process in main memory we saw a slightly more complex variation on this scheme with the ctss os in chapter with ctss when a process is swapped out part of the process image may remain in main memory thus the os must keep track of which portions of the image of each process are still in main memory modern operating systems presume paging hardware that allows noncontiguous physical memory to support partially resident processes at any given time a portion of a process image may be in main memory with the remainder in secondary memory therefore process tables maintained by the os must show the location of each page of each process image figure depicts the structure of the location information in the following way there is a primary process table with one entry for each process each entry contains at least a pointer to a process image if the process image contains multiple blocks this information is contained directly in the primary process table or is available by crossreference to entries in memory tables of course this depiction is generic a particular os will have its own way of organizing the location information process attributes a sophisticated multiprogramming system requires a great deal of information about each process as was explained this information can be considered to reside in a process control block different systems will organize this information in different ways and several examples of this appear at the end of this chapter and the next for now let us simply explore the type of information that might be of use to an os without considering in any detail how that information is organized table lists the typical categories of information required by the os for each process you may be somewhat surprised at the quantity of information required as you gain a greater appreciation of the responsibilities of the os this list should appear more reasonable we can group the process control block information into three general categories process identification processor state information process control information a brief overview of the concepts of pages segments and virtual memory is provided in the subsection on memory management in section this brief discussion slides over some details in particular in a system that uses virtual memory all of the process image for an active process is always in secondary memory when a portion of the image is loaded into main memory it is copied rather than moved thus the secondary memory retains a copy of all segments andor pages however if the main memory portion of the image is modified the secondary copy will be out of date until the main memory portion is copied back onto disk table typical elements of a process control block process identification identifiers numeric identifiers that may be stored with the process control block include identifier of this process identifier of the process that created this process parent process user identifier processor state information uservisible registers a uservisible register is one that may be referenced by means of the machine language that the processor executes while in user mode typically there are from to of these registers although some risc implementations have over control and status registers these are a variety of processor registers that are employed to control the operation of the processor these include program counter contains the address of the next instruction to be fetched condition codes result of the most recent arithmetic or logical operation eg sign zero carry equal overflow status information includes interrupt enableddisabled flags execution mode stack pointers each process has one or more lastinfirstout lifo system stacks associated with it a stack is used to store parameters and calling addresses for procedure and system calls the stack pointer points to the top of the stack process control information scheduling and state information this is information that is needed by the operating system to perform its scheduling function typical items of information process state defines the readiness of the process to be scheduled for execution eg running ready waiting halted priority one or more fields may be used to describe the scheduling priority of the process in some systems several values are required eg default current highestallowable schedulingrelated information this will depend on the scheduling algorithm used examples are the amount of time that the process has been waiting and the amount of time that the process executed the last time it was running event identity of event the process is awaiting before it can be resumed data structuring a process may be linked to other process in a queue ring or some other structure for example all processes in a waiting state for a particular priority level may be linked in a queue a process may exhibit a parentchild creatorcreated relationship with another process the process control block may contain pointers to other processes to support these structures interprocess communication various flags signals and messages may be associated with communication between two independent processes some or all of this information may be maintained in the process control block process privileges processes are granted privileges in terms of the memory that may be accessed and the types of instructions that may be executed in addition privileges may apply to the use of system utilities and services memory management this section may include pointers to segment andor page tables that describe the virtual memory assigned to this process resource ownership and utilization resources controlled by the process may be indicated such as opened files a history of utilization of the processor or other resources may also be included this information may be needed by the scheduler with respect to process identification in virtually all operating systems each process is assigned a unique numeric identifier which may simply be an index into the primary process table figure otherwise there must be a mapping that allows the os to locate the appropriate tables based on the process identifier this identifier is useful in several ways many of the other tables controlled by the os may use process identifiers to crossreference process tables for example the memory tables may be organized so as to provide a map of main memory with an indication of which process is assigned to each region similar references will appear in io and file tables when processes communicate with one another the process identifier informs the os of the destination of a particular communication when processes are allowed to create other processes identifiers indicate the parent and descendents of each process in addition to these process identifiers a process may be assigned a user identifier that indicates the user responsible for the job processor state information consists of the contents of processor registers while a process is running of course the information is in the registers when a process is interrupted all of this register information must be saved so that it can be restored when the process resumes execution the nature and number of registers involved depend on the design of the processor typically the register set will include uservisible registers control and status registers and stack pointers these are described in chapter of particular note all processor designs include a register or set of registers often known as the program status word psw that contains status information the psw typically contain condition codes plus other status information a good example of a processor status word is that on intel x processors referred to as the eflags register shown in figure and table this structure is used by any os including unix and windows running on an x processor the third major category of information in the process control block can be called for want of a better name process control information this is the additional information needed by the os to control and coordinate the various active processes the last part of table indicates the scope of this information as i v v a v r n io o d i t s z a p c d i i c m f t pl f f f f f f f f f p f id identification flag df direction flag vip virtual interrupt pending if interrupt enable flag vif virtual interrupt flag tf trap flag ac alignment check sf sign flag vm virtual mode zf zero flag rf resume flag af auxiliary carry flag nt nested task flag pf parity flag iopl io privilege level cf carry flag of overflow flag figure x eflags register table pentium eflags register bits control bits ac alignment check set if a word or doubleword is addressed on a nonword or nondoubleword boundary id identification flag if this bit can be set and cleared this processor supports the cpuid instruction this instruction provides information about the vendor family and model rf resume flag allows the programmer to disable debug exceptions so that the instruction can be restarted after a debug exception without immediately causing another debug exception iopl io privilege level when set causes the processor to generate an exception on all accesses to io devices during protected mode operation df direction flag determines whether string processing instructions increment or decrement the bit halfregisters si and di for bit operations or the bit registers esi and edi for bit operations if interrupt enable flag when set the processor will recognize external interrupts tf trap flag when set causes an interrupt after the execution of each instruction this is used for debugging operating mode bits nt nested task flag indicates that the current task is nested within another task in protected mode operation vm virtual mode allows the programmer to enable or disable virtual mode which determines whether the processor runs as an machine vip virtual interrupt pending used in virtual mode to indicate that one or more interrupts are awaiting service vif virtual interrupt flag used in virtual mode instead of if condition codes af auxiliary carry flag represents carrying or borrowing between halfbytes of an bit arithmetic or logic operation using the al register cf carry flag indicates carrying out or borrowing into the leftmost bit position following an arithmetic operation also modified by some of the shift and rotate operations of overflow flag indicates an arithmetic overflow after an addition or subtraction pf parity flag parity of the result of an arithmetic or logic operation indicates even parity indicates odd parity sf sign flag indicates the sign of the result of an arithmetic or logic operation zf zero flag indicates that the result of an arithmetic or logic operation is process process process identification identification identification processor state processor state processor state process information information information control block process control process control process control information information information user stack user stack user stack private user private user private user address space address space address space programs data programs data programs data shared address shared address shared address space space space process process process n figure user processes in virtual memory we examine the details of operating system functionality in succeeding chapters the need for the various items on this list should become clear figure suggests the structure of process images in virtual memory each process image consists of a process control block a user stack the private address space of the process and any other address space that the process shares with other processes in the figure each process image appears as a contiguous range of addresses in an actual implementation this may not be the case it will depend on the memory management scheme and the way in which control structures are organized by the os as indicated in table the process control block may contain structuring information including pointers that allow the linking of process control blocks thus the queues that were described in the preceding section could be implemented as linked lists of process control blocks for example the queueing structure of figure a could be implemented as suggested in figure the role of the process control modes of execution before continuing with our discussion of the way in which the os manages processes we need to distinguish between the mode of processor execution normally associated with the os and that normally associated with user programs most processors support at least two modes of execution certain instructions can only be executed in the moreprivileged mode these would include reading or altering a control register such as the program status word primitive io instructions and instructions that relate to memory management in addition certain regions of memory can only be accessed in the moreprivileged mode the lessprivileged mode is often referred to as the user mode because user programs typically would execute in this mode the moreprivileged mode is referred to as the system mode control mode or kernel mode this last term refers to the kernel of the os which is that portion of the os that encompasses the important system functions table lists the functions typically found in the kernel of an os the reason for using two modes should be clear it is necessary to protect the os and key operating system tables such as process control blocks from interference by user programs in the kernel mode the software has complete control of the processor and all its instructions registers and memory this level of control is not necessary and for safety is not desirable for user programs two questions arise how does the processor know in which mode it is to be executing and how is the mode changed regarding the first question typically there is a bit in the program status word psw that indicates the mode of execution this bit is changed in response to certain events typically when a user makes a call to an operating system service or when an interrupt triggers execution of an operating system routine the mode is set to the kernel mode and upon return from the service to the user process the mode is set to user mode as an example consider the intel itanium processor which implements the bit ia architecture the processor has a processor status register psr that includes a bit cpl current privilege level field level is the most privileged level while level is the least privileged level most operating systems such as linux use level for the kernel and one other level table typical functions of an operating system kernel process management process creation and termination process scheduling and dispatching process switching process synchronization and support for interprocess communication management of process control blocks memory management allocation of address space to processes swapping page and segment management io management buffer management allocation of io channels and devices to processes support functions interrupt handling accounting monitoring for user mode when an interrupt occurs the processor clears most of the bits in the psr including the cpl field this automatically sets the cpl to level at the end of the interrupthandling routine the final instruction that is executed is irt interrupt return this instruction causes the processor to restore the psr of the interrupted program which restores the privilege level of that program a similar sequence occurs when an application places a system call for the itanium an application places a system call by placing the system call identifier and the system call arguments in a predefined area and then executing a special instruction that has the effect of interrupting execution at the user level and transferring control to the kernel process creation in section we discussed the events that lead to the creation of a new process having discussed the data structures associated with a process we are now in a position to describe briefly the steps involved in actually creating the process once the os decides for whatever reason table to create a new process it can proceed as follows assign a unique process identifier to the new process at this time a new entry is added to the primary process table which contains one entry per process allocate space for the process this includes all elements of the process image thus the os must know how much space is needed for the private user address space programs and data and the user stack these values can be assigned by default based on the type of process or they can be set based on user request at job creation time if a process is spawned by another process the parent process can pass the needed values to the os as part of the processcreation request if any existing address space is to be shared by this new process the appropriate linkages must be set up finally space for a process control block must be allocated initialize the process control block the process identification portion contains the id of this process plus other appropriate ids such as that of the parent process the processor state information portion will typically be initialized with most entries zero except for the program counter set to the program entry point and system stack pointers set to define the process stack boundaries the process control information portion is initialized based on standard default values plus attributes that have been requested for this process for example the process state would typically be initialized to ready or ready suspend the priority may be set by default to the lowest priority unless an explicit request is made for a higher priority initially the process may own no resources io devices files unless there is an explicit request for these or unless they are inherited from the parent set the appropriate linkages for example if the os maintains each scheduling queue as a linked list then the new process must be put in the ready or readysuspend list create or expand other data structures for example the os may maintain an accounting file on each process to be used subsequently for billing andor performance assessment purposes process switching on the face of it the function of process switching would seem to be straightforward at some time a running process is interrupted and the os assigns another process to the running state and turns control over to that process however several design issues are raised first what events trigger a process switch another issue is that we must recognize the distinction between mode switching and process switching finally what must the os do to the various data structures under its control to achieve a process switch when to switch processes a process switch may occur any time that the os has gained control from the currently running process table suggests the possible events that may give control to the os first let us consider system interrupts actually we can distinguish as many systems do two kinds of system interrupts one of which is simply referred to as an interrupt and the other as a trap the former is due to some sort of event that is external to and independent of the currently running process such as the completion of an io operation the latter relates to an error or exception condition generated within the currently running process such as an illegal file access attempt with an ordinary interrupt control is first transferred to an interrupt handler which does some basic housekeeping and then branches to an os routine that is concerned with the particular type of interrupt that has occurred examples include the following clock interrupt the os determines whether the currently running process has been executing for the maximum allowable unit of time referred to as a time slice that is a time slice is the maximum amount of time that a process can execute before being interrupted if so this process must be switched to a ready state and another process dispatched io interrupt the os determines what io action has occurred if the io action constitutes an event for which one or more processes are waiting then the os moves all of the corresponding blocked processes to the ready state and blockedsuspend processes to the readysuspend state the os must then decide whether to resume execution of the process currently in the running state or to preempt that process for a higherpriority ready process memory fault the processor encounters a virtual memory address reference for a word that is not in main memory the os must bring in the block table mechanisms for interrupting the execution of a process mechanism cause use interrupt external to the execution of the reaction to an asynchronous external current instruction event trap associated with the execution of handling of an error or an exception the current instruction condition supervisor call explicit request call to an operating system function page or segment of memory containing the reference from secondary memory to main memory after the io request is issued to bring in the block of memory the process with the memory fault is placed in a blocked state the os then performs a process switch to resume execution of another process after the desired block is brought into memory that process is placed in the ready state with a trap the os determines if the error or exception condition is fatal if so then the currently running process is moved to the exit state and a process switch occurs if not then the action of the os will depend on the nature of the error and the design of the os it may attempt some recovery procedure or simply notify the user it may do a process switch or resume the currently running process finally the os may be activated by a supervisor call from the program being executed for example a user process is running and an instruction is executed that requests an io operation such as a file open this call results in a transfer to a routine that is part of the operating system code the use of a system call may place the user process in the blocked state mode switching in chapter we discussed the inclusion of an interrupt stage as part of the instruction cycle recall that in the interrupt stage the processor checks to see if any interrupts are pending indicated by the presence of an interrupt signal if no interrupts are pending the processor proceeds to the fetch stage and fetches the next instruction of the current program in the current process if an interrupt is pending the processor does the following it sets the program counter to the starting address of an interrupt handler program it switches from user mode to kernel mode so that the interrupt processing code may include privileged instructions the processor now proceeds to the fetch stage and fetches the first instruction of the interrupt handler program which will service the interrupt at this point typically the context of the process that has been interrupted is saved into that process control block of the interrupted program one question that may now occur to you is what constitutes the context that is saved the answer is that it must include any information that may be altered by the execution of the interrupt handler and that will be needed to resume the program that was interrupted thus the portion of the process control block that was referred to as processor state information must be saved this includes the program counter other processor registers and stack information does anything else need to be done that depends on what happens next the interrupt handler is typically a short program that performs a few basic tasks related to an interrupt for example it resets the flag or indicator that signals the presence of an interrupt it may send an acknowledgment to the entity that issued the interrupt such as an io module and it may do some basic housekeeping relating to the effects of the event that caused the interrupt for example if the interrupt relates to an io event the interrupt handler will check for an error condition if an error has occurred the interrupt handler may send a signal to the process that originally requested the io operation if the interrupt is by the clock then the handler will hand control over to the dispatcher which will want to pass control to another process because the time slice allotted to the currently running process has expired what about the other information in the process control block if this interrupt is to be followed by a switch to another process then some work will need to be done however in most operating systems the occurrence of an interrupt does not necessarily mean a process switch it is possible that after the interrupt handler has executed the currently running process will resume execution in that case all that is necessary is to save the processor state information when the interrupt occurs and restore that information when control is returned to the program that was running typically the saving and restoring functions are performed in hardware change of process state it is clear then that the mode switch is a concept distinct from that of the process switch a mode switch may occur without changing the state of the process that is currently in the running state in that case the context saving and subsequent restoral involve little overhead however if the currently running process is to be moved to another state ready blocked etc then the os must make substantial changes in its environment the steps involved in a full process switch are as follows save the context of the processor including program counter and other registers update the process control block of the process that is currently in the running state this includes changing the state of the process to one of the other states ready blocked readysuspend or exit other relevant fields must also be updated including the reason for leaving the running state and accounting information move the process control block of this process to the appropriate queue ready blocked on event i readysuspend select another process for execution this topic is explored in part four update the process control block of the process selected this includes changing the state of this process to running update memory management data structures this may be required depending on how address translation is managed this topic is explored in part three restore the context of the processor to that which existed at the time the selected process was last switched out of the running state by loading in the previous values of the program counter and other registers thus the process switch which involves a state change requires more effort than a mode switch the term context switch is often found in os literature and textbooks unfortunately although most of the literature uses this term to mean what is here called a process switch other sources use it to mean a mode switch or even a thread switch defined in the next chapter to avoid ambiguity the term is not used in this book execution of the operating system in chapter we pointed out two intriguing facts about operating systems the os functions in the same way as ordinary computer software in the sense that the os is a set of programs executed by the processor the os frequently relinquishes control and depends on the processor to restore control to the os if the os is just a collection of programs and if it is executed by the processor just like any other program is the os a process if so how is it controlled these interesting questions have inspired a number of design approaches figure illustrates a range of approaches that are found in various contemporary operating systems nonprocess kernel one traditional approach common on many older operating systems is to execute the kernel of the os outside of any process figure a with this approach when the currently running process is interrupted or issues a supervisor call the mode context of this process is saved and control is passed to the kernel the os has its own region of memory to use and its own system stack for controlling procedure calls and returns the os can perform any desired functions and restore the context of the interrupted process which causes execution to resume in the interrupted p p pn kernel a separate kernel p p pn os os os funcfuncfunctions tions tions processswitching functions b os functions execute within user processes p p pn os osk processswitching functions c os functions execute as separate processes figure relationship between operating system and user processes user process alternatively the os can complete the function of saving the environment of the process and proceed to schedule and dispatch another process whether this happens depends on the reason for the interruption and the circumstances at the time in any case the key point here is that the concept of process is considered to apply only to user programs the operating system code is executed as a separate entity that operates in privileged mode execution within user processes an alternative that is common with operating systems on smaller computers pcs workstations is to execute virtually all os software in the context of a user process the view is that the os is primarily a collection of routines that the user calls to perform various functions executed within the environment of the users process this is illustrated in figure b at any given point the os is managing n process images each image includes not only the regions illustrated in figure but also program data and stack areas for kernel programs figure suggests a typical process image structure for this strategy a separate kernel stack is used to manage callsreturns while the process is in kernel mode process identification processor state process control information block process control information user stack private user address space programs data kernel stack shared address space figure process image operating system executes within user space operating system code and data are in the shared address space and are shared by all user processes when an interrupt trap or supervisor call occurs the processor is placed in kernel mode and control is passed to the os to pass control from a user program to the os the mode context is saved and a mode switch takes place to an operating system routine however execution continues within the current user process thus a process switch is not performed just a mode switch within the same process if the os upon completion of its work determines that the current process should continue to run then a mode switch resumes the interrupted program within the current process this is one of the key advantages of this approach a user program has been interrupted to employ some operating system routine and then resumed and all of this has occurred without incurring the penalty of two process switches if however it is determined that a process switch is to occur rather than returning to the previously executing program then control is passed to a processswitching routine this routine may or may not execute in the current process depending on system design at some point however the current process has to be placed in a nonrunning state and another process designated as the running process during this phase it is logically most convenient to view execution as taking place outside of all processes in a way this view of the os is remarkable simply put at certain points in time a process will save its state information choose another process to run from among those that are ready and relinquish control to that process the reason this is not an arbitrary and indeed chaotic situation is that during the critical time the code that is executed in the user process is shared operating system code and not user code because of the concept of user mode and kernel mode the user can not tamper with or interfere with the operating system routines even though they are executing in the users process environment this further reminds us that there is a distinction between the concepts of process and program and that the relationship between the two is not one to one within a process both a user program and operating system programs may execute and the operating system programs that execute in the various user processes are identical processbased operating system another alternative illustrated in figure c is to implement the os as a collection of system processes as in the other options the software that is part of the kernel executes in a kernel mode in this case however major kernel functions are organized as separate processes again there may be a small amount of processswitching code that is executed outside of any process this approach has several advantages it imposes a program design discipline that encourages the use of a modular os with minimal clean interfaces between the modules in addition some noncritical operating system functions are conveniently implemented as separate processes for example we mentioned earlier a monitor program that records the level of utilization of various resources processor memory channels and the rate of progress of the user processes in the system because this program does not provide a particular service to any active process it can only be invoked by the os as a process the function can run at an assigned priority level and be interleaved with other processes under dispatcher control finally implementing the os as a set of processes is useful in a multiprocessor or multicomputer environment in which some of the operating system services can be shipped out to dedicated processors improving performance security issues an os associates a set of privileges with each process these privileges dictate what resources the process may access including regions of memory files privileged system instructions and so on typically a process that executes on behalf of a user has the privileges that the os recognizes for that user a system or utility process may have privileges assigned at configuration time on a typical system the highest level of privilege is referred to as administrator supervisor or root access root access provides access to all the functions and services of the operating system with root access a process has complete control of the system and can add or change programs and files monitor other processes send and receive network traffic and alter privileges a key security issue in the design of any os is to prevent or at least detect attempts by a user or a piece of malicious software malware from gaining unauthorized privileges on the system and in particular from gaining root access in this section we briefly summarize the threats and countermeasures related to this security issue part seven provides more detail system access threats system access threats fall into two general categories intruders and malicious software intruders one of the most common threats to security is the intruder the other is viruses often referred to as a hacker or cracker in an important early study of intrusion anderson ande identified three classes of intruders masquerader an individual who is not authorized to use the computer and who penetrates a systems access controls to exploit a legitimate users account misfeasor a legitimate user who accesses data programs or resources for which such access is not authorized or who is authorized for such access but misuses his or her privileges clandestine user an individual who seizes supervisory control of the system and uses this control to evade auditing and access controls or to suppress audit collection the masquerader is likely to be an outsider the misfeasor generally is an insider and the clandestine user can be either an outsider or an insider intruder attacks range from the benign to the serious at the benign end of the scale there are many people who simply wish to explore internets and see what is on unix systems the administrator or superuser account is called root hence the term root access out there at the serious end are individuals who are attempting to read privileged data perform unauthorized modifications to data or disrupt the system the objective of the intruder is to gain access to a system or to increase the range of privileges accessible on a system most initial attacks use system or software vulnerabilities that allow a user to execute code that opens a back door into the system intruders can get access to a system by exploiting attacks such as buffer overflows on a program that runs with certain privileges we introduce buffer overflow attacks in chapter alternatively the intruder attempts to acquire information that should have been protected in some cases this information is in the form of a user password with knowledge of some other users password an intruder can log in to a system and exercise all the privileges accorded to the legitimate user malicious software perhaps the most sophisticated types of threats to computer systems are presented by programs that exploit vulnerabilities in computing systems such threats are referred to as malicious software or malware in this context we are concerned with threats to application programs as well as utility programs such as editors and compilers and kernellevel programs malicious software can be divided into two categories those that need a host program and those that are independent the former referred to as parasitic are essentially fragments of programs that can not exist independently of some actual application program utility or system program viruses logic bombs and backdoors are examples the latter are selfcontained programs that can be scheduled and run by the operating system worms and bot programs are examples we can also differentiate between those software threats that do not replicate and those that do the former are programs or fragments of programs that are activated by a trigger examples are logic bombs backdoors and bot programs the latter consists of either a program fragment or an independent program that when executed may produce one or more copies of itself to be activated later on the same system or some other system viruses and worms are examples malicious software can be relatively harmless or may perform one or more of a number of harmful actions including destroying files and data in main memory bypassing controls to gain privileged access and providing a means for intruders to bypass access controls countermeasures intrusion detection rfc internet security glossary defines intrusion detection as follows a security service that monitors and analyzes system events for the purpose of finding and providing realtime or near realtime warning of attempts to access system resources in an unauthorized manner intrusion detection systems idss can be classified as follows hostbased ids monitors the characteristics of a single host and the events occurring within that host for suspicious activity networkbased ids monitors network traffic for particular network segments or devices and analyzes network transport and application protocols to identify suspicious activity an ids comprises three logical components sensors sensors are responsible for collecting data the input for a sensor may be any part of a system that could contain evidence of an intrusion types of input to a sensor include network packets log files and system call traces sensors collect and forward this information to the analyzer analyzers analyzers receive input from one or more sensors or from other analyzers the analyzer is responsible for determining if an intrusion has occurred the output of this component is an indication that an intrusion has occurred the output may include evidence supporting the conclusion that an intrusion occurred the analyzer may provide guidance about what actions to take as a result of the intrusion user interface the user interface to an ids enables a user to view output from the system or control the behavior of the system in some systems the user interface may equate to a manager director or console component intrusion detection systems are typically designed to detect human intruder behavior as well as malicious software behavior authentication in most computer security contexts user authentication is the fundamental building block and the primary line of defense user authentication is the basis for most types of access control and for user accountability rfc defines user authentication as follows the process of verifying an identity claimed by or for a system entity an authentication process consists of two steps identification step presenting an identifier to the security system identifiers should be assigned carefully because authenticated identities are the basis for other security services such as access control service verification step presenting or generating authentication information that corroborates the binding between the entity and the identifier for example user alice toklas could have the user identifier abtoklas this information needs to be stored on any server or computer system that alice wishes to use and could be known to system administrators and other users a typical item of authentication information associated with this user id is a password which is kept secret known only to alice and to the system if no one is able to obtain or guess alices password then the combination of alices user id and password enables administrators to set up alices access permissions and audit her activity because alices id is not secret system users can send her email but because her password is secret no one can pretend to be alice in essence identification is the means by which a user provides a claimed identity to the system user authentication is the means of establishing the validity of the claim there are four general means of authenticating a users identity which can be used alone or in combination something the individual knows examples include a password a personal identification number pin or answers to a prearranged set of questions something the individual possesses examples include electronic keycards smart cards and physical keys this type of authenticator is referred to as a token something the individual is static biometrics examples include recognition by fingerprint retina and face something the individual does dynamic biometrics examples include recognition by voice pattern handwriting characteristics and typing rhythm all of these methods properly implemented and used can provide secure user authentication however each method has problems an adversary may be able to guess or steal a password similarly an adversary may be able to forge or steal a token a user may forget a password or lose a token further there is a significant administrative overhead for managing password and token information on systems and securing such information on systems with respect to biometric authenticators there are a variety of problems including dealing with false positives and false negatives user acceptance cost and convenience access control access control implements a security policy that specifies who or what eg in the case of a process may have access to each specific system resource and the type of access that is permitted in each instance an access control mechanism mediates between a user or a process executing on behalf of a user and system resources such as applications operating systems firewalls routers files and databases the system must first authenticate a user seeking access typically the authentication function determines whether the user is permitted to access the system at all then the access control function determines if the specific requested access by this user is permitted a security administrator maintains an authorization database that specifies what type of access to which resources is allowed for this user the access control function consults this database to determine whether to grant access an auditing function monitors and keeps a record of user accesses to system resources firewalls firewalls can be an effective means of protecting a local system or network of systems from networkbased security threats while at the same time affording access to the outside world via wide area networks and the internet traditionally a firewall is a dedicated computer that interfaces with computers outside a network and has special security precautions built into it in order to protect sensitive files on computers within the network it is used to service outside network especially internet connections and dialin lines personal firewalls that are implemented in hardware or software and associated with a single workstation or pc are also common bell lists the following design goals for a firewall all traffic from inside to outside and vice versa must pass through the firewall this is achieved by physically blocking all access to the local network except via the firewall various configurations are possible as explained later in this chapter only authorized traffic as defined by the local security policy will be allowed to pass various types of firewalls are used which implement various types of security policies the firewall itself is immune to penetration this implies the use of a hardened system with a secured operating system trusted computer systems are suitable for hosting a firewall and often required in government applications unix svr process management unix system v makes use of a simple but powerful process facility that is highly visible to the user unix follows the model of figure b in which most of the os executes within the environment of a user process unix uses two categories of processes system processes and user processes system processes run in kernel mode and execute operating system code to perform administrative and housekeeping functions such as allocation of memory and process swapping user processes operate in user mode to execute user programs and utilities and in kernel mode to execute instructions that belong to the kernel a user process enters kernel mode by issuing a system call when an exception fault is generated or when an interrupt occurs process states a total of nine process states are recognized by the unix svr operating system these are listed in table and a state transition diagram is shown in figure table unix process states user running executing in user mode kernel running executing in kernel mode ready to run in ready to run as soon as the kernel schedules it memory asleep in memory unable to execute until an event occurs process is in main memory a blocked state ready to run process is ready to run but the swapper must swap the process into main memory swapped before the kernel can schedule it to execute sleeping swapped the process is awaiting an event and has been swapped to secondary storage a blocked state preempted process is returning from kernel to user mode but the kernel preempts it and does a process switch to schedule another process created process is newly created and not yet ready to run zombie process no longer exists but it leaves a record for its parent process to collect fork created preempted return enough not enough memory to user memory swapping system only user preempt running swap out return reschedule ready to run ready to run process in memory swap in swapped system call interrupt kernel running interrupt sleep wakeup wakeup interrupt return exit asleep in swap out sleep zombie memory swapped figure unix process state transition diagram based on figure in bach this figure is similar to figure b with the two unix sleeping states corresponding to the two blocked states the differences are as follows unix employs two running states to indicate whether the process is executing in user mode or kernel mode a distinction is made between the two states ready to run in memory and preempted these are essentially the same state as indicated by the dotted line joining them the distinction is made to emphasize the way in which the preempted state is entered when a process is running in kernel mode as a result of a supervisor call clock interrupt or io interrupt there will come a time when the kernel has completed its work and is ready to return control to the user program at this point the kernel may decide to preempt the current process in favor of one that is ready and of higher priority in that case the current process moves to the preempted state however for purposes of dispatching those processes in the preempted state and those in the ready to run in memory state form one queue preemption can only occur when a process is about to move from kernel mode to user mode while a process is running in kernel mode it may not be preempted this makes unix unsuitable for realtime processing chapter discusses the requirements for realtime processing two processes are unique in unix process is a special process that is created when the system boots in effect it is predefined as a data structure loaded at boot time it is the swapper process in addition process spawns process referred to as the init process all other processes in the system have process as an ancestor when a new interactive user logs on to the system it is process that creates a user process for that user subsequently the user process can create child processes in a branching tree so that any particular application can consist of a number of related processes process description a process in unix is a rather complex set of data structures that provide the os with all of the information necessary to manage and dispatch processes table summarizes the elements of the process image which are organized into three parts userlevel context register context and systemlevel context the userlevel context contains the basic elements of a users program and can be generated directly from a compiled object file the users program is separated into text and data areas the text area is readonly and is intended to hold the programs instructions while the process is executing the processor uses the user stack area for procedure calls and returns and parameter passing the shared memory area is a data area that is shared with other processes there is only one physical copy of a shared memory area but by the use of virtual memory it appears table unix process image userlevel context process text executable machine instructions of the program process data data accessible by the program of this process user stack contains the arguments local variables and pointers for functions executing in user mode shared memory memory shared with other processes used for interprocess communication register context program counter address of next instruction to be executed may be in kernel or user memory space of this process processor status contains the hardware status at the time of preemption contents and format are hardregister ware dependent stack pointer points to the top of the kernel or user stack depending on the mode of operation at the time or preemption generalpurpose hardware dependent registers systemlevel context process table entry defines state of a process this information is always accessible to the operating system u user area process control information that needs to be accessed only in the context of the process per process region defines the mapping from virtual to physical addresses also contains a permission table field that indicates the type of access allowed the process readonly readwrite or readexecute kernel stack contains the stack frame of kernel procedures as the process executes in kernel mode table unix process table entry process status current state of process pointers to u area and process memory area text data stack process size enables the operating system to know how much space to allocate the process user the real user id identifies the user who is responsible for the running process the effective identifiers user id may be used by a process to gain temporary privileges associated with a particular program while that program is being executed as part of the process the process operates with the effective user id process id of this process id of parent process these are set up when the process enters the identifiers created state during the fork system call event valid when a process is in a sleeping state when the event occurs the process is transferred descriptor to a readytorun state priority used for process scheduling signal enumerates signals sent to a process but not yet handled timers include process execution time kernel resource utilization and userset timer used to send alarm signal to a process plink pointer to the next link in the ready queue valid if process is ready to execute memory indicates whether process image is in main memory or swapped out if it is in memory status this field also indicates whether it may be swapped out or is temporarily locked into main memory to each sharing process that the shared memory region is in its address space when a process is not running the processor status information is stored in the register context area the systemlevel context contains the remaining information that the os needs to manage the process it consists of a static part which is fixed in size and stays with a process throughout its lifetime and a dynamic part which varies in size through the life of the process one element of the static part is the process table entry this is actually part of the process table maintained by the os with one entry per process the process table entry contains process control information that is accessible to the kernel at all times hence in a virtual memory system all process table entries are maintained in main memory table lists the contents of a process table entry the user area or u area contains additional process control information that is needed by the kernel when it is executing in the context of this process it is also used when paging processes to and from memory table shows the contents of this table the distinction between the process table entry and the u area reflects the fact that the unix kernel always executes in the context of some process much of the time the kernel will be dealing with the concerns of that process however some of the time such as when the kernel is performing a scheduling algorithm preparatory to dispatching another process it will need access to information about other processes the information in a process table can be accessed when the given process is not the current one the third static portion of the systemlevel context is the per process region table which is used by the memory management system finally the kernel stack is table unix u area process table indicates entry that corresponds to the u area pointer user identifiers real and effective user ids used to determine user privileges timers record time that the process and its descendants spent executing in user mode and in kernel mode signalhandler for each type of signal defined in the system indicates how the process will react to array receipt of that signal exit ignore execute specified user function control terminal indicates login terminal for this process if one exists error field records errors encountered during a system call return value contains the result of system calls io parameters describe the amount of data to transfer the address of the source or target data array in user space and file offsets for io file parameters current directory and current root describe the file system environment of the process user file records the files the process has opened descriptor table limit fields restrict the size of the process and the size of a file it can write permission modes mask mode settings on files the process creates fields the dynamic portion of the systemlevel context this stack is used when the process is executing in kernel mode and contains the information that must be saved and restored as procedure calls and interrupts occur process control process creation in unix is made by means of the kernel system call fork when a process issues a fork request the os performs the following functions bach it allocates a slot in the process table for the new process it assigns a unique process id to the child process it makes a copy of the process image of the parent with the exception of any shared memory it increments counters for any files owned by the parent to reflect that an additional process now also owns those files it assigns the child process to the ready to run state it returns the id number of the child to the parent process and a value to the child process all of this work is accomplished in kernel mode in the parent process when the kernel has completed these functions it can do one of the following as part of the dispatcher routine stay in the parent process control returns to user mode at the point of the fork call of the parent transfer control to the child process the child process begins executing at the same point in the code as the parent namely at the return from the fork call transfer control to another process both parent and child are left in the ready to run state it is perhaps difficult to visualize this method of process creation because both parent and child are executing the same passage of code the difference is this when the return from the fork occurs the return parameter is tested if the value is zero then this is the child process and a branch can be executed to the appropriate user program to continue execution if the value is nonzero then this is the parent process and the main line of execution can continue recommended reading good descriptions of unix process management are found in good and gray nehm is an interesting discussion of process states and the operating system primitives needed for process dispatching good goodheart b and cox j the magic garden explained the internals of unix system v release englewood cliffs nj prentice hall gray gray j interprocess communications in unix the nooks and crannies upper saddle river nj prentice hall nehm nehmer j dispatcher primitives for the construction of operating system kernels acta informatica vol key terms review questions and problems key terms blocked state privileged mode suspend state child process process swapping exit state process control block system mode interrupt process image task kernel mode process switch trace mode switch program status word trap new state ready state user mode parent process round robin preempt running state review questions what is an instruction trace what common events lead to the creation of a process for the processing model of figure briefly define each state what does it mean to preempt a process what is swapping and what is its purpose why does figure b have two blocked states list four characteristics of a suspended process for what types of entities does the os maintain tables of information for management purposes list three general categories of information in a process control block why are two modes user and kernel needed what are the steps performed by an os to create a new process what is the difference between an interrupt and a trap give three examples of an interrupt what is the difference between a mode switch and a process switch problems the following state transition table is a simplified model of process management with the labels representing transitions between states of ready run blocked and nonresident ready run blocked nonresident ready run blocked give an example of an event that can cause each of the above transitions draw a diagram if that helps assume that at time no system resources are being used except for the processor and memory now consider the following events at time p executes a command to read from disk unit at time ps time slice expires at time p executes a command to write to disk unit at time p executes a command to read from disk unit at time p executes a command to write to disk unit at time p is swapped out at time an interrupt occurs from disk unit ps read is complete at time an interrupt occurs from disk unit ps read is complete at time p terminates at time an interrupt occurs from disk unit ps write is complete at time p is swapped back in at time an interrupt occurs from disk unit ps write is complete for each time and identify which state each process is in if a process is blocked further identify the event on which is it blocked figure b contains seven states in principle one could draw a transition between any two states for a total of different transitions a list all of the possible transitions and give an example of what could cause each transition b list all of the impossible transitions and explain why for the sevenstate process model of figure b draw a queueing diagram similar to that of figure b consider the state transition diagram of figure b suppose that it is time for the os to dispatch a process and that there are processes in both the ready state and the readysuspend state and that at least one process in the readysuspend state has higher scheduling priority than any of the processes in the ready state two extreme policies are as follows always dispatch from a process in the ready state to minimize swapping and always give preference to the highestpriority process even though that may mean swapping when swapping is not necessary suggest an intermediate policy that tries to balance the concerns of priority and performance table shows the process states for the vaxvms operating system a can you provide a justification for the existence of so many distinct wait states b why do the following states not have resident and swappedout versions page fault wait collided page wait common event wait free page wait and resource wait c draw the state transition diagram and indicate the action or occurrence that causes each transition the vaxvms operating system makes use of four processor access modes to facilitate the protection and sharing of system resources among processesthe access mode determines instruction execution privileges what instructions the processor may execute memory access privileges which locations in virtual memory the current instruction may access table vaxvms process states process state process condition currently executing running process computable resident ready and resident in main memory computable outswapped ready but swapped out of main memory page fault wait process has referenced a page not in main memory and must wait for the page to be read in collided page wait process has referenced a shared page that is the cause of an existing page fault wait in another process or a private page that is in the process of being read in or written out common event wait waiting for shared event flag event flags are singlebit interprocess signaling mechanisms free page wait waiting for a free page in main memory to be added to the collection of pages in main memory devoted to this process the working set of the process hibernate wait resident process puts itself in a wait state hibernate wait outswapped hibernating process is swapped out of main memory local event wait resident process in main memory and waiting for local event flag usually io completion local event wait outswapped process in local event wait is swapped out of main memory suspended wait resident process is put into a wait state by another process suspended wait outswapped suspended process is swapped out of main memory resource wait process waiting for miscellaneous system resource the four modes are as follows kernel executes the kernel of the vms operating system which includes memory management interrupt handling and io operations executive executes many of the os service calls including file and record disk and tape management routines supervisor executes other os services such as responses to user commands user executes user programs plus utilities such as compilers editors linkers and debuggers a process executing in a lessprivileged mode often needs to call a procedure that executes in a moreprivileged mode for example a user program requires an operating system service this call is achieved by using a changemode chm instruction which causes an interrupt that transfers control to a routine at the new access mode a return is made by executing the rei return from exception or interrupt instruction a a number of operating systems have two modes kernel and user what are the advantages and disadvantages of providing four modes instead of two b can you make a case for even more than four modes the vms scheme discussed in the preceding problem is often referred to as a ring protection structure as illustrated in figure indeed the simple kerneluser scheme as described in section is a tworing structure silb points out a problem with this approach the main disadvantage of the ring hierarchical structure is that it does not allow us to enforce the needtoknow principle in particular if an object must chmx rei kernel executive supervisor user figure vaxvms access modes be accessible in domain dj but not accessible in domain di then we must have j i but this means that every segment accessible in di is also accessible in dj explain clearly what the problem is that is referred to in the preceding quote figure b suggests that a process can only be in one event queue at a time a is it possible that you would want to allow a process to wait on more than one event at the same time provide an example b in that case how would you modify the queueing structure of the figure to support this new feature in a number of early computers an interrupt caused the register values to be stored in fixed locations associated with the given interrupt signal under what circumstances is this a practical technique explain why it is inconvenient in general in section it was stated that unix is unsuitable for realtime applications because a process executing in kernel mode may not be preempted elaborate you have executed the following c program main int pid pid fork printf d n pid what are the possible outputs assuming the fork succeeded threads processes and threads multithreading thread functionality types of threads userlevel and kernellevel threads other arrangements multicore and multithreading performance of software on multicore application example valve game software windows thread and smp management process and thread objects multithreading thread states support for os subsystems symmetric multiprocessing support solaris thread and smp management multithreaded architecture motivation process structure thread execution interrupts as threads linux process and thread management linux tasks linux threads mac os x grand central dispatch summary recommended reading key terms review questions and problems the basic idea is that the several components in any complex system will perform particular subfunctions that contribute to the overall function the sciences of the artificial herbert simon learning objectives after studying this chapter you should be able to understand the distinction between process and thread describe the basic design issues for threads explain the difference between userlevel threads and kernellevel threads describe the thread management facility in windows describe the thread management facility in solaris describe the thread management facility in linux this chapter examines some more advanced concepts related to process management which are found in a number of contemporary operating systems we show that the concept of process is more complex and subtle than presented so far and in fact embodies two separate and potentially independent concepts one relating to resource ownership and another relating to execution this distinction has led to the development in many operating systems of a construct known as the thread processes and processes and threads the discussion so far has presented the concept of a process as embodying two characteristics resource ownership a process includes a virtual address space to hold the process image recall from chapter that the process image is the collection of program data stack and attributes defined in the process control block from time to time a process may be allocated control or ownership of resources such as main memory io channels io devices and files the os performs a protection function to prevent unwanted interference between processes with respect to resources schedulingexecution the execution of a process follows an execution path trace through one or more programs eg figure this execution may be interleaved with that of other processes thus a process has an execution state running ready etc and a dispatching priority and is the entity that is scheduled and dispatched by the os some thought should convince the reader that these two characteristics are independent and could be treated independently by the os this is done in a number of operating systems particularly recently developed systems to distinguish the two characteristics the unit of dispatching is usually referred to as a thread or lightweight process while the unit of resource ownership is usually referred to as a process or task multithreading multithreading refers to the ability of an os to support multiple concurrent paths of execution within a single process the traditional approach of a single thread of execution per process in which the concept of a thread is not recognized is referred to as a singlethreaded approach the two arrangements shown in the left half of figure are singlethreaded approaches msdos is an example of an os that supports a single user process and a single thread other operating systems such as some variants of unix support multiple user processes but only support one thread per process the right half of figure depicts multithreaded approaches a java runtime environment is an example of a system of one process with multiple threads of interest in this section is the use of multiple processes each of which supports multiple threads this approach is taken in windows solaris and many modern versions of unix among others in this section we give a general description one process one process one thread multiple threads multiple processes multiple processes one thread per process multiple threads per process instruction trace figure threads and processes ande alas even this degree of consistency is not maintained in ibms mainframe operating systems the concepts of address space and task respectively correspond roughly to the concepts of process and thread that we describe in this section also in the literature the term lightweight process is used as either equivalent to the term thread a particular type of thread known as a kernellevel thread or in the case of solaris an entity that maps userlevel threads to kernellevel threads of multithreading the details of the windows solaris and linux approaches are discussed later in this chapter in a multithreaded environment a process is defined as the unit of resource allocation and a unit of protection the following are associated with processes a virtual address space that holds the process image protected access to processors other processes for interprocess communication files and io resources devices and channels within a process there may be one or more threads each with the following a thread execution state running ready etc a saved thread context when not running one way to view a thread is as an independent program counter operating within a process an execution stack some perthread static storage for local variables access to the memory and resources of its process shared with all other threads in that process figure illustrates the distinction between threads and processes from the point of view of process management in a singlethreaded process model ie there is no distinct concept of thread the representation of a process includes its process control block and user address space as well as user and kernel stacks to manage the callreturn behavior of the execution of the process while the process is running it controls the processor registers the contents of these registers are saved when the process is not running in a multithreaded environment there is still a single process control block and user address space associated with the process but now there are separate stacks for each thread as well as a separate control singlethreaded multithreaded process model process model thread thread thread user thread thread thread process stack control control control control block block block block user kernel process user user user address stack control stack stack stack space block user kernel kernel kernel address stack stack stack space figure singlethreaded and multithreaded process models block for each thread containing register values priority and other threadrelated state information thus all of the threads of a process share the state and resources of that process they reside in the same address space and have access to the same data when one thread alters an item of data in memory other threads see the results if and when they access that item if one thread opens a file with read privileges other threads in the same process can also read from that file the key benefits of threads derive from the performance implications it takes far less time to create a new thread in an existing process than to create a brandnew process studies done by the mach developers show that thread creation is ten times faster than process creation in unix teva it takes less time to terminate a thread than a process it takes less time to switch between two threads within the same process than to switch between processes threads enhance efficiency in communication between different executing programs in most operating systems communication between independent processes requires the intervention of the kernel to provide protection and the mechanisms needed for communication however because threads within the same process share memory and files they can communicate with each other without invoking the kernel thus if there is an application or function that should be implemented as a set of related units of execution it is far more efficient to do so as a collection of threads rather than a collection of separate processes an example of an application that could make use of threads is a file server as each new file request comes in a new thread can be spawned for the file management program because a server will handle many requests many threads will be created and destroyed in a short period if the server runs on a multiprocessor computer then multiple threads within the same process can be executing simultaneously on different processors further because processes or threads in a file server must share file data and therefore coordinate their actions it is faster to use threads and shared memory than processes and message passing for this coordination the thread construct is also useful on a single processor to simplify the structure of a program that is logically doing several different functions letw gives four examples of the uses of threads in a singleuser multiprocessing system foreground and background work for example in a spreadsheet program one thread could display menus and read user input while another thread executes user commands and updates the spreadsheet this arrangement often increases the perceived speed of the application by allowing the program to prompt for the next command before the previous command is complete asynchronous processing asynchronous elements in the program can be implemented as threads for example as a protection against power failure one can design a word processor to write its random access memory ram buffer to disk once every minute a thread can be created whose sole job is periodic backup and that schedules itself directly with the os there is no need for fancy code in the main program to provide for time checks or to coordinate input and output speed of execution a multithreaded process can compute one batch of data while reading the next batch from a device on a multiprocessor system multiple threads from the same process may be able to execute simultaneously thus even though one thread may be blocked for an io operation to read in a batch of data another thread may be executing modular program structure programs that involve a variety of activities or a variety of sources and destinations of input and output may be easier to design and implement using threads in an os that supports threads scheduling and dispatching is done on a thread basis hence most of the state information dealing with execution is maintained in threadlevel data structures there are however several actions that affect all of the threads in a process and that the os must manage at the process level for example suspension involves swapping the address space of one process out of main memory to make room for the address space of another process because all threads in a process share the same address space all threads are suspended at the same time similarly termination of a process terminates all threads within that process thread functionality like processes threads have execution states and may synchronize with one another we look at these two aspects of thread functionality in turn thread states as with processes the key states for a thread are running ready and blocked generally it does not make sense to associate suspend states with threads because such states are processlevel concepts in particular if a process is swapped out all of its threads are necessarily swapped out because they all share the address space of the process there are four basic thread operations associated with a change in thread state ande spawn typically when a new process is spawned a thread for that process is also spawned subsequently a thread within a process may spawn another thread within the same process providing an instruction pointer and arguments for the new thread the new thread is provided with its own register context and stack space and placed on the ready queue block when a thread needs to wait for an event it will block saving its user registers program counter and stack pointers the processor may now turn to the execution of another ready thread in the same or a different process unblock when the event for which a thread is blocked occurs the thread is moved to the ready queue finish when a thread completes its register context and stacks are deallocated a significant issue is whether the blocking of a thread results in the blocking of the entire process in other words if one thread in a process is blocked does this prevent the running of any other thread in the same process even if that other thread is in a ready state clearly some of the flexibility and power of threads is lost if the one blocked thread blocks an entire process we return to this issue subsequently in our discussion of userlevel versus kernellevel threads but for now let us consider the performance benefits of threads that do not block an entire process figure based on one in klei shows a program that performs two remote procedure calls rpcs to two different hosts to obtain a combined result in a singlethreaded program the results are obtained in sequence so the program has to wait for a response from each server in turn rewriting the program to use a separate thread for each rpc results in a substantial speedup note that if this program operates on a uniprocessor the requests must be generated sequentially and the results processed in sequence however the program waits concurrently for the two replies time rpc rpc request request process server server a rpc using single thread rpc server request thread a process thread b process rpc request server b rpc using one thread per server on a uniprocessor blocked waiting for response to rpc blocked waiting for processor which is in use by thread b running figure remote procedure call rpc using threads an rpc is a technique by which two programs which may execute on different machines interact using procedure callreturn syntax and semantics both the called and calling program behave as if the partner program were running on the same machine rpcs are often used for clientserver applications and are discussed in chapter time io request time quantum request complete expires thread a process thread b process thread c process time quantum expires process created blocked ready running figure multithreading example on a uniprocessor on a uniprocessor multiprogramming enables the interleaving of multiple threads within multiple processes in the example of figure three threads in two processes are interleaved on the processor execution passes from one thread to another either when the currently running thread is blocked or when its time slice is exhausted thread synchronization all of the threads of a process share the same address space and other resources such as open files any alteration of a resource by one thread affects the environment of the other threads in the same process it is therefore necessary to synchronize the activities of the various threads so that they do not interfere with each other or corrupt data structures for example if two threads each try to add an element to a doubly linked list at the same time one element may be lost or the list may end up malformed the issues raised and the techniques used in the synchronization of threads are in general the same as for the synchronization of processes these issues and techniques are the subject of chapters and types of threads userlevel and kernellevel threads there are two broad categories of thread implementation userlevel threads ults and kernellevel threads klts the latter are also referred to in the literature as kernelsupported threads or lightweight processes userlevel threads in a pure ult facility all of the work of thread management is done by the application and the kernel is not aware of the existence of threads figure a illustrates the pure ult approach any application can be in this example thread c begins to run after thread a exhausts its time quantum even though thread b is also ready to run the choice between b and c is a scheduling decision a topic covered in part four the acronyms ult and klt are not widely used but are introduced for conciseness threads user user threads user library space space library space kernel kernel kernel space space space p p p p a pure userlevel b pure kernellevel c combined userlevel thread kernellevel thread p process figure userlevel and kernellevel threads programmed to be multithreaded by using a threads library which is a package of routines for ult management the threads library contains code for creating and destroying threads for passing messages and data between threads for scheduling thread execution and for saving and restoring thread contexts by default an application begins with a single thread and begins running in that thread this application and its thread are allocated to a single process managed by the kernel at any time that the application is running the process is in the running state the application may spawn a new thread to run within the same process spawning is done by invoking the spawn utility in the threads library control is passed to that utility by a procedure call the threads library creates a data structure for the new thread and then passes control to one of the threads within this process that is in the ready state using some scheduling algorithm when control is passed to the library the context of the current thread is saved and when control is passed from the library to a thread the context of that thread is restored the context essentially consists of the contents of user registers the program counter and stack pointers all of the activity described in the preceding paragraph takes place in user space and within a single process the kernel is unaware of this activity the kernel continues to schedule the process as a unit and assigns a single execution state ready running blocked etc to that process the following examples should clarify the relationship between thread scheduling and process scheduling suppose that process b is executing in its thread the states of the process and two ults that are part of the process are shown in figure a each of the following is a possible occurrence the application executing in thread makes a system call that blocks b for example an io call is made this causes control to transfer to the kernel the kernel invokes the io action places process b in the blocked state and switches to another process meanwhile according to the data structure maintained by a b thread thread thread thread ready running ready running ready running ready running blocked blocked blocked blocked process b process b ready running ready running blocked blocked c d thread thread thread thread ready running ready running ready running ready running blocked blocked blocked blocked process b process b ready running ready running blocked blocked figure examples of the relationships between userlevel thread states and process states the threads library thread of process b is still in the running state it is important to note that thread is not actually running in the sense of being executed on a processor but it is perceived as being in the running state by the threads library the corresponding state diagrams are shown in figure b a clock interrupt passes control to the kernel and the kernel determines that the currently running process b has exhausted its time slice the kernel places process b in the ready state and switches to another process meanwhile according to the data structure maintained by the threads library thread of process b is still in the running state the corresponding state diagrams are shown in figure c thread has reached a point where it needs some action performed by thread of process b thread enters a blocked state and thread transitions from ready to running the process itself remains in the running state the corresponding state diagrams are shown in figure d in cases and figures b and c when the kernel switches control back to process b execution resumes in thread also note that a process can be interrupted either by exhausting its time slice or by being preempted by a higherpriority process while it is executing code in the threads library thus a process may be in the midst of a thread switch from one thread to another when interrupted when that process is resumed execution continues within the threads library which completes the thread switch and transfers control to another thread within that process there are a number of advantages to the use of ults instead of klts including the following thread switching does not require kernel mode privileges because all of the thread management data structures are within the user address space of a single process therefore the process does not switch to the kernel mode to do thread management this saves the overhead of two mode switches user to kernel kernel back to user scheduling can be application specific one application may benefit most from a simple roundrobin scheduling algorithm while another might benefit from a prioritybased scheduling algorithm the scheduling algorithm can be tailored to the application without disturbing the underlying os scheduler ults can run on any os no changes are required to the underlying kernel to support ults the threads library is a set of applicationlevel functions shared by all applications there are two distinct disadvantages of ults compared to klts in a typical os many system calls are blocking as a result when a ult executes a system call not only is that thread blocked but also all of the threads within the process are blocked in a pure ult strategy a multithreaded application can not take advantage of multiprocessing a kernel assigns one process to only one processor at a time therefore only a single thread within a process can execute at a time in effect we have applicationlevel multiprogramming within a single process while this multiprogramming can result in a significant speedup of the application there are applications that would benefit from the ability to execute portions of code simultaneously there are ways to work around these two problems for example both problems can be overcome by writing an application as multiple processes rather than multiple threads but this approach eliminates the main advantage of threads each switch becomes a process switch rather than a thread switch resulting in much greater overhead another way to overcome the problem of blocking threads is to use a technique referred to as jacketing the purpose of jacketing is to convert a blocking system call into a nonblocking system call for example instead of directly calling a system io routine a thread calls an applicationlevel io jacket routine within this jacket routine is code that checks to determine if the io device is busy if it is the thread enters the blocked state and passes control through the threads library to another thread when this thread later is given control again the jacket routine checks the io device again kernellevel threads in a pure klt facility all of the work of thread management is done by the kernel there is no thread management code in the application level simply an application programming interface api to the kernel thread facility windows is an example of this approach figure b depicts the pure klt approach the kernel maintains context information for the process as a whole and for individual threads within the process scheduling by the kernel is done on a thread basis this approach overcomes the two principal drawbacks of the ult approach first the kernel can simultaneously schedule multiple threads from the same process on multiple processors second if one thread in a process is blocked the kernel can schedule another thread of the same process another advantage of the klt approach is that kernel routines themselves can be multithreaded the principal disadvantage of the klt approach compared to the ult approach is that the transfer of control from one thread to another within the same process requires a mode switch to the kernel to illustrate the differences table shows the results of measurements taken on a uniprocessor vax computer running a unixlike os the two benchmarks are as follows null fork the time to create schedule execute and complete a processthread that invokes the null procedure ie the overhead of forking a processthread and signalwait the time for a processthread to signal a waiting processthread and then wait on a condition ie the overhead of synchronizing two processesthreads together we see that there is an order of magnitude or more of difference between ults and klts and similarly between klts and processes table thread and process operation latencies s operation userlevel threads kernellevel threads processes null fork signal wait thus on the face of it while there is a significant speedup by using klt multithreading compared to singlethreaded processes there is an additional significant speedup by using ults however whether or not the additional speedup is realized depends on the nature of the applications involved if most of the thread switches in an application require kernel mode access then a ultbased scheme may not perform much better than a kltbased scheme combined approaches some operating systems provide a combined ult klt facility figure c in a combined system thread creation is done completely in user space as is the bulk of the scheduling and synchronization of threads within an application the multiple ults from a single application are mapped onto some smaller or equal number of klts the programmer may adjust the number of klts for a particular application and processor to achieve the best overall results in a combined approach multiple threads within the same application can run in parallel on multiple processors and a blocking system call need not block the entire process if properly designed this approach should combine the advantages of the pure ult and klt approaches while minimizing the disadvantages solaris is a good example of an os using this combined approach the current solaris version limits the ultklt relationship to be onetoone other arrangements as we have said the concepts of resource allocation and dispatching unit have traditionally been embodied in the single concept of the process that is as a relationship between threads and processes recently there has been much interest in providing for multiple threads within a single process which is a manytoone relationship however as table shows the other two combinations have also been investigated namely a manytomany relationship and a onetomany relationship manytomany relationship the idea of having a manytomany relationship between threads and processes has been explored in the experimental operating system trix pazz ward in trix there are the concepts of domain table relationship between threads and processes threads processes description example systems each thread of execution is a unique process with its traditional unix own address space and resources implementations m a process defines an address space and dynamic windows nt solaris resource ownership multiple threads may be created linux os os and executed within that process mach m a thread may migrate from one process environment ra clouds to another this allows a thread to be easily moved emerald among distinct systems mn combines attributes of m and m cases trix and thread a domain is a static entity consisting of an address space and ports through which messages may be sent and received a thread is a single execution path with an execution stack processor state and scheduling information as with the multithreading approaches discussed so far multiple threads may execute in a single domain providing the efficiency gains discussed earlier however it is also possible for a single user activity or application to be performed in multiple domains in this case a thread exists that can move from one domain to another the use of a single thread in multiple domains seems primarily motivated by a desire to provide structuring tools for the programmer for example consider a program that makes use of an io subprogram in a multiprogramming environment that allows userspawned processes the main program could generate a new process to handle io and then continue to execute however if the future progress of the main program depends on the outcome of the io operation then the main program will have to wait for the other io program to finish there are several ways to implement this application the entire program can be implemented as a single process this is a reasonable and straightforward solution there are drawbacks related to memory management the process as a whole may require considerable main memory to execute efficiently whereas the io subprogram requires a relatively small address space to buffer io and to handle the relatively small amount of program code because the io program executes in the address space of the larger program either the entire process must remain in main memory during the io operation or the io operation is subject to swapping this memory management effect would also exist if the main program and the io subprogram were implemented as two threads in the same address space the main program and io subprogram can be implemented as two separate processes this incurs the overhead of creating the subordinate process if the io activity is frequent one must either leave the subordinate process alive which consumes management resources or frequently create and destroy the subprogram which is inefficient treat the main program and the io subprogram as a single activity that is to be implemented as a single thread however one address space domain could be created for the main program and one for the io subprogram thus the thread can be moved between the two address spaces as execution proceeds the os can manage the two address spaces independently and no process creation overhead is incurred furthermore the address space used by the io subprogram could also be shared by other simple io programs the experiences of the trix developers indicate that the third option has merit and may be the most effective solution for some applications onetomany relationship in the field of distributed operating systems designed to control distributed computer systems there has been interest in the concept of a thread as primarily an entity that can move among address spaces a notable example of this research is the clouds operating system and especially its kernel known as ra dasg another example is the emerald system stee a thread in clouds is a unit of activity from the users perspective a process is a virtual address space with an associated process control block upon creation a thread starts executing in a process by invoking an entry point to a program in that process threads may move from one address space to another and actually span computer boundaries ie move from one computer to another as a thread moves it must carry with it certain information such as the controlling terminal global parameters and scheduling guidance eg priority the clouds approach provides an effective way of insulating both users and programmers from the details of the distributed environment a users activity may be represented as a single thread and the movement of that thread among computers may be dictated by the os for a variety of systemrelated reasons such as the need to access a remote resource and load balancing multicore and multithreading the use of a multicore system to support a single application with multiple threads such as might occur on a workstation a videogame console or a personal computer running a processorintense application raises issues of performance and application design in this section we first look at some of the performance implications of a multithreaded application on a multicore system and then describe a specific example of an application designed to exploit multicore capabilities performance of software on multicore the potential performance benefits of a multicore organization depend on the ability to effectively exploit the parallel resources available to the application let us focus first on a single application running on a multicore system amdahls law see appendix e states that speedup time to execute program on a single processor time to execute program on n parallel processors f f n the law assumes a program in which a fraction f of the execution time involves code that is inherently serial and a fraction f that involves code that is infinitely parallelizable with no scheduling overhead this law appears to make the prospect of a multicore organization attractive but as figure a shows even a small amount of serial code has a noticeable impact if only of the code is inherently serial f running the program on a multicore system with eight processors yields a performance gain of only a factor of in addition software typically incurs overhead as a result of communication the movement of processes or threads among address spaces or thread migration on different machines has become a hot topic in recent years chapter explores this topic relative speedup number of processors a speedup with and sequential portions relative speedup number of processors b speedup with overheads figure performance effect of multiple cores and distribution of work to multiple processors and cache coherence overhead this results in a curve where performance peaks and then begins to degrade because of the increased burden of the overhead of using multiple processors figure b from mcdo is a representative example however software engineers have been addressing this problem and there are numerous applications in which it is possible to effectively exploit a multicore system mcdo reports on a set of database applications in which great attention oracle dss way join tmc data mining db dss scan aggs oracle ad hoc insurance oltp scaling perfect scaling number of cpus figure scaling of database workloads on multipleprocessor hardware was paid to reducing the serial fraction within hardware architectures operating systems middleware and the database application software figure shows the result as this example shows database management systems and database applications are one area in which multicore systems can be used effectively many kinds of servers can also effectively use the parallel multicore organization because servers typically handle numerous relatively independent transactions in parallel in addition to generalpurpose server software a number of classes of applications benefit directly from the ability to scale throughput with the number of cores mcdo lists the following examples multithreaded native applications multithreaded applications are characterized by having a small number of highly threaded processes examples of threaded applications include lotus domino or siebel crm customer relationship manager multiprocess applications multiprocess applications are characterized by the presence of many singlethreaded processes examples of multiprocess applications include the oracle database sap and peoplesoft java applications java applications embrace threading in a fundamental way not only does the java language greatly facilitate multithreaded applications but the java virtual machine is a multithreaded process that provides scheduling and memory management for java applications java applications that can benefit directly from multicore resources include application servers such as suns java application server beas weblogic ibms websphere and the opensource tomcat application server all applications that use a java platform enterprise edition jee platform application server can immediately benefit from multicore technology multiinstance applications even if an individual application does not scale to take advantage of a large number of threads it is still possible to gain from multicore architecture by running multiple instances of the application in parallel if multiple application instances require some degree of isolation virtualization technology for the hardware of the operating system can be used to provide each of them with its own separate and secure environment application example valve game software valve is an entertainment and technology company that has developed a number of popular games as well as the source engine one of the most widely played game engines available source is an animation engine used by valve for its games and licensed for other game developers in recent years valve has reprogrammed the source engine software to use multithreading to exploit the power of multicore processor chips from intel and amd reim the revised source engine code provides more powerful support for valve games such as half life from valves perspective threading granularity options are defined as follows harr coarse threading individual modules called systems are assigned to individual processors in the source engine case this would mean putting rendering on one processor ai artificial intelligence on another physics on another and so on this is straightforward in essence each major module is single threaded and the principal coordination involves synchronizing all the threads with a timeline thread finegrained threading many similar or identical tasks are spread across multiple processors for example a loop that iterates over an array of data can be split up into a number of smaller parallel loops in individual threads that can be scheduled in parallel hybrid threading this involves the selective use of finegrained threading for some systems and single threading for other systems valve found that through coarse threading it could achieve up to twice the performance across two processors compared to executing on a single processor but this performance gain could only be achieved with contrived cases for realworld gameplay the improvement was on the order of a factor of valve also found that effective use of finegrained threading was difficult the time per work unit can be variable and managing the timeline of outcomes and consequences involved complex programming valve found that a hybrid threading approach was the most promising and would scale the best as multicore systems with or processors became available valve identified systems that operate very effectively being permanently assigned to a single processor an example is sound mixing which has little user interaction is not constrained by the frame configuration of windows and works on its own set of data other modules such as scene rendering can be organized into a number of threads so that the module can execute on a single processor but achieve greater performance as it is spread out over more and more processors render skybox main view monitor etc scene list for each object particles sim and draw character bone setup draw etc figure hybrid threading for rendering module figure illustrates the thread structure for the rendering module in this hierarchical structure higherlevel threads spawn lowerlevel threads as needed the rendering module relies on a critical part of the source engine the world list which is a database representation of the visual elements in the games world the first task is to determine what are the areas of the world that need to be rendered the next task is to determine what objects are in the scene as viewed from multiple angles then comes the processorintensive work the rendering module has to work out the rendering of each object from multiple points of view such as the players view the view of tv monitors and the point of view of reflections in water some of the key elements of the threading strategy for the rendering module are listed in leon and include the following construct scenerendering lists for multiple scenes in parallel eg the world and its reflection in water overlap graphics simulation compute character bone transformations for all characters in all scenes in parallel allow multiple threads to draw in parallel the designers found that simply locking key databases such as the world list for a thread was too inefficient over of the time a thread is trying to read from a data set and only of the time at most is spent in writing to a data set thus a concurrency mechanism known as the singlewritermultiplereaders model works effectively windows thread and smp management windows process design is driven by the need to provide support for a variety of os environments processes supported by different os environments differ in a number of ways including the following how processes are named whether threads are provided within processes how processes are represented how process resources are protected what mechanisms are used for interprocess communication and synchronization how processes are related to each other accordingly the native process structures and services provided by the windows kernel are relatively simple and general purpose allowing each os subsystem to emulate a particular process structure and functionality important characteristics of windows processes are the following windows processes are implemented as objects a process can be created as new process or as a copy of an existing process an executable process may contain one or more threads both process and thread objects have builtin synchronization capabilities figure based on one in russ illustrates the way in which a process relates to the resources it controls or uses each process is assigned a security access access token virtual address descriptors process object available handle table objects handle thread x handle file y handle section z figure a windows process and its resources token called the primary token of the process when a user first logs on windows creates an access token that includes the security id for the user every process that is created by or runs on behalf of this user has a copy of this access token windows uses the token to validate the users ability to access secured objects or to perform restricted functions on the system and on secured objects the access token controls whether the process can change its own attributes in this case the process does not have a handle opened to its access token if the process attempts to open such a handle the security system determines whether this is permitted and therefore whether the process may change its own attributes also related to the process is a series of blocks that define the virtual address space currently assigned to this process the process can not directly modify these structures but must rely on the virtual memory manager which provides a memoryallocation service for the process finally the process includes an object table with handles to other objects known to this process figure shows a single thread in addition the process has access to a file object and to a section object that defines a section of shared memory process and thread objects the objectoriented structure of windows facilitates the development of a generalpurpose process facility windows makes use of two types of processrelated objects processes and threads a process is an entity corresponding to a user job or application that owns resources such as memory and open files a thread is a dispatchable unit of work that executes sequentially and is interruptible so that the processor can turn to another thread each windows process is represented by an object whose general structure is shown in figure a each process is defined by a number of attributes and encapsulates a number of actions or services that it may perform a process will perform a service when called upon through a set of published interface methods when windows creates a new process it uses the object class or type defined for the windows process as a template to generate a new object instance at the time of creation attribute values are assigned table gives a brief definition of each of the object attributes for a process object a windows process must contain at least one thread to execute that thread may then create other threads in a multiprocessor system multiple threads from the same process may execute in parallel figure b depicts the object structure for a thread object and table defines the thread object attributes note that some of the attributes of a thread resemble those of a process in those cases the thread attribute value is derived from the process attribute value for example the thread processor affinity is the set of processors in a multiprocessor system that may execute this thread this set is equal to or a subset of the process processor affinity note that one of the attributes of a thread object is context which contains the values of the processor registers when the thread last ran this information enables threads to be suspended and resumed furthermore it is possible to alter the behavior of a thread by altering its context while it is suspended object type process object type thread process id thread id security descriptor thread context base priority dynamic priority object body default processor affinity object body base priority attributes quota limits attributes thread processor affinity execution time thread execution time io counters alert status vm operation counters suspension count exceptiondebugging ports impersonation token exit status termination port thread exit status create process open process create thread services query process information open thread set process information query thread information current process set thread information terminate process services current thread terminate thread get context a process object set context suspend resume alert thread test thread alert register termination port b thread object figure windows process and thread objects table windows process object attributes process id a unique value that identifies the process to the operating system security descriptor describes who created an object who can gain access to or use the object and who is denied access to the object base priority a baseline execution priority for the processs threads default processor affinity the default set of processors on which the processs threads can run quota limits the maximum amount of paged and nonpaged system memory paging file space and processor time a users processes can use execution time the total amount of time all threads in the process have executed io counters variables that record the number and type of io operations that the processs threads have performed vm operation counters variables that record the number and types of virtual memory operations that the processs threads have performed exceptiondebugging ports interprocess communication channels to which the process manager sends a message when one of the processs threads causes an exception normally these are connected to environment subsystem and debugger processes respectively exit status the reason for a processs termination table windows thread object attributes thread id a unique value that identifies a thread when it calls a server thread context the set of register values and other volatile data that defines the execution state of a thread dynamic priority the threads execution priority at any given moment base priority the lower limit of the threads dynamic priority thread processor affinity the set of processors on which the thread can run which is a subset or all of the processor affinity of the threads process thread execution time the cumulative amount of time a thread has executed in user mode and in kernel mode alert status a flag that indicates whether a waiting thread may execute an asynchronous procedure call suspension count the number of times the threads execution has been suspended without being resumed impersonation token a temporary access token allowing a thread to perform operations on behalf of another process used by subsystems termination port an interprocess communication channel to which the process manager sends a message when the thread terminates used by subsystems thread exit status the reason for a threads termination multithreading windows supports concurrency among processes because threads in different processes may execute concurrently appear to run at the same time moreover multiple threads within the same process may be allocated to separate processors and execute simultaneously actually run at the same time a multithreaded process achieves concurrency without the overhead of using multiple processes threads within the same process can exchange information through their common address space and have access to the shared resources of the process threads in different processes can exchange information through shared memory that has been set up between the two processes an objectoriented multithreaded process is an efficient means of implementing a server application for example one server process can service a number of clients concurrently thread states an existing windows thread is in one of six states figure ready a ready thread may be scheduled for execution the kernel dispatcher keeps track of all ready threads and schedules them in priority order standby a standby thread has been selected to run next on a particular processor the thread waits in this state until that processor is made available if the standby threads priority is high enough the running thread on that runnable pick to standby run switch ready preempted running resource unblockresume block terminate available resource available suspend transition unblock waiting terminated resource not available not runnable figure windows thread states processor may be preempted in favor of the standby thread otherwise the standby thread waits until the running thread blocks or exhausts its time slice running once the kernel dispatcher performs a thread switch the standby thread enters the running state and begins execution and continues execution until it is preempted by a higherpriority thread exhausts its time slice blocks or terminates in the first two cases it goes back to the ready state waiting a thread enters the waiting state when it is blocked on an event eg io it voluntarily waits for synchronization purposes or an environment subsystem directs the thread to suspend itself when the waiting condition is satisfied the thread moves to the ready state if all of its resources are available transition a thread enters this state after waiting if it is ready to run but the resources are not available for example the threads stack may be paged out of memory when the resources are available the thread goes to the ready state terminated a thread can be terminated by itself by another thread or when its parent process terminates once housekeeping chores are completed the thread is removed from the system or it may be retained by the executive for future reinitialization the windows executive is described in chapter it contains the base operating system services such as memory management process and thread management security io and interprocess communication support for os subsystems the generalpurpose process and thread facility must support the particular process and thread structures of the various os environments it is the responsibility of each os subsystem to exploit the windows process and thread features to emulate the process and thread facilities of its corresponding os this area of processthread management is complicated and we give only a brief overview here process creation begins with a request for a new process from an application the application issues a createprocess request to the corresponding protected subsystem which passes the request to the executive the executive creates a process object and returns a handle for that object to the subsystem when windows creates a process it does not automatically create a thread in the case of win a new process must always be created with an initial thread therefore for the win subsystem calls the windows process manager again to create a thread for the new process receiving a thread handle back from windows the appropriate thread and process information are then returned to the application in the case of posix threads are not supported therefore the posix subsystem obtains a thread for the new process from windows so that the process may be activated but returns only process information to the application the fact that the posix process is implemented using both a process and a thread from the windows executive is not visible to the application when a new process is created by the executive the new process inherits many of its attributes from the creating process however in the win environment this process creation is done indirectly an application client process issues its process creation request to the win subsystem then the subsystem in turn issues a process request to the windows executive because the desired effect is that the new process inherits characteristics of the client process and not of the server process windows enables the subsystem to specify the parent of the new process the new process then inherits the parents access token quota limits base priority and default processor affinity symmetric multiprocessing support windows supports smp hardware configurations the threads of any process including those of the executive can run on any processor in the absence of affinity restrictions explained in the next paragraph the kernel dispatcher assigns a ready thread to the next available processor this assures that no processor is idle or is executing a lowerpriority thread when a higherpriority thread is ready multiple threads from the same process can be executing simultaneously on multiple processors as a default the kernel dispatcher uses the policy of soft affinity in assigning threads to processors the dispatcher tries to assign a ready thread to the same processor it last ran on this helps reuse data still in that processors memory caches from the previous execution of the thread it is possible for an application to restrict its thread execution only to certain processors hard affinity solaris thread and smp management solaris implements multilevel thread support designed to provide considerable flexibility in exploiting processor resources multithreaded architecture solaris makes use of four separate threadrelated concepts process this is the normal unix process and includes the users address space stack and process control block userlevel threads implemented through a threads library in the address space of a process these are invisible to the os a userlevel thread ult is a usercreated unit of execution within a process lightweight processes a lightweight process lwp can be viewed as a mapping between ults and kernel threads each lwp supports ult and maps to one kernel thread lwps are scheduled by the kernel independently and may execute in parallel on multiprocessors kernel threads these are the fundamental entities that can be scheduled and dispatched to run on one of the system processors figure illustrates the relationship among these four entities note that there is always exactly one kernel thread for each lwp an lwp is visible within a process to the application thus lwp data structures exist within their respective process address space at the same time each lwp is bound to a single dispatchable kernel thread and the data structure for that kernel thread is maintained within the kernels address space process user user thread thread lightweight lightweight process lwp process lwp syscall syscall kernel kernel thread thread system calls kernel hardware figure processes and threads in solaris mcdo again the acronym ult is unique to this book and is not found in the solaris literature a process may consist of a single ult bound to a single lwp in this case there is a single thread of execution corresponding to a traditional unix process when concurrency is not required within a single process an application uses this process structure if an application requires concurrency its process contains multiple threads each bound to a single lwp which in turn are each bound to a single kernel thread in addition there are kernel threads that are not associated with lwps the kernel creates runs and destroys these kernel threads to execute specific system functions the use of kernel threads rather than kernel processes to implement system functions reduces the overhead of switching within the kernel from a process switch to a thread switch motivation the threelevel thread structure ult lwp kernel thread in solaris is intended to facilitate thread management by the os and to provide a clean interface to applications the ult interface can be a standard thread library a defined ult maps onto a lwp which is managed by the os and which has defined states of execution defined subsequently an lwp is bound to a kernel thread with a onetoone correspondence in execution states thus concurrency and execution are managed at the level of the kernel thread in addition an application has access to hardware through an application programming interface consisting of system calls the api allows the user to invoke kernel services to perform privileged tasks on behalf of the calling process such as read or write a file issue a control command to a device create a new process or thread allocate memory for the process to use and so on process structure figure compares in general terms the process structure of a traditional unix system with that of solaris on a typical unix implementation the process structure includes the process id the user ids a signal dispatch table which the kernel uses to decide what to do when sending a signal to a process file descriptors which describe the state of files in use by this process a memory map which defines the address space for this process and a processor state structure which includes the kernel stack for this process solaris retains this basic structure but replaces the processor state block with a list of structures containing one data block for each lwp the lwp data structure includes the following elements an lwp identifier the priority of this lwp and hence the kernel thread that supports it a signal mask that tells the kernel which signals will be accepted saved values of userlevel registers when the lwp is not running the kernel stack for this lwp which includes system call arguments results and error codes for each call level resource usage and profiling data pointer to the corresponding kernel thread pointer to the process structure unix process structure solaris process structure process id process id user ids user ids signal dispatch table signal dispatch table memory map memory map priority signal mask registers stack file descriptors file descriptors processor state lwp lwp lwp id lwp id priority priority signal mask signal mask registers registers stack stack figure process structure in traditional unix and solaris lewi thread execution figure shows a simplified view of both thread execution states these states reflect the execution status of both a kernel thread and the lwp bound to it as mentioned some kernel threads are not associated with an lwp the same execution diagram applies the states are as follows run the thread is runnable that is the thread is ready to execute onproc the thread is executing on a processor sleep the thread is blocked stop the thread is stopped zombie the thread has terminated free thread resources have been released and the thread is awaiting removal from the os thread data structure a thread moves from onproc to run if it is preempted by a higherpriority thread or because of time slicing a thread moves from onproc to sleep if it idl pinned threadcreate intr swtch run onproc syscall sleep preempt wakeup stop zombie free prun pstop exit reap figure solaris thread states is blocked and must await an event to return the run state blocking occurs if the thread invokes a system call and must wait for the system service to be performed a thread enters the stop state if its process is stopped this might be done for debugging purposes interrupts as threads most operating systems contain two fundamental forms of concurrent activity processes and interrupts processes or threads cooperate with each other and manage the use of shared data structures by means of a variety of primitives that enforce mutual exclusion only one process at a time can execute certain code or access certain data and that synchronize their execution interrupts are synchronized by preventing their handling for a period of time solaris unifies these two concepts into a single model namely kernel threads and the mechanisms for scheduling and executing kernel threads to do this interrupts are converted to kernel threads the motivation for converting interrupts to threads is to reduce overhead interrupt handlers often manipulate data shared by the rest of the kernel therefore while a kernel routine that accesses such data is executing interrupts must be blocked even though most interrupts will not affect that data typically the way this is done is for the routine to set the interrupt priority level higher to block interrupts and then lower the priority level after access is completed these operations take time the problem is magnified on a multiprocessor system the kernel must protect more objects and may need to block interrupts on all processors the solution in solaris can be summarized as follows solaris employs a set of kernel threads to handle interrupts as with any kernel thread an interrupt thread has its own identifier priority context and stack the kernel controls access to data structures and synchronizes among interrupt threads using mutual exclusion primitives of the type discussed in chapter that is the normal synchronization techniques for threads are used in handling interrupts interrupt threads are assigned higher priorities than all other types of kernel threads when an interrupt occurs it is delivered to a particular processor and the thread that was executing on that processor is pinned a pinned thread can not move to another processor and its context is preserved it is simply suspended until the interrupt is processed the processor then begins executing an interrupt thread there is a pool of deactivated interrupt threads available so that a new thread creation is not required the interrupt thread then executes to handle the interrupt if the handler routine needs access to a data structure that is currently locked in some fashion for use by another executing thread the interrupt thread must wait for access to that data structure an interrupt thread can only be preempted by another interrupt thread of higher priority experience with solaris interrupt threads indicates that this approach provides superior performance to the traditional interrupthandling strategy klei linux process and thread management linux tasks a process or task in linux is represented by a taskstruct data structure the taskstruct data structure contains information in a number of categories state the execution state of the process executing ready suspended stopped zombie this is described subsequently scheduling information information needed by linux to schedule processes a process can be normal or real time and has a priority realtime processes are scheduled before normal processes and within each category relative priorities can be used a counter keeps track of the amount of time a process is allowed to execute identifiers each process has a unique process identifier and also has user and group identifiers a group identifier is used to assign resource access privileges to a group of processes interprocess communication linux supports the ipc mechanisms found in unix svr described in chapter links each process includes a link to its parent process links to its siblings processes with the same parent and links to all of its children times and timers includes process creation time and the amount of processor time so far consumed by the process a process may also have associated one or more interval timers a process defines an interval timer by means of a system call as a result a signal is sent to the process when the timer expires a timer may be single use or periodic file system includes pointers to any files opened by this process as well as pointers to the current and the root directories for this process address space defines the virtual address space assigned to this process processorspecific context the registers and stack information that constitute the context of this process figure shows the execution states of a process these are as follows running this state value corresponds to two states a running process is either executing or it is ready to execute interruptible this is a blocked state in which the process is waiting for an event such as the end of an io operation the availability of a resource or a signal from another process uninterruptible this is another blocked state the difference between this and the interruptible state is that in an uninterruptible state a process is waiting directly on hardware conditions and therefore will not handle any signals stopped signal signal running state termination creation ready scheduling executing zombie event signal or event uninterruptible interruptible figure linux processthread model stopped the process has been halted and can only resume by positive action from another process for example a process that is being debugged can be put into the stopped state zombie the process has been terminated but for some reason still must have its task structure in the process table linux threads traditional unix systems support a single thread of execution per process while modern unix systems typically provide support for multiple kernellevel threads per process as with traditional unix systems older versions of the linux kernel offered no support for multithreading instead applications would need to be written with a set of userlevel library functions the most popular of which is known as pthread posix thread libraries with all of the threads mapping into a single kernellevel process we have seen that modern versions of unix offer kernellevel threads linux provides a unique solution in that it does not recognize a distinction between threads and processes using a mechanism similar to the lightweight processes of solaris userlevel threads are mapped into kernellevel processes multiple userlevel threads that constitute a single userlevel process are mapped into linux kernellevel processes that share the same group id this enables these processes to share resources such as files and memory and to avoid the need for a context switch when the scheduler switches among processes in the same group a new process is created in linux by copying the attributes of the current process a new process can be cloned so that it shares resources such as files signal handlers and virtual memory when the two processes share the same virtual memory they function as threads within a single process however no separate type of data structure is defined for a thread in place of the usual fork command processes are created in linux using the clone command this command includes a set of flags as arguments defined in table the traditional fork system call is implemented by linux as a clone system call with all of the clone flags cleared when the linux kernel performs a switch from one process to another it checks whether the address of the page directory of the current process is the same as that of the tobescheduled process if they are then they are sharing the same address space so that a context switch is basically just a jump from one location of code to another location of code although cloned processes that are part of the same process group can share the same memory space they can not share the same user stacks thus the clone call creates separate stack spaces for each process posix portable operating systems based on unix is an ieee api standard that includes a standard for a thread api libraries implementing the posix threads standard are often named pthreads pthreads are most commonly used on unixlike posix systems such as linux and solaris but microsoft windows implementations also exist table linux clone flags cloneclearid clear the task id clonedetached the parent does not want a sigchld signal sent on exit clonefiles share the table that identifies the open files clonefs share the table that identifies the root directory and the current working directory as well as the value of the bit mask used to mask the initial file permissions of a new file cloneidletask set pid to zero which refers to an idle task the idle task is employed when all available tasks are blocked waiting for resources clonenewns create a new namespace for the child cloneparent caller and new task share the same parent process cloneptrace if the parent process is being traced the child process will also be traced clonesettid write the tid back to user space clonesettls create a new tls for the child clonesighand share the table that identifies the signal handlers clonesysvsem share system v semundo semantics clonethread insert this process into the same thread group of the parent if this flag is true it implicitly enforces cloneparent clonevfork if set the parent does not get scheduled for execution until the child invokes the execve system call clonevm share the address space memory descriptor and all page tables mac os x grand central dispatch as was mentioned in chapter mac os x grand central dispatch gcd provides a pool of available threads designers can designate portions of applications called blocks that can be dispatched independently and run concurrently the os will provide as much concurrency as possible based on the number of cores available and the thread capacity of the system although other operating systems have implemented thread pools gcd provides a qualitative improvement in ease of use and efficiency a block is a simple extension to c or other languages such as c the purpose of defining a block is to define a selfcontained unit of work including code plus data here is a simple example of a block definition x printfhello worldn a block is denoted by a caret at the start of the function which is enclosed in curly brackets the above block definition defines x as a way of calling the function so that invoking the function x would print the words hello world blocks enable the programmer to encapsulate complex functions together with their arguments and data so that they can easily be referenced and passed around in a program much like a variable symbolically f f data blocks are scheduled and dispatched by means of queues the application makes use of system queues provided by gcd and may also set up private queues blocks are put onto a queue as they are encountered during program execution gcd then uses those queues to describe concurrency serialization and callbacks queues are lightweight userspace data structures which generally makes them far more efficient than manually managing threads and locks for example this queue has three blocks h g f queue depending on the queue and how it is defined gcd either treats these blocks as potentially concurrent activities or treats them as serial activities in either case blocks are dispatched on a firstinfirstout basis if this is a concurrent queue then the dispatcher assigns f to a thread as soon as one is available then g then h if this is a serial queue the dispatcher assigns f to a thread and then only assigns g to a thread after f has completed the use of predefined threads saves the cost of creating a new thread for each request reducing the latency associated with processing a block thread pools are automatically sized by the system to maximize the performance of the applications using gcd while minimizing the number of idle or competing threads h g f pool thread in addition to scheduling blocks directly the application can associate a single block and queue with an event source such as a timer network socket or file descriptor every time the source issues an event the block is scheduled if it is not much of the material in the remainder of this section is based on appl already running this allows rapid response without the expense of polling or parking a thread on the event source source e e e an example from sira indicates the ease of using gcd consider a documentbased application with a button that when clicked will analyze the current document and display some interesting statistics about it in the common case this analysis should execute in under a second so the following code is used to connect the button with an action inactionanalyzedocumentnsbutton sender nsdictionary stats mydoc analyze mymodel setdictstats mystatsview setneedsdisplayyes stats release the first line of the function body analyzes the document the second line updates the applications internal state and the third line tells the application that the statistics view needs to be updated to reflect this new state this code which follows a common pattern is executed in the main thread the design is acceptable so long as the analysis does not take too long because after the user clicks the button the main thread of the application needs to handle that user input as fast as possible so it can get back to the main event loop to process the next user action but if the user opens a very large or complex document the analyze step may take an unacceptably long amount of time a developer may be reluctant to alter the code to meet this unlikely event which may involve applicationglobal objects thread management callbacks argument marshalling context objects new variables and so on but with gcd a modest addition to the code produces the desired result ibactionanalyzedocumentnsbutton sender dispatchasyncdispatchgetglobalqueue nsdictionary stats mydoc analyze dispatchasyncdispatchgetmainqueue mymodel setdictstats mystatsview setneedsdisplayyes stats release all functions in gcd begin with dispatch the outer dispatch async call puts a task on a global concurrent queue this tells the os that the block can be assigned to a separate concurrent queue off the main queue and executed in parallel therefore the main thread of execution is not delayed when the analyze function is complete the inner dispatchasync call is encountered this directs the os to put the following block of code at the end of the main queue to be executed when it reaches the head of the queue so with very little work on the part of the programmer the desired requirement is met recommended reading lewi and klei provide good overviews of thread concepts and a discussion of programming strategies the former focuses more on concepts and the latter more on programming but both provide useful coverage of both topics pham discusses the windows nt thread facility in depth good coverage of unix threads concepts is found in robb klei kleiman s shah d and smallders b programming with threads upper saddle river nj prentice hall lewi lewis b and berg d threads primer upper saddle river nj prentice hall pham pham t and garg p multithreaded programming with windows nt upper saddle river nj prentice hall robb robbins k and robbins s unix systems programming communication concurrency and threads upper saddle river nj prentice hall key terms review questions and problems key terms kernellevel thread multithreading task lightweight process port thread message process userlevel thread review questions table lists typical elements found in a process control block for an unthreaded os of these which should belong to a thread control block and which should belong to a process control block for a multithreaded system list reasons why a mode switch between threads may be cheaper than a mode switch between processes what are the two separate and potentially independent characteristics embodied in the concept of process give four general examples of the use of threads in a singleuser multiprocessing system what resources are typically shared by all of the threads of a process list three advantages of ults over klts list two disadvantages of ults compared to klts define jacketing problems it was pointed out that two advantages of using multiple threads within a process are that less work is involved in creating a new thread within an existing process than in creating a new process and communication among threads within the same process is simplified is it also the case that a mode switch between two threads within the same process involves less work than a mode switch between two threads in different processes in the discussion of ults versus klts it was pointed out that a disadvantage of ults is that when a ult executes a system call not only is that thread blocked but also all of the threads within the process are blocked why is that so os is an obsolete os for pcs from ibm in os what is commonly embodied in the concept of process in other operating systems is split into three separate types of entities session processes and threads a session is a collection of one or more processes associated with a user interface keyboard display and mouse the session represents an interactive user application such as a word processing program or a spreadsheet this concept allows the personalcomputer user to open more than one application giving each one or more windows on the screen the os must keep track of which window and therefore which session is active so that keyboard and mouse input are routed to the appropriate session at any time one session is in foreground mode with other sessions in background mode all keyboard and mouse input is directed to one of the processes of the foreground session as dictated by the applications when a session is in foreground mode a process performing video output sends it directly to the hardware video buffer and thence to the users screen when the session is moved to the background the hardware video buffer is saved to a logical video buffer for that session while a session is in background if any of the threads of any of the processes of that session executes and produces screen output that output is directed to the logical video buffer when the session returns to foreground the screen is updated to reflect the current contents of the logical video buffer for the new foreground session there is a way to reduce the number of processrelated concepts in os from three to two eliminate sessions and associate the user interface keyboard mouse and screen with processes thus one process at a time is in foreground mode for further structuring processes can be broken up into threads a what benefits are lost with this approach b if you go ahead with this modification where do you assign resources memory files etc at the process or thread level consider an environment in which there is a onetoone mapping between userlevel threads and kernellevel threads that allows one or more threads within a process to issue blocking system calls while other threads continue to run explain why this model can make multithreaded programs run faster than their singlethreaded counterparts on a uniprocessor computer if a process exits and there are still threads of that process running will they continue to run the os mainframe operating system is structured around the concepts of address space and task roughly speaking a single address space corresponds to a single application and corresponds more or less to a process in other operating systems within an address space a number of tasks may be generated and execute concurrently this corresponds roughly to the concept of multithreading two data structures are key to managing this task structure an address space control block ascb contains information about an address space needed by os whether or not that address space is executing information in the ascb includes dispatching priority real and virtual memory allocated to this address space the number of ready tasks in this address space and whether each is swapped out a task control block tcb represents a user program in execution it contains information needed for managing a task within an address space including processor status information pointers to programs that are part of this task and task execution state ascbs are global structures maintained in system memory while tcbs are local structures maintained within their address space what is the advantage of splitting the control information into global and local portions many current language specifications such as for c and c are inadequate for multithreaded programs this can have an impact on compilers and the correctness of code as this problem illustrates consider the following declarations and function definition int globalpositives typedef struct list struct list next double val list void countpositiveslist l list p for p l p p p next if p val globalpositives now consider the case in which thread a performs countpositiveslist containing only negative values while thread b performs globalpositives a what does the function do b the c language only addresses singlethreaded execution does the use of two parallel threads create any problems or potential problems but some existing optimizing compilers including gcc which tends to be relatively conservative will optimize countpositives to something similar to void countpositiveslist l list p register int r r globalpositives for p l p p p next if p val r globalpositives r what problem or potential problem occurs with this compiled version of the program if threads a and b are executed concurrently consider the following code using the posix pthreads api threadc include pthreadh include stdlibh include unistdh include stdioh int myglobal void threadfunctionvoid arg int ij for i i i jmyglobal jj printf fflushstdout sleep myglobalj return null int mainvoid pthreadt mythread int i if pthreadcreate mythread null threadfunction null printfldquoerror creating thread abort for i i i myglobalmyglobal printfo fflushstdout sleep if pthreadjoin mythread null printferror joining thread abort printfnmyglobal equals dnmyglobal exit in main we first declare a variable called mythread which has a type of pthreadt this is essentially an id for a thread next the if statement creates a thread associated with mythread the call pthreadcreate returns zero on success and a nonzero value on failure the third argument of pthread create is the name of a function that the new thread will execute when it starts when this threadfunction returns the thread terminates meanwhile the main program itself defines a thread so that there are two threads executing the pthreadjoin function enables the main thread to wait until the new thread completes a what does this program accomplish b here is the output from the executed program thread oooooooooooooooooooo myglobal equals is this the output you would expect if not what has gone wrong the solaris documentation states that a ult may yield to another thread of the same priority isnt it possible that there will be a runnable thread of higher priority and that therefore the yield function should result in yielding to a thread of the same or higher priority in solaris and solaris there is a onetoone mapping between ults and lwps in solaris a single lwp supports one or more ults a what is the possible benefit of allowing a manytoone mapping of ults to lwps stop userlevel threads runnable continue wakeup dispatch stopped stop sleeping preempt stop sleep active time slice or preempt running stop dispatch wakeup blocking runnable system stopped call continue wakeup blocked stop lightweight processes figure solaris userlevel thread and lwp states b in solaris the thread execution state of a ult is distinct from that of its lwp explain why c figure shows the state transition diagrams for a ult and its associated lwp in solaris and explain the operation of the two diagrams and their relationships explain the rationale for the uninterruptible state in linux concurrency mutual exclusion and synchronization principles of concurrency a simple example race condition operating system concerns process interaction requirements for mutual exclusion mutual exclusion hardware support interrupt disabling special machine instructions semaphores mutual exclusion the producerconsumer problem implementation of semaphores monitors monitor with signal alternate model of monitors with notify and broadcast message passing synchronization addressing message format queueing discipline mutual exclusion readerswriters problem readers have priority writers have priority summary recommended reading key terms review questions and problems designing correct routines for controlling concurrent activities proved to be one of the most difficult aspects of systems programming the ad hoc techniques used by programmers of early multiprogramming and realtime systems were always vulnerable to subtle programming errors whose effects could be observed only when certain relatively rare sequences of actions occurredthe errors are particularly difficult to locate since the precise conditions under which they appear are very hard to reproduce the computer science and engineering research study mit press learning objectives after studying this chapter you should be able to discuss basic concepts related to concurrency such as race conditions os concerns and mutual exclusion requirements understand hardware approaches to supporting mutual exclusion define and explain semaphores define and explain monitors define and explain monitors explain the readerswriters problem the central themes of operating system design are all concerned with the management of processes and threads multiprogramming the management of multiple processes within a uniprocessor system multiprocessing the management of multiple processes within a multiprocessor distributed processing the management of multiple processes executing on multiple distributed computer systems the recent proliferation of clusters is a prime example of this type of system fundamental to all of these areas and fundamental to os design is concurrency concurrency encompasses a host of design issues including communication among processes sharing of and competing for resources such as memory files and io access synchronization of the activities of multiple processes and allocation of processor time to processes we shall see that these issues arise not just in multiprocessing and distributed processing environments but even in singleprocessor multiprogramming systems concurrency arises in three different contexts multiple applications multiprogramming was invented to allow processing time to be dynamically shared among a number of active applications structured applications as an extension of the principles of modular design and structured programming some applications can be effectively programmed as a set of concurrent processes operating system structure the same structuring advantages apply to systems programs and we have seen that operating systems are themselves often implemented as a set of processes or threads because of the importance of this topic four chapters and an appendix focus on concurrencyrelated issues chapters and deal with concurrency in multiprogramming and multiprocessing systems chapters and examine concurrency issues related to distributed processing this chapter begins with an introduction to the concept of concurrency and the implications of the execution of multiple concurrent processes we find that the basic requirement for support of concurrent processes is the ability to enforce mutual exclusion that is the ability to exclude all other processes from a course of action while one process is granted that ability next we examine some hardware mechanisms that can support mutual exclusion then we look at solutions that do not involve busy waiting and that can be supported either by the os or enforced by language compilers we examine three approaches semaphores monitors and message passing two classic problems in concurrency are used to illustrate the concepts and compare the approaches presented in this chapter the producerconsumer problem is introduced in section and used as a running example the chapter closes with the readerswriters problem our discussion of concurrency continues in chapter and we defer a discussion of the concurrency mechanisms of our example systems until the end of that chapter appendix a covers additional topics on concurrency table lists some key terms related to concurrency a set of animations that illustrate concepts in this chapter is available online click on the rotating globe at this books web site at williamstallingscomososehtml for access table some key terms related to concurrency atomic operation a function or action implemented as a sequence of one or more instructions that appears to be indivisible that is no other process can see an intermediate state or interrupt the operation the sequence of instruction is guaranteed to execute as a group or not execute at all having no visible effect on system state atomicity guarantees isolation from concurrent processes critical section a section of code within a process that requires access to shared resources and that must not be executed while another process is in a corresponding section of code deadlock a situation in which two or more processes are unable to proceed because each is waiting for one of the others to do something livelock a situation in which two or more processes continuously change their states in response to changes in the other processes without doing any useful work mutual exclusion the requirement that when one process is in a critical section that accesses shared resources no other process may be in a critical section that accesses any of those shared resources race condition a situation in which multiple threads or processes read and write a shared data item and the final result depends on the relative timing of their execution starvation a situation in which a runnable process is overlooked indefinitely by the scheduler although it is able to proceed it is never chosen for simplicity we generally refer to the concurrent execution of processes in fact as we have seen in the preceding chapter in some systems the fundamental unit of concurrency is a thread rather than a process principles of concurrency in a singleprocessor multiprogramming system processes are interleaved in time to yield the appearance of simultaneous execution figure a even though actual parallel processing is not achieved and even though there is a certain amount of overhead involved in switching back and forth between processes interleaved execution provides major benefits in processing efficiency and in program structuring in a multipleprocessor system it is possible not only to interleave the execution of multiple processes but also to overlap them figure b at first glance it may seem that interleaving and overlapping represent fundamentally different modes of execution and present different problems in fact both techniques can be viewed as examples of concurrent processing and both present the same problems in the case of a uniprocessor the problems stem from a basic characteristic of multiprogramming systems the relative speed of execution of processes can not be predicted it depends on the activities of other processes the way in which the os handles interrupts and the scheduling policies of the os the following difficulties arise the sharing of global resources is fraught with peril for example if two processes both make use of the same global variable and both perform reads and writes on that variable then the order in which the various reads and writes are executed is critical an example of this problem is shown in the following subsection it is difficult for the os to manage the allocation of resources optimally for example process a may request use of and be granted control of a particular io channel and then be suspended before using that channel it may be undesirable for the os simply to lock the channel and prevent its use by other processes indeed this may lead to a deadlock condition as described in chapter it becomes very difficult to locate a programming error because results are typically not deterministic and reproducible eg see lebl carr shen for a discussion of this point all of the foregoing difficulties present themselves in a multiprocessor system as well because here too the relative speed of execution of processes is unpredictable a multiprocessor system must also deal with problems arising from the simultaneous execution of multiple processes fundamentally however the problems are the same as those for uniprocessor systems this should become clear as the discussion proceeds a simple example consider the following procedure void echo chin getchar chout chin putcharchout this procedure shows the essential elements of a program that will provide a character echo procedure input is obtained from a keyboard one keystroke at a time each input character is stored in variable chin it is then transferred to variable chout and sent to the display any program can call this procedure repeatedly to accept user input and display it on the users screen now consider that we have a singleprocessor multiprogramming system supporting a single user the user can jump from one application to another and each application uses the same keyboard for input and the same screen for output because each application needs to use the procedure echo it makes sense for it to be a shared procedure that is loaded into a portion of memory global to all applications thus only a single copy of the echo procedure is used saving space the sharing of main memory among processes is useful to permit efficient and close interaction among processes however such sharing can lead to problems consider the following sequence process p invokes the echo procedure and is interrupted immediately after getchar returns its value and stores it in chin at this point the most recently entered character x is stored in variable chin process p is activated and invokes the echo procedure which runs to conclusion inputting and then displaying a single character y on the screen process p is resumed by this time the value x has been overwritten in chin and therefore lost instead chin contains y which is transferred to chout and displayed thus the first character is lost and the second character is displayed twice the essence of this problem is the shared global variable chin multiple processes have access to this variable if one process updates the global variable and then is interrupted another process may alter the variable before the first process can use its value suppose however that we permit only one process at a time to be in that procedure then the foregoing sequence would result in the following process p invokes the echo procedure and is interrupted immediately after the conclusion of the input function at this point the most recently entered character x is stored in variable chin process p is activated and invokes the echo procedure however because p is still inside the echo procedure although currently suspended p is blocked from entering the procedure therefore p is suspended awaiting the availability of the echo procedure at some later time process p is resumed and completes execution of echo the proper character x is displayed when p exits echo this removes the block on p when p is later resumed the echo procedure is successfully invoked this example shows that it is necessary to protect shared global variables and other shared global resources and that the only way to do that is to control the code that accesses the variable if we impose the discipline that only one process at a time may enter echo and that once in echo the procedure must run to completion before it is available for another process then the type of error just discussed will not occur how that discipline may be imposed is a major topic of this chapter this problem was stated with the assumption that there was a singleprocessor multiprogramming os the example demonstrates that the problems of concurrency occur even when there is a single processor in a multiprocessor system the same problems of protected shared resources arise and the same solution works first suppose that there is no mechanism for controlling access to the shared global variable processes p and p are both executing each on a separate processor both processes invoke the echo procedure the following events occur events on the same line take place in parallel process p process p chin getchar chin getchar chout chin chout chin putcharchout putcharchout the result is that the character input to p is lost before being displayed and the character input to p is displayed by both p and p again let us add the capability of enforcing the discipline that only one process at a time may be in echo then the following sequence occurs processes p and p are both executing each on a separate processor p invokes the echo procedure while p is inside the echo procedure p invokes echo because p is still inside the echo procedure whether p is suspended or executing p is blocked from entering the procedure therefore p is suspended awaiting the availability of the echo procedure at a later time process p completes execution of echo exits that procedure and continues executing immediately upon the exit of p from echo p is resumed and begins executing echo in the case of a uniprocessor system the reason we have a problem is that an interrupt can stop instruction execution anywhere in a process in the case of a multiprocessor system we have that same condition and in addition a problem can be caused because two processes may be executing simultaneously and both trying to access the same global variable however the solution to both types of problem is the same control access to the shared resource race condition a race condition occurs when multiple processes or threads read and write data items so that the final result depends on the order of execution of instructions in the multiple processes let us consider two simple examples as a first example suppose that two processes p and p share the global variable a at some point in its execution p updates a to the value and at some point in its execution p updates a to the value thus the two tasks are in a race to write variable a in this example the loser of the race the process that updates last determines the final value of a for our second example consider two process p and p that share global variables b and c with initial values b and c at some point in its execution p executes the assignment b b c and at some point in its execution p executes the assignment c b c note that the two processes update different variables however the final values of the two variables depend on the order in which the two processes execute these two assignments if p executes its assignment statement first then the final values are b and c if p executes its assignment statement first then the final values are b and c appendix a includes a discussion of race conditions using semaphores as an example operating system concerns what design and management issues are raised by the existence of concurrency we can list the following concerns the os must be able to keep track of the various processes this is done with the use of process control blocks and was described in chapter the os must allocate and deallocate various resources for each active process at times multiple processes want access to the same resource these resources include processor time this is the scheduling function discussed in part four memory most operating systems use a virtual memory scheme the topic is addressed in part three files discussed in chapter io devices discussed in chapter the os must protect the data and physical resources of each process against unintended interference by other processes this involves techniques that relate to memory files and io devices a general treatment of protection is found in part seven the functioning of a process and the output it produces must be independent of the speed at which its execution is carried out relative to the speed of other concurrent processes this is the subject of this chapter to understand how the issue of speed independence can be addressed we need to look at the ways in which processes can interact process interaction we can classify the ways in which processes interact on the basis of the degree to which they are aware of each others existence table lists three possible degrees of awareness plus the consequences of each processes unaware of each other these are independent processes that are not intended to work together the best example of this situation is the multiprogramming of multiple independent processes these can either be batch jobs or interactive sessions or a mixture although the processes are not working together the os needs to be concerned about competition for resources for example two independent applications may both want to access the same disk or file or printer the os must regulate these accesses processes indirectly aware of each other these are processes that are not necessarily aware of each other by their respective process ids but that share access to some object such as an io buffer such processes exhibit cooperation in sharing the common object processes directly aware of each other these are processes that are able to communicate with each other by process id and that are designed to work jointly on some activity again such processes exhibit cooperation conditions will not always be as clearcut as suggested in table rather several processes may exhibit aspects of both competition and cooperation nevertheless it is productive to examine each of the three items in the preceding list separately and determine their implications for the os table process interaction degree of awareness relationship influence that one potential control process has on the problems other processes unaware of competition results of one process mutual exclusion each other independent of the deadlock renewable action of others resource timing of process may starvation be affected processes indirectly cooperation by sharing results of one process mutual exclusion aware of each other may depend on infor deadlock renewable eg shared object mation obtained from resource others starvation timing of process may data coherence be affected processes directly aware cooperation by commu results of one process deadlock consumof each other have nication may depend on inforable resource communication mation obtained from starvation primitives available others to them timing of process may be affected competition among processes for resources concurrent processes come into conflict with each other when they are competing for the use of the same resource in its pure form we can describe the situation as follows two or more processes need to access a resource during the course of their execution each process is unaware of the existence of other processes and each is to be unaffected by the execution of the other processes it follows from this that each process should leave the state of any resource that it uses unaffected examples of resources include io devices memory processor time and the clock there is no exchange of information between the competing processes however the execution of one process may affect the behavior of competing processes in particular if two processes both wish access to a single resource then one process will be allocated that resource by the os and the other will have to wait therefore the process that is denied access will be slowed down in an extreme case the blocked process may never get access to the resource and hence will never terminate successfully in the case of competing processes three control problems must be faced first is the need for mutual exclusion suppose two or more processes require access to a single nonsharable resource such as a printer during the course of execution each process will be sending commands to the io device receiving status information sending data andor receiving data we will refer to such a resource as a critical resource and the portion of the program that uses it as a critical section of the program it is important that only one program at a time be allowed in its critical section we can not simply rely on the os to understand and enforce this restriction because the detailed requirements may not be obvious in the case of the printer for example we want any individual process to have control of the printer while it prints an entire file otherwise lines from competing processes will be interleaved the enforcement of mutual exclusion creates two additional control problems one is that of deadlock for example consider two processes p and p and two resources r and r suppose that each process needs access to both resources to perform part of its function then it is possible to have the following situation the os assigns r to p and r to p each process is waiting for one of the two resources neither will release the resource that it already owns until it has acquired the other resource and performed the function requiring both resources the two processes are deadlocked a final control problem is starvation suppose that three processes p p p each require periodic access to resource r consider the situation in which p is in possession of the resource and both p and p are delayed waiting for that resource when p exits its critical section either p or p should be allowed access to r assume that the os grants access to p and that p again requires access before p completes its critical section if the os grants access to p after p has finished and subsequently alternately grants access to p and p then p may indefinitely be denied access to the resource even though there is no deadlock situation control of competition inevitably involves the os because it is the os that allocates resources in addition the processes themselves will need to be able to process process process n void p void p void pn while true while true while true preceding code preceding code preceding code entercritical ra entercritical ra entercritical ra critical section critical section critical section exitcritical ra exitcritical ra exitcritical ra following code following code following code figure illustration of mutual exclusion express the requirement for mutual exclusion in some fashion such as locking a resource prior to its use any solution will involve some support from the os such as the provision of the locking facility figure illustrates the mutual exclusion mechanism in abstract terms there are n processes to be executed concurrently each process includes a critical section that operates on some resource ra and additional code preceding and following the critical section that does not involve access to ra because all processes access the same resource ra it is desired that only one process at a time be in its critical section to enforce mutual exclusion two functions are provided entercritical and exitcritical each function takes as an argument the name of the resource that is the subject of competition any process that attempts to enter its critical section while another process is in its critical section for the same resource is made to wait it remains to examine specific mechanisms for providing the functions entercritical and exitcritical for the moment we defer this issue while we consider the other cases of process interaction cooperation among processes by sharing the case of cooperation by sharing covers processes that interact with other processes without being explicitly aware of them for example multiple processes may have access to shared variables or to shared files or databases processes may use and update the shared data without reference to other processes but know that other processes may have access to the same data thus the processes must cooperate to ensure that the data they share are properly managed the control mechanisms must ensure the integrity of the shared data because data are held on resources devices memory the control problems of mutual exclusion deadlock and starvation are again present the only difference is that data items may be accessed in two different modes reading and writing and only writing operations must be mutually exclusive however over and above these problems a new requirement is introduced that of data coherence as a simple example consider a bookkeeping application in which various data items may be updated suppose two items of data a and b are to be maintained in the relationship a b that is any program that updates one value must also update the other to maintain the relationship now consider the following two processes p a a b b p b b a a if the state is initially consistent each process taken separately will leave the shared data in a consistent state now consider the following concurrent execution sequence in which the two processes respect mutual exclusion on each individual data item a and b a a b b b b a a at the end of this execution sequence the condition a b no longer holds for example if we start with a b at the end of this execution sequence we have a and b the problem can be avoided by declaring the entire sequence in each process to be a critical section thus we see that the concept of critical section is important in the case of cooperation by sharing the same abstract functions of entercritical and exitcritical discussed earlier figure can be used here in this case the argument for the functions could be a variable a file or any other shared object furthermore if critical sections are used to provide data integrity then there may be no specific resource or variable that can be identified as an argument in that case we can think of the argument as being an identifier that is shared among concurrent processes to identify critical sections that must be mutually exclusive cooperation among processes by communication in the first two cases that we have discussed each process has its own isolated environment that does not include the other processes the interactions among processes are indirect in both cases there is a sharing in the case of competition they are sharing resources without being aware of the other processes in the second case they are sharing values and although each process is not explicitly aware of the other processes it is aware of the need to maintain data integrity when processes cooperate by communication however the various processes participate in a common effort that links all of the processes the communication provides a way to synchronize or coordinate the various activities typically communication can be characterized as consisting of messages of some sort primitives for sending and receiving messages may be provided as part of the programming language or provided by the os kernel because nothing is shared between processes in the act of passing messages mutual exclusion is not a control requirement for this sort of cooperation however the problems of deadlock and starvation are still present as an example of deadlock two processes may be blocked each waiting for a communication from the other as an example of starvation consider three processes p p and p that exhibit the following behavior p is repeatedly attempting to communicate with either p or p and p and p are both attempting to communicate with p a sequence could arise in which p and p exchange information repeatedly while p is blocked waiting for a communication from p there is no deadlock because p remains active but p is starved requirements for mutual exclusion any facility or capability that is to provide support for mutual exclusion should meet the following requirements mutual exclusion must be enforced only one process at a time is allowed into its critical section among all processes that have critical sections for the same resource or shared object a process that halts in its noncritical section must do so without interfering with other processes it must not be possible for a process requiring access to a critical section to be delayed indefinitely no deadlock or starvation when no process is in a critical section any process that requests entry to its critical section must be permitted to enter without delay no assumptions are made about relative process speeds or number of processors a process remains inside its critical section for a finite time only there are a number of ways in which the requirements for mutual exclusion can be satisfied one approach is to leave the responsibility with the processes that wish to execute concurrently processes whether they are system programs or application programs would be required to coordinate with one another to enforce mutual exclusion with no support from the programming language or the os we can refer to these as software approaches although this approach is prone to high processing overhead and bugs it is nevertheless useful to examine such approaches to gain a better understanding of the complexity of concurrent processing this topic is covered in appendix a a second approach involves the use of specialpurpose machine instructions these have the advantage of reducing overhead but nevertheless will be shown to be unattractive as a generalpurpose solution they are covered in section a third approach is to provide some level of support within the os or a programming language three of the most important such approaches are examined in sections through mutual exclusion hardware support in this section we look at several interesting hardware approaches to mutual exclusion interrupt disabling in a uniprocessor system concurrent processes can not have overlapped execution they can only be interleaved furthermore a process will continue to run until it invokes an os service or until it is interrupted therefore to guarantee mutual exclusion it is sufficient to prevent a process from being interrupted this capability can be provided in the form of primitives defined by the os kernel for disabling and enabling interrupts a process can then enforce mutual exclusion in the following way compare figure while true disable interrupts critical section enable interrupts remainder because the critical section can not be interrupted mutual exclusion is guaranteed the price of this approach however is high the efficiency of execution could be noticeably degraded because the processor is limited in its ability to interleave processes another problem is that this approach will not work in a multiprocessor architecture when the computer includes more than one processor it is possible and typical for more than one process to be executing at a time in this case disabled interrupts do not guarantee mutual exclusion special machine instructions in a multiprocessor configuration several processors share access to a common main memory in this case there is not a masterslave relationship rather the processors behave independently in a peer relationship there is no interrupt mechanism between processors on which mutual exclusion can be based at the hardware level as was mentioned access to a memory location excludes any other access to that same location with this as a foundation processor designers have proposed several machine instructions that carry out two actions atomically such as reading and writing or reading and testing of a single memory location with one instruction fetch cycle during execution of the instruction access to the memory location is blocked for any other instruction referencing that location in this section we look at two of the most commonly implemented instructions others are described in rayn and ston compareswap instruction the compareswap instruction also called a compare and exchange instruction can be defined as follows herl the term atomic means that the instruction is treated as a single step that can not be interrupted int compareandswap int word int testval int newval int oldval oldval word if oldval testval word newval return oldval this version of the instruction checks a memory location word against a test value testval if the memory locations current value is testval it is replaced with newval otherwise it is left unchanged the old memory value is always returned thus the memory location has been updated if the returned value is the same as the test value this atomic instruction therefore has two parts a compare is made between a memory value and a test value if the values are the same a swap occurs the entire compareswap function is carried out atomically that is it is not subject to interruption another version of this instruction returns a boolean value true if the swap occurred false otherwise some version of this instruction is available on nearly all processor families x ia sparc ibm z series etc and most operating systems use this instruction for support of concurrency figure a shows a mutual exclusion protocol based on the use of this instruction a shared variable bolt is initialized to the only process that may enter its critical section is one that finds bolt equal to all other processes attempting program mutualexclusion program mutualexclusion const int n number of processes int const n number of processes int bolt int bolt void pint i void pint i while true int keyi while compareandswapbolt while true do nothing do exchange keyi bolt critical section while keyi bolt critical section remainder bolt remainder void main void main bolt parbegin p p pn bolt parbegin p p pn a compare and swap instruction b exchange instruction figure hardware support for mutual exclusion the construct parbegin p p pn means the following suspend the execution of the main program initiate concurrent execution of procedures p p pn when all of p p pn have terminated resume the main program to enter their critical section go into a busy waiting mode the term busy waiting or spin waiting refers to a technique in which a process can do nothing until it gets permission to enter its critical section but continues to execute an instruction or set of instructions that tests the appropriate variable to gain entrance when a process leaves its critical section it resets bolt to at this point one and only one of the waiting processes is granted access to its critical section the choice of process depends on which process happens to execute the compareswap instruction next exchange instruction the exchange instruction can be defined as follows void exchange int register int memory int temp temp memory memory register register temp the instruction exchanges the contents of a register with that of a memory location both the intel ia architecture pentium and the ia architecture itanium contain an xchg instruction figure b shows a mutual exclusion protocol based on the use of an exchange instruction a shared variable bolt is initialized to each process uses a local variable key that is initialized to the only process that may enter its critical section is one that finds bolt equal to it excludes all other processes from the critical section by setting bolt to when a process leaves its critical section it resets bolt to allowing another process to gain access to its critical section note that the following expression always holds because of the way in which the variables are initialized and because of the nature of the exchange algorithm bolt a keyi n i if bolt then no process is in its critical section if bolt then exactly one process is in its critical section namely the process whose key value equals properties of the machineinstruction approach the use of a special machine instruction to enforce mutual exclusion has a number of advantages it is applicable to any number of processes on either a single processor or multiple processors sharing main memory it is simple and therefore easy to verify it can be used to support multiple critical sections each critical section can be defined by its own variable there are some serious disadvantages busy waiting is employed thus while a process is waiting for access to a critical section it continues to consume processor time starvation is possible when a process leaves a critical section and more than one process is waiting the selection of a waiting process is arbitrary thus some process could indefinitely be denied access deadlock is possible consider the following scenario on a singleprocessor system process p executes the special instruction eg compareswap exchange and enters its critical section p is then interrupted to give the processor to p which has higher priority if p now attempts to use the same resource as p it will be denied access because of the mutual exclusion mechanism thus it will go into a busy waiting loop however p will never be dispatched because it is of lower priority than another ready process p because of the drawbacks of both the software and hardware solutions we need to look for other mechanisms semaphores we now turn to os and programming language mechanisms that are used to provide concurrency table summarizes mechanisms in common use we begin in this section with semaphores the next two sections discuss monitors and message passing the other mechanisms in table are discussed when treating specific os examples in chapters and table common concurrency mechanisms semaphore an integer value used for signaling among processes only three operations may be performed on a semaphore all of which are atomic initialize decrement and increment the decrement operation may result in the blocking of a process and the increment operation may result in the unblocking of a process also known as a counting semaphore or a general semaphore binary semaphore a semaphore that takes on only the values and mutex similar to a binary semaphore a key difference between the two is that the process that locks the mutex sets the value to zero must be the one to unlock it sets the value to condition variable a data type that is used to block a process or thread until a particular condition is true monitor a programming language construct that encapsulates variables access procedures and initialization code within an abstract data type the monitors variable may only be accessed via its access procedures and only one process may be actively accessing the monitor at any one time the access procedures are critical sections a monitor may have a queue of processes that are waiting to access it event flags a memory word used as a synchronization mechanism application code may associate a different event with each bit in a flag a thread can wait for either a single event or a combination of events by checking one or multiple bits in the corresponding flag the thread is blocked until all of the required bits are set and or until at least one of the bits is set or mailboxesmessages a means for two processes to exchange information and that may be used for synchronization spinlocks mutual exclusion mechanism in which a process executes in an infinite loop waiting for the value of a lock variable to indicate availability the first major advance in dealing with the problems of concurrent processes came in with dijkstras treatise dijk dijkstra was concerned with the design of an os as a collection of cooperating sequential processes and with the development of efficient and reliable mechanisms for supporting cooperation these mechanisms can just as readily be used by user processes if the processor and os make the mechanisms available the fundamental principle is this two or more processes can cooperate by means of simple signals such that a process can be forced to stop at a specified place until it has received a specific signal any complex coordination requirement can be satisfied by the appropriate structure of signals for signaling special variables called semaphores are used to transmit a signal via semaphore s a process executes the primitive semsignals to receive a signal via semaphore s a process executes the primitive semwaits if the corresponding signal has not yet been transmitted the process is suspended until the transmission takes place to achieve the desired effect we can view the semaphore as a variable that has an integer value upon which only three operations are defined a semaphore may be initialized to a nonnegative integer value the semwait operation decrements the semaphore value if the value becomes negative then the process executing the semwait is blocked otherwise the process continues execution the semsignal operation increments the semaphore value if the resulting value is less than or equal to zero then a process blocked by a semwait operation if any is unblocked other than these three operations there is no way to inspect or manipulate semaphores we explain these operations as follows to begin the semaphore has a zero or positive value if the value is positive that value equals the number of processes that can issue a wait and immediately continue to execute if the value is zero either by initialization or because a number of processes equal to the initial semaphore value have issued a wait the next process to issue a wait is blocked and the semaphore value goes negative each subsequent wait drives the semaphore value further into minus territory the negative value equals the number of processes waiting to be unblocked each signal unblocks one of the waiting processes when the semaphore value is negative down points out three interesting consequences of the semaphore definition in general there is no way to know before a process decrements a semaphore whether it will block or not in dijkstras original paper and in much of the literature the letter p is used for semwait and the letter v for semsignal these are the initials of the dutch words for test proberen and increment verhogen in some of the literature the terms wait and signal are used this book uses semwait and semsignal for clarity and to avoid confusion with similar wait and signal operations in monitors discussed subsequently struct semaphore int count queuetype queue void semwaitsemaphore s scount if scount place this process in squeue block this process void semsignalsemaphore s scount if scount remove a process p from squeue place process p on ready list figure a definition of semaphore primitives after a process increments a semaphore and another process gets woken up both processes continue running concurrently there is no way to know which process if either will continue immediately on a uniprocessor system when you signal a semaphore you dont necessarily know whether another process is waiting so the number of unblocked processes may be zero or one figure suggests a more formal definition of the primitives for semaphores the semwait and semsignal primitives are assumed to be atomic a more restricted version known as the binary semaphore is defined in figure a binary semaphore may only take on the values and and can be defined by the following three operations a binary semaphore may be initialized to or the semwaitb operation checks the semaphore value if the value is zero then the process executing the semwaitb is blocked if the value is one then the value is changed to zero and the process continues execution the semsignalb operation checks to see if any processes are blocked on this semaphore semaphore value equals if so then a process blocked by a semwaitb operation is unblocked if no processes are blocked then the value of the semaphore is set to one in principle it should be easier to implement the binary semaphore and it can be shown that it has the same expressive power as the general semaphore see problem to contrast the two types of semaphores the nonbinary semaphore is often referred to as either a counting semaphore or a general semaphore a concept related to the binary semaphore is the mutex a key difference between the two is that the process that locks the mutex sets the value to zero struct binarysemaphore enum zero one value queuetype queue void semwaitbbinarysemaphore s if svalue one svalue zero else place this process in squeue block this process void semsignalbsemaphore s if squeue is empty svalue one else remove a process p from squeue place process p on ready list figure a definition of binary semaphore primitives must be the one to unlock it sets the value to in contrast it is possible for one process to lock a binary semaphore and for another to unlock it for both counting semaphores and binary semaphores a queue is used to hold processes waiting on the semaphore the question arises of the order in which processes are removed from such a queue the fairest removal policy is firstinfirstout fifo the process that has been blocked the longest is released from the queue first a semaphore whose definition includes this policy is called a strong semaphore a semaphore that does not specify the order in which processes are removed from the queue is a weak semaphore figure based on one in denn is an example of the operation of a strong semaphore here processes a b and c depend on a result from process d initially a is running b c and d are ready and the semaphore count is indicating that one of ds results is available when a issues a semwait instruction on semaphore s the semaphore decrements to and a can continue to execute subsequently it rejoins the ready queue then b runs eventually issues a semwait instruction and is blocked allowing d to run when d completes a new result it issues a semsignal instruction which allows b to move to the ready queue d rejoins the ready queue and c begins to run but is blocked when it issues a semwait instruction similarly a and b run and are blocked on the semaphore allowing d to resume execution when d has a result it issues a semsignal which transfers c to the ready queue later cycles of d will release a and b from the blocked state in some of the literature and in some textbooks no distinction is made between a mutex and a binary semaphore however in practice a number of operating systems such as linux windows and solaris offer a mutex facility which conforms to the definition in this book processor a s c d b blocked queue semaphore ready queue processor b s a c d blocked queue semaphore ready queue processor d b s a c blocked queue semaphore ready queue processor d s b a c blocked queue semaphore ready queue processor c s d b a blocked queue semaphore ready queue processor d b a c s blocked queue semaphore ready queue processor d b a s c blocked queue semaphore ready queue figure example of semaphore mechanism program mutualexclusion const int n number of processes semaphore s void pint i while true semwaits critical section semsignals remainder void main parbegin p p pn figure mutual exclusion using semaphores for the mutual exclusion algorithm discussed in the next subsection and illustrated in figure strong semaphores guarantee freedom from starvation while weak semaphores do not we will assume strong semaphores because they are more convenient and because this is the form of semaphore typically provided by operating systems mutual exclusion figure shows a straightforward solution to the mutual exclusion problem using a semaphore s compare figure consider n processes identified in the array pi all of which need access to the same resource each process has a critical section used to access the resource in each process a semwaits is executed just before its critical section if the value of s becomes negative the process is blocked if the value is then it is decremented to and the process immediately enters its critical section because s is no longer positive no other process will be able to enter its critical section the semaphore is initialized to thus the first process that executes a semwait will be able to enter the critical section immediately setting the value of s to any other process attempting to enter the critical section will find it busy and will be blocked setting the value of s to any number of processes may attempt entry each such unsuccessful attempt results in a further decrement of the value of s when the process that initially entered its critical section departs s is incremented and one of the blocked processes if any is removed from the queue of blocked processes associated with the semaphore and put in a ready state when it is next scheduled by the os it may enter the critical section figure based on one in baco shows a possible sequence for three processes using the mutual exclusion discipline of figure in this example three processes a b c access a shared resource protected by the semaphore lock process a executes semwaitlock because the semaphore has a value of at the time of the semwait operation a can immediately enter its critical section and the semaphore takes on the value while a is in its critical section both b and c queue for value of semaphore lock semaphore lock a b c critical region semwaitlock normal execution blocked on semwaitlock semaphore b lock semwaitlock c b semsignallock c semsignallock semsignallock note that normal execution can proceed in parallel but that critical regions are serialized figure processes accessing shared data protected by a semaphore perform a semwait operation and are blocked pending the availability of the semaphore when a exits its critical section and performs semsignallock b which was the first process in the queue can now enter its critical section the program of figure can equally well handle a requirement that more than one process be allowed in its critical section at a time this requirement is met simply by initializing the semaphore to the specified value thus at any time the value of scount can be interpreted as follows scount scount is the number of processes that can execute semwaits without suspension if no semsignals is executed in the meantime such situations will allow semaphores to support synchronization as well as mutual exclusion scount the magnitude of scount is the number of processes suspended in squeue the producerconsumer problem we now examine one of the most common problems faced in concurrent processing the producerconsumer problem the general statement is this there are one or more producers generating some type of data records characters and placing these in a buffer there is a single consumer that is taking items out of the buffer one at a time the system is to be constrained to prevent the overlap of buffer operations that is only one agent producer or consumer may access the buffer at any one time the problem is to make sure that the producer wont try to add data into the buffer if its full and that the consumer wont try to remove data from an empty buffer we will look at a number of solutions to this problem to illustrate both the power and the pitfalls of semaphores to begin let us assume that the buffer is infinite and consists of a linear array of elements in abstract terms we can define the producer and consumer functions as follows producer consumer while true while true produce item v while in out bin v do nothing in w bout out consume item w figure illustrates the structure of buffer b the producer can generate items and store them in the buffer at its own pace each time an index in into the buffer is incremented the consumer proceeds in a similar fashion but must make sure that it does not attempt to read from an empty buffer hence the consumer makes sure that the producer has advanced beyond it in out before proceeding let us try to implement this system using binary semaphores figure is a first attempt rather than deal with the indices in and out we can simply keep track of the number of items in the buffer using the integer variable n in out the semaphore s is used to enforce mutual exclusion the semaphore delay is used to force the consumer to semwait if the buffer is empty this solution seems rather straightforward the producer is free to add to the buffer at any time it performs semwaitbs before appending and semsignalbs afterward to prevent the consumer or any other producer from b b b b b out in note shaded area indicates portion of buffer that is occupied figure infinite buffer for the producerconsumer problem program producerconsumer int n binarysemaphore s delay void producer while true produce semwaitbs append n if n semsignalbdelay semsignalbs void consumer semwaitbdelay while true semwaitbs take n semsignalbs consume if n semwaitbdelay void main n parbegin producer consumer figure an incorrect solution to the infinitebuffer producerconsumer problem using binary semaphores accessing the buffer during the append operation also while in the critical section the producer increments the value of n if n then the buffer was empty just prior to this append so the producer performs semsignalbdelay to alert the consumer of this fact the consumer begins by waiting for the first item to be produced using semwaitbdelay it then takes an item and decrements n in its critical section if the producer is able to stay ahead of the consumer a common situation then the consumer will rarely block on the semaphore delay because n will usually be positive hence both producer and consumer run smoothly there is however a flaw in this program when the consumer has exhausted the buffer it needs to reset the delay semaphore so that it will be forced to wait until the producer has placed more items in the buffer this is the purpose of the statement if n semwaitbdelay consider the scenario outlined in table in line the consumer fails to execute the semwaitb operation the consumer did indeed exhaust the buffer and set n to line but the producer has incremented n before the consumer can test it in line the result is a semsignalb not matched by a prior semwaitb the value of for n in line means that the consumer has consumed an item from the buffer that does not exist it would not do simply to move the conditional statement inside the critical section of the consumer because this could lead to deadlock eg after line of table table possible scenario for the program of figure producer consumer s n delay semwaitbs n if n semsignalbdelay semsignalbs semwaitbdelay semwaitbs n semsignalbs semwaitbs n if n semsignalbdelay semsignalbs if n semwaitbdelay semwaitbs n semsignalbs if n semwaitbdelay semwaitbs n semsignalbs note white areas represent the critical section controlled by semaphore s a fix for the problem is to introduce an auxiliary variable that can be set in the consumers critical section for use later on this is shown in figure a careful trace of the logic should convince you that deadlock can no longer occur a somewhat cleaner solution can be obtained if general semaphores also called counting semaphores are used as shown in figure the variable n is now a semaphore its value still is equal to the number of items in the buffer suppose now that in transcribing this program a mistake is made and the operations semsignals and semsignaln are interchanged this would require that the semsignaln operation be performed in the producers critical section without interruption by the consumer or another producer would this affect program producerconsumer int n binarysemaphore s delay void producer while true produce semwaitbs append n if n semsignalbdelay semsignalbs void consumer int m a local variable semwaitbdelay while true semwaitbs take n m n semsignalbs consume if m semwaitbdelay void main n parbegin producer consumer figure a correct solution to the infinitebuffer producerconsumer problem using binary semaphores the program no because the consumer must wait on both semaphores before proceeding in any case now suppose that the semwaitn and semwaits operations are accidentally reversed this produces a serious indeed a fatal flaw if the consumer ever enters its critical section when the buffer is empty ncount then no producer can ever append to the buffer and the system is deadlocked this is a good example of the subtlety of semaphores and the difficulty of producing correct designs finally let us add a new and realistic restriction to the producerconsumer problem namely that the buffer is finite the buffer is treated as a circular storage figure and pointer values must be expressed modulo the size of the buffer the following relationships hold block on unblock on producer insert in full buffer consumer item inserted consumer remove from empty buffer producer item removed program producerconsumer semaphore n s void producer while true produce semwaits append semsignals semsignaln void consumer while true semwaitn semwaits take semsignals consume void main parbegin producer consumer figure a solution to the infinitebuffer producerconsumer problem using semaphores the producer and consumer functions can be expressed as follows variable in and out are initialized to and n is the size of the buffer producer consumer while true while true produce item v while in out while in n out do nothing do nothing w bout bin v out out n in in n consume item w figure shows a solution using general semaphores the semaphore e has been added to keep track of the number of empty spaces another instructive example in the use of semaphores is the barbershop problem described in appendix a appendix a also includes additional examples of the problem of race conditions when using semaphores implementation of semaphores as was mentioned earlier it is imperative that the semwait and semsignal operations be implemented as atomic primitives one obvious way is to implement them b b b b b bn out in a b b b b b bn in out b figure finite circular buffer for the producerconsumer problem in hardware or firmware failing this a variety of schemes have been suggested the essence of the problem is one of mutual exclusion only one process at a time may manipulate a semaphore with either a semwait or semsignal operation thus any of the software schemes such as dekkers algorithm or petersons algorithm appendix a could be used this would entail a substantial processing overhead program boundedbuffer const int sizeofbuffer buffer size semaphore s n e sizeofbuffer void producer while true produce semwaite semwaits append semsignals semsignaln void consumer while true semwaitn semwaits take semsignals semsignale consume void main parbegin producer consumer figure a solution to the boundedbuffer producerconsumer problem using semaphores semwaits semwaits while compareandswapsflag inhibit interrupts do nothing scount scount if scount if scount place this process in squeue place this process in squeue block this process and allow inter block this process must also set rupts sflag to else sflag allow interrupts semsignals semsignals while compareandswapsflag inhibit interrupts do nothing scount scount if scount if scount remove a process p from squeue remove a process p from squeue place process p on ready list place process p on ready list allow interrupts sflag a compare and swap instruction b interrupts figure two possible implementations of semaphores another alternative is to use one of the hardwaresupported schemes for mutual exclusion for example figure a shows the use of a compareswap instruction in this implementation the semaphore is again a structure as in figure but now includes a new integer component sflag admittedly this involves a form of busy waiting however the semwait and semsignal operations are relatively short so the amount of busy waiting involved should be minor for a singleprocessor system it is possible to inhibit interrupts for the duration of a semwait or semsignal operation as suggested in figure b once again the relatively short duration of these operations means that this approach is reasonable monitors semaphores provide a primitive yet powerful and flexible tool for enforcing mutual exclusion and for coordinating processes however as figure suggests it may be difficult to produce a correct program using semaphores the difficulty is that semwait and semsignal operations may be scattered throughout a program and it is not easy to see the overall effect of these operations on the semaphores they affect the monitor is a programminglanguage construct that provides equivalent functionality to that of semaphores and that is easier to control the concept was first formally defined in hoar the monitor construct has been implemented in a number of programming languages including concurrent pascal pascalplus modula modula and java it has also been implemented as a program library this allows programmers to put a monitor lock on any object in particular for something like a linked list you may want to lock all linked lists with one lock or have one lock for each list or have one lock for each element of each list we begin with a look at hoares version and then examine a refinement monitor with signal a monitor is a software module consisting of one or more procedures an initialization sequence and local data the chief characteristics of a monitor are the following the local data variables are accessible only by the monitors procedures and not by any external procedure a process enters the monitor by invoking one of its procedures only one process may be executing in the monitor at a time any other processes that have invoked the monitor are blocked waiting for the monitor to become available the first two characteristics are reminiscent of those for objects in objectoriented software indeed an objectoriented os or programming language can readily implement a monitor as an object with special characteristics by enforcing the discipline of one process at a time the monitor is able to provide a mutual exclusion facility the data variables in the monitor can be accessed by only one process at a time thus a shared data structure can be protected by placing it in a monitor if the data in a monitor represent some resource then the monitor provides a mutual exclusion facility for accessing the resource to be useful for concurrent processing the monitor must include synchronization tools for example suppose a process invokes the monitor and while in the monitor must be blocked until some condition is satisfied a facility is needed by which the process is not only blocked but releases the monitor so that some other process may enter it later when the condition is satisfied and the monitor is again available the process needs to be resumed and allowed to reenter the monitor at the point of its suspension a monitor supports synchronization by the use of condition variables that are contained within the monitor and accessible only within the monitor condition variables are a special data type in monitors which are operated on by two functions cwaitc suspend execution of the calling process on condition c the monitor is now available for use by another process csignalc resume execution of some process blocked after a cwait on the same condition if there are several such processes choose one of them if there is no such process do nothing note that monitor wait and signal operations are different from those for the semaphore if a process in a monitor signals and no task is waiting on the condition variable the signal is lost figure illustrates the structure of a monitor although a process can enter the monitor by invoking any of its procedures we can think of the monitor as having a single entry point that is guarded so that only one process may be in the monitor at a time other processes that attempt to enter the monitor join a queue of queue of entering processes monitor waiting area entrance monitor condition c local data cwaitc condition variables procedure condition cn cwaitcn procedure k urgent queue csignal initialization code exit figure structure of a monitor processes blocked waiting for monitor availability once a process is in the monitor it may temporarily block itself on condition x by issuing cwaitx it is then placed in a queue of processes waiting to reenter the monitor when the condition changes and resume execution at the point in its program following the cwaitx call if a process that is executing in the monitor detects a change in the condition variable x it issues csignalx which alerts the corresponding condition queue that the condition has changed as an example of the use of a monitor let us return to the boundedbuffer producerconsumer problem figure shows a solution using a monitor the monitor module boundedbuffer controls the buffer used to store and retrieve characters the monitor includes two condition variables declared with the construct cond notfull is true when there is room to add at least one character to the buffer and notempty is true when there is at least one character in the buffer program producerconsumer monitor boundedbuffer char buffer n space for n items int nextin nextout buffer pointers int count number of items in buffer cond notfull notempty condition variables for synchronization void append char x if count n cwaitnotfull buffer is full avoid overflow buffernextin x nextin nextin n count one more item in buffer csignal nonempty resume any waiting consumer void take char x if count cwaitnotempty buffer is empty avoid underflow x buffernextout nextout nextout n count one fewer item in buffer csignal notfull resume any waiting producer monitor body nextin nextout count buffer initially empty void producer char x while true producex appendx void consumer char x while true takex consumex void main parbegin producer consumer figure a solution to the boundedbuffer producerconsumer problem using a monitor a producer can add characters to the buffer only by means of the procedure append inside the monitor the producer does not have direct access to buffer the procedure first checks the condition notfull to determine if there is space available in the buffer if not the process executing the monitor is blocked on that condition some other process producer or consumer may now enter the monitor later when the buffer is no longer full the blocked process may be removed from the queue reactivated and resume processing after placing a character in the buffer the process signals the notempty condition a similar description can be made of the consumer function this example points out the division of responsibility with monitors compared to semaphores in the case of monitors the monitor construct itself enforces mutual exclusion it is not possible for both a producer and a consumer simultaneously to access the buffer however the programmer must place the appropriate cwait and csignal primitives inside the monitor to prevent processes from depositing items in a full buffer or removing them from an empty one in the case of semaphores both mutual exclusion and synchronization are the responsibility of the programmer note that in figure a process exits the monitor immediately after executing the csignal function if the csignal does not occur at the end of the procedure then in hoares proposal the process issuing the signal is blocked to make the monitor available and placed in a queue until the monitor is free one possibility at this point would be to place the blocked process in the entrance queue so that it would have to compete for access with other processes that had not yet entered the monitor however because a process blocked on a csignal function has already partially performed its task in the monitor it makes sense to give this process precedence over newly entering processes by setting up a separate urgent queue figure one language that uses monitors concurrent pascal requires that csignal only appear as the last operation executed by a monitor procedure if there are no processes waiting on condition x then the execution of csignalx has no effect as with semaphores it is possible to make mistakes in the synchronization function of monitors for example if either of the csignal functions in the boundedbuffer monitor are omitted then processes entering the corresponding condition queue are permanently hung up the advantage that monitors have over semaphores is that all of the synchronization functions are confined to the monitor therefore it is easier to verify that the synchronization has been done correctly and to detect bugs furthermore once a monitor is correctly programmed access to the protected resource is correct for access from all processes in contrast with semaphores resource access is correct only if all of the processes that access the resource are programmed correctly alternate model of monitors with notify and broadcast hoares definition of monitors hoar requires that if there is at least one process in a condition queue a process from that queue runs immediately when another process issues a csignal for that condition thus the process issuing the csignal must either immediately exit the monitor or be blocked on the monitor there are two drawbacks to this approach if the process issuing the csignal has not finished with the monitor then two additional process switches are required one to block this process and another to resume it when the monitor becomes available process scheduling associated with a signal must be perfectly reliable when a csignal is issued a process from the corresponding condition queue must be activated immediately and the scheduler must ensure that no other process enters the monitor before activation otherwise the condition under which the process was activated could change for example in figure when a csignalnotempty is issued a process from the notempty queue must be activated before a new consumer enters the monitor another example a producer process may append a character to an empty buffer and then fail before signaling any processes in the notempty queue would be permanently hung up lampson and redell developed a different definition of monitors for the language mesa lamp their approach overcomes the problems just listed and supports several useful extensions the mesa monitor structure is also used in the modula systems programming language nels in mesa the csignal primitive is replaced by cnotify with the following interpretation when a process executing in a monitor executes cnotifyx it causes the x condition queue to be notified but the signaling process continues to execute the result of the notification is that the process at the head of the condition queue will be resumed at some convenient future time when the monitor is available however because there is no guarantee that some other process will not enter the monitor before the waiting process the waiting process must recheck the condition for example the procedures in the boundedbuffer monitor would now have the code of figure the if statements are replaced by while loops thus this arrangement results in at least one extra evaluation of the condition variable in return however there are no extra process switches and no constraints on when the waiting process must run after a cnotify one useful refinement that can be associated with the cnotify primitive is a watchdog timer associated with each condition primitive a process that has been waiting for the maximum timeout interval will be placed in a ready state regardless of whether the condition has been notified when activated the process checks the condition and continues if the condition is satisfied the timeout prevents the indefinite starvation of a process in the event that some other process fails before signaling a condition void append char x while count n cwaitnotfull buffer is full avoid overflow buffernextin x nextin nextin n count one more item in buffer cnotifynotempty notify any waiting consumer void take char x while count cwaitnotempty buffer is empty avoid underflow x buffernextout nextout nextout n count one fewer item in buffer cnotifynotfull notify any waiting producer figure boundedbuffer monitor code for mesa monitor with the rule that a process is notified rather than forcibly reactivated it is possible to add a cbroadcast primitive to the repertoire the broadcast causes all processes waiting on a condition to be placed in a ready state this is convenient in situations where a process does not know how many other processes should be reactivated for example in the producerconsumer program suppose that both the append and the take functions can apply to variable length blocks of characters in that case if a producer adds a block of characters to the buffer it need not know how many characters each waiting consumer is prepared to consume it simply issues a cbroadcast and all waiting processes are alerted to try again in addition a broadcast can be used when a process would have difficulty figuring out precisely which other process to reactivate a good example is a memory manager the manager has j bytes free a process frees up an additional k bytes but it does not know which waiting process can proceed with a total of k j bytes hence it uses broadcast and all processes check for themselves if there is enough memory free an advantage of lampsonredell monitors over hoare monitors is that the lampsonredell approach is less prone to error in the lampsonredell approach because each procedure checks the monitor variable after being signaled with the use of the while construct a process can signal or broadcast incorrectly without causing an error in the signaled program the signaled program will check the relevant variable and if the desired condition is not met continue to wait another advantage of the lampsonredell monitor is that it lends itself to a more modular approach to program construction for example consider the implementation of a buffer allocator there are two levels of conditions to be satisfied for cooperating sequential processes consistent data structures thus the monitor enforces mutual exclusion and completes an input or output operation before allowing another operation on the buffer level plus enough memory for this process to complete its allocation request in the hoare monitor each signal conveys the level condition but also carries the implicit message i have freed enough bytes for your particular allocate call to work now thus the signal implicitly carries the level condition if the programmer later changes the definition of the level condition it will be necessary to reprogram all signaling processes if the programmer changes the assumptions made by any particular waiting process ie waiting for a slightly different level invariant it may be necessary to reprogram all signaling processes this is unmodular and likely to cause synchronization errors eg wake up by mistake when the code is modified the programmer has to remember to modify all procedures in the monitor every time a small change is made to the level condition with a lampsonredell monitor a broadcast ensures the level condition and carries a hint that level might hold each process should check the level condition itself if a change is made in the level condition in either a waiter or a signaler there is no possibility of erroneous wakeup because each procedure checks its own level condition therefore the level condition can be hidden within each procedure with the hoare monitor the level condition must be carried from the waiter into the code of every signaling process which violates data abstraction and interprocedural modularity principles message passing when processes interact with one another two fundamental requirements must be satisfied synchronization and communication processes need to be synchronized to enforce mutual exclusion cooperating processes may need to exchange information one approach to providing both of these functions is message passing message passing has the further advantage that it lends itself to implementation in distributed systems as well as in sharedmemory multiprocessor and uniprocessor systems messagepassing systems come in many forms in this section we provide a general introduction that discusses features typically found in such systems the actual function of message passing is normally provided in the form of a pair of primitives send destination message receive source message this is the minimum set of operations needed for processes to engage in message passing a process sends information in the form of a message to another process designated by a destination a process receives information by executing the receive primitive indicating the source and the message a number of design issues relating to messagepassing systems are listed in table and examined in the remainder of this section table design characteristics of message systems for interprocess communication and synchronization synchronization format send content blocking length nonblocking fixed receive variable blocking nonblocking queueing discipline test for arrival fifo priority addressing direct send receive explicit implicit indirect static dynamic ownership synchronization the communication of a message between two processes implies some level of synchronization between the two the receiver can not receive a message until it has been sent by another process in addition we need to specify what happens to a process after it issues a send or receive primitive consider the send primitive first when a send primitive is executed in a process there are two possibilities either the sending process is blocked until the message is received or it is not similarly when a process issues a receive primitive there are two possibilities if a message has previously been sent the message is received and execution continues if there is no waiting message then either a the process is blocked until a message arrives or b the process continues to execute abandoning the attempt to receive thus both the sender and receiver can be blocking or nonblocking three combinations are common although any particular system will usually have only one or two combinations implemented blocking send blocking receive both the sender and receiver are blocked until the message is delivered this is sometimes referred to as a rendezvous this combination allows for tight synchronization between processes nonblocking send blocking receive although the sender may continue on the receiver is blocked until the requested message arrives this is probably the most useful combination it allows a process to send one or more messages to a variety of destinations as quickly as possible a process that must receive a message before it can do useful work needs to be blocked until such a message arrives an example is a server process that exists to provide a service or resource to other processes nonblocking send nonblocking receive neither party is required to wait the nonblocking send is more natural for many concurrent programming tasks for example if it is used to request an output operation such as printing it allows the requesting process to issue the request in the form of a message and then carry on one potential danger of the nonblocking send is that an error could lead to a situation in which a process repeatedly generates messages because there is no blocking to discipline the process these messages could consume system resources including processor time and buffer space to the detriment of other processes and the os also the nonblocking send places the burden on the programmer to determine that a message has been received processes must employ reply messages to acknowledge receipt of a message for the receive primitive the blocking version appears to be more natural for many concurrent programming tasks generally a process that requests a message will need the expected information before proceeding however if a message is lost which can happen in a distributed system or if a process fails before it sends an anticipated message a receiving process could be blocked indefinitely this problem can be solved by the use of the nonblocking receive however the danger of this approach is that if a message is sent after a process has already executed a matching receive the message will be lost other possible approaches are to allow a process to test whether a message is waiting before issuing a receive and allow a process to specify more than one source in a receive primitive the latter approach is useful if a process is waiting for messages from more than one source and can proceed if any of these messages arrive addressing clearly it is necessary to have a way of specifying in the send primitive which process is to receive the message similarly most implementations allow a receiving process to indicate the source of a message to be received the various schemes for specifying processes in send and receive primitives fall into two categories direct addressing and indirect addressing with direct addressing the send primitive includes a specific identifier of the destination process the receive primitive can be handled in one of two ways one possibility is to require that the process explicitly designate a sending process thus the process must know ahead of time from which process a message is expected this will often be effective for cooperating concurrent processes in other cases however it is impossible to specify the anticipated source process an example is a printerserver process which will accept a print request message from any other process for such applications a more effective approach is the use of implicit addressing in this case the source parameter of the receive primitive possesses a value returned when the receive operation has been performed the other general approach is indirect addressing in this case messages are not sent directly from sender to receiver but rather are sent to a shared data structure consisting of queues that can temporarily hold messages such queues are generally referred to as mailboxes thus for two processes to communicate one process sends a message to the appropriate mailbox and the other process picks up the message from the mailbox a strength of the use of indirect addressing is that by decoupling the sender and receiver it allows for greater flexibility in the use of messages the relationship between senders and receivers can be one to one many to one one to many or many to many figure a onetoone relationship allows a private communications link to be set up between two processes this insulates their interaction from erroneous interference from other processes a manytoone relationship is useful for clientserver interaction one process provides service to a number of other processes in this case the mailbox is often referred to as a port a onetomany relationship allows for one sender and multiple receivers it is useful for applications where a message or some information is to be broadcast to a set of processes a manytomany relationship allows multiple server processes to provide concurrent service to multiple clients the association of processes to mailboxes can be either static or dynamic ports are often statically associated with a particular process that is the port is created and assigned to the process permanently similarly a onetoone relationship is typically defined statically and permanently when there are many senders s s mailbox r port r sn a one to one b many to one r s r s mailbox mailbox rn sn rn c one to many d many to many figure indirect process communication the association of a sender to a mailbox may occur dynamically primitives such as connect and disconnect may be used for this purpose a related issue has to do with the ownership of a mailbox in the case of a port it is typically owned by and created by the receiving process thus when the process is destroyed the port is also destroyed for the general mailbox case the os may offer a createmailbox service such mailboxes can be viewed either as being owned by the creating process in which case they terminate with the process or as being owned by the os in which case an explicit command will be required to destroy the mailbox message format the format of the message depends on the objectives of the messaging facility and whether the facility runs on a single computer or on a distributed system for some operating systems designers have preferred short fixedlength messages to minimize processing and storage overhead if a large amount of data is to be passed the data can be placed in a file and the message then simply references that file a more flexible approach is to allow variablelength messages figure shows a typical message format for operating systems that support variablelength messages the message is divided into two parts a header which contains information about the message and a body which contains the actual contents of the message the header may contain an identification of the source and intended destination of the message a length field and a type field to discriminate among various types of messages there may also be additional control information message type destination id header source id message length control information body message contents figure general message format such as a pointer field so that a linked list of messages can be created a sequence number to keep track of the number and order of messages passed between source and destination and a priority field queueing discipline the simplest queueing discipline is firstinfirstout but this may not be sufficient if some messages are more urgent than others an alternative is to allow the specifying of message priority on the basis of message type or by designation by the sender another alternative is to allow the receiver to inspect the message queue and select which message to receive next mutual exclusion figure shows one way in which message passing can be used to enforce mutual exclusion compare figures and we assume the use of the blocking receive primitive and the nonblocking send primitive a set of concurrent processes share a mailbox box which can be used by all processes to send and receive program mutualexclusion const int n number of process void pint i message msg while true receive box msg critical section send box msg remainder void main create mailbox box send box null parbegin p p pn figure mutual exclusion using messages the mailbox is initialized to contain a single message with null content a process wishing to enter its critical section first attempts to receive a message if the mailbox is empty then the process is blocked once a process has acquired the message it performs its critical section and then places the message back into the mailbox thus the message functions as a token that is passed from process to process the preceding solution assumes that if more than one process performs the receive operation concurrently then if there is a message it is delivered to only one process and the others are blocked or if the message queue is empty all processes are blocked when a message is available only one blocked process is activated and given the message these assumptions are true of virtually all messagepassing facilities as an example of the use of message passing figure is a solution to the boundedbuffer producerconsumer problem using the basic mutualexclusion power of message passing the problem could have been solved with an algorithmic structure similar to that of figure instead the program of figure takes advantage of the ability of message passing to be used to pass data in addition to signals two mailboxes are used as the producer generates data it is sent as messages to the mailbox mayconsume as long as there is at least one message in that mailbox the consumer can consume hence mayconsume serves as the buffer the data in the buffer are organized as a queue of messages the size of the buffer is const int capacity buffering capacity null empty message int i void producer message pmsg while true receive mayproducepmsg pmsg produce send mayconsumepmsg void consumer message cmsg while true receive mayconsumecmsg consume cmsg send mayproducenull void main createmailbox mayproduce createmailbox mayconsume for int i i capacityi send mayproducenull parbegin producerconsumer figure a solution to the boundedbuffer producerconsumer problem using messages determined by the global variable capacity initially the mailbox mayproduce is filled with a number of null messages equal to the capacity of the buffer the number of messages in mayproduce shrinks with each production and grows with each consumption this approach is quite flexible there may be multiple producers and consumers as long as all have access to both mailboxes the system may even be distributed with all producer processes and the mayproduce mailbox at one site and all the consumer processes and the mayconsume mailbox at another readerswriters problem in dealing with the design of synchronization and concurrency mechanisms it is useful to be able to relate the problem at hand to known problems and to be able to test any solution in terms of its ability to solve these known problems in the literature several problems have assumed importance and appear frequently both because they are examples of common design problems and because of their educational value one such problem is the producerconsumer problem which has already been explored in this section we look at another classic problem the readerswriters problem the readerswriters problem is defined as follows there is a data area shared among a number of processes the data area could be a file a block of main memory or even a bank of processor registers there are a number of processes that only read the data area readers and a number that only write to the data area writers the conditions that must be satisfied are as follows any number of readers may simultaneously read the file only one writer at a time may write to the file if a writer is writing to the file no reader may read it thus readers are processes that are not required to exclude one another and writers are processes that are required to exclude all other processes readers and writers alike before proceeding let us distinguish this problem from two others the general mutual exclusion problem and the producerconsumer problem in the readerswriters problem readers do not also write to the data area nor do writers read the data area while writing a more general case which includes this case is to allow any of the processes to read or write the data area in that case we can declare any portion of a process that accesses the data area to be a critical section and impose the general mutual exclusion solution the reason for being concerned with the more restricted case is that more efficient solutions are possible for this case and that the less efficient solutions to the general problem are unacceptably slow for example suppose that the shared area is a library catalog ordinary users of the library read the catalog to locate a book one or more librarians are able to update the catalog in the general solution every access to the catalog would be treated as a critical section and users would be forced to read the catalog one at a time this would clearly impose intolerable delays at the same time it is important to prevent writers from interfering with each other and it is also required to prevent reading while writing is in progress to prevent the access of inconsistent information can the producerconsumer problem be considered simply a special case of the readerswriters problem with a single writer the producer and a single reader the consumer the answer is no the producer is not just a writer it must read queue pointers to determine where to write the next item and it must determine if the buffer is full similarly the consumer is not just a reader because it must adjust the queue pointers to show that it has removed a unit from the buffer we now examine two solutions to the problem readers have priority figure is a solution using semaphores showing one instance each of a reader and a writer the solution does not change for multiple readers and writers the writer process is simple the semaphore wsem is used to enforce mutual exclusion as long as one writer is accessing the shared data area no other writers and no readers may access it the reader process also makes use of wsem to enforce mutual exclusion however to allow multiple readers we require that when there are no readers reading the first reader that attempts to read should wait on wsem when program readersandwriters int readcount semaphore x wsem void reader while true semwait x readcount ifreadcount semwait wsem semsignal x readunit semwait x readcount ifreadcount semsignal wsem semsignal x void writer while true semwait wsem writeunit semsignal wsem void main readcount parbegin readerwriter figure a solution to the readerswriters problem using semaphore readers have priority there is already at least one reader reading subsequent readers need not wait before entering the global variable readcount is used to keep track of the number of readers and the semaphore x is used to assure that readcount is updated properly writers have priority in the previous solution readers have priority once a single reader has begun to access the data area it is possible for readers to retain control of the data area as long as there is at least one reader in the act of reading therefore writers are subject to starvation figure shows a solution that guarantees that no new readers are allowed access to the data area once at least one writer has declared a desire to write for program readersandwriters int readcountwritecount void reader while true semwait z semwait rsem semwait x readcount if readcount semwait wsem semsignal x semsignal rsem semsignal z readunit semwait x readcount if readcount semsignal wsem semsignal x void writer while true semwait y writecount if writecount semwait rsem semsignal y semwait wsem writeunit semsignal wsem semwait y writecount if writecount semsignal rsem semsignal y void main readcount writecount parbegin reader writer figure a solution to the readerswriters problem using semaphore writers have priority table state of the process queues for program of figure readers only in the system wsem set no queues writers only in the system wsem and rsem set writers queue on wsem both readers and writers with read first wsem set by reader rsem set by writer all writers queue on wsem one reader queues on rsem other readers queue on z both readers and writers with write first wsem set by writer rsem set by writer writers queue on wsem one reader queues on rsem other readers queue on z writers the following semaphores and variables are added to the ones already defined a semaphore rsem that inhibits all readers while there is at least one writer desiring access to the data area a variable writecount that controls the setting of rsem a semaphore y that controls the updating of writecount for readers one additional semaphore is needed a long queue must not be allowed to build up on rsem otherwise writers will not be able to jump the queue therefore only one reader is allowed to queue on rsem with any additional readers queueing on semaphore z immediately before waiting on rsem table summarizes the possibilities an alternative solution which gives writers priority and which is implemented using message passing is shown in figure in this case there is a controller process that has access to the shared data area other processes wishing to access the data area send a request message to the controller are granted access with an ok reply message and indicate completion of access with a finished message the controller is equipped with three mailboxes one for each type of message that it may receive the controller process services write request messages before read request messages to give writers priority in addition mutual exclusion must be enforced to do this the variable count is used which is initialized to some number greater than the maximum possible number of readers in this example we use a value of the action of the controller can be summarized as follows if count then no writer is waiting and there may or may not be readers active service all finished messages first to clear active readers then service write requests and then read requests if count then the only request outstanding is a write request allow the writer to proceed and wait for a finished message void readerint i void controller message rmsg while true while true rmsg i if count send readrequest rmsg if empty finished receive mboxi rmsg receive finished msg readunit count rmsg i send finished rmsg else if empty writerequest receive writerequest msg writerid msgid void writerint j count count message rmsg else if empty readrequest whiletrue receive readrequest msg rmsg j count send writerequest rmsg send msgid ok receive mboxj rmsg writeunit rmsg j if count send finished rmsg send writerid ok receive finished msg count while count receive finished msg count figure a solution to the readerswriters problem using message passing if count then a writer has made a request and is being made to wait to clear all active readers therefore only finished messages should be serviced summary the central themes of modern operating systems are multiprogramming multiprocessing and distributed processing fundamental to these themes and fundamental to the technology of os design is concurrency when multiple processes are executing concurrently either actually in the case of a multiprocessor system or virtually in the case of a singleprocessor multiprogramming system issues of conflict resolution and cooperation arise concurrent processes may interact in a number of ways processes that are unaware of each other may nevertheless compete for resources such as processor time or access to io devices processes may be indirectly aware of one another because they share access to a common object such as a block of main memory or a file finally processes may be directly aware of each other and cooperate by the exchange of information the key issues that arise in these interactions are mutual exclusion and deadlock mutual exclusion is a condition in which there is a set of concurrent processes only one of which is able to access a given resource or perform a given function at any time mutual exclusion techniques can be used to resolve conflicts such as competition for resources and to synchronize processes so that they can cooperate an example of the latter is the producerconsumer model in which one process is putting data into a buffer and one or more processes are extracting data from that buffer one approach to supporting mutual exclusion involves the use of specialpurpose machine instructions this approach reduces overhead but is still inefficient because it uses busy waiting another approach to supporting mutual exclusion is to provide features within the os two of the most common techniques are semaphores and message facilities semaphores are used for signaling among processes and can be readily used to enforce a mutualexclusion discipline messages are useful for the enforcement of mutual exclusion and also provide an effective means of interprocess communication recommended reading the misnamed little book of semaphores pages down provides numerous examples of the uses of semaphores available free online andr surveys many of the mechanisms described in this chapter ben provides a very clear and even entertaining discussion of concurrency mutual exclusion semaphores and other related topics a more formal treatment expanded to include distributed systems is contained in ben axfo is another readable and useful treatment it also contains a number of problems with workedout solutions rayn is a comprehensive and lucid collection of algorithms for mutual exclusion covering software eg dekker and hardware approaches as well as semaphores and messages hoar is a very readable classic that presents a formal approach to defining sequential processes and concurrency lamp is a lengthy formal treatment of mutual exclusion rudo is a useful aid in understanding concurrency baco is a wellorganized treatment of concurrency birr provides a good practical introduction to programming using concurrency buhr is an exhaustive survey of monitors kang is an instructive analysis of different scheduling policies for the readerswriters problem andr andrews g and schneider f concepts and notations for concurrent programming computing surveys march axfo axford t concurrent programming fundamental techniques for realtime and parallel software design new york wiley baco bacon j and harris t operating systems concurrent and distributed software design reading ma addisonwesley ben benari m principles of concurrent programming englewood cliffs nj prentice hall ben benari m principles of concurrent and distributed programming harlow england addisonwesley birr birrell a an introduction to programming with threads src research report compaq systems research center palo alto ca january available at httpwwwresearchcompaqcomsrc buhr buhr p and fortier m monitor classification acm computing surveys march down downey a the little book of semaphores wwwgreenteapresscom semaphores hoar hoare c communicating sequential processes englewood cliffs nj prenticehall kang kang s and lee j analysis and solution of nonpreemptive policies for scheduling readers and writers operating systems review july lamp lamport l the mutual exclusion problem journal of the acm april rayn raynal m algorithms for mutual exclusion cambridge ma mit press rudo rudolph b selfassessment procedure xxi concurrency communications of the acm may key terms review questions and problems key terms atomic critical resource nonblocking binary semaphore critical section race condition blocking deadlock semaphore busy waiting general semaphore spin waiting concurrency message passing starvation concurrent processes monitor strong semaphore coroutine mutual exclusion weak semaphore counting semaphore mutex review questions list four design issues for which the concept of concurrency is relevant what are three contexts in which concurrency arises what is the basic requirement for the execution of concurrent processes list three degrees of awareness between processes and briefly define each what is the distinction between competing processes and cooperating processes list the three control problems associated with competing processes and briefly define each list the requirements for mutual exclusion what operations can be performed on a semaphore what is the difference between binary and general semaphores what is the difference between strong and weak semaphores what is a monitor what is the distinction between blocking and nonblocking with respect to messages what conditions are generally associated with the readerswriters problem problems at the beginning of section it is stated that multiprogramming and multiprocessing present the same problems with respect to concurrency this is true as far as it goes however cite two differences in terms of concurrency between multiprogramming and multiprocessing processes and threads provide a powerful structuring tool for implementing programs that would be much more complex as simple sequential programs an earlier construct that is instructive to examine is the coroutine the purpose of this problem is to introduce coroutines and compare them to processes consider this simple problem from conw read column cards and print them on character lines with the following changes after every card image an extra blank is inserted and every adjacent pair of asterisks on a card is replaced by the character a develop a solution to this problem as an ordinary sequential program you will find that the program is tricky to write the interactions among the various elements of the program are uneven because of the conversion from a length of to furthermore the length of the card image after conversion will vary depending on the number of double asterisk occurrences one way to improve clarity and to minimize the potential for bugs is to write the application as three separate procedures the first procedure reads in card images pads each image with a blank and writes a stream of characters to a temporary file after all of the cards have been read the second procedure reads the temporary file does the character substitution and writes out a second temporary filethe third procedure reads the stream of characters from the second temporary file and prints lines of characters each b the sequential solution is unattractive because of the overhead of io and temporary files conway proposed a new form of program structure the coroutine that allows the application to be written as three programs connected by onecharacter buffers figure in a traditional procedure there is a masterslave relationship between the called and calling procedure the calling procedure may execute a call from any point in the procedure the called procedure is begun at its entry point and returns to the calling procedure at the point of call the coroutine exhibits a more symmetric relationship as each call is made execution takes up from the last active point in the called procedure because there is no sense in which a calling procedure is higher than the called there is no return rather any coroutine can pass control to any other coroutine with a resume command the first time a coroutine is invoked it is resumed at its entry point subsequently the coroutine is reactivated at the point of its own last resume command note that only one coroutine in a program can be in execution at one time and that the transition points are explicitly defined in the code so this is not an example of concurrent processing explain the operation of the program in figure c the program does not address the termination condition assume that the io routine readcard returns the value true if it has placed an character image in inbuf otherwise it returns false modify the program to include this contingency note that the last printed line may therefore contain less than characters d rewrite the solution as a set of three processes using semaphores char rs sp void squash char inbuf outbuf void read while true if rs while true sp rs readcard inbuf resume print for int i i i rs inbuf i else resume squash resume read if rs rs sp resume squash resume print else void print sp resume print while true sp rs for int j j j resume print outbuf j sp resume squash resume read output outbuf figure an application of coroutines consider the following program p p shared int x shared int x x x while while x x x x x x x x if x if x printfx is dx printfx is dx note that the scheduler in a uniprocessor system would implement pseudoparallel execution of these two concurrent processes by interleaving their instructions without restriction on the order of the interleaving a show a sequence ie trace the sequence of interleavings of statements such that the statement x is is printed b show a sequence such that the statement x is is printed you should remember that the incrementdecrements at the source language level are not done atomically that is the assembly language code ld rx load r from memory location x incr r increment r sto rx store the incremented value back in x implements the single c increment instruction x x consider the following program const int n int tally void total int count for count count n count tally void main tally parbegin total total write tally a determine the proper lower bound and upper bound on the final value of the shared variable tally output by this concurrent program assume processes can execute at any relative speed and that a value can only be incremented after it has been loaded into a register by a separate machine instruction b suppose that an arbitrary number of these processes are permitted to execute in parallel under the assumptions of part a what effect will this modification have on the range of final values of tally is busy waiting always less efficient in terms of using processor time than a blocking wait explain consider the following program boolean blocked int turn void p int id while true blockedid true while turn id while blockedid do nothing turn id critical section blockedid false remainder void main blocked false blocked false turn parbegin p p this software solution to the mutual exclusion problem for two processes is proposed in hyma find a counterexample that demonstrates that this solution is incorrect it is interesting to note that even the communications of the acm was fooled on this one a software approach to mutual exclusion is lamports bakery algorithm lamp so called because it is based on the practice in bakeries and other shops in which every customer receives a numbered ticket on arrival allowing each to be served in turn the algorithm is as follows boolean choosingn int numbern while true choosingi true numberi getmaxnumber n choosingi false for int j j n j while choosingj while numberj numberjj numberii critical section number i remainder the arrays choosing and number are initialized to false and respectively the ith element of each array may be read and written by process i but only read by other processes the notation a b c d is defined as a c or a c and b d a describe the algorithm in words b show that this algorithm avoids deadlock c show that it enforces mutual exclusion now consider a version of the bakery algorithm without the variable choosing then we have int numbern while true numberi getmaxnumber n for int j j n j while numberj numberjj numberii critical section number i remainder does this version violate mutual exclusion explain why or why not consider the following program which provides a software approach to mutual exclusion integer array control n integer k where k n and each element of control is either or all elements of control are initially zero the initial value of k is immaterial the program of the ith process i n is begin integer j l control i l li for jk step l until n l step l until k do begin if j i then goto l if control j then goto l end l control i for j step until n do if j i and control j then goto l l if control k and k i then goto l l k i critical section l for j k step until n step until k do if j k and control j then begin k j goto l end l control i l remainder of cycle goto l end this is referred to as the eisenbergmcguire algorithm explain its operation and its key features consider the first instance of the statement bolt in figure b a achieve the same result using the exchange instruction b which method is preferable when a special machine instruction is used to provide mutual exclusion in the fashion of figure there is no control over how long a process must wait before being granted access to its critical section devise an algorithm that uses the compareswap instruction but that guarantees that any process waiting to enter its critical section will do so within n turns where n is the number of processes that may require access to the critical section and a turn is an event consisting of one process leaving the critical section and another process being granted access consider the following definition of semaphores void semwaits if scount scount else place this process in squeue block void semsignal s if there is at least one process blocked on semaphore s remove a process p from squeue place process p on ready list else scount compare this set of definitions with that of figure note one difference with the preceding definition a semaphore can never take on a negative value is there any difference in the effect of the two sets of definitions when used in programs that is could you substitute one set for the other without altering the meaning of the program consider a sharable resource with the following characteristics as long as there are fewer than three processes using the resource new processes can start using it right away once there are three process using the resource all three must leave before any new processes can begin using it we realize that counters are needed to keep track of how many processes are waiting and active and that these counters are themselves shared resources that must be protected with mutual exclusion so we might create the following solution semaphore mutex block share variables semaphores int active waiting counters and boolean mustwait false state information semwaitmutex enter the mutual exclusion ifmustwait if there are or were then waiting we must wait but we must leave semsignalmutex the mutual exclusion first semwaitblock wait for all current users to depart semwaitmutex reenter the mutual exclusion waiting and update the waiting count active update active count and remember mustwait active if the count reached semsignalmutex leave the mutual exclusion critical section semwaitmutex enter mutual exclusion active and update the active count ifactive last one to leave int n if waiting n waiting else n if so unblock up to while n waiting processes semsignalblock n mustwait false all active processes have left semsignalmutex leave the mutual exclusion the solution appears to do everything right all accesses to the shared variables are protected by mutual exclusion processes do not block themselves while in the mutual exclusion new processes are prevented from using the resource if there are or were three active users and the last process to depart unblocks up to three waiting processes a the program is nevertheless incorrect explain why b suppose we change the if in line to a while does this solve any problem in the program do any difficulties remain now consider this correct solution to the preceding problem semaphore mutex block share variables semaphores int active waiting counters and boolean mustwait false state information semwaitmutex enter the mutual exclusion ifmustwait if there are or were then waiting we must wait but we must leave semsignalmutex the mutual exclusion first semwaitblock wait for all current users to depart else active update active count and mustwait active remember if the count reached semsignalmutex leave mutual exclusion critical section semwaitmutex enter mutual exclusion active and update the active count ifactive last one to leave int n if waiting n waiting else n if so see how many processes to unblock waiting n deduct this number from waiting count active n and set active to this number while n now unblock the processes semsignalblock one by one n mustwait active remember if the count is semsignalmutex leave the mutual exclusion a explain how this program works and why it is correct b this solution does not completely prevent newly arriving processes from cutting in line but it does make it less likely give an example of cutting in line c this program is an example of a general design pattern that is a uniform way to implement solutions to many concurrency problems using semaphores it has been referred to as the ill do it for you pattern describe the pattern now consider another correct solution to the preceding problem semaphore mutex block share variables semaphores int active waiting counters and boolean mustwait false state information semwaitmutex enter the mutual exclusion ifmustwait if there are or were then waiting we must wait but we must leave semsignalmutex the mutual exclusion first semwaitblock wait for all current users to depart waiting weve got the mutual exclusion update count active update active count and remember mustwait active if the count reached ifwaiting mustwait if there are others waiting semsignalblock and we dont yet have active unblock a waiting process else semsignalmutex otherwise open the mutual exclusion critical section semwaitmutex enter mutual exclusion active and update the active count ifactive if last one to leave mustwait false set up to let new processes enter ifwaiting mustwait if there are others waiting semsignalblock and we dont have active unblock a waiting process else semsignalmutex otherwise open the mutual exclusion a explain how this program works and why it is correct b does this solution differ from the preceding one in terms of the number of processes that can be unblocked at a time explain c this program is an example of a general design pattern that is a uniform way to implement solutions to many concurrency problems using semaphores it has been referred to as the pass the baton pattern describe the pattern it should be possible to implement general semaphores using binary semaphores we can use the operations semwaitb and semsignalb and two binary semaphores delay and mutex consider the following void semwaitsemaphore s semwaitbmutex s if s semsignalbmutex semwaitbdelay else semsignalbmutex void semsignalsemaphore s semwaitbmutex s if s semsignalbdelay semsignalbmutex initially s is set to the desired semaphore value each semwait operation decrements s and each semsignal operation increments s the binary semaphore mutex which is initialized to assures that there is mutual exclusion for the updating of s the binary semaphore delay which is initialized to is used to block processes there is a flaw in the preceding program demonstrate the flaw and propose a change that will fix it hint suppose two processes each call semwaits when s is initially and after the first has just performed semsignalbmutex but not performed semwaitbdelay the second call to semwaits proceeds to the same point all that you need to do is move a single line of the program in dijkstra put forward the conjecture that there was no solution to the mutual exclusion problem avoiding starvation applicable to an unknown but finite number of processes using a finite number of weak semaphores in j m morris refuted this conjecture by publishing an algorithm using three weak semaphores the behavior of the algorithm can be described as follows if one or several process are waiting in a semwaits operation and another process is executing semsignals the value of the semaphore s is not modified and one of the waiting processes is unblocked independently of semwaits apart from the three semaphores the algorithm uses two nonnegative integer variables as counters of the number of processes in certain sections of the algorithm thus semaphores a and b are initialized to while semaphore m and counters na and nm are initialized to the mutual exclusion semaphore b protects access to the shared variable na a process attempting to enter its critical section must cross two barriers represented by semaphores a and m counters na and nm respectively contain the number of processes ready to cross barrier a and those having already crossed barrier a but not yet barrier m in the second part of the protocol the nm processes blocked at m will enter their critical sections one by one using a cascade technique similar to that used in the first part define an algorithm that conforms to this description the following problem was once used on an exam jurassic park consists of a dinosaur museum and a park for safari riding there are m passengers and n singlepassenger cars passengers wander around the museum for a while then line up to take a ride in a safari car when a car is available it loads the one passenger it can hold and rides around the park for a random amount of time if the n cars are all out riding passengers around then a passenger who wants to ride waits if a car is ready to load but there are no waiting passengers then the car waits use semaphores to synchronize the m passenger processes and the n car processes the following skeleton code was found on a scrap of paper on the floor of the exam room grade it for correctness ignore syntax and missing variable declarations remember that p and v correspond to semwait and semsignal resource jurassicpark sem caravail cartaken carfilled passengerreleased process passengeri to numpassengers do true napintrandomwandertime pcaravail vcartaken pcarfilled ppassengerreleased od end passenger process carj to numcars do true vcaravail pcartaken vcarfilled napintrandomridetime vpassengerreleased od end car end jurassicpark in the commentary on figure and table it was stated that it would not do simply to move the conditional statement inside the critical section controlled by s of the consumer because this could lead to deadlock demonstrate this with a table similar to table consider the solution to the infinitebuffer producerconsumer problem defined in figure suppose we have the common case in which the producer and consumer are running at roughly the same speed the scenario could be producer append semsignal produce append semsignal produce consumer consume take semwait consume take semwait the producer always manages to append a new element to the buffer and signal during the consumption of the previous element by the consumer the producer is always appending to an empty buffer and the consumer is always taking the sole item in the bufferalthough the consumer never blocks on the semaphore a large number of calls to the semaphore mechanism is made creating considerable overhead construct a new program that will be more efficient under these circumstances hints allow n to have the value which is to mean that not only is the buffer empty but that the consumer has detected this fact and is going to block until the producer supplies fresh data the solution does not require the use of the local variable m found in figure consider figure would the meaning of the program change if the following were interchanged a semwaite semwaits b semsignals semsignaln c semwaitn semwaits d semsignals semsignale the following pseudocode is a correct implementation of the producerconsumer problem with a bounded buffer item buffer initially empty semaphore empty initialized to semaphore full initialized to binarysemaphore mutex initialized to void producer void consumer while true while true item produce c waitfull p waitempty waitmutex waitmutex c item take p appenditem signalmutex signalmutex c signalempty p signalfull consumeitem labels p p p and c c c refer to the lines of code shown above p and c each cover three lines of code semaphores empty and full are linear semaphores that can take unbounded negative and positive values there are multiple producer processes referred to as pa pb pc etc and multiple consumer processes referred to as ca cb cc etc each semaphore maintains a fifo firstinfirstout queue of blocked processes in the scheduling chart below each line represents the state of the buffer and semaphores after the scheduled execution has occurred to simplify we assume that scheduling is such that processes are never interrupted while executing a given portion of code p or p or c your task is to complete the following chart scheduled fulls state and emptys state step of execution queue buffer and queue initialization full ooo empty ca executes c full ca ooo empty cb executes c full ca cb ooo empty scheduled fulls state and emptys state step of execution queue buffer and queue pa executes p full ca cb ooo empty pa executes p full ca cb x oo empty pa executes p full cb ca x oo empty ca executes c full cb ooo empty ca executes c full cb ooo empty pb executes p full empty pa executes p full empty pa executes full empty pb executes full empty pb executes full empty pc executes p full empty cb executes full empty pc executes full empty cb executes full empty pa executes full empty pb executes pp full empty pc executes full empty pa executes p full empty pd executes p full empty ca executes cc full empty pa executes full empty cc executes cc full empty pa executes full empty cc executes c full empty pd executes pp full empty this problem demonstrates the use of semaphores to coordinate three types of processes santa claus sleeps in his shop at the north pole and can only be wakened by either all nine reindeer being back from their vacation in the south pacific or some of the elves having difficulties making toys to allow santa to get some sleep the elves can only wake him when three of them have problems when three elves are having their problems solved any other elves wishing to visit santa must wait for those elves to return if santa wakes up to find three elves waiting at his shops door along with the last reindeer having come back from the tropics santa has decided that the elves can wait until after christmas because it is more important to get his sleigh ready it is assumed that the reindeer do not want to leave the tropics and therefore they stay there until the last possible moment the last reindeer to arrive must get santa while the others wait in a warming hut before being harnessed to the sleigh solve this problem using semaphores show that message passing and semaphores have equivalent functionality by a implementing message passing using semaphores hint make use of a shared buffer area to hold mailboxes each one consisting of an array of message slots b implementing a semaphore using message passing hint introduce a separate synchronization process i am grateful to john trono of st michaels college in vermont for supplying this problem explain what is the problem with this implementation of the onewriter manyreaders problem int readcount shared and initialized to semaphore mutex wrt shared and initialized to writer readers semwaitmutex readcount readcount semwaitwrt if readcount then semwaitwrt writing performed semsignalmutex semsignalwrt reading performed semwaitmutex readcount readcount if readcount then upwrt semsignalmutex concurrency deadlock and starvation principles of deadlock reusable resources consumable resources resource allocation graphs the conditions for deadlock deadlock prevention mutual exclusion hold and wait no preemption circular wait deadlock avoidance process initiation denial resource allocation denial deadlock detection deadlock detection algorithm recovery an integrated deadlock strategy dining philosophers problem solution using semaphores solution using a monitor unix concurrency mechanisms linux kernel concurrency mechanisms solaris thread synchronization primitives windows concurrency mechanisms summary recommended reading key terms review questions and problems when two trains approach each other at a crossing both shall come to a full stop and neither shall start up again until the other has gone statute passed by the kansas state legislature early in the th century a treasury of railroad folklore b a botkin and alvin f harlow learning objectives after studying this chapter you should be able to list and explain the conditions for deadlock define deadlock prevention and describe deadlock prevention strategies related to each of the conditions for deadlock explain the difference between deadlock prevention and deadlock avoidance understand two approaches to deadlock avoidance explain the fundamental difference in approach between deadlock detection and deadlock prevention or avoidance understand how an integrated deadlock strategy can be designed analyze the dining philosophers problem explain the concurrency and synchronization methods used in unix linux solaris and windows this chapter examines two problems that plague all efforts to support concurrent processing deadlock and starvation we begin with a discussion of the underlying principles of deadlock and the related problem of starvation then we examine the three common approaches to dealing with deadlock prevention detection and avoidance we then look at one of the classic problems used to illustrate both synchronization and deadlock issues the dining philosophers problem as with chapter the discussion in this chapter is limited to a consideration of concurrency and deadlock on a single system measures to deal with distributed deadlock problems are assessed in chapter an animation illustrating deadlock is available online click on the rotating globe at williamstallingscomosose html for access principles of deadlock deadlock can be defined as the permanent blocking of a set of processes that either compete for system resources or communicate with each other a set of processes is deadlocked when each process in the set is blocked awaiting an event typically the freeing up of some requested resource that can only be triggered by another blocked process in the set deadlock is permanent because none of the events is ever triggered unlike other problems in concurrent process management there is no efficient solution in the general case c b d a a deadlock possible b deadlock figure illustration of deadlock all deadlocks involve conflicting needs for resources by two or more processes a common example is the traffic deadlock figure a shows a situation in which four cars have arrived at a fourway stop intersection at approximately the same time the four quadrants of the intersection are the resources over which control is needed in particular if all four cars wish to go straight through the intersection the resource requirements are as follows car traveling north needs quadrants a and b car needs quadrants b and c car needs quadrants c and d car needs quadrants d and a the rule of the road in the united states is that a car at a fourway stop should defer to a car immediately to its right this rule works if there are only two or three cars at the intersection for example if only the northbound and westbound cars arrive at the intersection the northbound car will wait and the westbound car proceeds however if all four cars arrive at about the same time and all four follow the rule each will refrain from entering the intersection this causes a potential deadlock it is only a potential deadlock because the necessary resources are available for any of the cars to proceed if one car eventually chooses to proceed it can do so however if all four cars ignore the rules and proceed cautiously into the intersection at the same time then each car seizes one resource one quadrant but can not proceed because the required second resource has already been seized by another car this is an actual deadlock let us now look at a depiction of deadlock involving processes and computer resources figure based on one in baco which we refer to as a joint progress diagram illustrates the progress of two processes competing for two progress of q release a p and q a want a required release b get a b deadlock p and q required inevitable want b get b progress get a get b release a release b of p both p and q want resource a a both p and q want resource b required b required deadlockinevitable region possible progress path of p and q horizontal portion of path indicates p is executing and q is waiting vertical portion of path indicates q is executing and p is waiting figure example of deadlock resources each process needs exclusive use of both resources for a certain period of time two processes p and q have the following general form process p process q get a get b get b get a release a release b release b release a in figure the xaxis represents progress in the execution of p and the yaxis represents progress in the execution of q the joint progress of the two processes is therefore represented by a path that progresses from the origin in a northeasterly direction for a uniprocessor system only one process at a time may execute and the path consists of alternating horizontal and vertical segments with a horizontal segment representing a period when p executes and q waits and a vertical segment representing a period when q executes and p waits the figure indicates areas in which both p and q require resource a upward slanted lines both p and q require resource b downward slanted lines and both p and q require both resources because we assume that each process requires exclusive control of any resource these are all forbidden regions that is it is impossible for any path representing the joint execution progress of p and q to enter these regions the figure shows six different execution paths these can be summarized as follows q acquires b and then a and then releases b and a when p resumes execution it will be able to acquire both resources q acquires b and then a p executes and blocks on a request for a q releases b and a when p resumes execution it will be able to acquire both resources q acquires b and then p acquires a deadlock is inevitable because as execution proceeds q will block on a and p will block on b p acquires a and then q acquires b deadlock is inevitable because as execution proceeds q will block on a and p will block on b p acquires a and then b q executes and blocks on a request for b p releases a and b when q resumes execution it will be able to acquire both resources p acquires a and then b and then releases a and b when q resumes execution it will be able to acquire both resources the grayshaded area of figure which can be referred to as a fatal region applies to the commentary on paths and if an execution path enters this fatal region then deadlock is inevitable note that the existence of a fatal region depends on the logic of the two processes however deadlock is only inevitable if the joint progress of the two processes creates a path that enters the fatal region whether or not deadlock occurs depends on both the dynamics of the execution and on the details of the application for example suppose that p does not need both resources at the same time so that the two processes have the following form process p process q get a get b release a get a get b release b release b release a this situation is reflected in figure some thought should convince you that regardless of the relative timing of the two processes deadlock can not occur as shown the joint progress diagram can be used to record the execution history of two processes that share resources in cases where more than two processes progress of q release a a release p and q required b want a get a p and q want b b required get b progress get a release a get b release b of p a required b required both p and q want resource a both p and q want resource b possible progress path of p and q horizontal portion of path indicates p is executing and q is waiting vertical portion of path indicates q is executing and p is waiting figure example of no deadlock baco may compete for the same resource a higherdimensional diagram would be required the principles concerning fatal regions and deadlock would remain the same reusable resources two general categories of resources can be distinguished reusable and consumable a reusable resource is one that can be safely used by only one process at a time and is not depleted by that use processes obtain resource units that they later release for reuse by other processes examples of reusable resources include processors io channels main and secondary memory devices and data structures such as files databases and semaphores as an example of deadlock involving reusable resources consider two processes that compete for exclusive access to a disk file d and a tape drive t the programs engage in the operations depicted in figure deadlock occurs if each process holds one resource and requests the other for example deadlock occurs if the multiprogramming system interleaves the execution of the two processes as follows p p q q p q step process p action step process q action p request d q request t p lock d q lock t p request t q request d p lock t q lock d p perform function q perform function p unlock d q unlock t p unlock t q unlock d figure example of two processes competing for reusable resources it may appear that this is a programming error rather than a problem for the os designer however we have seen that concurrent program design is challenging such deadlocks do occur and the cause is often embedded in complex program logic making detection difficult one strategy for dealing with such a deadlock is to impose system design constraints concerning the order in which resources can be requested another example of deadlock with a reusable resource has to do with requests for main memory suppose the space available for allocation is kbytes and the following sequence of requests occurs p p request kbytes request kbytes request kbytes request kbytes deadlock occurs if both processes progress to their second request if the amount of memory to be requested is not known ahead of time it is difficult to deal with this type of deadlock by means of system design constraints the best way to deal with this particular problem is in effect to eliminate the possibility by using virtual memory which is discussed in chapter consumable resources a consumable resource is one that can be created produced and destroyed consumed typically there is no limit on the number of consumable resources of a particular type an unblocked producing process may create any number of such resources when a resource is acquired by a consuming process the resource ceases to exist examples of consumable resources are interrupts signals messages and information in io buffers as an example of deadlock involving consumable resources consider the following pair of processes in which each process attempts to receive a message from the other process and then send a message to the other process p p receive p receive p send p m send p m deadlock occurs if the receive is blocking ie the receiving process is blocked until the message is received once again a design error is the cause of the deadlock such errors may be quite subtle and difficult to detect furthermore it may take a rare combination of events to cause the deadlock thus a program table summary of deadlock detection prevention and avoidance approaches for operating systems islo resource allocation different approach policy schemes major advantages major disadvantages requesting all works well for processes inefficient resources at that perform a single delays process initiation once burst of activity future resource require no preemption necessary ments must be known by processes prevention conservative preemption convenient when preempts more often undercommits applied to resources than necessary resources whose state can be saved and restored easily resource feasible to enforce via disallows incremental ordering compiletime checks resource requests needs no runtime computation since problem is solved in system design avoidance midway manipulate to no preemption future resource requirebetween that find at least necessary ments must be known of detection one safe path by os and prevention processes can be blocked for long periods detection very liberal invoke peri never delays process inherent preemption requested odically to initiation losses resources are test for facilitates online granted where deadlock handling possible could be in use for a considerable period of time even years before the deadlock actually occurs there is no single effective strategy that can deal with all types of deadlock table summarizes the key elements of the most important approaches that have been developed prevention avoidance and detection we examine each of these in turn after first introducing resource allocation graphs and then discussing the conditions for deadlock resource allocation graphs a useful tool in characterizing the allocation of resources to processes is the resource allocation graph introduced by holt holt the resource allocation graph is a directed graph that depicts a state of the system of resources and processes with each process and each resource represented by a node a graph edge directed from a process to a resource indicates a resource that has been requested by the process but not yet granted figure a within a resource node a dot is shown for each instance of that resource examples of resource types that may have multiple instances are io devices that are allocated by a resource management module in the os a graph edge directed from a reusable resource node dot to a process indicates a request that has been granted figure b that is the process p requests ra p held by ra a resource is requested b resource is held ra ra requests held by requests held by p p p p held by requests held by requests rb rb c circular wait d no deadlock figure examples of resource allocation graphs p p p p ra rb rc rd figure resource allocation graph for figure b has been assigned one unit of that resource a graph edge directed from a consumable resource node dot to a process indicates that the process is the producer of that resource figure c shows an example deadlock there is only one unit each of resources ra and rb process p holds rb and requests ra while p holds ra but requests rb figure d has the same topology as figure c but there is no deadlock because multiple units of each resource are available the resource allocation graph of figure corresponds to the deadlock situation in figure b note that in this case we do not have a simple situation in which two processes each have one resource the other needs rather in this case there is a circular chain of processes and resources that results in deadlock the conditions for deadlock three conditions of policy must be present for a deadlock to be possible mutual exclusion only one process may use a resource at a time no process may access a resource unit that has been allocated to another process hold and wait a process may hold allocated resources while awaiting assignment of other resources no preemption no resource can be forcibly removed from a process holding it in many ways these conditions are quite desirable for example mutual exclusion is needed to ensure consistency of results and the integrity of a database similarly preemption should not be done arbitrarily for example when data resources are involved preemption must be supported by a rollback recovery mechanism which restores a process and its resources to a suitable previous state from which the process can eventually repeat its actions the first three conditions are necessary but not sufficient for a deadlock to exist for deadlock to actually take place a fourth condition is required circular wait a closed chain of processes exists such that each process holds at least one resource needed by the next process in the chain eg figure c and figure the fourth condition is actually a potential consequence of the first three that is given that the first three conditions exist a sequence of events may occur that lead to an unresolvable circular wait the unresolvable circular wait is in fact the definition of deadlock the circular wait listed as condition is unresolvable because the first three conditions hold thus the four conditions taken together constitute necessary and sufficient conditions for deadlock to clarify this discussion it is useful to return to the concept of the joint progress diagram such as the one shown in figure recall that we defined a fatal region as one such that once the processes have progressed into that region those processes will deadlock a fatal region exists only if all of the first three conditions listed above are met if one or more of these conditions are not met there is no fatal region and deadlock can not occur thus these are necessary conditions for deadlock for deadlock to occur there must not only be a fatal region but also a sequence of resource requests that has led into the fatal region if a circular wait condition occurs then in fact the fatal region has been entered thus all four conditions listed above are sufficient for deadlock to summarize possibility of deadlock existence of deadlock mutual exclusion mutual exclusion no preemption no preemption hold and wait hold and wait circular wait three general approaches exist for dealing with deadlock first one can prevent deadlock by adopting a policy that eliminates one of the conditions conditions through second one can avoid deadlock by making the appropriate dynamic choices based on the current state of resource allocation third one can attempt to detect the presence of deadlock conditions through hold and take action to recover we discuss each of these approaches in turn computer system overview basic elements evolution of the microprocessor instruction execution interrupts interrupts and the instruction cycle interrupt processing multiple interrupts the memory hierarchy cache memory motivation cache principles cache design direct memory access multiprocessor and multicore organization symmetric multiprocessors multicore computers recommended reading and web sites key terms review questions and problems appendix a performance characteristics of twolevel memories locality operation of twolevel memory performance no artifact designed by man is so convenient for this kind of functional description as a digital computer almost the only ones of its properties that are detectable in its behavior are the organizational properties almost no interesting statement that one can make about on operating computer bears any particular relation to the specific nature of the hardwarea computer is an organization of elementary functional components in which to a high approximation only the function performed by those components is relevant to the behavior of the whole system the sciences of the artificial herbert simon learning objectives after studying this chapter you should be able to describe the basic elements of a computer system and their interrelationship explain the steps taken by a processor to execute an instruction understand the concept of interrupts and how and why a processor uses interrupts list and describe the levels of a typical computer memory hierarchy explain the basic characteristics of multiprocessor and multicore organizations discuss the concept of locality and analyze the performance of a multilevel memory hierarchy understand the operation of a stack and its use to support procedure call and return an operating system os exploits the hardware resources of one or more processors to provide a set of services to system users the os also manages secondary memory and io inputoutput devices on behalf of its users accordingly it is important to have some understanding of the underlying computer system hardware before we begin our examination of operating systems this chapter provides an overview of computer system hardware in most areas the survey is brief as it is assumed that the reader is familiar with this subject however several areas are covered in some detail because of their importance to topics covered later in the book further topics are covered in appendix c deadlock prevention the strategy of deadlock prevention is simply put to design a system in such a way that the possibility of deadlock is excluded we can view deadlock prevention methods as falling into two classes an indirect method of deadlock prevention is to prevent the occurrence of one of the three necessary conditions listed previously items through a direct method of deadlock prevention is to prevent the occurrence of a circular wait item we now examine techniques related to each of the four conditions virtually all textbooks simply list these four conditions as the conditions needed for deadlock but such a presentation obscures some of the subtler issues item the circular wait condition is fundamentally different from the other three conditions items through are policy decisions while item is a circumstance that might occur depending on the sequencing of requests and releases by the involved processes linking circular wait with the three necessary conditions leads to inadequate distinction between prevention and avoidance see shub and shub for a discussion mutual exclusion in general the first of the four listed conditions can not be disallowed if access to a resource requires mutual exclusion then mutual exclusion must be supported by the os some resources such as files may allow multiple accesses for reads but only exclusive access for writes even in this case deadlock can occur if more than one process requires write permission hold and wait the holdandwait condition can be prevented by requiring that a process request all of its required resources at one time and blocking the process until all requests can be granted simultaneously this approach is inefficient in two ways first a process may be held up for a long time waiting for all of its resource requests to be filled when in fact it could have proceeded with only some of the resources second resources allocated to a process may remain unused for a considerable period during which time they are denied to other processes another problem is that a process may not know in advance all of the resources that it will require there is also the practical problem created by the use of modular programming or a multithreaded structure for an application an application would need to be aware of all resources that will be requested at all levels or in all modules to make the simultaneous request no preemption this condition can be prevented in several ways first if a process holding certain resources is denied a further request that process must release its original resources and if necessary request them again together with the additional resource alternatively if a process requests a resource that is currently held by another process the os may preempt the second process and require it to release its resources this latter scheme would prevent deadlock only if no two processes possessed the same priority this approach is practical only when applied to resources whose state can be easily saved and restored later as is the case with a processor circular wait the circularwait condition can be prevented by defining a linear ordering of resource types if a process has been allocated resources of type r then it may subsequently request only those resources of types following r in the ordering to see that this strategy works let us associate an index with each resource type then resource ri precedes rj in the ordering if i j now suppose that two processes a and b are deadlocked because a has acquired ri and requested rj and b has acquired rj and requested ri this condition is impossible because it implies i j and j i as with holdandwait prevention circularwait prevention may be inefficient slowing down processes and denying resource access unnecessarily deadlock avoidance an approach to solving the deadlock problem that differs subtly from deadlock prevention is deadlock avoidance in deadlock prevention we constrain resource requests to prevent at least one of the four conditions of deadlock this is either done indirectly by preventing one of the three necessary policy conditions mutual exclusion hold and wait no preemption or directly by preventing circular wait this leads to inefficient use of resources and inefficient execution of processes deadlock avoidance on the other hand allows the three necessary conditions but makes judicious choices to assure that the deadlock point is never reached as such avoidance allows more concurrency than prevention with deadlock avoidance a decision is made dynamically whether the current resource allocation request will if granted potentially lead to a deadlock deadlock avoidance thus requires knowledge of future process resource requests in this section we describe two approaches to deadlock avoidance do not start a process if its demands might lead to deadlock do not grant an incremental resource request to a process if this allocation might lead to deadlock process initiation denial consider a system of n processes and m different types of resources let us define the following vectors and matrices resource r rr c rm total amount of each resource in the system available v vv c vm total amount of each resource not allocated to any process c c c c m claim c c c c c m cij requirement of process i for resource j f f f f c n c n c c nm a a c am allocation a a a c am f f f f aij current allocation to process i of resource j an an c anm the matrix claim gives the maximum requirement of each process for each resource with one row dedicated to each process this information must be the term avoidance is a bit confusing in fact one could consider the strategies discussed in this section to be examples of deadlock prevention because they indeed prevent the occurrence of a deadlock declared in advance by a process for deadlock avoidance to work similarly the matrix allocation gives the current allocation to each process the following relationships hold n rj vj a aij for all j all resources are either available or allocated i cij rj for all ij no process can claim more than the total amount of resources in the system aij c ij for all ij no process is allocated more resources of any type than the process originally claimed to need with these quantities defined we can define a deadlock avoidance policy that refuses to start a new process if its resource requirements might lead to deadlock start a new process pn only if n rj c n j a cij for all j i that is a process is only started if the maximum claim of all current processes plus those of the new process can be met this strategy is hardly optimal because it assumes the worst that all processes will make their maximum claims together resource allocation denial the strategy of resource allocation denial referred to as the bankers algorithm was first proposed in dijk let us begin by defining the concepts of state and safe state consider a system with a fixed number of processes and a fixed number of resources at any time a process may have zero or more resources allocated to it the state of the system reflects the current allocation of resources to processes thus the state consists of the two vectors resource and available and the two matrices claim and allocation defined earlier a safe state is one in which there is at least one sequence of resource allocations to processes that does not result in a deadlock ie all of the processes can be run to completion an unsafe state is of course a state that is not safe the following example illustrates these concepts figure a shows the state of a system consisting of four processes and three resources the total amount of resources r r and r are and units respectively in the current state allocations have been made to the four processes leaving unit of r dijkstra used this name because of the analogy of this problem to one in banking with customers who wish to borrow money corresponding to processes and the money to be borrowed corresponding to resources stated as a banking problem the bank has a limited reserve of money to lend and a list of customers each with a line of credit a customer may choose to borrow against the line of credit a portion at a time and there is no guarantee that the customer will make any repayment until after having taken out the maximum amount of loan the banker can refuse a loan to a customer if there is a risk that the bank will have insufficient funds to make further loans that will permit the customers to repay eventually r r r r r r r r r p p p p p p p p p p p p claim matrix c allocation matrix a ca r r r r r r resource vector r available vector v a initial state r r r r r r r r r p p p p p p p p p p p p claim matrix c allocation matrix a ca r r r r r r resource vector r available vector v b p runs to completion r r r r r r r r r p p p p p p p p p p p p claim matrix c allocation matrix a ca r r r r r r resource vector r available vector v c p runs to completion r r r r r r r r r p p p p p p p p p p p p claim matrix c allocation matrix a ca r r r r r r resource vector r available vector v d p runs to completion figure determination of a safe state and unit of r available is this a safe state to answer this question we ask an intermediate question can any of the four processes be run to completion with the resources available that is can the difference between the maximum requirement and current allocation for any process be met with the available resources in terms of the matrices and vectors introduced earlier the condition to be met for process i is c ij aij vj for all j clearly this is not possible for p which has only unit of r and requires more units of r units of r and units of r however by assigning one unit of r to process p p has its maximum required resources allocated and can run to completion let us assume that this is accomplished when p completes its resources can be returned to the pool of available resources the resulting state is shown in figure b now we can ask again if any of the remaining processes can be completed in this case each of the remaining processes could be completed suppose we choose p allocate the required resources complete p and return all of ps resources to the available pool we are left in the state shown in figure c next we can complete p resulting in the state of figure d finally we can complete p at this point all of the processes have been run to completion thus the state defined by figure a is a safe state these concepts suggest the following deadlock avoidance strategy which ensures that the system of processes and resources is always in a safe state when a process makes a request for a set of resources assume that the request is granted update the system state accordingly and then determine if the result is a safe state if so grant the request and if not block the process until it is safe to grant the request consider the state defined in figure a suppose p makes a request for one additional unit of r and one additional unit of r if we assume the request is granted then the resulting state is that of figure a we have already seen that this is a safe state therefore it is safe to grant the request now let us return to the state of figure a and suppose that p makes the request for one additional unit each of r and r if we assume that the request is granted we are left in the state of figure b is this a safe state the answer is no because each process will need at least one additional unit of r and there are none available thus on the basis of deadlock avoidance the request by p should be denied and p should be blocked it is important to point out that figure b is not a deadlocked state it merely has the potential for deadlock it is possible for example that if p were run from this state it would subsequently release one unit of r and one unit of r prior to needing these resources again if that happened the system would return to a safe state thus the deadlock avoidance strategy does not predict deadlock with certainty it merely anticipates the possibility of deadlock and assures that there is never such a possibility r r r r r r r r r p p p p p p p p p p p p claim matrix c allocation matrix a ca r r r r r r resource vector r available vector v a initial state r r r r r r r r r p p p p p p p p p p p p claim matrix c allocation matrix a ca r r r r r r resource vector r available vector v b p requests one unit each of r and r figure determination of an unsafe state figure gives an abstract version of the deadlock avoidance logic the main algorithm is shown in part b with the state of the system defined by the data structure state request is a vector defining the resources requested by process i first a check is made to assure that the request does not exceed the original claim of the process if the request is valid the next step is to determine if it is possible to fulfill the request ie there are sufficient resources available if it is not possible then the process is suspended if it is possible the final step is to determine if it is safe to fulfill the request to do this the resources are tentatively assigned to process i to form newstate then a test for safety is made using the algorithm in figure c deadlock avoidance has the advantage that it is not necessary to preempt and rollback processes as in deadlock detection and is less restrictive than deadlock prevention however it does have a number of restrictions on its use the maximum resource requirement for each process must be stated in advance the processes under consideration must be independent that is the order in which they execute must be unconstrained by any synchronization requirements there must be a fixed number of resources to allocate no process may exit while holding resources struct state int resourcem int availablem int claimnm int allocnm a global data structures if alloc i request claim i error total request claim else if request available suspend process else simulate alloc define newstate by alloc i alloc i request available available request if safe newstate carry out allocation else restore original state suspend process b resource alloc algorithm boolean safe state s int currentavailm process restnumber of processes currentavail available rest all processes possible true while possible find a process pk in rest such that claim k alloc k currentavail if found simulate execution of pk currentavail currentavail alloc k rest rest pk else possible false return rest null c test for safety algorithm bankers algorithm figure deadlock avoidance logic deadlock detection deadlock prevention strategies are very conservative they solve the problem of deadlock by limiting access to resources and by imposing restrictions on processes at the opposite extreme deadlock detection strategies do not limit resource access or restrict process actions with deadlock detection requested resources are granted to processes whenever possible periodically the os performs an algorithm that allows it to detect the circular wait condition described earlier in condition and illustrated in figure deadlock detection algorithm a check for deadlock can be made as frequently as each resource request or less frequently depending on how likely it is for a deadlock to occur checking at each resource request has two advantages it leads to early detection and the algorithm is relatively simple because it is based on incremental changes to the state of the system on the other hand such frequent checks consume considerable processor time a common algorithm for deadlock detection is one described in coff the allocation matrix and available vector described in the previous section are used in addition a request matrix q is defined such that qij represents the amount of resources of type j requested by process i the algorithm proceeds by marking processes that are not deadlocked initially all processes are unmarked then the following steps are performed mark each process that has a row in the allocation matrix of all zeros initialize a temporary vector w to equal the available vector find an index i such that process i is currently unmarked and the ith row of q is less than or equal to w that is qik wk for k m if no such row is found terminate the algorithm if such a row is found mark process i and add the corresponding row of the allocation matrix to w that is set wk wk aik for k m return to step a deadlock exists if and only if there are unmarked processes at the end of the algorithm each unmarked process is deadlocked the strategy in this algorithm is to find a process whose resource requests can be satisfied with the available resources and then assume that those resources are granted and that the process runs to completion and releases all of its resources the algorithm then looks for another process to satisfy note that this algorithm does not guarantee to prevent deadlock that will depend on the order in which future requests are granted all that it does is determine if deadlock currently exists we can use figure to illustrate the deadlock detection algorithm the algorithm proceeds as follows mark p because p has no allocated resources set w r r r r r r r r r r r r r r r p p p p resource vector p p p p request matrix q allocation matrix a r r r r r available vector figure example for deadlock detection the request of process p is less than or equal to w so mark p and set w w no other unmarked process has a row in q that is less than or equal to w therefore terminate the algorithm the algorithm concludes with p and p unmarked indicating that these processes are deadlocked recovery once deadlock has been detected some strategy is needed for recovery the following are possible approaches listed in order of increasing sophistication abort all deadlocked processes this is believe it or not one of the most common if not the most common solution adopted in operating systems back up each deadlocked process to some previously defined checkpoint and restart all processes this requires that rollback and restart mechanisms be built in to the system the risk in this approach is that the original deadlock may recur however the nondeterminancy of concurrent processing may ensure that this does not happen successively abort deadlocked processes until deadlock no longer exists the order in which processes are selected for abortion should be on the basis of some criterion of minimum cost after each abortion the detection algorithm must be reinvoked to see whether deadlock still exists successively preempt resources until deadlock no longer exists as in a costbased selection should be used and reinvocation of the detection algorithm is required after each preemption a process that has a resource preempted from it must be rolled back to a point prior to its acquisition of that resource for and the selection criteria could be one of the following choose the process with the least amount of processor time consumed so far least amount of output produced so far most estimated time remaining least total resources allocated so far lowest priority some of these quantities are easier to measure than others estimated time remaining is particularly suspect also other than by means of the priority measure there is no indication of the cost to the user as opposed to the cost to the system as a whole basic elements at a top level a computer consists of processor memory and io components with one or more modules of each type these components are interconnected in some fashion to achieve the main function of the computer which is to execute programs thus there are four main structural elements processor controls the operation of the computer and performs its data processing functions when there is only one processor it is often referred to as the central processing unit cpu main memory stores data and programs this memory is typically volatile that is when the computer is shut down the contents of the memory are lost in contrast the contents of disk memory are retained even when the computer system is shut down main memory is also referred to as real memory or primary memory io modules move data between the computer and its external environment the external environment consists of a variety of devices including secondary memory devices eg disks communications equipment and terminals system bus provides for communication among processors main memory and io modules figure depicts these toplevel components one of the processors functions is to exchange data with memory for this purpose it typically makes use of two internal to the processor registers a memory address register mar which specifies the address in memory for the next read or write and a memory buffer register mbr which contains the data to be written into memory or which receives cpu main memory system bus pc mar instruction instruction ir mbr instruction io ar execution data unit io br data data data io module n n pc program counter ir instruction register buffers mar memory address register mbr memory buffer register io ar inputoutput address register io br inputoutput buffer register figure computer components toplevel view the data read from memory similarly an io address register ioar specifies a particular io device an io buffer register iobr is used for the exchange of data between an io module and the processor a memory module consists of a set of locations defined by sequentially numbered addresses each location contains a bit pattern that can be interpreted as either an instruction or data an io module transfers data from external devices to processor and memory and vice versa it contains internal buffers for temporarily holding data until they can be sent on an integrated deadlock strategy as table suggests there are strengths and weaknesses to all of the strategies for dealing with deadlock rather than attempting to design an os facility that employs only one of these strategies it might be more efficient to use different strategies in different situations howa suggests one approach group resources into a number of different resource classes use the linear ordering strategy defined previously for the prevention of circular wait to prevent deadlocks between resource classes within a resource class use the algorithm that is most appropriate for that class as an example of this technique consider the following classes of resources swappable space blocks of memory on secondary storage for use in swapping processes process resources assignable devices such as tape drives and files main memory assignable to processes in pages or segments internal resources such as io channels the order of the preceding list represents the order in which resources are assigned the order is a reasonable one considering the sequence of steps that a process may follow during its lifetime within each class the following strategies could be used swappable space prevention of deadlocks by requiring that all of the required resources that may be used be allocated at one time as in the holdandwait prevention strategy this strategy is reasonable if the maximum storage requirements are known which is often the case deadlock avoidance is also a possibility process resources avoidance will often be effective in this category because it is reasonable to expect processes to declare ahead of time the resources that they will require in this class prevention by means of resource ordering within this class is also possible main memory prevention by preemption appears to be the most appropriate strategy for main memory when a process is preempted it is simply swapped to secondary memory freeing space to resolve the deadlock internal resources prevention by means of resource ordering can be used dining philosophers problem we now turn to the dining philosophers problem introduced by dijkstra dijk five philosophers live in a house where a table is laid for them the life of each philosopher consists principally of thinking and eating and through years of thought all of the philosophers had agreed that the only food that contributed to their thinking efforts was spaghetti due to a lack of manual skill each philosopher requires two forks to eat spaghetti the eating arrangements are simple figure a round table on which is set a large serving bowl of spaghetti five plates one for each philosopher and five forks a philosopher wishing to eat goes to his or her assigned place at the table and using the two forks on either side of the plate takes and eats some spaghetti the problem devise a ritual algorithm that will allow the philosophers to eat the algorithm must satisfy mutual exclusion no two philosophers can use the same fork at the same time while avoiding deadlock and starvation in this case the term has literal as well as algorithmic meaning this problem may not seem important or relevant in itself however it does illustrate basic problems in deadlock and starvation furthermore attempts to develop solutions reveal many of the difficulties in concurrent programming eg see ging in addition the dining philosophers problem can be seen as representative of problems dealing with the coordination of shared resources which may p p p p p figure dining arrangement for philosophers occur when an application includes concurrent threads of execution accordingly this problem is a standard test case for evaluating approaches to synchronization solution using semaphores figure suggests a solution using semaphores each philosopher picks up first the fork on the left and then the fork on the right after the philosopher is finished eating the two forks are replaced on the table this solution alas leads to deadlock if all of the philosophers are hungry at the same time they all sit down they all pick up the fork on their left and they all reach out for the other fork which is not there in this undignified position all philosophers starve to overcome the risk of deadlock we could buy five additional forks a more sanitary solution or teach the philosophers to eat spaghetti with just one fork as another approach we could consider adding an attendant who only allows four philosophers at a time into the dining room with at most four seated philosophers at least one philosopher will have access to two forks figure shows such a solution again using semaphores this solution is free of deadlock and starvation solution using a monitor figure shows a solution to the dining philosophers problem using a monitor a vector of five condition variables is defined one condition variable per fork these condition variables are used to enable a philosopher to wait for the availability of a fork in addition there is a boolean vector that records the availability status of each fork true means the fork is available the monitor consists of two procedures the getforks procedure is used by a philosopher to seize his or her left and program diningphilosophers semaphore fork int i void philosopher int i while true think wait forki wait fork i mod eat signalfork i mod signalforki void main parbegin philosopher philosopher philosopher philosopher philosopher figure a first solution to the dining philosophers problem program diningphilosophers semaphore fork semaphore room int i void philosopher int i while true think wait room wait forki wait fork i mod eat signal fork i mod signal forki signal room void main parbegin philosopher philosopher philosopher philosopher philosopher figure a second solution to the dining philosophers problem right forks if either fork is unavailable the philosopher process is queued on the appropriate condition variable this enables another philosopher process to enter the monitor the releaseforks procedure is used to make two forks available note that the structure of this solution is similar to that of the semaphore solution proposed in figure in both cases a philosopher seizes first the left fork and then the right fork unlike the semaphore solution this monitor solution does not suffer from deadlock because only one process at a time may be in the monitor for example the first philosopher process to enter the monitor is guaranteed that it can pick up the right fork after it picks up the left fork before the next philosopher to the right has a chance to seize its left fork which is this philosophers right fork unix concurrency mechanisms unix provides a variety of mechanisms for interprocessor communication and synchronization here we look at the most important of these pipes messages shared memory semaphores signals monitor diningcontroller cond forkready condition variable for synchronization boolean fork true availability status of each fork void getforksint pid pid is the philosopher id number int left pid int right pid grant the left fork if forkleft cwaitforkreadyleft queue on condition variable forkleft false grant the right fork if forkright cwaitforkreadyright queue on condition variable forkright false void releaseforksint pid int left pid int right pid release the left fork if emptyforkreadyleftno one is waiting for this fork forkleft true else awaken a process waiting on this fork csignalforkreadyleft release the right fork if emptyforkreadyrightno one is waiting for this fork forkright true else awaken a process waiting on this fork csignalforkreadyright void philosopherk to the five philosopher clients while true think getforksk client requests two forks via monitor eat spaghetti releaseforksk client releases forks via the monitor figure a solution to the dining philosophers problem using a monitor pipes messages and shared memory can be used to communicate data between processes whereas semaphores and signals are used to trigger actions by other processes pipes one of the most significant contributions of unix to the development of operating systems is the pipe inspired by the concept of coroutines ritc a pipe is a circular buffer allowing two processes to communicate on the producerconsumer model thus it is a firstinfirstout queue written by one process and read by another when a pipe is created it is given a fixed size in bytes when a process attempts to write into the pipe the write request is immediately executed if there is sufficient room otherwise the process is blocked similarly a reading process is blocked if it attempts to read more bytes than are currently in the pipe otherwise the read request is immediately executed the os enforces mutual exclusion that is only one process can access a pipe at a time there are two types of pipes named and unnamed only related processes can share unnamed pipes while either related or unrelated processes can share named pipes messages a message is a block of bytes with an accompanying type unix provides msgsnd and msgrcv system calls for processes to engage in message passing associated with each process is a message queue which functions like a mailbox the message sender specifies the type of message with each message sent and this can be used as a selection criterion by the receiver the receiver can either retrieve messages in firstinfirstout order or by type a process will block when trying to send a message to a full queue a process will also block when trying to read from an empty queue if a process attempts to read a message of a certain type and fails because no message of that type is present the process is not blocked shared memory the fastest form of interprocess communication provided in unix is shared memory this is a common block of virtual memory shared by multiple processes processes read and write shared memory using the same machine instructions they use to read and write other portions of their virtual memory space permission is readonly or readwrite for a process determined on a perprocess basis mutual exclusion constraints are not part of the sharedmemory facility but must be provided by the processes using the shared memory semaphores the semaphore system calls in unix system v are a generalization of the semwait and semsignal primitives defined in chapter several operations can be performed simultaneously and the increment and decrement operations can be values greater than the kernel does all of the requested operations atomically no other process may access the semaphore until all operations have completed a semaphore consists of the following elements current value of the semaphore process id of the last process to operate on the semaphore number of processes waiting for the semaphore value to be greater than its current value number of processes waiting for the semaphore value to be zero associated with the semaphore are queues of processes blocked on that semaphore semaphores are actually created in sets with a semaphore set consisting of one or more semaphores there is a semctl system call that allows all of the semaphore values in the set to be set at the same time in addition there is a semop system call that takes as an argument a list of semaphore operations each defined on one of the semaphores in a set when this call is made the kernel performs the indicated operations one at a time for each operation the actual function is specified by the value semop the following are the possibilities if semop is positive the kernel increments the value of the semaphore and awakens all processes waiting for the value of the semaphore to increase if semop is the kernel checks the semaphore value if the semaphore value equals the kernel continues with the other operations on the list otherwise the kernel increments the number of processes waiting for this semaphore to be and suspends the process to wait for the event that the value of the semaphore equals if semop is negative and its absolute value is less than or equal to the semaphore value the kernel adds semop a negative number to the semaphore value if the result is the kernel awakens all processes waiting for the value of the semaphore to equal if semop is negative and its absolute value is greater than the semaphore value the kernel suspends the process on the event that the value of the semaphore increases this generalization of the semaphore provides considerable flexibility in performing process synchronization and coordination signals a signal is a software mechanism that informs a process of the occurrence of asynchronous events a signal is similar to a hardware interrupt but does not employ priorities that is all signals are treated equally signals that occur at the same time are presented to a process one at a time with no particular ordering processes may send each other signals or the kernel may send signals internally a signal is delivered by updating a field in the process table for the process to which the signal is being sent because each signal is maintained as a single bit signals of a given type can not be queued a signal is processed just after a process wakes up to run or whenever the process is preparing to return from a system call a process may respond to a signal by performing some default action eg termination executing a signalhandler function or ignoring the signal table lists signals defined for unix svr table unix signals value name description sighup hang up sent to process when kernel assumes that the user of that process is doing no useful work sigint interrupt sigquit quit sent by user to induce halting of process and production of core dump sigill illegal instruction sigtrap trace trap triggers the execution of code for process tracing sigiot iot instruction sigemt emt instruction sigfpe floatingpoint exception sigkill kill terminate process sigbus bus error sigsegv segmentation violation process attempts to access location outside its virtual address space sigsys bad argument to system call sigpipe write on a pipe that has no readers attached to it sigalrm alarm clock issued when a process wishes to receive a signal after a period of time sigterm software termination sigusr userdefined signal sigusr userdefined signal sigchld death of a child sigpwr power failure linux kernel concurrency mechanisms linux includes all of the concurrency mechanisms found in other unix systems such as svr including pipes messages shared memory and signals in addition linux includes a rich set of concurrency mechanisms specifically intended for use when a thread is executing in kernel mode that is these are mechanisms used within the kernel to provide concurrency in the execution of kernel code this section examines the linux kernel concurrency mechanisms atomic operations linux provides a set of operations that guarantee atomic operations on a variable these operations can be used to avoid simple race conditions an atomic operation executes without interruption and without interference on a uniprocessor system a thread performing an atomic operation can not be interrupted once the operation has started until the operation is finished in addition on a multiprocessor system the variable being operated on is locked from access by other threads until this operation is completed two types of atomic operations are defined in linux integer operations which operate on an integer variable and bitmap operations which operate on one bit in a bitmap table these operations must be implemented on any architecture that implements linux for some architectures there are corresponding assembly language instructions for the atomic operations on other architectures an operation that locks the memory bus is used to guarantee that the operation is atomic for atomic integer operations a special data type is used atomict the atomic integer operations can be used only on this data type and no other operations table linux atomic operations atomic integer operations atomicinit int i at declaration initialize an atomict to i int atomicreadatomict v read integer value of v void atomicsetatomict v int i set the value of v to integer i void atomicaddint i atomict v add i to v void atomicsubint i atomict v subtract i from v void atomicincatomict v add to v void atomicdecatomict v subtract from v int atomicsubandtestint i subtract i from v return if the result is zero atomict v return otherwise int atomicaddnegativeint i add i to v return if the result is negative atomict v return otherwise used for implementing semaphores int atomicdecandtestatomict v subtract from v return if the result is zero return otherwise int atomicincandtestatomict v add to v return if the result is zero return otherwise atomic bitmap operations void setbitint nr void addr set bit nr in the bitmap pointed to by addr void clearbitint nr void addr clear bit nr in the bitmap pointed to by addr void changebitint nr void addr invert bit nr in the bitmap pointed to by addr int testandsetbitint nr set bit nr in the bitmap pointed to by addr void addr return the old bit value int testandclearbitint nr clear bit nr in the bitmap pointed to by addr void addr return the old bit value int testandchangebitint nr invert bit nr in the bitmap pointed to by addr void addr return the old bit value int testbitint nr void addr return the value of bit nr in the bitmap pointed to by addr are allowed on this data type love lists the following advantages for these restrictions the atomic operations are never used on variables that might in some circumstances be unprotected from race conditions variables of this data type are protected from improper use by nonatomic operations the compiler can not erroneously optimize access to the value eg by using an alias rather than the correct memory address this data type serves to hide architecturespecific differences in its implementation a typical use of the atomic integer data type is to implement counters the atomic bitmap operations operate on one of a sequence of bits at an arbitrary memory location indicated by a pointer variable thus there is no equivalent to the atomict data type needed for atomic integer operations atomic operations are the simplest of the approaches to kernel synchronization more complex locking mechanisms can be built on top of them spinlocks the most common technique used for protecting a critical section in linux is the spinlock only one thread at a time can acquire a spinlock any other thread attempting to acquire the same lock will keep trying spinning until it can acquire the lock in essence a spinlock is built on an integer location in memory that is checked by each thread before it enters its critical section if the value is the thread sets the value to and enters its critical section if the value is nonzero the thread continually checks the value until it is zero the spinlock is easy to implement but has the disadvantage that lockedout threads continue to execute in a busywaiting mode thus spinlocks are most effective in situations where the wait time for acquiring a lock is expected to be very short say on the order of less than two context changes the basic form of use of a spinlock is the following spinlocklock critical section spinunlocklock basic spinlocks the basic spinlock as opposed to the readerwriter spinlock explained subsequently comes in four flavors table plain if the critical section of code is not executed by interrupt handlers or if the interrupts are disabled during the execution of the critical section then the plain spinlock can be used it does not affect the interrupt state on the processor on which it is run irq if interrupts are always enabled then this spinlock should be used irqsave if it is not known if interrupts will be enabled or disabled at the time of execution then this version should be used when a lock is acquired the current state of interrupts on the local processor is saved to be restored when the lock is released table linux spinlocks void spinlockspinlockt lock acquires the specified lock spinning if needed until it is available void spinlockirqspinlockt lock like spinlock but also disables interrupts on the local processor void spinlockirqsavespinlockt lock like spinlockirq but also saves the current unsigned long flags interrupt state in flags void spinlockbhspinlockt lock like spinlock but also disables the execution of all bottom halves void spinunlockspinlockt lock releases given lock void spinunlockirqspinlockt lock releases given lock and enables local interrupts void spinunlockirqrestorespinlockt releases given lock and restores local interrupts lock unsigned long flags to given previous state void spinunlockbhspinlockt lock releases given lock and enables bottom halves void spinlockinitspinlockt lock initializes given spinlock int spintrylockspinlockt lock tries to acquire specified lock returns nonzero if lock is currently held and zero otherwise int spinislockedspinlockt lock returns nonzero if lock is currently held and zero otherwise bh when an interrupt occurs the minimum amount of work necessary is performed by the corresponding interrupt handler a piece of code called the bottom half performs the remainder of the interruptrelated work allowing the current interrupt to be enabled as soon as possible the bh spinlock is used to disable and then enable bottom halves to avoid conflict with the protected critical section the plain spinlock is used if the programmer knows that the protected data is not accessed by an interrupt handler or bottom half otherwise the appropriate nonplain spinlock is used spinlocks are implemented differently on a uniprocessor system versus a multiprocessor system for a uniprocessor system the following considerations apply if kernel preemption is turned off so that a thread executing in kernel mode can not be interrupted then the locks are deleted at compile time they are not needed if kernel preemption is enabled which does permit interrupts then the spinlocks again compile away ie no test of a spinlock memory location occurs but are simply implemented as code that enablesdisables interrupts on a multiple processor system the spinlock is compiled into code that does in fact test the spinlock location the use of the spinlock mechanism in a program allows it to be independent of whether it is executed on a uniprocessor or multiprocessor system readerwriter spinlock the readerwriter spinlock is a mechanism that allows a greater degree of concurrency within the kernel than the basic spinlock the readerwriter spinlock allows multiple threads to have simultaneous access to the same data structure for reading only but gives exclusive access to the spinlock for a thread that intends to update the data structure each readerwriter spinlock consists of a bit reader counter and an unlock flag with the following interpretation counter flag interpretation the spinlock is released and available for use spinlock has been acquired for writing by one thread n n spinlock has been acquired for reading by n threads n n not valid as with the basic spinlock there are plain irq and irqsave versions of the readerwriter spinlock note that the readerwriter spinlock favors readers over writers if the spinlock is held for readers then so long as there is at least one reader the spinlock can not be preempted by a writer furthermore new readers may be added to the spinlock even while a writer is waiting semaphores at the user level linux provides a semaphore interface corresponding to that in unix svr internally linux provides an implementation of semaphores for its own use that is code that is part of the kernel can invoke kernel semaphores these kernel semaphores can not be accessed directly by the user program via system calls they are implemented as functions within the kernel and are thus more efficient than uservisible semaphores linux provides three types of semaphore facilities in the kernel binary semaphores counting semaphores and readerwriter semaphores binary and counting semaphores the binary and counting semaphores defined in linux table have the same functionality as described for such semaphores in chapter the function names down and up are used for the functions referred to in chapter as semwait and semsignal respectively a counting semaphore is initialized using the semainit function which gives the semaphore a name and assigns an initial value to the semaphore binary semaphores called mutexes in linux are initialized using the initmutex and init mutexlocked functions which initialize the semaphore to or respectively linux provides three versions of the down semwait operation the down function corresponds to the traditional semwait operation that is the thread tests the semaphore and blocks if the semaphore is not available the thread will awaken when a corresponding up operation on this semaphore occurs note that this function name is used for an operation on either a counting semaphore or a binary semaphore the downinterruptible function allows the thread to receive and respond to a kernel signal while being blocked on the down operation if the thread is woken up by a signal the downinterruptible function increments the count value of the semaphore and returns an error code known in linux as eintr this alerts the thread that the invoked semaphore function has aborted in effect the thread has been forced to give up the semaphore this feature is useful for device drivers and other services in which it is convenient to override a semaphore operation the downtrylock function makes it possible to try to acquire a semaphore without being blocked if the semaphore is available it is acquired otherwise this function returns a nonzero value without blocking the thread readerwriter semaphores the readerwriter semaphore divides users into readers and writers it allows multiple concurrent readers with no writers but only a single writer with no concurrent readers in effect the semaphore functions as a counting semaphore for readers but a binary semaphore mutex for writers table shows the basic readerwriter semaphore operations the readerwriter semaphore uses uninterruptible sleep so there is only one version of each of the down operations table linux semaphores traditional semaphores void semainitstruct semaphore sem initializes the dynamically created semaphore to the int count given count void initmutexstruct semaphore sem initializes the dynamically created semaphore with a count of initially unlocked void initmutexlockedstruct semainitializes the dynamically created semaphore with a phore sem count of initially locked void downstruct semaphore sem attempts to acquire the given semaphore entering uninterruptible sleep if semaphore is unavailable int downinterruptiblestruct attempts to acquire the given semaphore entersemaphore sem ing interruptible sleep if semaphore is unavailable returnseintr value if a signal other than the result of an up operation is received int downtrylockstruct semaphore attempts to acquire the given semaphore and sem returns a nonzero value if semaphore is unavailable void upstruct semaphore sem releases the given semaphore readerwriter semaphores void initrwsemstruct rwsemaphore initializes the dynamically created semaphore with a rwsem count of void downreadstruct rwsemaphore down operation for readers rwsem void upreadstruct rwsemaphore up operation for readers rwsem void downwritestruct rwsemaphore down operation for writers rwsem void upwritestruct rwsemaphore up operation for writers rwsem barriers in some architectures compilers andor the processor hardware may reorder memory accesses in source code to optimize performance these reorderings are done to optimize the use of the instruction pipeline in the processor the reordering algorithms contain checks to ensure that data dependencies are not violated for example the code a b may be reordered so that memory location b is updated before memory location a is updated however the code a b a will not be reordered even so there are occasions when it is important that reads or writes are executed in the order specified because of use of the information that is made by another thread or a hardware device to enforce the order in which instructions are executed linux provides the memory barrier facility table lists the most important functions that are defined for this facility the rmb operation insures that no reads occur across the barrier defined by the place of the rmb in the code similarly the wmb operation insures that no writes occur across the barrier defined by the place of the wmb in the code the mb operation provides both a load and store barrier two important points to note about the barrier operations the barriers relate to machine instructions namely loads and stores thus the higherlevel language instruction a b involves both a load read from location b and a store write to location a the rmb wmb and mb operations dictate the behavior of both the compiler and the processor in the case of the compiler the barrier operation dictates that the compiler not reorder instructions during the compile process in the case of the processor the barrier operation dictates that any instructions pending in the pipeline before the barrier must be committed for execution before any instructions encountered after the barrier table linux memory barrier operations rmb prevents loads from being reordered across the barrier wmb prevents stores from being reordered across the barrier mb prevents loads and stores from being reordered across the barrier barrier prevents the compiler from reordering loads or stores across the barrier smprmb on smp provides a rmb and on up provides a barrier smpwmb on smp provides a wmb and on up provides a barrier smpmb on smp provides a mb and on up provides a barrier note smp symmetric multiprocessor up uniprocessor the barrier operation is a lighterweight version of the mb operation in that it only controls the compilers behavior this would be useful if it is known that the processor will not perform undesirable reorderings for example the intel x processors do not reorder writes the smprmb smpwmb and smpmb operations provide an optimization for code that may be compiled on either a uniprocessor up or a symmetric multiprocessor smp these instructions are defined as the usual memory barriers for an smp but for a up they are all treated only as compiler barriers the smp operations are useful in situations in which the data dependencies of concern will only arise in an smp context solaris thread synchronization primitives in addition to the concurrency mechanisms of unix svr solaris supports four thread synchronization primitives mutual exclusion mutex locks semaphores multiple readers single writer readerswriter locks condition variables solaris implements these primitives within the kernel for kernel threads they are also provided in the threads library for userlevel threads figure shows the data structures for these primitives the initialization functions for the primitives fill in some of the data members once a synchronization object is created there are essentially only two operations that can be performed enter acquire lock and release unlock there are no mechanisms in the kernel or the threads library to enforce mutual exclusion or to prevent deadlock if a thread attempts to access a piece of data or code that is supposed to be protected but does not use the appropriate synchronization primitive then such access occurs if a thread locks an object and then fails to unlock it no kernel action is taken all of the synchronization primitives require the existence of a hardware instruction that allows an object to be tested and set in one atomic operation mutual exclusion lock a mutex is used to ensure that only one thread at a time can access the resource protected by the mutex the thread that locks the mutex must be the one that unlocks it a thread attempts to acquire a mutex lock by executing the mutex enter primitive if mutexenter can not set the lock because it is already set by another thread the blocking action depends on typespecific information stored in the mutex object the default blocking policy is a spinlock a blocked thread polls the status of the lock while executing in a busy waiting loop an interruptbased blocking mechanism is optional in this latter case the mutex includes a turnstile id that identifies a queue of threads sleeping on this lock type octet owner octets wlock octet lock octet waiters octets waiters octets union octets statistic pointer or typespecific info octets number of write requests possibly a turnstile id lock type filler or statistics pointer thread owner octets a mutex lock c readerwriter lock type octet wlock octet waiters octets waiters octets d condition variable count octets b semaphore figure solaris synchronization data structures the operations on a mutex lock are mutexenter acquires the lock potentially blocking if it is already held mutexexit releases the lock potentially unblocking a waiter mutextryenter acquires the lock if it is not already held the mutextryenter primitive provides a nonblocking way of performing the mutual exclusion function this enables the programmer to use a busywait approach for userlevel threads which avoids blocking the entire process because one thread is blocked semaphores solaris provides classic counting semaphores with the following primitives semap decrements the semaphore potentially blocking the thread semav increments the semaphore potentially unblocking a waiting thread sematryp decrements the semaphore if blocking is not required again the sematryp primitive permits busy waiting readerswriter lock the readerswriter lock allows multiple threads to have simultaneous readonly access to an object protected by the lock it also allows a single thread to access the object for writing at one time while excluding all readers when the lock is acquired for writing it takes on the status of write lock all threads attempting access for reading or writing must wait if one or more readers have acquired the lock its status is read lock the primitives are as follows rwenter attempts to acquire a lock as reader or writer rwexit releases a lock as reader or writer rwtryenter acquires the lock if blocking is not required rwdowngrade a thread that has acquired a write lock converts it to a read lock any waiting writer remains waiting until this thread releases the lock if there are no waiting writers the primitive wakes up any pending readers rwtryupgrade attempts to convert a reader lock into a writer lock condition variables a condition variable is used to wait until a particular condition is true condition variables must be used in conjunction with a mutex lock this implements a monitor of the type illustrated in figure the primitives are as follows cvwait blocks until the condition is signaled cvsignal wakes up one of the threads blocked in cvwait cvbroadcast wakes up all of the threads blocked in cvwait cvwait releases the associated mutex before blocking and reacquires it before returning because reacquisition of the mutex may be blocked by other threads waiting for the mutex the condition that caused the wait must be retested thus typical usage is as follows mutexenterm while somecondition cvwaitcv m mutexexitm this allows the condition to be a complex expression because it is protected by the mutex windows concurrency mechanisms windows provides synchronization among threads as part of the object architecture the most important methods of synchronization are executive dispatcher objects usermode critical sections slim readerwriter locks condition variables and lockfree operations dispatcher objects make use of wait functions we first describe wait functions and then look at the synchronization methods wait functions the wait functions allow a thread to block its own execution the wait functions do not return until the specified criteria have been met the type of wait function determines the set of criteria used when a wait function is called it checks whether the wait criteria have been met if the criteria have not been met the calling thread enters the wait state it uses no processor time while waiting for the criteria to be met the most straightforward type of wait function is one that waits on a single object the waitforsingleobject function requires a handle to one synchronization object the function returns when one of the following occurs the specified object is in the signaled state the timeout interval elapses the timeout interval can be set to infinite to specify that the wait will not time out dispatcher objects the mechanism used by the windows executive to implement synchronization facilities is the family of dispatcher objects which are listed with brief descriptions in table table windows synchronization objects set to signaled state effect on waiting object type definition when threads notification an announcement that a thread sets the event all released event system event has occurred synchronization an announcement that a thread sets the event one thread released event system event has occurred mutex a mechanism that provides owning thread or other one thread released mutual exclusion capabilities thread releases the equivalent to a binary semaphore mutex semaphore a counter that regulates the number semaphore count drops all released of threads that can use a resource to zero waitable timer a counter that records the passage set time arrives or time all released of time interval expires file an instance of an opened file or io operation completes all released io device process a program invocation including last thread terminates all released the address space and resources required to run the program thread an executable entity within a process thread terminates all released note shaded rows correspond to objects that exist for the sole purpose of synchronization the first five object types in the table are specifically designed to support synchronization the remaining object types have other uses but also may be used for synchronization each dispatcher object instance can be in either a signaled or unsignaled state a thread can be blocked on an object in an unsignaled state the thread is released when the object enters the signaled state the mechanism is straightforward a thread issues a wait request to the windows executive using the handle of the synchronization object when an object enters the signaled state the windows executive releases one or all of the thread objects that are waiting on that dispatcher object the event object is useful in sending a signal to a thread indicating that a particular event has occurred for example in overlapped input and output the system sets a specified event object to the signaled state when the overlapped operation has been completed the mutex object is used to enforce mutually exclusive access to a resource allowing only one thread object at a time to gain access it therefore functions as a binary semaphore when the mutex object enters the signaled state only one of the threads waiting on the mutex is released mutexes can be used to synchronize threads running in different processes like mutexes semaphore objects may be shared by threads in multiple processes the windows semaphore is a counting semaphore in essence the waitable timer object signals at a certain time andor at regular intervals critical sections critical sections provide a synchronization mechanism similar to that provided by mutex objects except that critical sections can be used only by the threads of a single process event mutex and semaphore objects can also be used in a singleprocess application but critical sections provide a much faster more efficient mechanism for mutualexclusion synchronization the process is responsible for allocating the memory used by a critical section typically this is done by simply declaring a variable of type criticalsection before the threads of the process can use it initialize the critical section by using the initializecriticalsection function a thread uses the entercriticalsection or tryentercriticalsection function to request ownership of a critical section it uses the leavecriticalsection function to release ownership of a critical section if the critical section is currently owned by another thread entercriticalsection waits indefinitely for ownership in contrast when a mutex object is used for mutual exclusion the wait functions accept a specified timeout interval the tryentercriticalsection function attempts to enter a critical section without blocking the calling thread critical sections use a sophisticated algorithm when trying to acquire the mutex if the system is a multiprocessor the code will attempt to acquire a spinlock this works well in situations where the critical section is acquired for only a short time effectively the spinlock optimizes for the case where the thread that currently owns the critical section is executing on another processor if the spinlock can not be acquired within a reasonable number of iterations a dispatcher object is used to block the thread so that the kernel can dispatch another thread onto the processor the dispatcher object is only allocated as a last resort most critical sections are needed for correctness but in practice are rarely contended by lazily allocating the dispatcher object the system saves significant amounts of kernel virtual memory slim readwriter locks and condition variables windows vista added a user mode readerwriter like critical sections the reader writer lock enters the kernel to block only after attempting to use a spinlock it is slim in the sense that it normally only requires allocation of a single pointersized piece of memory to use an srw lock a process declares a variable of type srwlock and a calls initializesrwlock to initialize it threads call acquiresrwlockexclusive or acquiresrwlockshared to acquire the lock and releasesrwlockexclusive or releasesrwlockshared to release it windows also has condition variables the process must declare a conditionvariable and initialize it in some thread by calling initializeconditionvariable condition variables can be used with either critical sections or srw locks so there are two methods sleepconditionvariablecs and sleepconditionvariablesrw which sleep on the specified condition and releases the specified lock as an atomic operation there are two wake methods wakeconditionvariable and wake allconditionvariable which wake one or all of the sleeping threads respectively condition variables are used as follows acquire exclusive lock while predicate false sleepconditionvariable perform the protected operation release the lock lockfree synchronization windows also relies heavily on interlocked operations for synchronization interlocked operations use hardware facilities to guarantee that memory locations can be read modified and written in a single atomic operation examples include interlockedincrement and interlockedcompareexchange the latter allows a memory location to be updated only if it hasnt changed values since being read many of the synchronization primitives use interlocked operations within their implementation but these operations are also available to programmers for situations where they want to synchronize without taking a software lock these socalled lockfree synchronization primitives have the advantage that a thread can never be switched away from a processor say at the end of its timeslice while still holding a lock thus they can not block another thread from running more complex lockfree primitives can be built out of the interlocked operations most notably windows slists which provide a lockfree lifo queue slists are managed using functions like interlockedpushentryslist and interlockedpopentryslist evolution of the microprocessor the hardware revolution that brought about desktop and handheld computing was the invention of the microprocessor which contained a processor on a single chip though originally much slower than multichip processors microprocessors have continually evolved to the point that they are now much faster for most computations due to the physics involved in moving information around in subnanosecond timeframes not only have microprocessors become the fastest general purpose processors available they are now multiprocessors each chip called a socket contains multiple processors called cores each with multiple levels of large memory caches and multiple logical processors sharing the execution units of each core as of it is not unusual for even a laptop to have or cores each with hardware threads for a total of or logical processors although processors provide very good performance for most forms of computing there is increasing demand for numerical computation graphical processing units gpus provide efficient computation on arrays of data using singleinstruction multiple data simd techniques pioneered in supercomputers gpus are no longer used just for rendering advanced graphics but they are also used for general numerical processing such as physics simulations for games or computations on large spreadsheets simultaneously the cpus themselves are gaining the capability of operating on arrays of data with increasingly powerful vector units integrated into the processor architecture of the x and amd families processors and gpus are not the end of the computational story for the modern pc digital signal processors dsps are also present for dealing with streaming signals such as audio or video dsps used to be embedded in io devices like modems but they are now becoming firstclass computational devices especially in handhelds other specialized computational devices fixed function units coexist with the cpu to support other standard computations such as encodingdecoding speech and video codecs or providing support for encryption and security to satisfy the requirements of handheld devices the classic microprocessor is giving way to the system on a chip soc where not just the cpus and caches are on the same chip but also many of the other components of the system such as dsps gpus io devices such as radios and codecs and main memory recommended reading the classic paper on deadlocks holt is still well worth a read as is coff another good survey is islo corb is a thorough treatment of deadlock detection dimi is a nice overview of deadlocks two papers by levine levia levib clarify some of the concepts used in discussions of deadlock shub is a useful overview of deadlock abra describes a deadlock detection package the concurrency mechanisms in unix svr linux and solaris are well covered in gray love and mcdo respectively hall is a thorough treatment of unix concurrency and interprocess communication mechanisms abra abramson t detecting potential deadlocks dr dobbs journal january coff coffman e elphick m and shoshani a system deadlocks computing surveys june corb corbett j evaluating deadlock detection methods for concurrent software ieee transactions on software engineering march dimi dimitoglou g deadlocks and methods for their detection prevention and recovery in modern operating systems operating systems review july gray gray j interprocess communications in unix the nooks and crannies upper saddle river nj prentice hall hall hall b beejs guide to unix ipc document available in premium content section for this book holt holt r some deadlock properties of computer systems computing surveys september islo isloor s and marsland t the deadlock problem an overview computer september levia levine g defining deadlock operating systems review january levib levine g defining deadlock with fungible resources operating systems review july love love r linux kernel development upper saddle river nj addisonwesley mcdo mcdougall r and mauro j solaris internals solaris and opensolaris kernel architecture palo alto ca sun microsystems press shub shub c a unified treatment of deadlock journal of computing in small colleges october available through the acm digital library key terms review questions and problems key terms bankers algorithm deadlock prevention pipe circular wait hold and wait preemption consumable resource joint progress diagram resource allocation graph deadlock memory barrier reusable resource deadlock avoidance message spinlock deadlock detection mutual exclusion starvation review questions give examples of reusable and consumable resources what are the three conditions that must be present for deadlock to be possible what are the four conditions that create deadlock how can the holdandwait condition be prevented list two ways in which the nopreemption condition can be prevented how can the circular wait condition be prevented what is the difference among deadlock avoidance detection and prevention problems show that the four conditions of deadlock apply to figure a show how each of the techniques of prevention avoidance and detection can be applied to figure for figure provide a narrative description of each of the six depicted paths similar to the description of the paths of figure provided in section it was stated that deadlock can not occur for the situation reflected in figure justify that statement given the following state for the bankers algorithm processes p through p resource types a instances b instances c instances d instances snapshot at time t available a b c d current allocation maximum demand process a b c d a b c d p p p p p p a verify that the available array has been calculated correctly b calculate the need matrix c show that the current state is safe that is show a safe sequence of processes in addition to the sequence show how the available working array changes as each process terminates d given the request from process p should this request be granted why or why not in the code below three processes are competing for six resources labeled a to f a using a resource allocation graph figures and show the possibility of a deadlock in this implementation b modify the order of some of the get requests to prevent the possibility of any deadlock you can not move requests across procedures only change the order inside each procedure use a resource allocation graph to justify your answer void p void p void p while true while true while true geta getd getc getb gete getf getc getb getd critical region critical region critical region use a b c use d e b use c f d releasea released releasec releaseb releasee releasef releasec releaseb released a spooling system figure consists of an input process i a user process p and an output process o connected by two buffers the processes exchange data in i input p output o buffer buffer figure a spooling system blocks of equal size these blocks are buffered on a disk using a floating boundary between the input and the output buffers depending on the speed of the processes the communication primitives used ensure that the following resource constraint is satisfied i o max where max maximum number of blocks on disk i number of input blocks on disk o number of output blocks on disk the following is known about the processes as long as the environment supplies data process i will eventually input it to the disk provided disk space becomes available as long as input is available on the disk process p will eventually consume it and output a finite amount of data on the disk for each block input provided disk space becomes available as long as output is available on the disk process o will eventually consume it show that this system can become deadlocked suggest an additional resource constraint that will prevent the deadlock in problem but still permit the boundary between input and output buffers to vary in accordance with the present needs of the processes in the the multiprogramming system dijk a drum precursor to the disk for secondary storage is divided into input buffers processing areas and output buffers with floating boundaries depending on the speed of the processes involvedthe current state of the drum can be characterized by the following parameters max maximum number of pages on drum i number of input pages on drum p number of processing pages on drum o number of output pages on drum reso minimum number of pages reserved for output resp minimum number of pages reserved for processing formulate the necessary resource constraints that guarantee that the drum capacity is not exceeded and that a minimum number of pages is reserved permanently for output and processing in the the multiprogramming system a page can make the following state transitions empty input buffer input production input buffer processing area input consumption processing area output buffer output production output buffer empty output consumption empty processing area procedure call processing area empty procedure return a define the effect of these transitions in terms of the quantities i o and p b can any of them lead to a deadlock if the assumptions made in problem about input processes user processes and output processes hold consider a system with a total of units of memory allocated to three processes as shown process max hold apply the bankers algorithm to determine whether it would be safe to grant each of the following requests if yes indicate a sequence of terminations that could be guaranteed possible if no show the reduction of the resulting allocation table a a fourth process arrives with a maximum memory need of and an initial need of units b a fourth process arrives with a maximum memory need of and an initial need of units evaluate the bankers algorithm for its usefulness in an os a pipeline algorithm is implemented so that a stream of data elements of type t produced by a process p passes through a sequence of processes p p pn which operates on the elements in that order a define a generalized message buffer that contains all the partially consumed data elements and write an algorithm for process pi i n of the form repeat receive from predecessor consume element send to successor forever assume p receives input elements sent by pn the algorithm should enable the processes to operate directly on messages stored in the buffer so that copying is unnecessary b show that the processes can not be deadlocked with respect to the common buffer suppose the following two processes foo and bar are executed concurrently and share the semaphore variables s and r each initialized to and the integer variable x initialized to void foo void bar do do semwaits semwaitr semwaitr semwaits x x semsignals semsignals semsignalr semsignalr while while a can the concurrent execution of these two processes result in one or both being blocked forever if yes give an execution sequence in which one or both are blocked forever b can the concurrent execution of these two processes result in the indefinite postponement of one of them if yes give an execution sequence in which one is indefinitely postponed consider a system consisting of four processes and a single resource the current state of the claim and allocation matrices are c a what is the minimum number of units of the resource needed to be available for this state to be safe consider the following ways of handling deadlock bankers algorithm detect deadlock and kill thread releasing all resources reserve all resources in advance restart thread and release all resources if thread needs to wait resource ordering and detect deadlock and roll back threads actions a one criterion to use in evaluating different approaches to deadlock is which approach permits the greatest concurrency in other words which approach allows the most threads to make progress without waiting when there is no deadlock give a rank order from to for each of the ways of handling deadlock just listed where allows the greatest degree of concurrency comment on your ordering b another criterion is efficiency in other words which requires the least processor overhead rank order the approaches from to with being the most efficient assuming that deadlock is a very rare event comment on your ordering does your ordering change if deadlocks occur frequently comment on the following solution to the dining philosophers problem a hungry philosopher first picks up his left fork if his right fork is also available he picks up his right fork and starts eating otherwise he puts down his left fork again and repeats the cycle suppose that there are two types of philosophers one type always picks up his left fork first a lefty and the other type always picks up his right fork first a righty the behavior of a lefty is defined in figure the behavior of a righty is as follows begin repeat think wait fork i mod wait forki eat signal forki signal fork i mod forever end prove the following a any seating arrangement of lefties and righties with at least one of each avoids deadlock b any seating arrangement of lefties and righties with at least one of each prevents starvation figure shows another solution to the dining philosophers problem using monitors compare to figure and report your conclusions in table some of the linux atomic operations do not involve two accesses to a variable such as atomicreadatomict v a simple read operation is obviously atomic in any architecture therefore why is this operation added to the repertoire of atomic operations consider the following fragment of code on a linux system readlockmrrwlock writelockmrrwlock where mrrwlock is a readerwriter lock what is the effect of this code monitor diningcontroller enum states thinking hungry eating state cond needfork condition variable void getforksint pid pid is the philosopher id number statepid hungry announce that im hungry if statepid eating statepid eating cwaitneedforkpid wait if either neighbor is eating statepid eating proceed if neither neighbor is eating void releaseforksint pid statepid thinking give right higher neighbor a chance to eat if statepid hungry statepid eating csignalneedforkpid give left lower neighbor a chance to eat else if statepid hungry statepid eating csignalneedforkpid void philosopherk to the five philosopher clients while true think getforksk client requests two forks via monitor eat spaghetti releaseforksk client releases forks via the monitor figure another solution to the dining philosophers problem using a monitor the two variables a and b have initial values of and respectively the following code is for a linux system thread thread a mb b c b rmb d a what possible errors are avoided by the use of the memory barriers chapter memory management memory management requirements relocation protection sharing logical organization physical organization memory partitioning fixed partitioning dynamic partitioning buddy system relocation paging segmentation security issues buffer overflow attacks defending against buffer overflows summary recommended reading key terms review questions and problems appendix a loading and linking i can not guarantee that i carry all the facts in my mind intense mental concentration has a curious way of blotting out what has passed each of my cases displaces the last and mlle carre has blurred my recollection of baskerville hall tomorrow some other little problem may be submitted to my notice which will in turn dispossess the fair french lady and the infamous upwood the hound of the baskervilles arthur conan doyle learning objectives after studying this chapter you should be able to discuss the principal requirements for memory management understand the reason for memory partitioning and explain the various techniques that are used understand and explain the concept of paging understand and explain the concept of segmentation assess the relative advantages of paging and segmentation summarize key security issues related to memory management describe the concepts of loading and linking in a uniprogramming system main memory is divided into two parts one part for the operating system resident monitor kernel and one part for the program currently being executed in a multiprogramming system the user part of memory must be further subdivided to accommodate multiple processes the task of subdivision is carried out dynamically by the operating system and is known as memory management effective memory management is vital in a multiprogramming system if only a few processes are in memory then for much of the time all of the processes will be waiting for io and the processor will be idle thus memory needs to be allocated to ensure a reasonable supply of ready processes to consume available processor time we begin with the requirements that memory management is intended to satisfy next we discuss a variety of simple schemes that have been used for memory management table introduces some key terms for our discussion a set of animations that illustrate concepts in this chapter is available online click on the rotating globe at williamstallingscomososehtml for access table memory management terms frame a fixedlength block of main memory page a fixedlength block of data that resides in secondary memory such as disk a page of data may temporarily be copied into a frame of main memory segment a variablelength block of data that resides in secondary memory an entire segment may temporarily be copied into an available region of main memory segmentation or the segment may be divided into pages which can be individually copied into main memory combined segmentation and paging memory management requirements while surveying the various mechanisms and policies associated with memory management it is helpful to keep in mind the requirements that memory management is intended to satisfy these requirements include the following relocation protection sharing logical organization physical organization relocation in a multiprogramming system the available main memory is generally shared among a number of processes typically it is not possible for the programmer to know in advance which other programs will be resident in main memory at the time of execution of his or her program in addition we would like to be able to swap active processes in and out of main memory to maximize processor utilization by providing a large pool of ready processes to execute once a program is swapped out to disk it would be quite limiting to specify that when it is next swapped back in it must be placed in the same main memory region as before instead we may need to relocate the process to a different area of memory thus we can not know ahead of time where a program will be placed and we must allow for the possibility that the program may be moved about in main memory due to swapping these facts raise some technical concerns related to addressing as illustrated in figure the figure depicts a process image for simplicity let us assume that the process image occupies a contiguous region of main memory clearly the operating system will need to know the location of process control information and of the execution stack as well as the entry point to begin execution of the program for this process because the operating system is managing memory and is responsible for bringing this process into main memory these addresses are easy to come by in addition however the processor must deal with memory process control process control block information entry point to program branch program instruction increasing address values reference to data data current top of stack stack figure addressing requirements for a process references within the program branch instructions contain an address to reference the instruction to be executed next data reference instructions contain the address of the byte or word of data referenced somehow the processor hardware and operating system software must be able to translate the memory references found in the code of the program into actual physical memory addresses reflecting the current location of the program in main memory protection each process should be protected against unwanted interference by other processes whether accidental or intentional thus programs in other processes should not be able to reference memory locations in a process for reading or writing purposes without permission in one sense satisfaction of the relocation requirement increases the difficulty of satisfying the protection requirement because the location of a program in main memory is unpredictable it is impossible to check absolute addresses at compile time to assure protection furthermore most programming languages allow the dynamic calculation of addresses at run time eg by computing an array subscript or a pointer into a data structure hence all memory references generated by a process must be checked at run time to ensure that they refer only to the memory space allocated to that process fortunately we shall see that mechanisms that support relocation also support the protection requirement normally a user process can not access any portion of the operating system neither program nor data again usually a program in one process can not branch to an instruction in another process without special arrangement a program in one process can not access the data area of another process the processor must be able to abort such instructions at the point of execution note that the memory protection requirement must be satisfied by the processor hardware rather than the operating system software this is because the os can not anticipate all of the memory references that a program will make even if such anticipation were possible it would be prohibitively time consuming to screen each program in advance for possible memoryreference violations thus it is only possible to assess the permissibility of a memory reference data access or branch at the time of execution of the instruction making the reference to accomplish this the processor hardware must have that capability sharing any protection mechanism must have the flexibility to allow several processes to access the same portion of main memory for example if a number of processes are executing the same program it is advantageous to allow each process to access the same copy of the program rather than have its own separate copy processes that are cooperating on some task may need to share access to the same data structure the memory management system must therefore allow controlled access to shared areas of memory without compromising essential protection again we will see that the mechanisms used to support relocation support sharing capabilities logical organization almost invariably main memory in a computer system is organized as a linear or onedimensional address space consisting of a sequence of bytes or words secondary memory at its physical level is similarly organized while this organization closely mirrors the actual machine hardware it does not correspond to the way in which programs are typically constructed most programs are organized into modules some of which are unmodifiable read only execute only and some of which contain data that may be modified if the operating system and computer hardware can effectively deal with user programs and data in the form of modules of some sort then a number of advantages can be realized modules can be written and compiled independently with all references from one module to another resolved by the system at run time with modest additional overhead different degrees of protection read only execute only can be given to different modules it is possible to introduce mechanisms by which modules can be shared among processes the advantage of providing sharing on a module level is that this corresponds to the users way of viewing the problem and hence it is easy for the user to specify the sharing that is desired the tool that most readily satisfies these requirements is segmentation which is one of the memory management techniques explored in this chapter physical organization as we discussed in section computer memory is organized into at least two levels referred to as main memory and secondary memory main memory provides fast access at relatively high cost in addition main memory is volatile that is it does not provide permanent storage secondary memory is slower and cheaper than main memory and is usually not volatile thus secondary memory of large capacity can be provided for longterm storage of programs and data while a smaller main memory holds programs and data currently in use in this twolevel scheme the organization of the flow of information between main and secondary memory is a major system concern the responsibility for this flow could be assigned to the individual programmer but this is impractical and undesirable for two reasons the main memory available for a program plus its data may be insufficient in that case the programmer must engage in a practice known as overlaying in which the program and data are organized in such a way that various modules can be assigned the same region of memory with a main program responsible for switching the modules in and out as needed even with the aid of compiler tools overlay programming wastes programmer time in a multiprogramming environment the programmer does not know at the time of coding how much space will be available or where that space will be it is clear then that the task of moving information between the two levels of memory should be a system responsibility this task is the essence of memory management instruction execution a program to be executed by a processor consists of a set of instructions stored in memory in its simplest form instruction processing consists of two steps the processor reads fetches instructions from memory one at a time and executes each instruction program execution consists of repeating the process of instruction fetch and instruction execution instruction execution may involve several operations and depends on the nature of the instruction the processing required for a single instruction is called an instruction cycle using a simplified twostep description the instruction cycle is depicted in figure the two steps are referred to as the fetch stage and the execute stage program execution halts only if the processor is turned off some sort of unrecoverable error occurs or a program instruction that halts the processor is encountered at the beginning of each instruction cycle the processor fetches an instruction from memory typically the program counter pc holds the address of the next instruction to be fetched unless instructed otherwise the processor always increments the pc after each instruction fetch so that it will fetch the next instruction in sequence ie the instruction located at the next higher memory address for example consider a simplified computer in which each instruction occupies one bit word of memory assume that the program counter is set to location the processor will next fetch the instruction at location on succeeding instruction cycles it will fetch instructions from locations and so on this sequence may be altered as explained subsequently the fetched instruction is loaded into the instruction register ir the instruction contains bits that specify the action the processor is to take the processor interprets the instruction and performs the required action in general these actions fall into four categories processormemory data may be transferred from processor to memory or from memory to processor processorio data may be transferred to or from a peripheral device by transferring between the processor and an io module data processing the processor may perform some arithmetic or logic operation on data control an instruction may specify that the sequence of execution be altered for example the processor may fetch an instruction from location which fetch stage execute stage start fetch next execute halt instruction instruction figure basic instruction cycle opcode address a instruction format s magnitude b integer format program counter pc address of instruction instruction register ir instruction being executed accumulator ac temporary storage c internal cpu registers load ac from memory store ac to memory add to ac from memory d partial list of opcodes figure characteristics of a hypothetical machine specifies that the next instruction will be from location the processor sets the program counter to thus on the next fetch stage the instruction will be fetched from location rather than an instructions execution may involve a combination of these actions consider a simple example using a hypothetical processor that includes the characteristics listed in figure the processor contains a single data register called the accumulator ac both instructions and data are bits long and memory is organized as a sequence of bit words the instruction format provides bits for the opcode allowing as many as different opcodes represented by a single hexadecimal digit the opcode defines the operation the processor is to perform with the remaining bits of the instruction format up to k words of memory denoted by three hexadecimal digits can be directly addressed figure illustrates a partial program execution showing the relevant portions of memory and processor registers the program fragment shown adds the contents of the memory word at address to the contents of the memory word at address and stores the result in the latter location three instructions which can be described as three fetch and three execute stages are required the pc contains the address of the first instruction this instruction the value in hexadecimal is loaded into the ir and the pc is incremented note that this process involves the use of a memory address register mar a basic refresher on number systems decimal binary hexadecimal can be found at the computer science student resource site at computersciencestudentcom fetch stage execute stage memory cpu registers memory cpu registers pc pc ac ac ir ir step step memory cpu registers memory cpu registers pc pc ac ac ir ir step step memory cpu registers memory cpu registers pc pc ac ac ir ir step step figure example of program execution contents of memory and registers in hexadecimal and a memory buffer register mbr for simplicity these intermediate registers are not shown the first bits first hexadecimal digit in the ir indicate that the ac is to be loaded from memory the remaining bits three hexadecimal digits specify the address which is the next instruction is fetched from location and the pc is incremented the old contents of the ac and the contents of location are added and the result is stored in the ac the next instruction is fetched from location and the pc is incremented the contents of the ac are stored in location in this example three instruction cycles each consisting of a fetch stage and an execute stage are needed to add the contents of location to the contents of with a more complex set of instructions fewer instruction cycles would be needed most modern processors include instructions that contain more than one address thus the execution stage for a particular instruction may involve more than one reference to memory also instead of memory references an instruction may specify an io operation memory partitioning the principal operation of memory management is to bring processes into main memory for execution by the processor in almost all modern multiprogramming systems this involves a sophisticated scheme known as virtual memory virtual memory is in turn based on the use of one or both of two basic techniques segmentation and paging before we can look at these virtual memory techniques we must prepare the ground by looking at simpler techniques that do not involve virtual memory table summarizes all the techniques examined in this chapter and the next one of these techniques partitioning has been used in several variations in some nowobsolete operating systems the other two techniques simple paging and simple segmentation are not used by themselves however it will clarify the discussion of virtual memory if we look first at these two techniques in the absence of virtual memory considerations fixed partitioning in most schemes for memory management we can assume that the os occupies some fixed portion of main memory and that the rest of main memory is available for use by multiple processes the simplest scheme for managing this available memory is to partition it into regions with fixed boundaries partition sizes figure shows examples of two alternatives for fixed partitioning one possibility is to make use of equalsize partitions in this case any process whose size is less than or equal to the partition size can be loaded into table memory management techniques technique description strengths weaknesses fixed main memory is divided into simple to implement little inefficient use of memory partitioning a number of static partitions operating system overhead due to internal fragmentaat system generation time tion maximum number of a process may be loaded active processes is fixed into a partition of equal or greater size dynamic partitions are created no internal fragmentation inefficient use of processor partitioning dynamically so that each more efficient use of main due to the need for comprocess is loaded into a memory paction to counter external partition of exactly the same fragmentation size as that process simple paging main memory is divided no external fragmentation a small amount of internal into a number of equalsize fragmentation frames each process is divided into a number of equalsize pages of the same length as frames a process is loaded by loading all of its pages into available not necessarily contiguous frames simple each process is divided into no internal fragmentation external fragmentation segmentation a number of segments a improved memory utilizaprocess is loaded by loadtion and reduced overhead ing all of its segments into compared to dynamic dynamic partitions that need partitioning not be contiguous virtual memory as with simple paging no external fragmentation overhead of complex paging except that it is not necessary higher degree of multipromemory management to load all of the pages of a gramming large virtual process nonresident pages address space that are needed are brought in later automatically virtual memory as with simple segmentano internal fragmentation overhead of complex segmentation tion except that it is not higher degree of multipromemory management necessary to load all of gramming large virtual the segments of a process address space protection nonresident segments that and sharing support are needed are brought in later automatically any available partition if all partitions are full and no process is in the ready or running state the operating system can swap a process out of any of the partitions and load in another process so that there is some work for the processor there are two difficulties with the use of equalsize fixed partitions a program may be too big to fit into a partition in this case the programmer must design the program with the use of overlays so that only a portion of the program need be in main memory at any one time when a module is needed operating system operating system m m m m m m m m m m m m m m m m a equalsize partitions b unequalsize partitions figure example of fixed partitioning of a mbyte memory that is not present the users program must load that module into the programs partition overlaying whatever programs or data are there main memory utilization is extremely inefficient any program no matter how small occupies an entire partition in our example there may be a program whose length is less than mbytes yet it occupies an mbyte partition whenever it is swapped in this phenomenon in which there is wasted space internal to a partition due to the fact that the block of data loaded is smaller than the partition is referred to as internal fragmentation both of these problems can be lessened though not solved by using unequalsize partitions figure b in this example programs as large as mbytes can be accommodated without overlays partitions smaller than mbytes allow smaller programs to be accommodated with less internal fragmentation placement algorithm with equalsize partitions the placement of processes in memory is trivial as long as there is any available partition a process can be loaded into that partition because all partitions are of equal size it does not matter which partition is used if all partitions are occupied with processes that are not ready to run then one of these processes must be swapped out to make room for a new process which one to swap out is a scheduling decision this topic is explored in part four with unequalsize partitions there are two possible ways to assign processes to partitions the simplest way is to assign each process to the smallest partition within which it will fit in this case a scheduling queue is needed for each partition to hold swappedout processes destined for that partition figure a the advantage of this approach is that processes are always assigned in such a way as to minimize wasted memory within a partition internal fragmentation although this technique seems optimum from the point of view of an individual partition it is not optimum from the point of view of the system as a whole in figure b for example consider a case in which there are no processes with a size between and m at a certain point in time in that case the m partition will remain unused even though some smaller process could have been assigned to it thus a preferable approach would be to employ a single queue for all processes figure b when it is time to load a process into main memory the smallest available partition that will hold the process is selected if all partitions are occupied then a swapping decision must be made preference might be given to swapping out of the smallest partition that will hold the incoming process it is also possible to operating operating system system new new processes processes a one process queue per partition b single queue figure memory assignment for fixed partitioning this assumes that one knows the maximum amount of memory that a process will require this is not always the case if it is not known how large a process may become the only alternatives are an overlay scheme or the use of virtual memory consider other factors such as priority and a preference for swapping out blocked processes versus ready processes the use of unequalsize partitions provides a degree of flexibility to fixed partitioning in addition it can be said that fixedpartitioning schemes are relatively simple and require minimal os software and processing overhead however there are disadvantages the number of partitions specified at system generation time limits the number of active not suspended processes in the system because partition sizes are preset at system generation time small jobs will not utilize partition space efficiently in an environment where the main storage requirement of all jobs is known beforehand this may be reasonable but in most cases it is an inefficient technique the use of fixed partitioning is almost unknown today one example of a successful operating system that did use this technique was an early ibm mainframe operating system osmft multiprogramming with a fixed number of tasks dynamic partitioning to overcome some of the difficulties with fixed partitioning an approach known as dynamic partitioning was developed again this approach has been supplanted by more sophisticated memory management techniques an important operating system that used this technique was ibms mainframe operating system osmvt multiprogramming with a variable number of tasks with dynamic partitioning the partitions are of variable length and number when a process is brought into main memory it is allocated exactly as much memory as it requires and no more an example using mbytes of main memory is shown in figure initially main memory is empty except for the os a the first three processes are loaded in starting where the operating system ends and occupying just enough space for each process b c d this leaves a hole at the end of memory that is too small for a fourth process at some point none of the processes in memory is ready the operating system swaps out process e which leaves sufficient room to load a new process process f because process is smaller than process another small hole is created later a point is reached at which none of the processes in main memory is ready but process in the readysuspend state is available because there is insufficient room in memory for process the operating system swaps process out g and swaps process back in h as this example shows this method starts out well but eventually it leads to a situation in which there are a lot of small holes in memory as time goes on memory becomes more and more fragmented and memory utilization declines this phenomenon is referred to as external fragmentation indicating that the memory that is external to all partitions becomes increasingly fragmented this is in contrast to internal fragmentation referred to earlier one technique for overcoming external fragmentation is compaction from time to time the os shifts the processes so that they are contiguous and so that all of the free memory is together in one block for example in figure h compaction operating m operating operating operating system system system system process m process m process m m process m process m m m process m m a b c d operating operating operating operating system system system system process process process m m m m m m process m process m process m m m m process m process m process m process m m m m m e f g h figure the effect of dynamic partitioning will result in a block of free memory of length m this may well be sufficient to load in an additional process the difficulty with compaction is that it is a timeconsuming procedure and wasteful of processor time note that compaction implies the need for a dynamic relocation capability that is it must be possible to move a program from one region to another in main memory without invalidating the memory references in the program see appendix a placement algorithm because memory compaction is time consuming the os designer must be clever in deciding how to assign processes to memory how to plug the holes when it is time to load or swap a process into main memory and if there is more than one free block of memory of sufficient size then the operating system must decide which free block to allocate three placement algorithms that might be considered are bestfit firstfit and nextfit all of course are limited to choosing among free blocks of main memory that are equal to or larger than the process to be brought in bestfit chooses the block that is closest in size to the request firstfit begins to scan memory from the m m m first fit m m m best fit last m allocated m block k m m m m allocated block free block m possible new allocation m next fit m m a before b after figure example memory configuration before and after allocation of mbyte block beginning and chooses the first available block that is large enough nextfit begins to scan memory from the location of the last placement and chooses the next available block that is large enough figure a shows an example memory configuration after a number of placement and swappingout operations the last block that was used was a mbyte block from which a mbyte partition was created figure b shows the difference between the best first and nextfit placement algorithms in satisfying a mbyte allocation request bestfit will search the entire list of available blocks and make use of the mbyte block leaving a mbyte fragment firstfit results in a mbyte fragment and nextfit results in a mbyte fragment which of these approaches is best will depend on the exact sequence of process swappings that occurs and the size of those processes however some general comments can be made see also bren shor and bays the firstfit algorithm is not only the simplest but usually the best and fastest as well the nextfit algorithm tends to produce slightly worse results than the firstfit the nextfit algorithm will more frequently lead to an allocation from a free block at the end of memory the result is that the largest block of free memory which usually appears at the end of the memory space is quickly broken up into small fragments thus compaction may be required more frequently with nextfit on the other hand the firstfit algorithm may litter the front end with small free partitions that need to be searched over on each subsequent firstfit pass the bestfit algorithm despite its name is usually the worst performer because this algorithm looks for the smallest block that will satisfy the requirement it guarantees that the fragment left behind is as small as possible although each memory request always wastes the smallest amount of memory the result is that main memory is quickly littered by blocks too small to satisfy memory allocation requests thus memory compaction must be done more frequently than with the other algorithms replacement algorithm in a multiprogramming system using dynamic partitioning there will come a time when all of the processes in main memory are in a blocked state and there is insufficient memory even after compaction for an additional process to avoid wasting processor time waiting for an active process to become unblocked the os will swap one of the processes out of main memory to make room for a new process or for a process in a readysuspend state therefore the operating system must choose which process to replace because the topic of replacement algorithms will be covered in some detail with respect to various virtual memory schemes we defer a discussion of replacement algorithms until then buddy system both fixed and dynamic partitioning schemes have drawbacks a fixed partitioning scheme limits the number of active processes and may use space inefficiently if there is a poor match between available partition sizes and process sizes a dynamic partitioning scheme is more complex to maintain and includes the overhead of compaction an interesting compromise is the buddy system knut pete in a buddy system memory blocks are available of size k words l k u where l smallest size block that is allocated u largest size block that is allocated generally u is the size of the entire memory available for allocation to begin the entire space available for allocation is treated as a single block of size u if a request of size s such that u s u is made then the entire block is allocated otherwise the block is split into two equal buddies of size u if u s u then the request is allocated to one of the two buddies otherwise one of the buddies is split in half again this process continues until the smallest block greater than or equal to s is generated and allocated to the request at any time the buddy system maintains a list of holes unallocated blocks of each size i a hole may be removed from the i list by splitting it in half to create two buddies of size i in the i list whenever a pair of buddies on the i list both become unallocated they are removed from that list and coalesced into a single block on the i list presented with a request for an allocation of size k such that i k i the following recursive algorithm is used to find a hole of size i void getholeint i if i u failure if ilist empty getholei split hole into buddies put buddies on ilist take first hole on ilist figure gives an example using a mbyte initial block the first request a is for kbytes for which a k block is needed the initial block is divided into two k buddies the first of these is divided into two k buddies and the first of these is divided into two k buddies one of which is allocated to a the next request b requires a k block such a block is already available and is allocated the process continues with splitting and coalescing occurring as needed note that when e is released two k buddies are coalesced into a k block which is immediately coalesced with its buddy figure shows a binary tree representation of the buddy allocation immediately after the release b request the leaf nodes represent the current partitioning of the memory if two buddies are leaf nodes then at least one must be allocated otherwise they would be coalesced into a larger block mbyte block m request k a k k k k request k a k k b k k request k a k c k k b k k request k a k c k k b k d k k release b a k c k k k d k k release a k c k k k d k k request k e k c k k k d k k release c e k k k d k k release e k d k k release d m figure example of buddy system m k k k k a k c k k k d k k leaf node for leaf node for nonleaf node allocated block unallocated block figure tree representation of buddy system the buddy system is a reasonable compromise to overcome the disadvantages of both the fixed and variable partitioning schemes but in contemporary operating systems virtual memory based on paging and segmentation is superior however the buddy system has found application in parallel systems as an efficient means of allocation and release for parallel programs eg see john a modified form of the buddy system is used for unix kernel memory allocation described in chapter relocation before we consider ways of dealing with the shortcomings of partitioning we must clear up one loose end which relates to the placement of processes in memory when the fixed partition scheme of figure a is used we can expect that a process will always be assigned to the same partition that is whichever partition is selected when a new process is loaded will always be used to swap that process back into memory after it has been swapped out in that case a simple relocating loader such as is described in appendix a can be used when the process is first loaded all relative memory references in the code are replaced by absolute main memory addresses determined by the base address of the loaded process in the case of equalsize partitions figure and in the case of a single process queue for unequalsize partitions figure b a process may occupy different partitions during the course of its life when a process image is first created it is loaded into some partition in main memory later the process may be swapped out when it is subsequently swapped back in it may be assigned to a different partition than the last time the same is true for dynamic partitioning observe in figure c and figure h that process occupies two different regions of memory on the two occasions when it is brought in furthermore when compaction is used processes are shifted while they are in main memory thus the locations of instructions and data referenced by a process are not fixed they will change each time a process is swapped in or shifted to solve this problem a distinction is made among several types of addresses a logical address is a reference to a memory location independent of the current assignment of data to memory a translation must be made to a physical address before the memory access can be achieved a relative address is a particular example of logical address in which the address is expressed as a location relative to some known point usually a value in a processor register a physical address or absolute address is an actual location in main memory programs that employ relative addresses in memory are loaded using dynamic runtime loading see appendix a for a discussion typically all of the memory references in the loaded process are relative to the origin of the program thus a hardware mechanism is needed for translating relative addresses to physical main memory addresses at the time of execution of the instruction that contains the reference figure shows the way in which this address translation is typically accomplished when a process is assigned to the running state a special processor register sometimes called the base register is loaded with the starting address in main memory of the program there is also a bounds register that indicates the ending location relative address base register process control block adder program absolute address bounds register comparator data interrupt to operating system stack process image in main memory figure hardware support for relocation of the program these values must be set when the program is loaded into memory or when the process image is swapped in during the course of execution of the process relative addresses are encountered these include the contents of the instruction register instruction addresses that occur in branch and call instructions and data addresses that occur in load and store instructions each such relative address goes through two steps of manipulation by the processor first the value in the base register is added to the relative address to produce an absolute address second the resulting address is compared to the value in the bounds register if the address is within bounds then the instruction execution may proceed otherwise an interrupt is generated to the operating system which must respond to the error in some fashion the scheme of figure allows programs to be swapped in and out of memory during the course of execution it also provides a measure of protection each process image is isolated by the contents of the base and bounds registers and safe from unwanted accesses by other processes paging both unequal fixedsize and variablesize partitions are inefficient in the use of memory the former results in internal fragmentation the latter in external fragmentation suppose however that main memory is partitioned into equal fixedsize chunks that are relatively small and that each process is also divided into small fixedsize chunks of the same size then the chunks of a process known as pages could be assigned to available chunks of memory known as frames or page frames we show in this section that the wasted space in memory for each process is due to internal fragmentation consisting of only a fraction of the last page of a process there is no external fragmentation figure illustrates the use of pages and frames at a given point in time some of the frames in memory are in use and some are free a list of free frames is maintained by the os process a stored on disk consists of four pages when it is time to load this process the os finds four free frames and loads the four pages of process a into the four frames figure b process b consisting of three pages and process c consisting of four pages are subsequently loaded then process b is suspended and is swapped out of main memory later all of the processes in main memory are blocked and the os needs to bring in a new process process d which consists of five pages now suppose as in this example that there are not sufficient unused contiguous frames to hold the process does this prevent the operating system from loading d the answer is no because we can once again use the concept of logical address a simple base address register will no longer suffice rather the operating system maintains a page table for each process the page table shows the frame location for each page of the process within the program each logical address consists of a page number and an offset within the page recall that in the case of simple partition a logical address is the location of a word relative to the beginning of the program the processor translates that into a physical address with paging the logicaltophysical address translation is still done by processor hardware now the processor must know how to access the page table of the current process presented with a logical frame main memory main memory main memory number a a a a a a a a b b b a fifteen available frames b load process a c load process b main memory main memory main memory a a a a a a a a a a a a b d b d b d c c c c c c c c c c c c d d d load process c e swap out b f load process d figure assignment of process to free frames address page number offset the processor uses the page table to produce a physical address frame number offset continuing our example the five pages of process d are loaded into frames and figure shows the various page tables at this time a page table contains one entry for each page of the process so that the table is easily indexed by the page number starting at page each page table entry contains the number of the frame in main memory if any that holds the corresponding page in addition the os maintains a single freeframe list of all the frames in main memory that are currently unoccupied and available for pages thus we see that simple paging as described here is similar to fixed partitioning the differences are that with paging the partitions are rather small a free frame process b list process a page table process c page table page table process d page table figure data structures for the example of figure at time epoch f program may occupy more than one partition and these partitions need not be contiguous to make this paging scheme convenient let us dictate that the page size hence the frame size must be a power of with the use of a page size that is a power of it is easy to demonstrate that the relative address which is defined with reference to the origin of the program and the logical address expressed as a page number and offset are the same an example is shown in figure in this example bit addresses are used and the page size is k bytes the relative address in binary form is with a page size of k an offset field of bits is needed leaving bits for the page number thus a program can consist of a maximum of pages of k bytes each as figure b shows relative address corresponds to an offset of on page which yields the same bit number the consequences of using a page size that is a power of are twofold first the logical addressing scheme is transparent to the programmer the assembler and logical address logical address relative address page offset segment offset page segment bytes user process bytes page segment bytes page internal fragmentation a partitioning c segmentation b paging page size k figure logical addresses the linker each logical address page number offset of a program is identical to its relative address second it is a relatively easy matter to implement a function in hardware to perform dynamic address translation at run time consider an address of n m bits where the leftmost n bits are the page number and the rightmost m bits are the offset in our example figure b n and m the following steps are needed for address translation extract the page number as the leftmost n bits of the logical address use the page number as an index into the process page table to find the frame number k the starting physical address of the frame is k m and the physical address of the referenced byte is that number plus the offset this physical address need not be calculated it is easily constructed by appending the frame number to the offset in our example we have the logical address which is page number offset suppose that this page is residing in main memory frame binary then the physical address is frame number offset figure a bit logical address bit page bit offset process page table bit physical address a paging bit logical address bit segment bit offset length base process segment table bit physical address b segmentation figure examples of logicaltophysical address translation to summarize with simple paging main memory is divided into many small equalsize frames each process is divided into framesize pages smaller processes require fewer pages larger processes require more when a process is brought in all of its pages are loaded into available frames and a page table is set up this approach solves many of the problems inherent in partitioning segmentation a user program can be subdivided using segmentation in which the program and its associated data are divided into a number of segments it is not required that all segments of all programs be of the same length although there is a maximum segment length as with paging a logical address using segmentation consists of two parts in this case a segment number and an offset because of the use of unequalsize segments segmentation is similar to dynamic partitioning in the absence of an overlay scheme or the use of virtual memory it would be required that all of a programs segments be loaded into memory for execution the difference compared to dynamic partitioning is that with segmentation a program may occupy more than one partition and these partitions need not be contiguous segmentation eliminates internal fragmentation but like dynamic partitioning it suffers from external fragmentation however because a process is broken up into a number of smaller pieces the external fragmentation should be less whereas paging is invisible to the programmer segmentation is usually visible and is provided as a convenience for organizing programs and data typically the programmer or compiler will assign programs and data to different segments for purposes of modular programming the program or data may be further broken down into multiple segments the principal inconvenience of this service is that the programmer must be aware of the maximum segment size limitation another consequence of unequalsize segments is that there is no simple relationship between logical addresses and physical addresses analogous to paging a simple segmentation scheme would make use of a segment table for each process and a list of free blocks of main memory each segment table entry would have to give the starting address in main memory of the corresponding segment the entry should also provide the length of the segment to assure that invalid addresses are not used when a process enters the running state the address of its segment table is loaded into a special register used by the memory management hardware consider an address of n m bits where the leftmost n bits are the segment number and the rightmost m bits are the offset in our example figure c n and m thus the maximum segment size is the following steps are needed for address translation extract the segment number as the leftmost n bits of the logical address use the segment number as an index into the process segment table to find the starting physical address of the segment compare the offset expressed in the rightmost m bits to the length of the segment if the offset is greater than or equal to the length the address is invalid the desired physical address is the sum of the starting physical address of the segment plus the offset in our example we have the logical address which is segment number offset suppose that this segment is residing in main memory starting at physical address then the physical address is figure b to summarize with simple segmentation a process is divided into a number of segments that need not be of equal size when a process is brought in all of its segments are loaded into available regions of memory and a segment table is set up security issues main memory and virtual memory are system resources subject to security threats and for which security countermeasures need to be taken the most obvious security requirement is the prevention of unauthorized access to the memory contents of processes if a process has not declared a portion of its memory to be sharable then no other process should have access to the contents of that portion of memory if a process declares that a portion of memory may be shared by other designated processes then the security service of the os must ensure that only the designated processes have access the security threats and countermeasures discussed in chapter are relevant to this type of memory protection in this section we summarize another threat that involves memory protection part seven provides more detail buffer overflow attacks one serious security threat related to memory management remains to be introduced buffer overflow also known as a buffer overrun which is defined in the nist national institute of standards and technology glossary of key information security terms as follows buffer overrun a condition at an interface under which more input can be placed into a buffer or dataholding area than the capacity allocated overwriting other information attackers exploit such a condition to crash a system or to insert specially crafted code that allows them to gain control of the system a buffer overflow can occur as a result of a programming error when a process attempts to store data beyond the limits of a fixedsized buffer and consequently overwrites adjacent memory locations these locations could hold other program variables or parameters or program control flow data such as return addresses and pointers to previous stack frames the buffer could be located on the stack in the heap or in the data section of the process the consequences of this error include corruption of data used by the program unexpected transfer of control in the program possibly memory access violations and very likely eventual program termination when done deliberately as part of an attack on a system the transfer of control could be to code of the attackers choosing resulting in the ability to execute arbitrary code with the privileges of the attacked process buffer overflow attacks are one of the most prevalent and dangerous types of security attacks to illustrate the basic operation of a common type of buffer overflow known as stack overflow consider the c main function given in figure a this contains three variables valid str and str whose values will typically be saved in adjacent memory locations their order and location depends on the type of variable local or global the language and compiler used and the target machine architecture for this example we assume that they are saved in consecutive memory locations from highest to lowest as shown in figure this is typically the case for local variables in a c function on common processor architectures such as the intel pentium family the purpose of the code fragment is to call the function nexttagstr to copy into str some expected tag value int mainint argc char argv int valid false char str char str nexttagstr getsstr if strncmpstr str valid true printfbuffer strs strs validdn str str valid a basic buffer overflow c code cc g o buffer bufferc buffer start buffer strstart strstart valid buffer evilinputvalue buffer strtvalue strevilinputvalue valid buffer badinputbadinput buffer strbadinput strbadinputbadinput valid b basic buffer overflow example runs figure basic buffer overflow example in this example the flag variable is saved as an integer rather than a boolean this is done both because it is the classic c style and to avoid issues of word alignment in its storage the buffers are deliberately small to accentuate the buffer overflow issue being illustrated address and data values are specified in hexadecimal in this and related figures data values are also shown in ascii where appropriate memory before after contains address gets str gets str value of bffffbf fcffbf fcffbf argv bffffbf argc bffffbec cbd cbd return addr bffffbe fcffbf fcffbf old base ptr bffffbe valid bffffbe d d bffffbdc e str t nput bffffbd str star badi bffffbd e str nput bffffbd str v badi figure basic buffer overflow stack values lets assume this will be the string start it then reads the next line from the standard input for the program using the c library gets function and then compares the string read with the expected tag if the next line did indeed contain just the string start this comparison would succeed and the variable valid would be set to true this case is shown in the first of the three example program runs in figure b any other input tag would leave it with the value false such a code fragment might be used to parse some structured network protocol interaction or formatted text file the problem with this code exists because the traditional c library gets function does not include any checking on the amount of data copied it reads the next line of text from the programs standard input up until the first newline character occurs and copies it into the supplied buffer followed by the null terminator in c the logical values false and true are simply integers with the values and or indeed any nonzero value respectively symbolic defines are often used to map these symbolic names to their underlying value as was done in this program the newline nl or linefeed lf character is the standard end of line terminator for unix systems and hence for c and is the character with the ascii value xa used with c strings if more than seven characters are present on the input line when read in they will along with the terminating null character require more room than is available in the str buffer consequently the extra characters will overwrite the values of the adjacent variable str in this case for example if the input line contained evilinputvalue the result will be that str will be overwritten with the characters tvalue and str will use not only the eight characters allocated to it but seven more from str as well this can be seen in the second example run in figure b the overflow has resulted in corruption of a variable not directly used to save the input because these strings are not equal valid also retains the value false further if or more characters were input additional memory locations would be overwritten the preceding example illustrates the basic behavior of a buffer overflow at its simplest any unchecked copying of data into a buffer could result in corruption of adjacent memory locations which may be other variables or possibly program control addresses and data even this simple example could be taken further knowing the structure of the code processing it an attacker could arrange for the overwritten value to set the value in str equal to the value placed in str resulting in the subsequent comparison succeeding for example the input line could be the string badinputbadinput this results in the comparison succeeding as shown in the third of the three example program runs in figure b and illustrated in figure with the values of the local variables before and after the call to gets note also that the terminating null for the input string was written to the memory location following str this means the flow of control in the program will continue as if the expected tag was found when in fact the tag read was something completely different this will almost certainly result in program behavior that was not intended how serious this is depends very much on the logic in the attacked program one dangerous possibility occurs if instead of being a tag the values in these buffers were an expected and supplied password needed to access privileged features if so the buffer overflow provides the attacker with a means of accessing these features without actually knowing the correct password to exploit any type of buffer overflow such as those we have illustrated here the attacker needs to identify a buffer overflow vulnerability in some program that can be triggered using externally sourced data under the attackers control and to understand how that buffer will be stored in the processes memory and hence the potential for corrupting adjacent memory locations and potentially altering the flow of execution of the program identifying vulnerable programs may be done by inspection of program source tracing the execution of programs as they process oversized input or using tools such as fuzzing which we discuss in part seven to automatically identify potentially strings in c are stored in an array of characters and terminated with the null character which has the ascii value x any remaining locations in the array are undefined and typically contain whatever value was previously saved in that area of memory this can be clearly seen in the value in the variable str in the before column of figure vulnerable programs what the attacker does with the resulting corruption of memory varies considerably depending on what values are being overwritten defending against buffer overflows finding and exploiting a stack buffer overflow is not that difficult the large number of exploits over the previous couple of decades clearly illustrates this there is consequently a need to defend systems against such attacks by either preventing them or at least detecting and aborting such attacks countermeasures can be broadly classified into two categories compiletime defenses which aim to harden programs to resist attacks in new programs runtime defenses which aim to detect and abort attacks in existing programs while suitable defenses have been known for a couple of decades the very large existing base of vulnerable software and systems hinders their deployment hence the interest in runtime defenses which can be deployed in operating systems and updates and can provide some protection for existing vulnerable programs recommended reading because partitioning has been supplanted by virtual memory techniques most os books offer only cursory coverage one of the more complete and interesting treatments is in mile a thorough discussion of partitioning strategies is found in knut the topics of linking and loading are covered in many books on program development computer architecture and operating systems a particularly detailed treatment is beck clar also contains a good discussion a thorough practical discussion of this topic with numerous os examples is levi beck beck l system software reading ma addisonwesley clar clarke d and merusi d system software programming the way things work upper saddle river nj prentice hall knut knuth d the art of computer programming volume fundamental algorithms reading ma addisonwesley levi levine j linkers and loaders san francisco morgan kaufmann mile milenkovic m operating systems concepts and design new york mcgrawhill key terms review questions and problems key terms absolute loading linkage editor physical address buddy system linking physical organization compaction loading protection dynamic linking logical address relative address dynamic partitioning logical organization relocatable loading dynamic runtime loading memory management relocation external fragmentation page segment fixed partitioning page table segmentation frame paging sharing internal fragmentation partitioning review questions what requirements is memory management intended to satisfy why is the capability to relocate processes desirable why is it not possible to enforce memory protection at compile time what are some reasons to allow two or more processes to all have access to a particular region of memory in a fixedpartitioning scheme what are the advantages of using unequalsize partitions what is the difference between internal and external fragmentation what are the distinctions among logical relative and physical addresses what is the difference between a page and a frame what is the difference between a page and a segment problems in section we listed five objectives of memory management and in section we listed five requirements argue that each list encompasses all of the concerns addressed in the other consider a fixed partitioning scheme with equalsize partitions of bytes and a total main memory size of bytes a process table is maintained that includes a pointer to a partition for each resident process how many bits are required for the pointer consider a dynamic partitioning scheme show that on average the memory contains half as many holes as segments to implement the various placement algorithms discussed for dynamic partitioning section a list of the free blocks of memory must be kept for each of the three methods discussed bestfit firstfit nextfit what is the average length of the search another placement algorithm for dynamic partitioning is referred to as worstfit in this case the largest free block of memory is used for bringing in a process a discuss the pros and cons of this method compared to first next and bestfit b what is the average length of the search for worstfit this diagram shows an example of memory configuration under dynamic partitioning after a number of placement and swappingout operations have been carried out addresses go from left to right gray areas indicate blocks occupied by processes white areas indicate free memory blocks the last process placed is mbyte and is marked with an x only one process was swapped out after that m m x m m m m m a what was the maximum size of the swapped out process b what was the size of the free block just before it was partitioned by x c a new mbyte allocation request must be satisfied next indicate the intervals of memory where a partition will be created for the new process under the following four placement algorithms bestfit firstfit nextfit worstfit for each algorithm draw a horizontal segment under the memory strip and label it clearly a mbyte block of memory is allocated using the buddy system a show the results of the following sequence in a figure similar to figure request request request return a request return b return d return c b show the binary tree representation following return b consider a buddy system in which a particular block under the current allocation has an address of a if the block is of size what is the binary address of its buddy b if the block is of size what is the binary address of its buddy let buddykx address of the buddy of the block of size k whose address is x write a general expression for buddykx the fibonacci sequence is defined as follows f f fn fn fn n a could this sequence be used to establish a buddy system b what would be the advantage of this system over the binary buddy system described in this chapter during the course of execution of a program the processor will increment the contents of the instruction register program counter by one word after each instruction fetch but will alter the contents of that register if it encounters a branch or call instruction that causes execution to continue elsewhere in the program now consider figure there are two alternatives with respect to instruction addresses maintain a relative address in the instruction register and do the dynamic address translation using the instruction register as input when a successful branch or call is encountered the relative address generated by that branch or call is loaded into the instruction register maintain an absolute address in the instruction register when a successful branch or call is encountered dynamic address translation is employed with the results stored in the instruction register which approach is preferable consider a simple paging system with the following parameters bytes of physical memory page size of bytes pages of logical address space a how many bits are in a logical address b how many bytes in a frame c how many bits in the physical address specify the frame d how many entries in the page table e how many bits in each page table entry assume each page table entry contains a validinvalid bit write the binary translation of the logical address under the following hypothetical memory management schemes and explain your answer a a paging system with a address page size using a page table in which the frame number happens to be four times smaller than the page number b a segmentation system with a kaddress maximum segment size using a segment table in which bases happen to be regularly placed at real addresses segment consider a simple segmentation system that has the following segment table starting address length bytes for each of the following logical addresses determine the physical address or indicate if a segment fault occurs a b c d e consider a memory in which contiguous segments s ssn are placed in their order of creation from one end of the store to the other as suggested by the following figure s s sn hole when segment sn is being created it is placed immediately after segment sn even though some of the segments s ssn may already have been deleted when the boundary between segments in use or deleted and the hole reaches the other end of the memory the segments in use are compacted a show that the fraction of time f spent on compacting obeys the following inequality f f where k t kf s where s average length of a segment in words t average lifetime of a segment in memory references f fraction of the memory that is unused under equilibrium conditions hint find the average speed at which the boundary crosses the memory and assume that the copying of a single word requires at least two memory references b find f for f t and s appendix a loading and linking the first step in the creation of an active process is to load a program into main memory and create a process image figure figure depicts a scenario typical for most systemsthe application consists of a number of compiled or assembled modules in objectcode form these are linked to resolve any references between modules at the same time references to library routines are resolved the library routines themselves may be incorporated into the program or referenced as shared code that must be supplied by the operating system at run time in this appendix we summarize the key features of linkers and loaders for clarity in the presentation we begin with a description of the loading task when a single program module is involved no linking is required loading in figure the loader places the load module in main memory starting at location x in loading the program the addressing requirement illustrated in figure must be satisfied in general three approaches can be taken absolute loading relocatable loading dynamic runtime loading absolute loading an absolute loader requires that a given load module always be loaded into the same location in main memory thus in the load module presented to the loader all address references must be to specific or absolute main process control block program program data data object code stack process image in main memory figure the loading function static library dynamic library x module load loader linker module module dynamic runtime library linker loader module n main memory figure a linking and loading scenario memory addresses for example if x in figure is location then the first word in a load module destined for that region of memory has address the assignment of specific address values to memory references within a program can be done either by the programmer or at compile or assembly time table a there are several disadvantages to the former approach first every programmer would have to know the intended assignment strategy for placing modules into main memory second if any modifications are made to the program that involve insertions or deletions in the body of the module then all of the addresses will have to be altered accordingly it is preferable to allow memory references within programs to be expressed symbolically and then resolve those symbolic references at the time of compilation or assembly this is illustrated in figure every reference to an instruction or item of data is initially represented by a symbol in preparing the module for input to an absolute loader the assembler or compiler will convert all of these references to specific addresses in this example for a module to be loaded starting at location as shown in figure b relocatable loading the disadvantage of binding memory references to specific addresses prior to loading is that the resulting load module can only be placed in one region of main memory however when many programs share main memory it may not be desirable to decide ahead of time into which region of memory a particular module should be loaded it is better to make that decision at load time thus we need a load module that can be located anywhere in main memory to satisfy this new requirement the assembler or compiler produces not actual main memory addresses absolute addresses but addresses that are relative to some known point such as the start of the program this technique is illustrated in figure c the start of the load module is assigned the relative address and table address binding a loader binding time function programming time all actual physical addresses are directly specified by the programmer in the program itself compile or assembly time the program contains symbolic address references and these are converted to actual physical addresses by the compiler or assembler load time the compiler or assembler produces relative addresses the loader translates these to absolute addresses at the time of program loading run time the loaded program retains relative addresses these are converted dynamically to absolute addresses by processor hardware b linker linkage time function programming time no external program or data references are allowed the programmer must place into the program the source code for all subprograms that are referenced compile or assembly time the assembler must fetch the source code of every subroutine that is referenced and assemble them as a unit load module creation all object modules have been assembled using relative addresses these modules are linked together and all references are restated relative to the origin of the final load module load time external references are not resolved until the load module is to be loaded into main memory at that time referenced dynamic link modules are appended to the load module and the entire package is loaded into main or virtual memory run time external references are not resolved until the external call is executed by the processor at that time the process is interrupted and the desired module is linked to the calling program symbolic absolute relative main memory addresses addresses addresses addresses program program program x program jump x jump jump jump x x load y load load load data data data data y x a object module b absolute load module c relative load module d relative load module loaded into main memory starting at location x figure absolute and relocatable load modules all other memory references within the module are expressed relative to the beginning of the module with all memory references expressed in relative format it becomes a simple task for the loader to place the module in the desired location if the module is to be loaded beginning at location x then the loader must simply add x to each memory reference as it loads the module into memory to assist in this task the load module must include information that tells the loader where the address references are and how they are to be interpreted usually relative to the program origin but also possibly relative to some other point in the program such as the current location this set of information is prepared by the compiler or assembler and is usually referred to as the relocation dictionary dynamic runtime loading relocatable loaders are common and provide obvious benefits relative to absolute loaders however in a multiprogramming environment even one that does not depend on virtual memory the relocatable loading scheme is inadequate we have referred to the need to swap process images in and out of main memory to maximize the utilization of the processor to maximize main memory utilization we would like to be able to swap the process image back into different locations at different times thus a program once loaded may be swapped out to disk and then swapped back in at a different location this would be impossible if memory references had been bound to absolute addresses at the initial load time the alternative is to defer the calculation of an absolute address until it is actually needed at run time for this purpose the load module is loaded into main memory with all memory references in relative form figure c it is not until an instruction is actually executed that the absolute address is calculated to assure that this function does not degrade performance it must be done by special processor hardware rather than software this hardware is described in section dynamic address calculation provides complete flexibility a program can be loaded into any region of main memory subsequently the execution of the program can be interrupted and the program can be swapped out of main memory to be later swapped back in at a different location linking the function of a linker is to take as input a collection of object modules and produce a load module consisting of an integrated set of program and data modules to be passed to the loader in each object module there may be address references to locations in other modules each such reference can only be expressed symbolically in an unlinked object module the linker creates a single load module that is the contiguous joining of all of the object modules each intramodule reference must be changed from a symbolic address to a reference to a location within the overall load module for example module a in figure a contains a procedure invocation of module b when these modules are combined in the load module this symbolic reference to module b is changed to a specific reference to the location of the entry point of b within the load module linkage editor the nature of this address linkage will depend on the type of load module to be created and when the linkage occurs table b if as is relative addresses module a module a external call b jsr l reference to length l module b return l return l module b module b jsr l m call c length m lm return return lm module c module c length n lmn return return b load module a object modules figure the linking function usually the case a relocatable load module is desired then linkage is usually done in the following fashion each compiled or assembled object module is created with references relative to the beginning of the object module all of these modules are put together into a single relocatable load module with all references relative to the origin of the load module this module can be used as input for relocatable loading or dynamic runtime loading a linker that produces a relocatable load module is often referred to as a linkage editor figure illustrates the linkage editor function dynamic linker as with loading it is possible to defer some linkage functions the term dynamic linking is used to refer to the practice of deferring the linkage of some external modules until after the load module has been created thus the load module contains unresolved references to other programs these references can be resolved either at load time or run time for loadtime dynamic linking involving upper dynamic library in figure the following steps occur the load module application module to be loaded is read into memory any reference to an external module target module causes the loader to find the target module load it and alter the reference to a relative address in memory from the beginning of the application module there are several advantages to this approach over what might be called static linking it becomes easier to incorporate changed or upgraded versions of the target module which may be an operating system utility or some other generalpurpose routine with static linking a change to such a supporting module would require the relinking of the entire application module not only is this inefficient but it may be impossible in some circumstances for example in the personal computer field most commercial software is released in load module form source and object versions are not released having target code in a dynamic link file paves the way for automatic code sharing the operating system can recognize that more than one application is using the same target code because it loaded and linked that code it can use that information to load a single copy of the target code and link it to both applications rather than having to load one copy for each application it becomes easier for independent software developers to extend the functionality of a widely used operating system such as linux a developer can come up with a new function that may be useful to a variety of applications and package it as a dynamic link module with runtime dynamic linking involving lower dynamic library in figure some of the linking is postponed until execution time external references to target modules remain in the loaded program when a call is made to the absent module the operating system locates the module loads it and links it to the calling module such modules are typically shareable in the windows environment these are call dynamiclink libraries dlls thus if one process is already making use of a dynamicallylinked shared module then that module is in main memory and a new process can simply link to the alreadyloaded module the use of dlls can lead to a problem commonly referred to as dll hell dll hell occurs if two or more processes are sharing a dll module but expect different versions of the module for example an application or system function might be reinstalled and bring in with it an older version of a dll file we have seen that dynamic loading allows an entire load module to be moved around however the structure of the module is static being unchanged throughout the execution of the process and from one execution to the next however in some cases it is not possible to determine prior to execution which object modules will be required this situation is typified by transactionprocessing applications such as an airline reservation system or a banking application the nature of the transaction dictates which program modules are required and they are loaded as appropriate and linked with the main program the advantage of the use of such a dynamic linker is that it is not necessary to allocate memory for program units unless those units are referenced this capability is used in support of segmentation systems one additional refinement is possible an application need not know the names of all the modules or entry points that may be called for example a charting program may be written to work with a variety of plotters each of which is driven by a different driver package the application can learn the name of the plotter that is currently installed on the system from another process or by looking it up in a configuration file this allows the user of the application to install a new plotter that did not exist at the time the application was written interrupts virtually all computers provide a mechanism by which other modules io memory may interrupt the normal sequencing of the processor table lists the most common classes of interrupts interrupts are provided primarily as a way to improve processor utilization for example most io devices are much slower than the processor suppose that the processor is transferring data to a printer using the instruction cycle scheme of figure after each write operation the processor must pause and remain idle until the printer catches up the length of this pause may be on the order of many thousands or even millions of instruction cycles clearly this is a very wasteful use of the processor to give a specific example consider a pc that operates at ghz which would allow roughly instructions per second a typical hard disk has a rotational speed of revolutions per minute for a halftrack rotation time of ms which is million times slower than the processor figure a illustrates this state of affairs the user program performs a series of write calls interleaved with processing the solid vertical lines represent segments of code in a program code segments and refer to sequences of instructions that do not involve io the write calls are to an io routine that is a system utility and that will perform the actual io operation the io program consists of three sections a sequence of instructions labeled in the figure to prepare for the actual io operation this may include copying the data to be output into a special buffer and preparing the parameters for a device command the actual io command without the use of interrupts once this command is issued the program must wait for the io device to perform the requested function or periodically check the status or poll the io device the program might wait by simply repeatedly performing a test operation to determine if the io operation is done a sequence of instructions labeled in the figure to complete the operation this may include setting a flag indicating the success or failure of the operation table classes of interrupts program generated by some condition that occurs as a result of an instruction execution such as arithmetic overflow division by zero attempt to execute an illegal machine instruction and reference outside a users allowed memory space timer generated by a timer within the processor this allows the operating system to perform certain functions on a regular basis io generated by an io controller to signal normal completion of an operation or to signal a variety of error conditions hardware failure generated by a failure such as power failure or memory parity error a discussion of the uses of numerical prefixes such as giga and tera is contained in a supporting document at the computer science student resource site at computersciencestudentcom user io user io user io program program program program program program io io io write command write command write command a end interrupt interrupt b handler handler write write write a end end b write write write a no interrupts b interrupts short io wait c interrupts long io wait figure program flow of control without and with interrupts the dashed line represents the path of execution followed by the processor that is this line shows the sequence in which instructions are executed thus after the first write instruction is encountered the user program is interrupted and execution continues with the io program after the io program execution is complete execution resumes in the user program immediately following the write instruction because the io operation may take a relatively long time to complete the io program is hung up waiting for the operation to complete hence the user program is stopped at the point of the write call for some considerable period of time interrupts and the instruction cycle with interrupts the processor can be engaged in executing other instructions while an io operation is in progress consider the flow of control in figure b as before the user program reaches a point at which it makes a system call in the form of a write call the io program that is invoked in this case consists only of the preparation code and the actual io command after these few instructions have been executed control returns to the user program meanwhile the external device is busy accepting data from computer memory and printing it this io operation is conducted concurrently with the execution of instructions in the user program when the external device becomes ready to be serviced that is when it is ready to accept more data from the processor the io module for that external device sends an interrupt request signal to the processor the processor responds by suspending operation of the current program branching off to a routine to service user program interrupt handler interrupt i occurs here i m figure transfer of control via interrupts that particular io device known as an interrupt handler and resuming the original execution after the device is serviced the points at which such interrupts occur are indicated by in figure b note that an interrupt can occur at any point in the main program not just at one specific instruction for the user program an interrupt suspends the normal sequence of execution when the interrupt processing is completed execution resumes figure thus the user program does not have to contain any special code to accommodate interrupts the processor and the os are responsible for suspending the user program and then resuming it at the same point to accommodate interrupts an interrupt stage is added to the instruction cycle as shown in figure compare figure in the interrupt stage the processor checks to see if any interrupts have occurred indicated by the presence of an interrupt signal if no interrupts are pending the processor proceeds to the fetch stage and fetches the next instruction of the current program if an interrupt is pending fetch stage execute stage interrupt stage interrupts disabled check for start fetch next execute interrupt instruction instruction interrupts initiate interrupt enabled handler halt figure instruction cycle with interrupts the processor suspends execution of the current program and executes an interrupthandler routine the interrupthandler routine is generally part of the os typically this routine determines the nature of the interrupt and performs whatever actions are needed in the example we have been using the handler determines which io module generated the interrupt and may branch to a program that will write more data out to that io module when the interrupthandler routine is completed the processor can resume execution of the user program at the point of interruption it is clear that there is some overhead involved in this process extra instructions must be executed in the interrupt handler to determine the nature of the interrupt and to decide on the appropriate action nevertheless because of the relatively large amount of time that would be wasted by simply waiting on an io operation the processor can be employed much more efficiently with the use of interrupts to appreciate the gain in efficiency consider figure which is a timing diagram based on the flow of control in figures a and b figures b and time processor io a io wait operation operation b a io operation processor io wait operation b b with interrupts circled numbers refer to numbers in figure b a without interrupts circled numbers refer to numbers in figure a figure program timing short io wait assume that the time required for the io operation is relatively short less than the time to complete the execution of instructions between write operations in the user program the more typical case especially for a slow device such as a printer is that the io operation will take much more time than executing a sequence of user instructions figure c indicates this state of affairs in this case the user program reaches the second write call before the io operation spawned by the first call is complete the result is that the user program is hung up at that point when the preceding io operation is completed this new write call may be processed and a new io operation may be started figure shows the timing for this situation with and without the use of interrupts we can see that there is still a gain in efficiency because part of the time during which the io operation is underway overlaps with the execution of user instructions time processor io wait operation io operation processor wait processor io io wait operation operation processor wait b with interrupts circled numbers refer to numbers in figure c a without interrupts circled numbers refer to numbers in figure a figure program timing long io wait interrupt processing an interrupt triggers a number of events both in the processor hardware and in software figure shows a typical sequence when an io device completes an io operation the following sequence of hardware events occurs the device issues an interrupt signal to the processor the processor finishes execution of the current instruction before responding to the interrupt as indicated in figure the processor tests for a pending interrupt request determines that there is one and sends an acknowledgment signal to the device that issued the interrupt the acknowledgment allows the device to remove its interrupt signal the processor next needs to prepare to transfer control to the interrupt routine to begin it saves information needed to resume the current program at the point of interrupt the minimum information required is the program status word psw and the location of the next instruction to be executed which hardware software device controller or other system hardware issues an interrupt save remainder of process state information processor finishes execution of current instruction process interrupt processor signals acknowledgment of interrupt restore process state information processor pushes psw and pc onto control stack restore old psw and pc processor loads new pc value based on interrupt figure simple interrupt processing the psw contains status information about the currently running process including memory usage information condition codes and other status information such as an interrupt enabledisable bit and a kerneluser mode bit see appendix c for further discussion is contained in the program counter pc these can be pushed onto a control stack see appendix p the processor then loads the program counter with the entry location of the interrupthandling routine that will respond to this interrupt depending on the computer architecture and os design there may be a single program one for each type of interrupt or one for each device and each type of interrupt if there is more than one interrupthandling routine the processor must determine which one to invoke this information may have been included in the original interrupt signal or the processor may have to issue a request to the device that issued the interrupt to get a response that contains the needed information once the program counter has been loaded the processor proceeds to the next instruction cycle which begins with an instruction fetch because the instruction fetch is determined by the contents of the program counter control is transferred to the interrupthandler program the execution of this program results in the following operations at this point the program counter and psw relating to the interrupted program have been saved on the control stack however there is other information that is considered part of the state of the executing program in particular the contents of the processor registers need to be saved because these registers may be used by the interrupt handler so all of these values plus any other state information need to be saved typically the interrupt handler will begin by saving the contents of all registers on the stack other state information that must be saved is discussed in chapter figure a shows a simple example in this case a user program is interrupted after the instruction at location n the contents of all of the registers plus the address of the next instruction n a total of m words are pushed onto the control stack the stack pointer is updated to point to the new top of stack and the program counter is updated to point to the beginning of the interrupt service routine the interrupt handler may now proceed to process the interrupt this includes an examination of status information relating to the io operation or other event that caused an interrupt it may also involve sending additional commands or acknowledgments to the io device when interrupt processing is complete the saved register values are retrieved from the stack and restored to the registers eg see figure b the final act is to restore the psw and program counter values from the stack as a result the next instruction to be executed will be from the previously interrupted program it is important to save all of the state information about the interrupted program for later resumption this is because the interrupt is not a routine called from the program rather the interrupt can occur at any time and therefore at any point in the execution of a user program its occurrence is unpredictable tm tm n control y control stack stack t t n yl program program counter counter y start general y start general interrupt interrupt service registers service registers yl return routine yl return routine t tm stack stack pointer pointer processor processor tm t n users n users n program n program main main memory memory a interrupt occurs after instruction b return from interrupt at location n figure changes in memory and registers for an interrupt multiple interrupts so far we have discussed the occurrence of a single interrupt suppose however that one or more interrupts can occur while an interrupt is being processed for example a program may be receiving data from a communications line and printing results at the same time the printer will generate an interrupt every time that it completes a print operation the communication line controller will generate an interrupt every time a unit of data arrives the unit could either be a single character or a block depending on the nature of the communications discipline in any case it is possible for a communications interrupt to occur while a printer interrupt is being processed two approaches can be taken to dealing with multiple interrupts the first is to disable interrupts while an interrupt is being processed a disabled interrupt simply means that the processor ignores any new interrupt request signal if an interrupt occurs during this time it generally remains pending and will be checked by the processor after the processor has reenabled interrupts thus if an interrupt occurs when a user program is executing then interrupts are disabled immediately after the interrupthandler routine completes interrupts are reenabled before resuming the user program and the processor checks to see if additional interrupts have occurred this approach is simple as interrupts are handled in strict sequential order figure a interrupt user program handler x interrupt handler y a sequential interrupt processing interrupt user program handler x interrupt handler y b nested interrupt processing figure transfer of control with multiple interrupts the drawback to the preceding approach is that it does not take into account relative priority or timecritical needs for example when input arrives from the communications line it may need to be absorbed rapidly to make room for more input if the first batch of input has not been processed before the second batch arrives data may be lost because the buffer on the io device may fill and overflow a second approach is to define priorities for interrupts and to allow an interrupt of higher priority to cause a lowerpriority interrupt handler to be interrupted figure b as an example of this second approach consider a system with three io devices a printer a disk and a communications line with increasing priorities of and respectively figure based on an example in tane illustrates a possible sequence a user program begins at t at t a printer interrupt occurs user information is placed on the control stack and execution continues at the printer interrupt service routine isr while this routine is still executing at t a communications interrupt occurs because the communications line has higher priority than the printer the interrupt request is honored the printer isr is interrupted its state is pushed onto the stack and execution continues at the communications isr while this routine is executing a disk interrupt occurs t because this interrupt is of lower priority it is simply held and the communications isr runs to completion when the communications isr is complete t the previous processor state is restored which is the execution of the printer isr however before even a single instruction in that routine can be executed the processor honors the higherpriority disk interrupt and transfers control to the disk isr only when that routine is complete t is the printer isr resumed when that routine completes t control finally returns to the user program user program printer communication interrupt service routine interrupt service routine t t t t t t disk interrupt service routine t figure example time sequence of multiple interrupts virtual memory hardware and control structures locality and virtual memory paging segmentation combined paging and segmentation protection and sharing operating system software fetch policy placement policy replacement policy resident set management cleaning policy load control unix and solaris memory management paging system kernel memory allocator linux memory management linux virtual memory kernel memory allocation windows memory management windows virtual address map windows paging summary recommended reading and web sites key terms review questions and problems youre gon na need a bigger boat steven spielberg jaws learning objectives after studying this chapter you should be able to define virtual memory describe the hardware and control structures that support virtual memory describe the various os mechanisms used to implement virtual memory describe the virtual memory management mechanisms in unix linux and windows chapter introduced the concepts of paging and segmentation and analyzed their shortcomings we now move to a discussion of virtual memory an analysis of this topic is complicated by the fact that memory management is a complex interrelationship between processor hardware and operating system software we focus first on the hardware aspect of virtual memory looking at the use of paging segmentation and combined paging and segmentation then we look at the issues involved in the design of a virtual memory facility in operating systems table defines some key terms related to virtual memory a set of animations that illustrate concepts in this chapter is available online click on the rotating globe at williamstallingscomososehtml for access hardware and control structures comparing simple paging and simple segmentation on the one hand with fixed and dynamic partitioning on the other we see the foundation for a fundamental breakthrough in memory management two characteristics of paging and segmentation are the keys to this breakthrough table virtual memory terminology virtual memory a storage allocation scheme in which secondary memory can be addressed as though it were part of main memory the addresses a program may use to reference memory are distinguished from the addresses the memory system uses to identify physical storage sites and programgenerated addresses are translated automatically to the corresponding machine addresses the size of virtual storage is limited by the addressing scheme of the computer system and by the amount of secondary memory available and not by the actual number of main storage locations virtual address the address assigned to a location in virtual memory to allow that location to be accessed as though it were part of main memory virtual address space the virtual storage assigned to a process address space the range of memory addresses available to a process real address the address of a storage location in main memory all memory references within a process are logical addresses that are dynamically translated into physical addresses at run time this means that a process may be swapped in and out of main memory such that it occupies different regions of main memory at different times during the course of execution a process may be broken up into a number of pieces pages or segments and these pieces need not be contiguously located in main memory during execution the combination of dynamic runtime address translation and the use of a page or segment table permits this now we come to the breakthrough if the preceding two characteristics are present then it is not necessary that all of the pages or all of the segments of a process be in main memory during execution if the piece segment or page that holds the next instruction to be fetched and the piece that holds the next data location to be accessed are in main memory then at least for a time execution may proceed let us consider how this may be accomplished for now we can talk in general terms and we will use the term piece to refer to either page or segment depending on whether paging or segmentation is employed suppose that it is time to bring a new process into memory the os begins by bringing in only one or a few pieces to include the initial program piece and the initial data piece to which those instructions refer the portion of a process that is actually in main memory at any time is called the resident set of the process as the process executes things proceed smoothly as long as all memory references are to locations that are in the resident set using the segment or page table the processor always is able to determine whether this is so if the processor encounters a logical address that is not in main memory it generates an interrupt indicating a memory access fault the os puts the interrupted process in a blocking state for the execution of this process to proceed later the os must bring into main memory the piece of the process that contains the logical address that caused the access fault for this purpose the os issues a disk io read request after the io request has been issued the os can dispatch another process to run while the disk io is performed once the desired piece has been brought into main memory an io interrupt is issued giving control back to the os which places the affected process back into a ready state it may immediately occur to you to question the efficiency of this maneuver in which a process may be executing and have to be interrupted for no other reason than that you have failed to load in all of the needed pieces of the process for now let us defer consideration of this question with the assurance that efficiency is possible instead let us ponder the implications of our new strategy there are two implications the second more startling than the first and both lead to improved system utilization more processes may be maintained in main memory because we are only going to load some of the pieces of any particular process there is room for more processes this leads to more efficient utilization of the processor because it is more likely that at least one of the more numerous processes will be in a ready state at any particular time a process may be larger than all of main memory one of the most fundamental restrictions in programming is lifted without the scheme we have been discussing a programmer must be acutely aware of how much memory is available if the program being written is too large the programmer must devise ways to structure the program into pieces that can be loaded separately in some sort of overlay strategy with virtual memory based on paging or segmentation that job is left to the os and the hardware as far as the programmer is concerned he or she is dealing with a huge memory the size associated with disk storage the os automatically loads pieces of a process into main memory as required because a process executes only in main memory that memory is referred to as real memory but a programmer or user perceives a potentially much larger memory that which is allocated on disk this latter is referred to as virtual memory virtual memory allows for very effective multiprogramming and relieves the user of the unnecessarily tight constraints of main memory table summarizes characteristics of paging and segmentation with and without the use of virtual memory table characteristics of paging and segmentation virtual memory virtual memory simple paging paging simple segmentation segmentation main memory partimain memory partimain memory not main memory not tioned into small fixedtioned into small fixedpartitioned partitioned size chunks called frames size chunks called frames program broken into program broken into program segments speciprogram segments specipages by the compiler pages by the compiler fied by the programmer fied by the programmer or memory management or memory management to the compiler ie the to the compiler ie the system system decision is made by the decision is made by the programmer programmer internal fragmentation internal fragmentation no internal no internal within frames within frames fragmentation fragmentation no external no external external fragmentation external fragmentation fragmentation fragmentation operating system must operating system must operating system must operating system must maintain a page table maintain a page table maintain a segment table maintain a segment table for each process showing for each process showing for each process showfor each process showwhich frame each page which frame each page ing the load address and ing the load address and occupies occupies length of each segment length of each segment operating system must operating system must operating system must operating system must maintain a free frame maintain a free frame maintain a list of free maintain a list of free list list holes in main memory holes in main memory processor uses page processor uses page processor uses segment processor uses segment number offset to calcunumber offset to calcunumber offset to calcunumber offset to calculate absolute address late absolute address late absolute address late absolute address all the pages of a not all pages of a process all the segments of a not all segments of a process must be in main need be in main memory process must be in main process need be in main memory for process to frames for the process to memory for process to memory for the process run unless overlays are run pages may be read run unless overlays are to run segments may be used in as needed used read in as needed reading a page into reading a segment into main memory may main memory may require require writing a page writing one or more segout to disk ments out to disk locality and virtual memory the benefits of virtual memory are attractive but is the scheme practical at one time there was considerable debate on this point but experience with numerous operating systems has demonstrated beyond doubt that virtual memory does work accordingly virtual memory based on either paging or paging plus segmentation has become an essential component of contemporary operating systems to understand the key issue and why virtual memory was a matter of much debate let us examine again the task of the os with respect to virtual memory consider a large process consisting of a long program plus a number of arrays of data over any short period of time execution may be confined to a small section of the program eg a subroutine and access to perhaps only one or two arrays of data if this is so then it would clearly be wasteful to load in dozens of pieces for that process when only a few pieces will be used before the program is suspended and swapped out we can make better use of memory by loading in just a few pieces then if the program branches to an instruction or references a data item on a piece not in main memory a fault is triggered this tells the os to bring in the desired piece thus at any one time only a few pieces of any given process are in memory and therefore more processes can be maintained in memory furthermore time is saved because unused pieces are not swapped in and out of memory however the os must be clever about how it manages this scheme in the steady state practically all of main memory will be occupied with process pieces so that the processor and os have direct access to as many processes as possible thus when the os brings one piece in it must throw another out if it throws out a piece just before it is used then it will just have to go get that piece again almost immediately too much of this leads to a condition known as thrashing the system spends most of its time swapping pieces rather than executing instructions the avoidance of thrashing was a major research area in the s and led to a variety of complex but effective algorithms in essence the os tries to guess based on recent history which pieces are least likely to be used in the near future this reasoning is based on belief in the principle of locality which was introduced in chapter see especially appendix a to summarize the principle of locality states that program and data references within a process tend to cluster hence the assumption that only a few pieces of a process will be needed over a short period of time is valid also it should be possible to make intelligent guesses about which pieces of a process will be needed in the near future which avoids thrashing one way to confirm the principle of locality is to look at the performance of processes in a virtual memory environment figure is a rather famous diagram that dramatically illustrates the principle of locality hatf note that during the lifetime of the process references are confined to a subset of pages thus we see that the principle of locality suggests that a virtual memory scheme may work for virtual memory to be practical and effective two ingredients are needed first there must be hardware support for the paging andor segmentation scheme to be employed second the os must include software for managing the movement of pages andor segments between secondary memory and main memory in this section we examine the hardware aspect and look at the page numbers execution time figure paging behavior necessary control structures which are created and maintained by the os but are used by the memory management hardware an examination of the os issues is provided in the next section paging the term virtual memory is usually associated with systems that employ paging although virtual memory based on segmentation is also used and is discussed next the use of paging to achieve virtual memory was first reported for the atlas computer kilb and soon came into widespread commercial use in the discussion of simple paging we indicated that each process has its own page table and when all of its pages are loaded into main memory the page table for a process is created and loaded into main memory each page table entry pte contains the frame number of the corresponding page in main memory a page table is also needed for a virtual memory scheme based on paging again it is typical to associate a unique page table with each process in this case however the page table entries become more complex figure a because only some of the pages of a process may be in main memory a bit is needed in each page table entry to indicate whether the corresponding page is present p in main memory or not if the bit indicates that the page is in memory then the entry also includes the frame number of that page the page table entry includes a modify m bit indicating whether the contents of the corresponding page have been altered since the page was last loaded into main memory if there has been no change then it is not necessary to write the page out when it comes time to replace the page in the frame that it currently occupies other control bits may also be present for example if protection or sharing is managed at the page level then bits for that purpose will be required virtual address page number offset page table entry p m other control bits frame number a paging only virtual address segment number offset segment table entry p m other control bits length segment base b segmentation only virtual address segment number page number offset segment table entry control bits length segment base page table entry p m other control bits frame number p present bit m modified bit c combined segmentation and paging figure typical memory management formats page table structure the basic mechanism for reading a word from memory involves the translation of a virtual or logical address consisting of page number and offset into a physical address consisting of frame number and offset using a page table because the page table is of variable length depending on the size of the process we can not expect to hold it in registers instead it must be in main memory to be accessed figure suggests a hardware implementation when a particular process is running a register holds the starting address of the page table for that process the page number of a virtual address is used to index that table and look up the corresponding frame number this is combined with the offset portion of the virtual address to produce the desired real address typically the page number field is longer than the frame number field n m in most systems there is one page table per process but each process can occupy huge amounts of virtual memory for example in the vax architecture each process can have up to gbytes of virtual memory using byte pages means that as many as page table entries are required per process clearly the amount of memory devoted to page tables alone could be unacceptably high to overcome this problem most virtual memory schemes store page tables in virtual memory rather than real memory this means that page tables are subject to paging just as other pages are when a process is running at least a part of its page table must be in main memory including the page table entry of the currently executing page some processors make use of a twolevel scheme to organize large page tables in this scheme there is a page directory in which each entry points to a page table thus if the length of the page directory is x and if the maximum length of a page table is y then a process can virtual address physical address page offset frame offset register n bits page table ptr page table m bits offset page page frame frame program paging mechanism main memory figure address translation in a paging system kbyte root page table mbyte user page table gbyte user address space figure a twolevel hierarchical page table consist of up to x y pages typically the maximum length of a page table is restricted to be equal to one page for example the pentium processor uses this approach figure shows an example of a twolevel scheme typical for use with a bit address if we assume bytelevel addressing and kbyte pages then the gbyte virtual address space is composed of pages if each of these pages is mapped by a byte page table entry we can create a user page table composed of ptes requiring mbytes this huge user page table occupying pages can be kept in virtual memory and mapped by a root page table with ptes occupying kbytes of main memory figure shows the steps involved in address virtual address bits bits bits frame offset root page table ptr page frame kbyte page root page table table contains contains ptes ptes program paging mechanism main memory figure address translation in a twolevel paging system translation for this scheme the root page always remains in main memory the first bits of a virtual address are used to index into the root page to find a pte for a page of the user page table if that page is not in main memory a page fault occurs if that page is in main memory then the next bits of the virtual address index into the user pte page to find the pte for the page that is referenced by the virtual address inverted page table a drawback of the type of page tables that we have been discussing is that their size is proportional to that of the virtual address space an alternative approach to the use of one or multiplelevel page tables is the use of an inverted page table structure variations on this approach are used on the powerpc ultrasparc and the ia architecture an implementation of the mach operating system on the rtpc also uses this technique in this approach the page number portion of a virtual address is mapped into a hash value using a simple hashing function the hash value is a pointer to the inverted page table which contains the page table entries there is one entry in the inverted page table for each real memory page frame rather than one per virtual page thus a fixed proportion of real memory is required for the tables regardless of the number of processes or virtual pages supported because more than one virtual address may map into the same hash table entry a chaining technique is used for managing the overflow the hashing technique results in chains that are typically short between one and two entries the page tables structure is called inverted because it indexes page table entries by frame number rather than by virtual page number figure shows a typical implementation of the inverted page table approach for a physical memory size of m frames the inverted page table contains m entries so that the ith entry refers to frame i each entry in the page table includes the following page number this is the page number portion of the virtual address process identifier the process that owns this page the combination of page number and process identifier identify a page within the virtual address space of a particular process control bits this field includes flags such as valid referenced and modified and protection and locking information chain pointer this field is null perhaps indicated by a separate bit if there are no chained entries for this entry otherwise the field contains the index value number between and m of the next entry in the chain in this example the virtual address includes an nbit page number with n m the hash function maps the nbit page number into an mbit quantity which is used to index into the inverted page table translation lookaside buffer in principle every virtual memory reference can cause two physical memory accesses one to fetch the appropriate page table entry and one to fetch the desired data thus a straightforward virtual memory see appendix f for a discussion of hashing virtual address n bits page offset control n bits bits process hash m bits page id chain function i j m frame offset inverted page table m bits one entry for each real address physical memory frame figure inverted page table structure scheme would have the effect of doubling the memory access time to overcome this problem most virtual memory schemes make use of a special highspeed cache for page table entries usually called a translation lookaside buffer tlb this cache functions in the same way as a memory cache see chapter and contains those page table entries that have been most recently used the organization of the resulting paging hardware is illustrated in figure given a virtual address the processor will first examine the tlb if the desired page table entry is present tlb hit then the frame number is retrieved and the real address is formed if the desired page table entry is not found tlb miss then the processor uses the page number to index the process page table and examine the corresponding page table entry if the present bit is set then the page is in main memory and the processor can retrieve the frame number from the page table entry to form the real address the processor also updates the tlb to include this new page table entry finally if the present bit is not set then the desired page is not in main memory and a memory access fault called a page fault is issued at this point we leave the realm of hardware and invoke the os which loads the needed page and updates the page table figure is a flowchart that shows the use of the tlb the flowchart shows that if the desired page is not in main memory a page fault interrupt causes the page fault handling routine to be invoked to keep the flowchart simple the fact that the os may dispatch another process while disk io is underway is not shown by the principle of locality most virtual memory references will be to locations in main memory secondary virtual address memory page offset translation lookaside buffer tlb hit offset load page table page tlb miss frame offset real address page fault figure use of a translation lookaside buffer recently used pages therefore most references will involve page table entries in the cache studies of the vax tlb have shown that this scheme can significantly improve performance clar saty there are a number of additional details concerning the actual organization of the tlb because the tlb contains only some of the entries in a full page table we can not simply index into the tlb based on page number instead each entry in the tlb must include the page number as well as the complete page table entry the processor is equipped with hardware that allows it to interrogate simultaneously a number of tlb entries to determine if there is a match on page number this technique is referred to as associative mapping and is contrasted with the direct mapping or indexing used for lookup in the page table in figure the design of the tlb also must consider the way in which entries are organized in the tlb and which entry to replace when a new entry is brought in these issues must be considered in any hardware cache design this topic is not pursued here the reader may consult a treatment of cache design for further details eg stal finally the virtual memory mechanism must interact with the cache system not the tlb cache but the main memory cache this is illustrated in figure a virtual address will generally be in the form of a page number offset first the memory system consults the tlb to see if the matching page table entry is present if it is the real physical address is generated by combining the frame number with the offset if not the entry is accessed from a page table once the real address is start return to faulted instruction cpu checks the tlb page table yes entry in tlb no access page table page fault handling routine os instructs cpu no page to read the page in main from disk memory yes cpu activates update tlb io hardware page transferred from disk to cpu generates main memory physical address memory yes full no perform page replacement page tables updated figure operation of paging and translation lookaside buffer tlb generated which is in the form of a tag and a remainder the cache is consulted to see if the block containing that word is present if so it is returned to the cpu if not the word is retrieved from main memory the reader should be able to appreciate the complexity of the cpu hardware involved in a single memory reference the virtual address is translated into a real address this involves reference to a page table entry which may be in the tlb in main memory or on disk the referenced word may be in cache main memory or on disk if the referenced word is only on disk the page containing the word must see figure typically a tag is just the leftmost bits of the real address again for a more detailed discussion of caches see stal virtual address virtual address page offset page offset page pt entries frame offset translation lookaside buffer frame offset real address real address page table a direct mapping b associative mapping figure direct versus associative lookup for page table entries tlb operation virtual address page offset tlb tlb miss tlb hit cache operation real address tag remainder cache hit value miss main memory page table value figure translation lookaside buffer and cache operation be loaded into main memory and its block loaded into the cache in addition the page table entry for that page must be updated page size an important hardware design decision is the size of page to be used there are several factors to consider one is internal fragmentation clearly the smaller the page size the lesser is the amount of internal fragmentation to optimize the use of main memory we would like to reduce internal fragmentation on the other hand the smaller the page the greater is the number of pages required per process more pages per process means larger page tables for large programs in a heavily multiprogrammed environment this may mean that some portion of the page tables of active processes must be in virtual memory not in main memory thus there may be a double page fault for a single reference to memory first to bring in the needed portion of the page table and second to bring in the process page another factor is that the physical characteristics of most secondarymemory devices which are rotational favor a larger page size for more efficient block transfer of data complicating these matters is the effect of page size on the rate at which page faults occur this behavior in general terms is depicted in figure a and is based on the principle of locality if the page size is very small then ordinarily a relatively large number of pages will be available in main memory for a process after a time the pages in memory will all contain portions of the process near recent references thus the page fault rate should be low as the size of the page is increased each individual page will contain locations further and further from any particular recent reference thus the effect of the principle of locality is weakened and the page fault rate begins to rise eventually however the page fault rate will begin to fall as the size of a page approaches the size of the entire process point p in the diagram when a single page encompasses the entire process there will be no page faults a further complication is that the page fault rate is also determined by the number of frames allocated to a process figure b shows that for a fixed page page fault rate page fault rate p w n a page size b number of page frames allocated p size of entire process w working set size n total number of pages in process figure typical paging behavior of a program table example page sizes computer page size atlas bit words honeywellmultics bit words ibm xa and esa kbytes vax family bytes ibm as bytes dec alpha kbytes mips kbytes to mbytes ultrasparc kbytes to mbytes pentium kbytes or mbytes intel itanium kbytes to mbytes intel core i kbytes to gbyte size the fault rate drops as the number of pages maintained in main memory grows thus a software policy the amount of memory to allocate to each process interacts with a hardware design decision page size table lists the page sizes used on some machines finally the design issue of page size is related to the size of physical main memory and program size at the same time that main memory is getting larger the address space used by applications is also growing the trend is most obvious on personal computers and workstations where applications are becoming increasingly complex furthermore contemporary programming techniques used in large programs tend to decrease the locality of references within a process huck for example objectoriented techniques encourage the use of many small program and data modules with references scattered over a relatively large number of objects over a relatively short period of time multithreaded applications may result in abrupt changes in the instruction stream and in scattered memory references for a given size of tlb as the memory size of processes grows and as locality decreases the hit ratio on tlb accesses declines under these circumstances the tlb can become a performance bottleneck eg see chen one way to improve tlb performance is to use a larger tlb with more entries however tlb size interacts with other aspects of the hardware design such as the main memory cache and the number of memory accesses per instruction cycle tall the upshot is that tlb size is unlikely to grow as rapidly as main memory size an alternative is to use larger page sizes so that each page table entry in the tlb refers to a larger block of memory but we have just seen that the use of large page sizes can lead to performance degradation the parameter w represents working set size a concept discussed in section accordingly a number of designers have investigated the use of multiple page sizes tall khal and several microprocessor architectures support multiple pages sizes including mips r alpha ultrasparc pentium and ia multiple page sizes provide the flexibility needed to use a tlb effectively for example large contiguous regions in the address space of a process such as program instructions may be mapped using a small number of large pages rather than a large number of small pages while thread stacks may be mapped using the small page size however most commercial operating systems still support only one page size regardless of the capability of the underlying hardware the reason for this is that page size affects many aspects of the os thus a change to multiple page sizes is a complex undertaking see gana for a discussion segmentation virtual memory implications segmentation allows the programmer to view memory as consisting of multiple address spaces or segments segments may be of unequal indeed dynamic size memory references consist of a segment number offset form of address this organization has a number of advantages to the programmer over a nonsegmented address space it simplifies the handling of growing data structures if the programmer does not know ahead of time how large a particular data structure will become it is necessary to guess unless dynamic segment sizes are allowed with segmented virtual memory the data structure can be assigned its own segment and the os will expand or shrink the segment as needed if a segment that needs to be expanded is in main memory and there is insufficient room the os may move the segment to a larger area of main memory if available or swap it out in the latter case the enlarged segment would be swapped back in at the next opportunity it allows programs to be altered and recompiled independently without requiring the entire set of programs to be relinked and reloaded again this is accomplished using multiple segments it lends itself to sharing among processes a programmer can place a utility program or a useful table of data in a segment that can be referenced by other processes it lends itself to protection because a segment can be constructed to contain a welldefined set of programs or data the programmer or system administrator can assign access privileges in a convenient fashion organization in the discussion of simple segmentation we indicated that each process has its own segment table and when all of its segments are loaded into main memory the segment table for a process is created and loaded into main memory each segment table entry contains the starting address of the corresponding segment in main memory as well as the length of the segment the same device a segment table is needed when we consider a virtual memory scheme based on segmentation again it is typical to associate a unique segment table with each process in this case however the segment table entries become more complex figure b because only some of the segments of a process may be in main memory a bit is needed in each segment table entry to indicate whether the corresponding segment is present in main memory or not if the bit indicates that the segment is in memory then the entry also includes the starting address and length of that segment another control bit in the segmentation table entry is a modify bit indicating whether the contents of the corresponding segment have been altered since the segment was last loaded into main memory if there has been no change then it is not necessary to write the segment out when it comes time to replace the segment in the frame that it currently occupies other control bits may also be present for example if protection or sharing is managed at the segment level then bits for that purpose will be required the basic mechanism for reading a word from memory involves the translation of a virtual or logical address consisting of segment number and offset into a physical address using a segment table because the segment table is of variable length depending on the size of the process we can not expect to hold it in registers instead it must be in main memory to be accessed figure suggests a hardware implementation of this scheme note similarity to figure when a particular process is running a register holds the starting address of the segment table for that process the segment number of a virtual address is used to index that table and look up the corresponding main memory address for the start of the segment this is added to the offset portion of the virtual address to produce the desired real address combined paging and segmentation both paging and segmentation have their strengths paging which is transparent to the programmer eliminates external fragmentation and thus provides efficient use of main memory in addition because the pieces that are moved in and out of virtual address physical address seg offset d base d register seg table ptr segment table d segment seg length base program segmentation mechanism main memory figure address translation in a segmentation system main memory are of fixed equal size it is possible to develop sophisticated memory management algorithms that exploit the behavior of programs as we shall see segmentation which is visible to the programmer has the strengths listed earlier including the ability to handle growing data structures modularity and support for sharing and protection to combine the advantages of both some systems are equipped with processor hardware and os software to provide both in a combined pagingsegmentation system a users address space is broken up into a number of segments at the discretion of the programmer each segment is in turn broken up into a number of fixedsize pages which are equal in length to a main memory frame if a segment has length less than that of a page the segment occupies just one page from the programmers point of view a logical address still consists of a segment number and a segment offset from the systems point of view the segment offset is viewed as a page number and page offset for a page within the specified segment figure suggests a structure to support combined pagingsegmentation note similarity to figure associated with each process is a segment table and a number of page tables one per process segment when a particular process is running a register holds the starting address of the segment table for that process presented with a virtual address the processor uses the segment number portion to index into the process segment table to find the page table for that segment then the page number portion of the virtual address is used to index the page table and look up the corresponding frame number this is combined with the offset portion of the virtual address to produce the desired real address figure c suggests the segment table entry and page table entry formats as before the segment table entry contains the length of the segment it also contains virtual address seg page offset frame offset seg table ptr segment page table table page offset page seg frame program segmentation paging main memory mechanism mechanism figure address translation in a segmentationpaging system a base field which now refers to a page table the present and modified bits are not needed because these matters are handled at the page level other control bits may be used for purposes of sharing and protection the page table entry is essentially the same as is used in a pure paging system each page number is mapped into a corresponding frame number if the page is present in main memory the modified bit indicates whether this page needs to be written back out when the frame is allocated to another page there may be other control bits dealing with protection or other aspects of memory management protection and sharing segmentation lends itself to the implementation of protection and sharing policies because each segment table entry includes a length as well as a base address a program can not inadvertently access a main memory location beyond the limits of a segment to achieve sharing it is possible for a segment to be referenced in the segment tables of more than one process the same mechanisms are of course available in a paging system however in this case the page structure of programs and data is not visible to the programmer making the specification of protection and sharing requirements more awkward figure illustrates the types of protection relationships that can be enforced in such a system address main memory k dispatcher k no access allowed k process a k k branch instruction not allowed process b reference to data allowed k process c reference to data not allowed k figure protection relationships between segments more sophisticated mechanisms can also be provided a common scheme is to use a ringprotection structure of the type we referred to in chapter figure in this scheme lowernumbered or inner rings enjoy greater privilege than highernumbered or outer rings typically ring is reserved for kernel functions of the os with applications at a higher level some utilities or os services may occupy an intermediate ring basic principles of the ring system are as follows a program may access only data that reside on the same ring or a less privileged ring a program may call services residing on the same or a more privileged ring operating system software the design of the memory management portion of an os depends on three fundamental areas of choice whether or not to use virtual memory techniques the use of paging or segmentation or both the algorithms employed for various aspects of memory management the choices made in the first two areas depend on the hardware platform available thus earlier unix implementations did not provide virtual memory because the processors on which the system ran did not support paging or segmentation neither of these techniques is practical without hardware support for address translation and other basic functions two additional comments about the first two items in the preceding list first with the exception of operating systems for some of the older personal computers such as msdos and specialized systems all important operating systems provide virtual memory second pure segmentation systems are becoming increasingly rare when segmentation is combined with paging most of the memory management issues confronting the os designer are in the area of paging thus we can concentrate in this section on the issues associated with paging the choices related to the third item are the domain of operating system software and are the subject of this section table lists the key design elements that we examine in each case the key issue is one of performance we would like to minimize the rate at which page faults occur because page faults cause considerable software overhead at a minimum the overhead includes deciding which resident page or pages to replace and the io of exchanging pages also the os must schedule another process to run during the page io causing a process switch accordingly we would like to arrange matters so that during the time that a process is executing the probability of referencing a word on a missing page is minimized in all of the areas referred to in table there is no definitive policy that works best protection and sharing are usually dealt with at the segment level in a combined segmentationpaging system we will deal with these issues in later chapters table operating system policies for virtual memory fetch policy resident set management demand paging resident set size prepaging fixed variable placement policy replacement scope global replacement policy local basic algorithms optimal cleaning policy least recently used lru demand firstinfirstout fifo precleaning clock page buffering load control degree of multiprogramming as we shall see the task of memory management in a paging environment is fiendishly complex furthermore the performance of any particular set of policies depends on main memory size the relative speed of main and secondary memory the size and number of processes competing for resources and the execution behavior of individual programs this latter characteristic depends on the nature of the application the programming language and compiler employed the style of the programmer who wrote it and for an interactive program the dynamic behavior of the user thus the reader must expect no final answers here or anywhere for smaller systems the os designer should attempt to choose a set of policies that seems good over a wide range of conditions based on the current state of knowledge for larger systems particularly mainframes the operating system should be equipped with monitoring and control tools that allow the site manager to tune the operating system to get good results based on site conditions fetch policy the fetch policy determines when a page should be brought into main memory the two common alternatives are demand paging and prepaging with demand paging a page is brought into main memory only when a reference is made to a location on that page if the other elements of memory management policy are good the following should happen when a process is first started there will be a flurry of page faults as more and more pages are brought in the principle of locality suggests that most future references will be to pages that have recently been brought in thus after a time matters should settle down and the number of page faults should drop to a very low level with prepaging pages other than the one demanded by a page fault are brought in prepaging exploits the characteristics of most secondary memory devices such as disks which have seek times and rotational latency if the pages of a process are stored contiguously in secondary memory then it is more efficient to bring in a number of contiguous pages at one time rather than bringing them in one at a time over an extended period of course this policy is ineffective if most of the extra pages that are brought in are not referenced the prepaging policy could be employed either when a process first starts up in which case the programmer would somehow have to designate desired pages or every time a page fault occurs this latter course would seem preferable because it is invisible to the programmer however the utility of prepaging has not been established maek prepaging should not be confused with swapping when a process is swapped out of memory and put in a suspended state all of its resident pages are moved out when the process is resumed all of the pages that were previously in main memory are returned to main memory placement policy the placement policy determines where in real memory a process piece is to reside in a pure segmentation system the placement policy is an important design issue policies such as bestfit firstfit and so on which were discussed in chapter are possible alternatives however for a system that uses either pure paging or paging combined with segmentation placement is usually irrelevant because the address translation hardware and the main memory access hardware can perform their functions for any pageframe combination with equal efficiency there is one area in which placement does become a concern and this is a subject of research and development on a socalled nonuniform memory access numa multiprocessor the distributed shared memory of the machine can be referenced by any processor on the machine but the time for accessing a particular physical location varies with the distance between the processor and the memory module thus performance depends heavily on the extent to which data reside close to the processors that use them laro bolo cox for numa systems an automatic placement strategy is desirable to assign pages to the memory module that provides the best performance replacement policy in most operating system texts the treatment of memory management includes a section entitled replacement policy which deals with the selection of a page in main memory to be replaced when a new page must be brought in this topic is sometimes difficult to explain because several interrelated concepts are involved how many page frames are to be allocated to each active process whether the set of pages to be considered for replacement should be limited to those of the process that caused the page fault or encompass all the page frames in main memory among the set of pages considered which particular page should be selected for replacement we shall refer to the first two concepts as resident set management which is dealt with in the next subsection and reserve the term replacement policy for the third concept which is discussed in this subsection the area of replacement policy is probably the most studied of any area of memory management when all of the frames in main memory are occupied and it is necessary to bring in a new page to satisfy a page fault the replacement policy determines which page currently in memory is to be replaced all of the policies have as their objective that the page that is removed should be the page least likely to be referenced in the near future because of the principle of locality there is often a high correlation between recent referencing history and nearfuture referencing patterns thus most policies try to predict future behavior on the basis of past behavior one tradeoff that must be considered is that the more elaborate and sophisticated the replacement policy the greater will be the hardware and software overhead to implement it frame locking one restriction on replacement policy needs to be mentioned before looking at various algorithms some of the frames in main memory may be locked when a frame is locked the page currently stored in that frame may not be replaced much of the kernel of the os as well as key control structures are held in locked frames in addition io buffers and other timecritical areas may be locked into main memory frames locking is achieved by associating a lock bit with each frame this bit may be kept in a frame table as well as being included in the current page table basic algorithms regardless of the resident set management strategy discussed in the next subsection there are certain basic algorithms that are used for the selection of a page to replace replacement algorithms that have been discussed in the literature include optimal least recently used lru firstinfirstout fifo clock the optimal policy selects for replacement that page for which the time to the next reference is the longest it can be shown that this policy results in the fewest number of page faults bela clearly this policy is impossible to implement because it would require the os to have perfect knowledge of future events however it does serve as a standard against which to judge realworld algorithms figure gives an example of the optimal policy the example assumes a fixed frame allocation fixed resident set size for this process of three frames the execution of the process requires reference to five distinct pages the page address stream formed by executing the program is which means that the first page referenced is the second page referenced is and so on the optimal policy produces three page faults after the frame allocation has been filled the least recently used lru policy replaces the page in memory that has not been referenced for the longest time by the principle of locality this should be the page least likely to be referenced in the near future and in fact the lru policy does nearly as well as the optimal policy the problem with this approach is the difficulty in implementation one approach would be to tag each page with the page address stream opt f f f lru f f f f fifo f f f f f f clock f f f f f f page fault occurring after the frame allocation is initially filled figure behavior of four page replacement algorithms time of its last reference this would have to be done at each memory reference both instruction and data even if the hardware would support such a scheme the overhead would be tremendous alternatively one could maintain a stack of page references again an expensive prospect figure shows an example of the behavior of lru using the same page address stream as for the optimal policy example in this example there are four page faults the firstinfirstout fifo policy treats the page frames allocated to a process as a circular buffer and pages are removed in roundrobin style all that is required is a pointer that circles through the page frames of the process this is therefore one of the simplest page replacement policies to implement the logic behind this choice other than its simplicity is that one is replacing the page that has been in memory the longest a page fetched into memory a long time ago may have now fallen out of use this reasoning will often be wrong because there will often be regions of program or data that are heavily used throughout the life of a program those pages will be repeatedly paged in and out by the fifo algorithm continuing our example in figure the fifo policy results in six page faults note that lru recognizes that pages and are referenced more frequently than other pages whereas fifo does not although the lru policy does nearly as well as an optimal policy it is difficult to implement and imposes significant overhead on the other hand the fifo policy is very simple to implement but performs relatively poorly over the years os designers have tried a number of other algorithms to approximate the performance of lru while imposing little overhead many of these algorithms are variants of a scheme referred to as the clock policy the simplest form of clock policy requires the association of an additional bit with each frame referred to as the use bit when a page is first loaded into a frame in memory the use bit for that frame is set to whenever the page is subsequently referenced after the reference that generated the page fault its use bit is set to for the page replacement algorithm the set of frames that are candidates for replacement this process local scope all of main memory global scope is considered to be a circular buffer with which a pointer is associated when a page is replaced the pointer is set to indicate the next frame in the buffer after the one just updated when it comes time to replace a page the os scans the buffer to find a frame with a use bit set to each time it encounters a frame with a use bit of it resets that bit to and continues on if any of the frames in the buffer have a use bit of at the beginning of this process the first such frame encountered is chosen for replacement if all of the frames have a use bit of then the pointer will make one complete cycle through the buffer setting all the use bits to and stop at its original position replacing the page in that frame we can see that this policy is similar to fifo except that in the clock policy any frame with a use bit of is passed over by the algorithm the policy is referred to as a clock policy because we can visualize the page frames as laid out in a circle a number of operating systems have employed some variation of this simple clock policy eg multics corb figure provides an example of the simple clock policy mechanism a circular buffer of n main memory frames is available for page replacement just prior to the replacement of a page from the buffer with incoming page the next frame pointer points at frame which contains page the clock policy is now executed because the use bit for page in frame is equal to this page is not replaced instead the use bit is set to and the pointer advances similarly page in frame is not replaced its use bit is set to and the pointer advances in the next frame frame the use bit is set to therefore page is replaced with page the use bit is set to for this frame and the pointer advances to frame completing the page replacement procedure the behavior of the clock policy is illustrated in figure the presence of an asterisk indicates that the corresponding use bit is equal to and the arrow indicates the current position of the pointer note that the clock policy is adept at protecting frames and from replacement figure shows the results of an experiment reported in baer which compares the four algorithms that we have been discussing it is assumed that the number of page frames assigned to a process is fixed the results are based on the execution of references in a fortran program using a page size of words baer ran the experiment with frame allocations of and frames the differences among the four policies are most striking at small allocations with the concept of scope is discussed in the subsection replacement scope subsequently first frame in circular buffer of n frames that are candidates for replacement page page use use page use next frame page pointer use page page use use page page use use page page use use a state of buffer just prior to a page replacement n page page use use page use page use page page use use page page use use page page use use b state of buffer just after the next page replacement figure example of clock policy operation fifo being over a factor of worse than optimal all four curves have the same shape as the idealized behavior shown in figure b in order to run efficiently we would like to be to the right of the knee of the curve with a small page fault rate while keeping a small frame allocation to the left of the knee of the curve these two constraints indicate that a desirable mode of operation would be at the knee of the curve almost identical results have been reported in fink again showing a maximum spread of about a factor of finkels approach was to simulate the effects of various policies on a synthesized pagereference string of references selected page faults per references fifo clock lru opt number of frames allocated figure comparison of fixedallocation local page replacement algorithms from a virtual space of pages to approximate the effects of the principle of locality an exponential distribution for the probability of referencing a particular page was imposed finkel observes that some might be led to conclude that there is little point in elaborate page replacement algorithms when only a factor of is at stake but he notes that this difference will have a noticeable effect either on main memory requirements to avoid degrading operating system performance or operating system performance to avoid enlarging main memory the clock algorithm has also been compared to these other algorithms when a variable allocation and either global or local replacement scope see the following discussion of replacement policy is used carr carr the clock algorithm was found to approximate closely the performance of lru the clock algorithm can be made more powerful by increasing the number of bits that it employs in all processors that support paging a modify bit is associated with every page in main memory and hence with every frame of main memory this bit is needed so that when a page has been modified it is not replaced until it has been written back into secondary memory we can exploit this bit in the clock algorithm in the following way if we take the use and modify bits into account each frame falls into one of four categories not accessed recently not modified u m accessed recently not modified u m not accessed recently modified u m accessed recently modified u m with this classification the clock algorithm performs as follows beginning at the current position of the pointer scan the frame buffer during this scan make no changes to the use bit the first frame encountered with u m is selected for replacement on the other hand if we reduce the number of bits employed to zero the clock algorithm degenerates to fifo if step fails scan again looking for the frame with u m the first such frame encountered is selected for replacement during this scan set the use bit to on each frame that is bypassed if step fails the pointer should have returned to its original position and all of the frames in the set will have a use bit of repeat step and if necessary step this time a frame will be found for the replacement in summary the page replacement algorithm cycles through all of the pages in the buffer looking for one that has not been modified since being brought in and has not been accessed recently such a page is a good bet for replacement and has the advantage that because it is unmodified it does not need to be written back out to secondary memory if no candidate page is found in the first sweep the algorithm cycles through the buffer again looking for a modified page that has not been accessed recently even though such a page must be written out to be replaced because of the principle of locality it may not be needed again anytime soon if this second pass fails all of the frames in the buffer are marked as having not been accessed recently and a third sweep is performed this strategy was used on an earlier version of the macintosh virtual memory scheme gold illustrated in figure the advantage of this algorithm over first frame in circular buffer for this process n page page not accessed not accessed recently recently modified modified page not accessed recently not modified page page not accessed accessed recently recently not modified not modified page page not accessed accessed recently recently last not modified not modified replaced next replaced page page not accessed not accessed recently page recently modified page accessed modified accessed recently recently not modified not modified figure the clock page replacement algorithm gold the simple clock algorithm is that pages that are unchanged are given preference for replacement because a page that has been modified must be written out before being replaced there is an immediate saving of time page buffering although lru and the clock policies are superior to fifo they both involve complexity and overhead not suffered with fifo in addition there is the related issue that the cost of replacing a page that has been modified is greater than for one that has not because the former must be written back out to secondary memory an interesting strategy that can improve paging performance and allow the use of a simpler page replacement policy is page buffering the vax vms approach is representative the page replacement algorithm is simple fifo to improve performance a replaced page is not lost but rather is assigned to one of two lists the free page list if the page has not been modified or the modified page list if it has note that the page is not physically moved about in main memory instead the entry in the page table for this page is removed and placed in either the free or modified page list the free page list is a list of page frames available for reading in pages vms tries to keep some small number of frames free at all times when a page is to be read in the page frame at the head of the list is used destroying the page that was there when an unmodified page is to be replaced it remains in memory and its page frame is added to the tail of the free page list similarly when a modified page is to be written out and replaced its page frame is added to the tail of the modified page list the important aspect of these maneuvers is that the page to be replaced remains in memory thus if the process references that page it is returned to the resident set of that process at little cost in effect the free and modified page lists act as a cache of pages the modified page list serves another useful function modified pages are written out in clusters rather than one at a time this significantly reduces the number of io operations and therefore the amount of disk access time a simpler version of page buffering is implemented in the mach operating system rash in this case no distinction is made between modified and unmodified pages replacement policy and cache size as discussed earlier main memory size is getting larger and the locality of applications is decreasing in compensation cache sizes have been increasing large cache sizes even multimegabyte ones are now feasible design alternatives borg with a large cache the replacement of virtual memory pages can have a performance impact if the page frame selected for replacement is in the cache then that cache block is lost as well as the page that it holds in systems that use some form of page buffering it is possible to improve cache performance by supplementing the page replacement policy with a policy for page placement in the page buffer most operating systems place pages by selecting an arbitrary page frame from the page buffer typically a firstinfirstout discipline is used a study reported in kess shows that a careful page placement strategy can result in fewer cache misses than naive placement several page placement algorithms are examined in kess the details are beyond the scope of this book as they depend on the details of cache structure and policies the essence of these strategies is to bring consecutive pages into main memory in such a way as to minimize the number of page frames that are mapped into the same cache slots resident set management resident set size with paged virtual memory it is not necessary and indeed may not be possible to bring all of the pages of a process into main memory to prepare it for execution thus the os must decide how many pages to bring in that is how much main memory to allocate to a particular process several factors come into play the smaller the amount of memory allocated to a process the more processes that can reside in main memory at any one time this increases the probability that the os will find at least one ready process at any given time and hence reduces the time lost due to swapping if a relatively small number of pages of a process are in main memory then despite the principle of locality the rate of page faults will be rather high see figure b beyond a certain size additional allocation of main memory to a particular process will have no noticeable effect on the page fault rate for that process because of the principle of locality with these factors in mind two sorts of policies are to be found in contemporary operating systems a fixedallocation policy gives a process a fixed number of frames in main memory within which to execute that number is decided at initial load time process creation time and may be determined based on the type of process interactive batch type of application or may be based on guidance from the programmer or system manager with a fixedallocation policy whenever a page fault occurs in the execution of a process one of the pages of that process must be replaced by the needed page a variableallocation policy allows the number of page frames allocated to a process to be varied over the lifetime of the process ideally a process that is suffering persistently high levels of page faults indicating that the principle of locality only holds in a weak form for that process will be given additional page frames to reduce the page fault rate whereas a process with an exceptionally low page fault rate indicating that the process is quite well behaved from a locality point of view will be given a reduced allocation with the hope that this will not noticeably increase the page fault rate the use of a variableallocation policy relates to the concept of replacement scope as explained in the next subsection the variableallocation policy would appear to be the more powerful one however the difficulty with this approach is that it requires the os to assess the behavior of active processes this inevitably requires software overhead in the os and is dependent on hardware mechanisms provided by the processor platform replacement scope the scope of a replacement strategy can be categorized as global or local both types of policies are activated by a page fault when there are no free page frames a local replacement policy chooses only among the resident pages of the process that generated the page fault in selecting a page to replace a global replacement policy considers all unlocked pages in main memory as candidates for replacement regardless of which process owns a particular page while it happens that local policies are easier to analyze there is no convincing evidence that they perform better than global policies which are attractive because of their simplicity of implementation and minimal overhead carr maek there is a correlation between replacement scope and resident set size table a fixed resident set implies a local replacement policy to hold the size of a resident set fixed a page that is removed from main memory must be replaced by another page from the same process a variableallocation policy can clearly employ a global replacement policy the replacement of a page from one process in main memory with that of another causes the allocation of one process to grow by one page and that of the other to shrink by one page we shall also see that variable allocation and local replacement is a valid combination we now examine these three combinations fixed allocation local scope for this case we have a process that is running in main memory with a fixed number of frames when a page fault occurs the os must choose which page from among the currently resident pages for this process is to be replaced replacement algorithms such as those discussed in the preceding subsection can be used with a fixedallocation policy it is necessary to decide ahead of time the amount of allocation to give to a process this could be decided on the basis of the type of application and the amount requested by the program the drawback to this approach is twofold if allocations tend to be too small then there will be a high page fault rate causing the entire multiprogramming system to run slowly if allocations tend to be unnecessarily large then there will be too few programs in main memory and there will be either considerable processor idle time or considerable time spent in swapping table resident set management local replacement global replacement fixed allocation number of frames allocated to a not possible process is fixed page to be replaced is chosen from among the frames allocated to that process variable allocation the number of frames allocated to a page to be replaced is chosen from all process may be changed from time to available frames in main memory this time to maintain the working set of the causes the size of the resident set of process processes to vary page to be replaced is chosen from among the frames allocated to that process variable allocation global scope this combination is perhaps the easiest to implement and has been adopted in a number of operating systems at any given time there are a number of processes in main memory each with a certain number of frames allocated to it typically the os also maintains a list of free frames when a page fault occurs a free frame is added to the resident set of a process and the page is brought in thus a process experiencing page faults will gradually grow in size which should help reduce overall page faults in the system the difficulty with this approach is in the replacement choice when there are no free frames available the os must choose a page currently in memory to replace the selection is made from among all of the frames in memory except for locked frames such as those of the kernel using any of the policies discussed in the preceding subsection the page selected for replacement can belong to any of the resident processes there is no discipline to determine which process should lose a page from its resident set therefore the process that suffers the reduction in resident set size may not be optimum one way to counter the potential performance problems of a variableallocation globalscope policy is to use page buffering in this way the choice of which page to replace becomes less significant because the page may be reclaimed if it is referenced before the next time that a block of pages are overwritten variable allocation local scope the variableallocation localscope strategy attempts to overcome the problems with a globalscope strategy it can be summarized as follows when a new process is loaded into main memory allocate to it a certain number of page frames as its resident set based on application type program request or other criteria use either prepaging or demand paging to fill up the allocation when a page fault occurs select the page to replace from among the resident set of the process that suffers the fault from time to time reevaluate the allocation provided to the process and increase or decrease it to improve overall performance with this strategy the decision to increase or decrease a resident set size is a deliberate one and is based on an assessment of the likely future demands of active processes because of this evaluation such a strategy is more complex than a simple global replacement policy however it may yield better performance the key elements of the variableallocation localscope strategy are the criteria used to determine resident set size and the timing of changes one specific strategy that has received much attention in the literature is known as the working set strategy although a true working set strategy would be difficult to implement it is useful to examine it as a baseline for comparison the working set is a concept introduced and popularized by denning denn denn dennb it has had a profound impact on virtual memory management design the working set with parameter for a process at virtual time t which we designate as wt is the set of pages of that process that have been referenced in the last virtual time units sequence of page references window size w figure working set of process as defined by window size virtual time is defined as follows consider a sequence of memory references r r in which ri is the page that contains the ith virtual address generated by a given process time is measured in memory references thus t measures the processs internal virtual time let us consider each of the two variables of w the variable is a window of virtual time over which the process is observed the working set size will be a nondecreasing function of the window size the result is illustrated in figure based on bach which shows a sequence of page references for a process the dots indicate time units in which the working set does not change note that the larger the window size the larger is the working set this can be expressed in the following relationship wt wt the working set is also a function of time if a process executes over time units and uses only a single page then wt a working set can also grow as large as the number of pages n of the process if many different pages are rapidly addressed and if the window size allows thus wt minn figure indicates the way in which the working set size can vary over time for a fixed value of for many programs periods of relatively stable working set working set size time transient transient transient transient stable stable stable stable figure typical graph of working set size maek sizes alternate with periods of rapid change when a process first begins executing it gradually builds up to a working set as it references new pages eventually by the principle of locality the process should stabilize on a certain set of pages subsequent transient periods reflect a shift of the program to a new locality during the transition phase some of the pages from the old locality remain within the window causing a surge in the size of the working set as new pages are referenced as the window slides past these page references the working set size declines until it contains only those pages from the new locality this concept of a working set can be used to guide a strategy for resident set size monitor the working set of each process periodically remove from the resident set of a process those pages that are not in its working set this is essentially an lru policy a process may execute only if its working set is in main memory ie if its resident set includes its working set this strategy is appealing because it takes an accepted principle the principle of locality and exploits it to achieve a memory management strategy that should minimize page faults unfortunately there are a number of problems with the working set strategy the past does not always predict the future both the size and the membership of the working set will change over time eg see figure a true measurement of working set for each process is impractical it would be necessary to timestamp every page reference for every process using the virtual time of that process and then maintain a timeordered queue of pages for each process the optimal value of is unknown and in any case would vary nevertheless the spirit of this strategy is valid and a number of operating systems attempt to approximate a working set strategy one way to do this is to focus not on the exact page references but on the page fault rate of a process as figure b illustrates the page fault rate falls as we increase the resident set size of a process the working set size should fall at a point on this curve such as indicated by w in the figure therefore rather than monitor the working set size directly we can achieve comparable results by monitoring the page fault rate the line of reasoning is as follows if the page fault rate for a process is below some minimum threshold the system as a whole can benefit by assigning a smaller resident set size to this process because more page frames are available for other processes without harming the process by causing it to incur increased page faults if the page fault rate for a process is above some maximum threshold the process can benefit from an increased resident set size by incurring fewer faults without degrading the system an algorithm that follows this strategy is the page fault frequency pff algorithm chu gupt it requires a use bit to be associated with each page in memory the bit is set to when that page is accessed when a page fault occurs the os notes the virtual time since the last page fault for that process this could be done by maintaining a counter of page references a threshold f is defined if the amount of time since the last page fault is less than f then a page is added to the resident set of the process otherwise discard all pages with a use bit of and shrink the resident set accordingly at the same time reset the use bit on the remaining pages of the process to the strategy can be refined by using two thresholds an upper threshold that is used to trigger a growth in the resident set size and a lower threshold that is used to trigger a contraction in the resident set size the time between page faults is the reciprocal of the page fault rate although it would seem to be better to maintain a running average of the page fault rate the use of a single time measurement is a reasonable compromise that allows decisions about resident set size to be based on the page fault rate if such a strategy is supplemented with page buffering the resulting performance should be quite good nevertheless there is a major flaw in the pff approach which is that it does not perform well during the transient periods when there is a shift to a new locality with pff no page ever drops out of the resident set before f virtual time units have elapsed since it was last referenced during interlocality transitions the rapid succession of page faults causes the resident set of a process to swell before the pages of the old locality are expelled the sudden peaks of memory demand may produce unnecessary process deactivations and reactivations with the corresponding undesirable switching and swapping overheads an approach that attempts to deal with the phenomenon of interlocality transition with a similar relatively low overhead to that of pff is the variableinterval sampled working set vsws policy ferr the vsws policy evaluates the working set of a process at sampling instances based on elapsed virtual time at the beginning of a sampling interval the use bits of all the resident pages for the process are reset at the end only the pages that have been referenced during the interval will have their use bit set these pages are retained in the resident set of the process throughout the next interval while the others are discarded thus the resident set size can only decrease at the end of an interval during each interval any faulted pages are added to the resident set thus the resident set remains fixed or grows during the interval the vsws policy is driven by three parameters m the minimum duration of the sampling interval l the maximum duration of the sampling interval q the number of page faults that are allowed to occur between sampling instances the vsws policy is as follows if the virtual time since the last sampling instance reaches l then suspend the process and scan the use bits if prior to an elapsed virtual time of l q page faults occur a if the virtual time since the last sampling instance is less than m then wait until the elapsed virtual time reaches m to suspend the process and scan the use bits b if the virtual time since the last sampling instance is greater than or equal to m suspend the process and scan the use bits the parameter values are to be selected so that the sampling will normally be triggered by the occurrence of the qth page fault after the last scan case b the other two parameters m and l provide boundary protection for exceptional conditions the vsws policy tries to reduce the peak memory demands caused by abrupt interlocality transitions by increasing the sampling frequency and hence the rate at which unused pages drop out of the resident set when the page fault rate increases experience with this technique in the bull mainframe operating system gcos indicates that this approach is as simple to implement as pff and more effective pizz cleaning policy a cleaning policy is the opposite of a fetch policy it is concerned with determining when a modified page should be written out to secondary memory two common alternatives are demand cleaning and precleaning with demand cleaning a page is written out to secondary memory only when it has been selected for replacement a precleaning policy writes modified pages before their page frames are needed so that pages can be written out in batches there is a danger in following either policy to the full with precleaning a page is written out but remains in main memory until the page replacement algorithm dictates that it be removed precleaning allows the writing of pages in batches but it makes little sense to write out hundreds or thousands of pages only to find that the majority of them have been modified again before they are replaced the transfer capacity of secondary memory is limited and should not be wasted with unnecessary cleaning operations on the other hand with demand cleaning the writing of a dirty page is coupled to and precedes the reading in of a new page this technique may minimize page writes but it means that a process that suffers a page fault may have to wait for two page transfers before it can be unblocked this may decrease processor utilization a better approach incorporates page buffering this allows the adoption of the following policy clean only pages that are replaceable but decouple the cleaning and replacement operations with page buffering replaced pages can be placed on two lists modified and unmodified the pages on the modified list can periodically be written out in batches and moved to the unmodified list a page on the unmodified list is either reclaimed if it is referenced or lost when its frame is assigned to another page load control load control is concerned with determining the number of processes that will be resident in main memory which has been referred to as the multiprogramming level the load control policy is critical in effective memory management if too few processes are resident at any one time then there will be many occasions when all processes are blocked and much time will be spent in swapping on the other hand if too many processes are resident then on average the size of the resident set of each process will be inadequate and frequent faulting will occur the result is thrashing multiprogramming level thrashing is illustrated in figure as the multiprogramming level increases from a small value one would expect to see processor utilization rise because there is less chance that all resident processes are blocked however a point is reached at which the average resident set is inadequate at this point the number of page faults rises dramatically and processor utilization collapses there are a number of ways to approach this problem a working set or pff algorithm implicitly incorporates load control only those processes whose resident set is sufficiently large are allowed to execute in providing the required resident set processor utilization multiprogramming level figure multiprogramming effects size for each active process the policy automatically and dynamically determines the number of active programs another approach suggested by denning and his colleagues dennb is known as the l s criterion which adjusts the multiprogramming level so that the mean time between faults equals the mean time required to process a page fault performance studies indicate that this is the point at which processor utilization attained a maximum a policy with a similar effect proposed in lero is the criterion which attempts to keep utilization of the paging device at approximately again performance studies indicate that this is a point of maximum processor utilization another approach is to adapt the clock page replacement algorithm described earlier figure carr describes a technique using a global scope that involves monitoring the rate at which the pointer scans the circular buffer of frames if the rate is below a given lower threshold this indicates one or both of two circumstances few page faults are occurring resulting in few requests to advance the pointer for each request the average number of frames scanned by the pointer is small indicating that there are many resident pages not being referenced and are readily replaceable in both cases the multiprogramming level can safely be increased on the other hand if the pointer scan rate exceeds an upper threshold this indicates either a high fault rate or difficulty in locating replaceable pages which implies that the multiprogramming level is too high process suspension if the degree of multiprogramming is to be reduced one or more of the currently resident processes must be suspended swapped out carr lists six possibilities lowestpriority process this implements a scheduling policy decision and is unrelated to performance issues faulting process the reasoning is that there is a greater probability that the faulting task does not have its working set resident and performance would suffer least by suspending it in addition this choice has an immediate payoff because it blocks a process that is about to be blocked anyway and it eliminates the overhead of a page replacement and io operation last process activated this is the process least likely to have its working set resident process with the smallest resident set this will require the least future effort to reload however it penalizes programs with strong locality largest process this obtains the most free frames in an overcommitted memory making additional deactivations unlikely soon process with the largest remaining execution window in most process scheduling schemes a process may only run for a certain quantum of time before being interrupted and placed at the end of the ready queue this approximates a shortestprocessingtimefirst scheduling discipline as in so many other areas of os design which policy to choose is a matter of judgment and depends on many other design factors in the os as well as the characteristics of the programs being executed unix and solaris memory management because unix is intended to be machine independent its memory management scheme will vary from one system to the next earlier versions of unix simply used variable partitioning with no virtual memory scheme current implementations of unix and solaris make use of paged virtual memory in svr and solaris there are actually two separate memory management schemes the paging system provides a virtual memory capability that allocates page frames in main memory to processes and also allocates page frames to disk block buffers although this is an effective memory management scheme for user processes and disk io a paged virtual memory scheme is less suited to managing the memory allocation for the kernel for this latter purpose a kernel memory allocator is used we examine these two mechanisms in turn paging system data structures for paged virtual memory unix makes use of a number of data structures that with minor adjustment are machine independent figure and table copy modrefepropage frame number age on ify rence valid tect write a page table entry swap device number device block number type of storage b disk block descriptor page state reference logical block pfdata count device number pointer c page frame data table entry reference pagestorage count unit number d swapuse table entry figure unix svr memory management formats table unix svr memory management parameters page table entry page frame number refers to frame in real memory age indicates how long the page has been in memory without being referenced the length and contents of this field are processor dependent copy on write set when more than one process shares a page if one of the processes writes into the page a separate copy of the page must first be made for all other processes that share the page this feature allows the copy operation to be deferred until necessary and avoided in cases where it turns out not to be necessary modify indicates page has been modified reference indicates page has been referenced this bit is set to when the page is first loaded and may be periodically reset by the page replacement algorithm valid indicates page is in main memory protect indicates whether write operation is allowed disk block descriptor swap device number logical device number of the secondary device that holds the corresponding page this allows more than one device to be used for swapping device block number block location of page on swap device type of storage storage may be swap unit or executable file in the latter case there is an indication as to whether the virtual memory to be allocated should be cleared first page frame data table entry page state indicates whether this frame is available or has an associated page in the latter case the status of the page is specified on swap device in executable file or dma in progress reference count number of processes that reference the page logical device logical device that contains a copy of the page block number block location of the page copy on the logical device pfdata pointer pointer to other pfdata table entries on a list of free pages and on a hash queue of pages swapuse table entry reference count number of page table entries that point to a page on the swap device pagestorage unit number page identifier on storage unit page table typically there will be one page table per process with one entry for each page in virtual memory for that process disk block descriptor associated with each page of a process is an entry in this table that describes the disk copy of the virtual page page frame data table describes each frame of real memory and is indexed by frame number this table is used by the replacement algorithm swapuse table there is one swapuse table for each swap device with one entry for each page on the device most of the fields defined in table are selfexplanatory a few warrant further comment the age field in the page table entry is an indication of how long it has been since a program referenced this frame however the number of bits and the frequency of update of this field are implementation dependent therefore there is no universal unix use of this field for page replacement policy the type of storage field in the disk block descriptor is needed for the following reason when an executable file is first used to create a new process only a portion of the program and data for that file may be loaded into real memory later as page faults occur new portions of the program and data are loaded it is only at the time of first loading that virtual memory pages are created and assigned to locations on one of the devices to be used for swapping at that time the os is told whether it needs to clear set to the locations in the page frame before the first loading of a block of the program or data page replacement the page frame data table is used for page replacement several pointers are used to create lists within this table all of the available frames are linked together in a list of free frames available for bringing in pages when the number of available frames drops below a certain threshold the kernel will steal a number of frames to compensate the page replacement algorithm used in svr is a refinement of the clock policy algorithm figure known as the twohanded clock algorithm figure the algorithm uses the reference bit in the page table entry for each page in memory that is eligible not locked to be swapped out this bit is set to when the page is first brought in and set to when the page is referenced for a read or write one hand in the clock algorithm the fronthand sweeps through the pages on the list of eligible pages and sets the reference bit to on each page sometime later the backhand sweeps through the same list and checks the reference bit if the bit is set to then that page has been referenced since the fronthand swept by these frames are ignored if the bit is still set to then the page has not been referenced in the time interval between the visit by fronthand and backhand these pages are placed on a list to be paged out two parameters determine the operation of the algorithm scanrate the rate at which the two hands scan through the page list in pages per second handspread the gap between fronthand and backhand these two parameters have default values set at boot time based on the amount of physical memory the scanrate parameter can be altered to meet changing end of beginning page list of page list handspread fronthand backhand figure twohanded clock page replacement algorithm conditions the parameter varies linearly between the values slowscan and fastscan set at configuration time as the amount of free memory varies between the values lotsfree and minfree in other words as the amount of free memory shrinks the clock hands move more rapidly to free up more pages the handspread parameter determines the gap between the fronthand and the backhand and therefore together with scanrate determines the window of opportunity to use a page before it is swapped out due to lack of use kernel memory allocator the kernel generates and destroys small tables and buffers frequently during the course of execution each of which requires dynamic memory allocation vaha lists the following examples the pathname translation routing may allocate a buffer to copy a pathname from user space the allocb routine allocates streams buffers of arbitrary size many unix implementations allocate zombie structures to retain exit status and resource usage information about deceased processes in svr and solaris the kernel allocates many objects such as proc structures vnodes and file descriptor blocks dynamically when needed most of these blocks are significantly smaller than the typical machine page size and therefore the paging mechanism would be inefficient for dynamic kernel memory allocation for svr a modification of the buddy system described in section is used in buddy systems the cost to allocate and free a block of memory is low compared to that of bestfit or firstfit policies knut however in the case of kernel memory management the allocation and free operations must be made as fast as possible the drawback of the buddy system is the time required to fragment and coalesce blocks barkley and lee at att proposed a variation known as a lazy buddy system bark and this is the technique adopted for svr the authors observed that unix often exhibits steadystate behavior in kernel memory demand that is the amount of demand for blocks of a particular size varies slowly in time therefore if a block of size i is released and is immediately coalesced with its buddy into a block of size i the kernel may next request a block of size i which may necessitate splitting the larger block again to avoid this unnecessary coalescing and splitting the lazy buddy system defers coalescing until it seems likely that it is needed and then coalesces as many blocks as possible the lazy buddy system uses the following parameters ni current number of blocks of size i ai current number of blocks of size i that are allocated occupied gi current number of blocks of size i that are globally free these are blocks that are eligible for coalescing if the buddy of such a block becomes globally free then the two blocks will be coalesced into a globally free block of size i all free blocks holes in the standard buddy system could be considered globally free li current number of blocks of size i that are locally free these are blocks that are not eligible for coalescing even if the buddy of such a block becomes free the two blocks are not coalesced rather the locally free blocks are retained in anticipation of future demand for a block of that size the following relationship holds ni ai gi li in general the lazy buddy system tries to maintain a pool of locally free blocks and only invokes coalescing if the number of locally free blocks exceeds a threshold if there are too many locally free blocks then there is a chance that there will be a lack of free blocks at the next level to satisfy demand most of the time when a block is freed coalescing does not occur so there is minimal bookkeeping and operational costs when a block is to be allocated no distinction is made between locally and globally free blocks again this minimizes bookkeeping the criterion used for coalescing is that the number of locally free blocks of a given size should not exceed the number of allocated blocks of that size ie we must have li ai this is a reasonable guideline for restricting the growth of locally free blocks and experiments in bark confirm that this scheme results in noticeable savings to implement the scheme the authors define a delay variable as follows di ai li ni li gi figure shows the algorithm initial value of di is after an operation the value of di is updated as follows i if the next operation is a block allocate request if there is any free block select one to allocate if the selected block is locally free then di di else di di otherwise first get two blocks by splitting a larger one into two recursive operation allocate one and mark the other locally free di remains unchanged but d may change for other block sizes because of the recursive call ii if the next operation is a block free request case di mark it locally free and free it locally di case di mark it globally free and free it globally coalesce if possible di case di mark it globally free and free it globally coalesce if possible select one locally free block of size i and free it globally coalesce if possible di figure lazy buddy system algorithm linux memory management linux shares many of the characteristics of the memory management schemes of other unix implementations but has its own unique features overall the linux memory management scheme is quite complex dube in this section we give a brief overview of the two main aspects of linux memory management process virtual memory and kernel memory allocation linux virtual memory virtual memory addressing linux makes use of a threelevel page table structure consisting of the following types of tables each individual table is the size of one page page directory an active process has a single page directory that is the size of one page each entry in the page directory points to one page of the page middle directory the page directory must be in main memory for an active process page middle directory the page middle directory may span multiple pages each entry in the page middle directory points to one page in the page table page table the page table may also span multiple pages each page table entry refers to one virtual page of the process to use this threelevel page table structure a virtual address in linux is viewed as consisting of four fields figure the leftmost most significant field is used as an index into the page directory the next field serves as an index into the page middle directory the third field serves as an index into the page table the fourth field gives the offset within the selected page of memory the linux page table structure is platform independent and was designed to accommodate the bit alpha processor which provides hardware support for three levels of paging with bit addresses the use of only two levels of pages on the alpha would result in very large page tables and directories the bit pentium x architecture has a twolevel hardware paging mechanism the linux software accommodates the twolevel scheme by defining the size of the page middle directory as one note that all references to an extra level of indirection are optimized away at compile time not at run time therefore there is no performance overhead for using generic threelevel design on platforms which support only two levels in hardware page allocation to enhance the efficiency of reading in and writing out pages to and from main memory linux defines a mechanism for dealing with contiguous blocks of pages mapped into contiguous blocks of page frames for this purpose the buddy system is used the kernel maintains a list of contiguous page frame groups of fixed size a group may consist of or page frames as pages virtual address global directory middle directory page table offset page table page middle page frame directory in physical memory page directory cr register figure address translation in linux virtual memory scheme are allocated and deallocated in main memory the available groups are split and merged using the buddy algorithm page replacement algorithm the linux page replacement algorithm is based on the clock algorithm described in section see figure in the simple clock algorithm a use bit and a modify bit are associated with each page in main memory in the linux scheme the use bit is replaced with an bit age variable each time that a page is accessed the age variable is incremented in the background linux periodically sweeps through the global page pool and decrements the age variable for each page as it rotates through all the pages in main memory a page with an age of is an old page that has not been referenced in some time and is the best candidate for replacement the larger the value of age the more frequently a page has been used in recent times and the less eligible it is for replacement thus the linux algorithm is a form of least frequently used policy kernel memory allocation the linux kernel memory capability manages physical main memory page frames its primary function is to allocate and deallocate frames for particular uses possible owners of a frame include userspace processes ie the frame is part of the virtual memory of a process that is currently resident in real memory dynamically allocated kernel data static kernel code and the page cache the foundation of kernel memory allocation for linux is the page allocation mechanism used for user virtual memory management as in the virtual memory scheme a buddy algorithm is used so that memory for the kernel can be allocated and deallocated in units of one or more pages because the minimum amount of memory that can be allocated in this fashion is one page the page allocator alone would be inefficient because the kernel requires small shortterm memory chunks in odd sizes to accommodate these small chunks linux uses a scheme known as slab allocation bonw within an allocated page on a pentiumx machine the page size is kbytes and chunks within a page may be allocated of sizes and bytes the slab allocator is relatively complex and is not examined in detail here a good description can be found in vaha in essence linux maintains a set of linked lists one for each size of chunk chunks may be split and aggregated in a manner similar to the buddy algorithm and moved between lists accordingly windows memory management the windows virtual memory manager controls how memory is allocated and how paging is performed the memory manager is designed to operate over a variety of platforms and to use page sizes ranging from kbytes to kbytes intel the page cache has properties similar to a disk buffer described in this chapter as well as a disk cache described in chapter we defer a discussion of the linux page cache to chapter and amd platforms have kbytes per page and intel itanium platforms have kbytes per page windows virtual address map on bit platforms each windows user process sees a separate bit address space allowing gbytes of virtual memory per process by default half of this memory is reserved for the os so each user actually has gbytes of available virtual address space and all processes share most of the upper gbytes of system space when running in kernelmode large memory intensive applications on both clients and servers can run more effectively using bit windows other than netbooks most modern pcs use the amd processor architecture which is capable of running as either a bit or bit system figure shows the default virtual address space seen by a normal bit user process it consists of four regions x to xffff set aside to help programmers catch nullpointer assignments x to xffeffff available user address space this space is divided into pages that may be loaded into main memory kbyte region for nullpointer assignments inaccessible gbyte user address space unreserved usable kbyte region for badpointer assignments inaccessible gbyte region for the operating system inaccessible xffffffff figure windows default bit virtual address space xfff to xfffffff a guard page inaccessible to the user this page makes it easier for the os to check on outofbounds pointer references x to xffffffff system address space this gbyte process is used for the windows executive kernel hal and device drivers on bit platforms tbytes of user address space is available in windows windows paging when a process is created it can in principle make use of the entire user space of almost gbytes or tbytes on bit windows this space is divided into fixedsize pages any of which can be brought into main memory but the os manages the addresses in contiguous regions allocated on kbyte boundaries a region can be in one of three states available addresses not currently used by this process reserved addresses that the virtual memory manager has set aside for a process so they can not be allocated to another use eg saving contiguous space for a stack to grow committed addresses that the virtual memory manager has initialized for use by the process to access virtual memory pages these pages can reside either on disk or in physical memory when on disk they can be either kept in files mapped pages or occupy space in the paging file ie the disk file to which it writes pages when removing them from main memory the distinction between reserved and committed memory is useful because it reduces the amount of total virtual memory space needed by the system allowing the page file to be smaller and allows programs to reserve addresses without making them accessible to the program or having them charged against their resource quotas the resident set management scheme used by windows is variable allocation local scope see table when a process is first activated it is assigned data structures to manage its working set as the pages needed by the process are brought into physical memory the memory manager uses the data structures to keep track of the pages assigned to the process working sets of active processes are adjusted using the following general conventions when main memory is plentiful the virtual memory manager allows the resident sets of active processes to grow to do this when a page fault occurs a new physical page is added to the process but no older page is swapped out resulting in an increase of the resident set of that process by one page when memory becomes scarce the virtual memory manager recovers memory for the system by removing less recently used pages out of the working sets of active processes reducing the size of those resident sets even when memory is plentiful windows watches for large processes that are rapidly increasing their memory usage the system begins to remove pages that have not been recently used from the process this policy makes the system more responsive because a new program will not suddenly cause a scarcity of memory and make the user wait while the system tries to reduce the resident sets of the processes that are already running summary to use the processor and the io facilities efficiently it is desirable to maintain as many processes in main memory as possible in addition it is desirable to free programmers from size restrictions in program development the way to address both of these concerns is virtual memory with virtual memory all address references are logical references that are translated at run time to real addresses this allows a process to be located anywhere in main memory and for that location to change over time virtual memory also allows a process to be broken up into pieces these pieces need not be contiguously located in main memory during execution and indeed it is not even necessary for all of the pieces of the process to be in main memory during execution two basic approaches to providing virtual memory are paging and segmentation with paging each process is divided into relatively small fixedsize pages segmentation provides for the use of pieces of varying size it is also possible to combine segmentation and paging in a single memory management scheme a virtual memory management scheme requires both hardware and software support the hardware support is provided by the processor the support includes dynamic translation of virtual addresses to physical addresses and the generation of an interrupt when a referenced page or segment is not in main memory such an interrupt triggers the memory management software in the os a number of design issues relate to os support for memory management fetch policy process pages can be brought in on demand or a prepaging policy can be used which clusters the input activity by bringing in a number of pages at once placement policy with a pure segmentation system an incoming segment must be fit into an available space in memory replacement policy when memory is full a decision must be made as to which page or pages are to be replaced resident set management the os must decide how much main memory to allocate to a particular process when that process is swapped in this can be a static allocation made at process creation time or it can change dynamically cleaning policy modified process pages can be written out at the time of replacement or a precleaning policy can be used which clusters the output activity by writing out a number of pages at once load control load control is concerned with determining the number of processes that will be resident in main memory at any given time recommended reading and web sites as might be expected virtual memory receives good coverage in most books on operating systems mile provides a good summary of various research areas carr provides an excellent indepth examination of performance issues the classic paper denn is still well worth a read dowd provides an instructive performance analysis of various page replacement algorithms jacoa is a good survey of issues in virtual memory design it includes a discussion of inverted page tables jacob looks at virtual memory hardware organizations in various microprocessors it is a sobering experience to read ibm which gives a detailed account of the tools and options available to a site manager in optimizing the virtual memory policies of mvs the document illustrates the complexity of the problem vaha is one of the best treatments of the memory management schemes used in the various flavors of unix gorm is a thorough treatment of linux memory management carr carr r virtual memory management ann arbor mi umi research press denn denning p virtual memory computing surveys september dowd dowdy l and lowery c ps to operating systems upper saddle river nj prentice hall gorm gorman m understanding the linux virtual memory manager upper saddle river nj prentice hall ibm ibm national technical support large systems multiple virtual storage mvs virtual storage tuning cookbook dallas systems center technical bulletin g june jacoa jacob b and mudge t virtual memory issues of implementation computer june jacob jacob b and mudge t virtual memory in contemporary microprocessors ieee micro august mile milenkovic m operating systems concepts and design new york mcgrawhill vaha vahalia u unix internals the new frontiers upper saddle river nj prentice hall recommended web site the memory management reference a good source of documents and links on all aspects of memory management key terms review questions and problems key terms associative mapping page resident set management demand paging page fault segment external fragmentation page placement policy segment table fetch policy page replacement policy segmentation frame page table slab allocation hash table paging thrashing hashing prepaging translation lookaside buffer internal fragmentation real memory virtual memory locality resident set working set review questions what is the difference between simple paging and virtual memory paging explain thrashing why is the principle of locality crucial to the use of virtual memory what elements are typically found in a page table entry briefly define each element what is the purpose of a translation lookaside buffer briefly define the alternative page fetch policies what is the difference between resident set management and page replacement policy what is the relationship between fifo and clock page replacement algorithms what is accomplished by page buffering why is it not possible to combine a global replacement policy and a fixed allocation policy what is the difference between a resident set and a working set what is the difference between demand cleaning and precleaning problems suppose the page table for the process currently executing on the processor looks like the following all numbers are decimal everything is numbered starting from zero and all addresses are memory byte addresses the page size is bytes virtual page page frame number valid bit reference bit modify bit number a describe exactly how in general a virtual address generated by the cpu is translated into a physical main memory address b what physical address if any would each of the following virtual addresses correspond to do not try to handle any page faults if any i ii iii consider the following program define size int asize size bsize size csize size int register i j for j j size j for i i size i ci j ai j bi j assume that the program is running on a system using demand paging and the page size is kilobyte each integer is bytes long it is clear that each array requires a page space as an example a a a a a a and a a will be stored in the first data page a similar storage pattern can be derived for the rest of array a and for arrays b and c assume that the system allocates a page working set for this process one of the pages will be used by the program and three pages can be used for the data also two index registers are assigned for i and j so no memory accesses are needed for references to these two variables a discuss how frequently the page fault would occur in terms of number of times ci j ai j bi j are executed b can you modify the program to minimize the page fault frequency c what will be the frequency of page faults after your modification a how much memory space is needed for the user page table of figure b assume you want to implement a hashed inverted page table for the same addressing scheme as depicted in figure using a hash function that maps the bit page number into a bit hash value the table entry contains the page number the frame number and a chain pointer if the page table allocates space for up to overflow entries per hashed entry how much memory space does the hashed inverted page table take consider the following string of page references complete a figure similar to figure showing the frame allocation for a fifo firstinfirstout b lru least recently used c clock d optimal assume the page reference string continues with e list the total number of page faults and the miss rate for each policy count page faults only after all frames have been initialized a process references five pages a b c d and e in the following order a b c d a b e a b c d e assume that the replacement algorithm is firstinfirstout and find the number of page transfers during this sequence of references starting with an empty main memory with three page frames repeat for four page frames a process contains eight virtual pages on disk and is assigned a fixed allocation of four page frames in main memory the following page trace occurs a show the successive pages residing in the four frames using the lru replacement policy compute the hit ratio in main memory assume that the frames are initially empty b repeat part a for the fifo replacement policy c compare the two hit ratios and comment on the effectiveness of using fifo to approximate lru with respect to this particular trace in the vax user page tables are located at virtual addresses in the system spacewhat is the advantage of having user page tables in virtual rather than main memory what is the disadvantage suppose the program statement for i i n i ai bi ci is executed in a memory with page size of words let n using a machine that has a full range of registertoregister instructions and employs index registers write a hypothetical program to implement the foregoing statement then show the sequence of page references during execution the ibm system architecture uses a twolevel memory structure and refers to the two levels as segments and pages although the segmentation approach lacks many of the features described earlier in this chapter for the basic architecture the page size may be either kbytes or kbytes and the segment size is fixed at either kbytes or mbyte for the xa and esa architectures the page size is kbytes and the segment size is mbytewhich advantages of segmentation does this scheme lack what is the benefit of segmentation for the assuming a page size of kbytes and that a page table entry takes bytes how many levels of page tables would be required to map a bit address space if the top level page table fits into a single page consider a system with memory mapping done on a page basis and using a singlelevel page table assume that the necessary page table is always in memory a if a memory reference takes ns how long does a paged memory reference take b now we add an mmu that imposes an overhead of ns on a hit or a miss if we assume that of all memory references hit in the mmu tlb what is the effective memory access time emat c explain how the tlb hit rate affects the emat consider a page reference string for a process with a working set of m frames initially all empty the page reference string is of length p with n distinct page numbers in it for any page replacement algorithm a what is a lower bound on the number of page faults b what is an upper bound on the number of page faults in discussing a page replacement algorithm one author makes an analogy with a snowplow moving around a circular track snow is falling uniformly on the track and a lone snowplow continually circles the track at constant speedthe snow that is plowed w the track disappears from the system a for which of the page replacement algorithms discussed in section is this a useful analogy b what does this analogy suggest about the behavior of the page replacement algorithm in question in the s architecture a storage key is a control field associated with each pagesized frame of real memorytwo bits of that key that are relevant for page replacement are the reference bit and the change bit the reference bit is set to when any address within the frame is accessed for read or write and is set to when a new page is loaded into the frame the change bit is set to when a write operation is performed on any location within the frame suggest an approach for determining which page frames are leastrecentlyused making use of only the reference bit consider the following sequence of page references each element in the sequence represents a page number k define the mean working set size after the kth reference as sk k a wt t k and define the missing page probability after the kth reference as mk k a ft t where ft if a page fault occurs at virtual time t and otherwise a draw a diagram similar to that of figure for the reference sequence just defined for the values b plot s as a function of c plot m as a function of a key to the performance of the vsws resident set management policy is the value of q experience has shown that with a fixed value of q for a process there are considerable differences in page fault frequencies at various stages of execution furthermore if a single value of q is used for different processes dramatically different frequencies of page faults occur these differences strongly indicate that a mechanism that would dynamically adjust the value of q during the lifetime of a process would improve the behavior of the algorithm suggest a simple mechanism for this purpose assume that a task is divided into four equalsized segments and that the system builds an eightentry page descriptor table for each segment thus the system has a combination of segmentation and paging assume also that the page size is kbytes a what is the maximum size of each segment b what is the maximum logical address space for the task c assume that an element in physical location abc is accessed by this task what is the format of the logical address that the task generates for it what is the maximum physical address space for the system consider a paged logical address space composed of pages of kbytes each mapped into a mbyte physical memory space a what is the format of the processors logical address b what is the length and width of the page table disregarding the access rights bits c what is the effect on the page table if the physical memory space is reduced by half the unix kernel will dynamically grow a processs stack in virtual memory as needed but it will never try to shrink it consider the case in which a program calls a c subroutine that allocates a local array on the stack that consumes k the kernel will expand the stack segment to accommodate it when the subroutine returns the stack pointer is adjusted and this space could be released by the kernel but it is not released explain why it would be possible to shrink the stack at this point and why the unix kernel does not shrink it chapter uniprocessor scheduling types of processor scheduling longterm scheduling mediumterm scheduling shortterm scheduling scheduling algorithms shortterm scheduling criteria the use of priorities alternative scheduling policies performance comparison fairshare scheduling traditional unix scheduling summary recommended reading key terms review questions and problems i take a two hour nap from one oclock to four yogi berra learning objectives after studying this chapter you should be able to explain the differences among long medium and shortterm scheduling assess the performance of different scheduling policies understand the scheduling technique used in traditional unix in a multiprogramming system multiple processes exist concurrently in main memory each process alternates between using a processor and waiting for some event to occur such as the completion of an io operation the processor or processors are kept busy by executing one process while the others wait the key to multiprogramming is scheduling in fact four types of scheduling are typically involved table one of these io scheduling is more conveniently addressed in chapter where io is discussed the remaining three types of scheduling which are types of processor scheduling are addressed in this chapter and the next this chapter begins with an examination of the three types of processor scheduling showing how they are related we see that longterm scheduling and mediumterm scheduling are driven primarily by performance concerns related to the degree of multiprogramming these issues are dealt with to some extent in chapter and in more detail in chapters and thus the remainder of this chapter concentrates on shortterm scheduling and is limited to a consideration of scheduling on a uniprocessor system because the use of multiple processors adds additional complexity it is best to focus on the uniprocessor case first so that the differences among scheduling algorithms can be clearly seen section looks at the various algorithms that may be used to make shortterm scheduling decisions a set of animations that illustrate concepts in this chapter is available online click on the rotating globe at williamstallingscomosose html for access table types of scheduling longterm scheduling the decision to add to the pool of processes to be executed mediumterm scheduling the decision to add to the number of processes that are partially or fully in main memory shortterm scheduling the decision as to which available process will be executed by the processor io scheduling the decision as to which processs pending io request shall be handled by an available io device new longterm longterm scheduling scheduling ready ready running exit suspend mediumterm shortterm scheduling scheduling blocked blocked suspend mediumterm scheduling figure scheduling and process state transitions types of processor scheduling the aim of processor scheduling is to assign processes to be executed by the processor or processors over time in a way that meets system objectives such as response time throughput and processor efficiency in many systems this scheduling activity is broken down into three separate functions long medium and shortterm scheduling the names suggest the relative time scales with which these functions are performed figure relates the scheduling functions to the process state transition diagram first shown in figure b longterm scheduling is performed when a new process is created this is a decision whether to add a new process to the set of processes that are currently active mediumterm scheduling is a part of the swapping function this is a decision whether to add a process to those that are at least partially in main memory and therefore available for execution shortterm scheduling is the actual decision of which ready process to execute next figure reorganizes the state transition diagram of figure b to suggest the nesting of scheduling functions scheduling affects the performance of the system because it determines which processes will wait and which will progress this point of view is presented in figure which shows the queues involved in the state transitions of a process fundamentally scheduling is a matter of managing queues to minimize queueing delay and to optimize performance in a queueing environment longterm scheduling the longterm scheduler determines which programs are admitted to the system for processing thus it controls the degree of multiprogramming once admitted a job for simplicity figure shows new processes going directly to the ready state whereas figures and show the option of either the ready state or the readysuspend state running ready blocked short term blocked suspend ready suspend medium term long term new exit figure levels of scheduling or user program becomes a process and is added to the queue for the shortterm scheduler in some systems a newly created process begins in a swappedout condition in which case it is added to a queue for the mediumterm scheduler in a batch system or for the batch portion of an os newly submitted jobs are routed to disk and held in a batch queue the longterm scheduler creates processes from the queue when it can there are two decisions involved the scheduler must decide when the os can take on one or more additional processes and the scheduler must decide which job or jobs to accept and turn into processes we briefly consider these two decisions the decision as to when to create a new process is generally driven by the desired degree of multiprogramming the more processes that are created the smaller is the percentage of time that each process can be executed ie more processes are competing for the same amount of processor time thus the longterm scheduler may limit the degree of multiprogramming to provide satisfactory service longterm timeout scheduling batch ready queue shortterm release jobs scheduling processor mediumterm scheduling interactive ready suspend queue users mediumterm scheduling blocked suspend queue blocked queue event event wait occurs figure queueing diagram for scheduling to the current set of processes each time a job terminates the scheduler may decide to add one or more new jobs additionally if the fraction of time that the processor is idle exceeds a certain threshold the longterm scheduler may be invoked the decision as to which job to admit next can be on a simple firstcomefirstserved fcfs basis or it can be a tool to manage system performance the criteria used may include priority expected execution time and io requirements for example if the information is available the scheduler may attempt to keep a mix of processorbound and iobound processes also the decision can depend on which io resources are to be requested in an attempt to balance io usage for interactive programs in a timesharing system a process creation request can be generated by the act of a user attempting to connect to the system timesharing users are not simply queued up and kept waiting until the system can accept them rather the os will accept all authorized comers until the system is saturated using some predefined measure of saturation at that point a connection request is met with a message indicating that the system is full and the user should try again later mediumterm scheduling mediumterm scheduling is part of the swapping function the issues involved are discussed in chapters and typically the swappingin decision is based on the need to manage the degree of multiprogramming on a system that does not a process is regarded as processor bound if it mainly performs computational work and occasionally uses io devices a process is regarded as io bound if the time it takes to execute the process depends primarily on the time spent waiting for io operations use virtual memory memory management is also an issue thus the swappingin decision will consider the memory requirements of the swappedout processes shortterm scheduling in terms of frequency of execution the longterm scheduler executes relatively infrequently and makes the coarsegrained decision of whether or not to take on a new process and which one to take the mediumterm scheduler is executed somewhat more frequently to make a swapping decision the shortterm scheduler also known as the dispatcher executes most frequently and makes the finegrained decision of which process to execute next the shortterm scheduler is invoked whenever an event occurs that may lead to the blocking of the current process or that may provide an opportunity to preempt a currently running process in favor of another examples of such events include clock interrupts io interrupts operating system calls signals eg semaphores scheduling algorithms shortterm scheduling criteria the main objective of shortterm scheduling is to allocate processor time in such a way as to optimize one or more aspects of system behavior generally a set of criteria is established against which various scheduling policies may be evaluated the commonly used criteria can be categorized along two dimensions first we can make a distinction between useroriented and systemoriented criteria useroriented criteria relate to the behavior of the system as perceived by the individual user or process an example is response time in an interactive system response time is the elapsed time between the submission of a request until the response begins to appear as output this quantity is visible to the user and is naturally of interest to the user we would like a scheduling policy that provides good service to various users in the case of response time a threshold may be defined say two seconds then a goal of the scheduling mechanism should be to maximize the number of users who experience an average response time of two seconds or less other criteria are system oriented that is the focus is on effective and efficient utilization of the processor an example is throughput which is the rate at which processes are completed this is certainly a worthwhile measure of system performance and one that we would like to maximize however it focuses on system performance rather than service provided to the user thus throughput is of concern to a system administrator but not to the user population whereas useroriented criteria are important on virtually all systems systemoriented criteria are generally of minor importance on singleuser systems on a singleuser system it probably is not important to achieve high processor utilization or high throughput as long as the responsiveness of the system to user applications is acceptable another dimension along which criteria can be classified is those that are performance related and those that are not directly performance related performancerelated criteria are quantitative and generally can be readily measured examples include response time and throughput criteria that are not performance related are either qualitative in nature or do not lend themselves readily to measurement and analysis an example of such a criterion is predictability we would like for the service provided to users to exhibit the same characteristics over time independent of other work being performed by the system to some extent this criterion can be measured by calculating variances as a function of workload however this is not nearly as straightforward as measuring throughput or response time as a function of workload table summarizes key scheduling criteria these are interdependent and it is impossible to optimize all of them simultaneously for example providing good table scheduling criteria user oriented performance related turnaround time this is the interval of time between the submission of a process and its completion includes actual execution time plus time spent waiting for resources including the processor this is an appropriate measure for a batch job response time for an interactive process this is the time from the submission of a request until the response begins to be received often a process can begin producing some output to the user while continuing to process the request thus this is a better measure than turnaround time from the users point of view the scheduling discipline should attempt to achieve low response time and to maximize the number of interactive users receiving acceptable response time deadlines when process completion deadlines can be specified the scheduling discipline should subordinate other goals to that of maximizing the percentage of deadlines met user oriented other predictability a given job should run in about the same amount of time and at about the same cost regardless of the load on the system a wide variation in response time or turnaround time is distracting to users it may signal a wide swing in system workloads or the need for system tuning to cure instabilities system oriented performance related throughput the scheduling policy should attempt to maximize the number of processes completed per unit of time this is a measure of how much work is being performed this clearly depends on the average length of a process but is also influenced by the scheduling policy which may affect utilization processor utilization this is the percentage of time that the processor is busy for an expensive shared system this is a significant criterion in singleuser systems and in some other systems such as realtime systems this criterion is less important than some of the others system oriented other fairness in the absence of guidance from the user or other systemsupplied guidance processes should be treated the same and no process should suffer starvation enforcing priorities when processes are assigned priorities the scheduling policy should favor higherpriority processes balancing resources the scheduling policy should keep the resources of the system busy processes that will underutilize stressed resources should be favored this criterion also involves mediumterm and longterm scheduling response time may require a scheduling algorithm that switches between processes frequently this increases the overhead of the system reducing throughput thus the design of a scheduling policy involves compromising among competing requirements the relative weights given the various requirements will depend on the nature and intended use of the system in most interactive operating systems whether single user or time shared adequate response time is the critical requirement because of the importance of this requirement and because the definition of adequacy will vary from one application to another the topic is explored further in appendix g the use of priorities in many systems each process is assigned a priority and the scheduler will always choose a process of higher priority over one of lower priority figure illustrates the use of priorities for clarity the queueing diagram is simplified ignoring the existence of multiple blocked queues and of suspended states compare figure a instead of a single ready queue we provide a set of queues in descending order of priority rq rq rqn with priorityrqi priorityrqj for i j when a scheduling selection is to be made the scheduler will start at the highestpriority ready queue rq if there are one or more processes in the queue a process is selected using some scheduling policy if rq is empty then rq is examined and so on rq release dispatch processor rq admit rqn preemption event wait event occurs blocked queue figure priority queueing in unix and many other systems larger priority values represent lower priority processes unless otherwise stated we follow that convention some systems such as windows use the opposite convention a higher number means a higher priority one problem with a pure priority scheduling scheme is that lowerpriority processes may suffer starvation this will happen if there is always a steady supply of higherpriority ready processes if this behavior is not desirable the priority of a process can change with its age or execution history we will give one example of this subsequently alternative scheduling policies table presents some summary information about the various scheduling policies that are examined in this subsection the selection function determines which process among ready processes is selected next for execution the function may be based on priority resource requirements or the execution characteristics of the process in the latter case three quantities are significant w time spent in system so far waiting e time spent in execution so far s total service time required by the process including e generally this quantity must be estimated or supplied by the user for example the selection function maxw indicates an fcfs discipline table characteristics of various scheduling policies fcfs round spn srt hrrn feedback robin selection maxw constant mins mins e max a w s b see text function s decision nonpreemptive nonpreemptive nonpreemptive mode preemptive at time preemptive at arrival preemptive at time quantum quantum not may be low not throughput emphasized if quantum high high high emphasized is too small may be high especially if provides provides there is good good provides provides response a large response response good good not time variance time for time for response response emphasized in process short short time time execution processes processes times overhead minimum minimum can be high can be high can be high can be high penalizes short penalizes penalizes may favor effect on processes fair long long good io bound processes penalizes treatment processes processes balance processes io bound processes starvation no no possible possible no possible the decision mode specifies the instants in time at which the selection function is exercised there are two general categories nonpreemptive in this case once a process is in the running state it continues to execute until a it terminates or b it blocks itself to wait for io or to request some os service preemptive the currently running process may be interrupted and moved to the ready state by the os the decision to preempt may be performed when a new process arrives when an interrupt occurs that places a blocked process in the ready state or periodically based on a clock interrupt preemptive policies incur greater overhead than nonpreemptive ones but may provide better service to the total population of processes because they prevent any one process from monopolizing the processor for very long in addition the cost of preemption may be kept relatively low by using efficient processswitching mechanisms as much help from hardware as possible and by providing a large main memory to keep a high percentage of programs in main memory as we describe the various scheduling policies we will use the set of processes in table as a running example we can think of these as batch jobs with the service time being the total execution time required alternatively we can consider these to be ongoing processes that require alternate use of the processor and io in a repetitive fashion in this latter case the service times represent the processor time required in one cycle in either case in terms of a queueing model this quantity corresponds to the service time for the example of table figure shows the execution pattern for each policy for one cycle and table summarizes some key results first the finish time of each process is determined from this we can determine the turnaround time in terms of the queueing model turnaround time tat is the residence time tr or total time that the item spends in the system waiting time plus service time a more useful figure is the normalized turnaround time which is the ratio of turnaround time to service time this value indicates the relative table process scheduling example process arrival time service time a b c d e see appendix h for a summary of queueing model terminology and chapter for a more detailed discussion of queueing analysis firstcomefirst a served fcfs b c d e roundrobin a rr q b c d e roundrobin a rr q b c d e shortest process a next spn b c d e shortest remaining a time srt b c d e highest response a ratio next hrrn b c d e feedback a q b c d e feedback a q i b c d e figure a comparison of scheduling policies delay experienced by a process typically the longer the process execution time the greater is the absolute amount of delay that can be tolerated the minimum possible value for this ratio is increasing values correspond to a decreasing level of service table a comparison of scheduling policies process a b c d e arrival time service time ts mean fcfs finish time turnaround time tr trts rr q finish time turnaround time tr trts rr q finish time turnaround time tr trts spn finish time turnaround time tr trts srt finish time turnaround time tr trts hrrn finish time turnaround time tr trts fb q finish time turnaround time tr trts fb q i finish time turnaround time tr trts firstcomefirstserved the simplest scheduling policy is firstcomefirstserved fcfs also known as firstinfirstout fifo or a strict queueing scheme as each process becomes ready it joins the ready queue when the currently running process ceases to execute the process that has been in the ready queue the longest is selected for running fcfs performs much better for long processes than short ones consider the following example based on one in fink arrival service finish turnaround process time time ts start time time time tr tr ts w x y z mean the normalized turnaround time for process y is way out of line compared to the other processes the total time that it is in the system is times the required processing time this will happen whenever a short process arrives just after a long process on the other hand even in this extreme example long processes do not fare poorly process z has a turnaround time that is almost double that of y but its normalized residence time is under another difficulty with fcfs is that it tends to favor processorbound processes over iobound processes consider that there is a collection of processes one of which mostly uses the processor processor bound and a number of which favor io io bound when a processorbound process is running all of the io bound processes must wait some of these may be in io queues blocked state but may move back to the ready queue while the processorbound process is executing at this point most or all of the io devices may be idle even though there is potentially work for them to do when the currently running process leaves the running state the ready iobound processes quickly move through the running state and become blocked on io events if the processorbound process is also blocked the processor becomes idle thus fcfs may result in inefficient use of both the processor and the io devices fcfs is not an attractive alternative on its own for a uniprocessor system however it is often combined with a priority scheme to provide an effective scheduler thus the scheduler may maintain a number of queues one for each priority level and dispatch within each queue on a firstcomefirstserved basis we see one example of such a system later in our discussion of feedback scheduling round robin a straightforward way to reduce the penalty that short jobs suffer with fcfs is to use preemption based on a clock the simplest such policy is round robin a clock interrupt is generated at periodic intervals when the interrupt occurs the currently running process is placed in the ready queue and the next ready job is selected on a fcfs basis this technique is also known as time slicing because each process is given a slice of time before being preempted with round robin the principal design issue is the length of the time quantum or slice to be used if the quantum is very short then short processes will move through the system relatively quickly on the other hand there is processing overhead involved in handling the clock interrupt and performing the scheduling and dispatching function thus very short time quanta should be avoided one useful guide is that the time quantum should be slightly greater than the time required for a typical interaction or process function if it is less then most processes will require at least two time quanta figure illustrates the effect this has on response time note that in the limiting case of a time quantum that is longer than the longestrunning process round robin degenerates to fcfs figure and table show the results for our example using time quanta q of and time units note that process e which is the shortest job enjoys significant improvement for a time quantum of round robin is particularly effective in a generalpurpose timesharing system or transaction processing system one drawback to round robin is its relative time process allocated interaction time quantum complete response time qs s quantum q a time quantum greater than typical interaction process allocated process process allocated interaction time quantum preempted time quantum complete q other processes run s b time quantum less than typical interaction figure effect of size of preemption time quantum treatment of processorbound and iobound processes generally an iobound process has a shorter processor burst amount of time spent executing between io operations than a processorbound process if there is a mix of processorbound and iobound processes then the following will happen an iobound process uses a processor for a short period and then is blocked for io it waits for the io operation to complete and then joins the ready queue on the other hand a processorbound process generally uses a complete time quantum while executing and immediately returns to the ready queue thus processorbound processes tend to receive an unfair portion of processor time which results in poor performance for iobound processes inefficient use of io devices and an increase in the variance of response time hald suggests a refinement to round robin that he refers to as a virtual round robin vrr and that avoids this unfairness figure illustrates the scheme new processes arrive and join the ready queue which is managed on an fcfs basis when a running process times out it is returned to the ready queue when a process is blocked for io it joins an io queue so far this is as usual the new feature is an fcfs auxiliary queue to which processes are moved after being released from an io block when a dispatching decision is to be made processes in the auxiliary queue get preference over those in the main ready queue when a process is dispatched from the auxiliary queue it runs no longer than a time equal to the basic time quantum minus the total time spent running since it was last selected from the timeout ready queue admit dispatch release processor auxiliary queue io io wait occurs io queue io io wait occurs io queue io n io n wait occurs io n queue figure queueing diagram for virtual roundrobin scheduler main ready queue performance studies by the authors indicate that this approach is indeed superior to round robin in terms of fairness shortest process next another approach to reducing the bias in favor of long processes inherent in fcfs is the shortest process next spn policy this is a nonpreemptive policy in which the process with the shortest expected processing time is selected next thus a short process will jump to the head of the queue past longer jobs figure and table show the results for our example note that process e receives service much earlier than under fcfs overall performance is also significantly improved in terms of response time however the variability of response times is increased especially for longer processes and thus predictability is reduced one difficulty with the spn policy is the need to know or at least estimate the required processing time of each process for batch jobs the system may require the programmer to estimate the value and supply it to the os if the programmers estimate is substantially under the actual running time the system may abort the job in a production environment the same jobs run frequently and statistics may be gathered for interactive processes the os may keep a running average of each burst for each process the simplest calculation would be the following n sn n a ti i where ti processor execution time for the ith instance of this process total execution time for batch job processor burst time for interactive job si predicted value for the ith instance s predicted value for first instance not calculated to avoid recalculating the entire summation each time we can rewrite equation as sn tn n sn n n note that each term in this summation is given equal weight that is each term is multiplied by the same constant n typically we would like to give greater weight to more recent instances because these are more likely to reflect future behavior a common technique for predicting a future value on the basis of a time series of past values is exponential averaging sn atn asn where is a constant weighting factor that determines the relative weight given to more recent observations relative to older observations compare with equation by using a constant value of independent of the number of past observations equation considers all past values but the less recent ones have less weight to see this more clearly consider the following expansion of equation sn atn aatn c aiatn i c ans because both and are less than each successive term in the preceding equation is smaller for example for equation becomes sn tn tn tn tn c ns the older the observation the less it is counted in to the average the size of the coefficient as a function of its position in the expansion is shown in figure the larger the value of the greater is the weight given to the more recent observations for virtually all of the weight is given to the four most recent observations whereas for the averaging is effectively spread out over the eight or so most recent observations the advantage of using a value of close to is that the average will quickly reflect a rapid change in the observed quantity the disadvantage is that if there is a brief surge in the value of the observed quantity and it then settles back to some average value the use of a large value of will result in jerky changes in the average figure compares simple averaging with exponential averaging for two different values of in figure a the observed value begins at grows gradually to a value of and then stays there in figure b the observed value begins at declines gradually to and then stays there in both cases we start out with an estimate of s this gives greater priority to new processes note that exponential averaging tracks changes in process behavior faster than does simple averaging and that the larger value of results in a more rapid reaction to the change in the observed value a risk with spn is the possibility of starvation for longer processes as long as there is a steady supply of shorter processes on the other hand although spn reduces the bias in favor of longer jobs it still is not desirable for a timesharing or transaction processing environment because of the lack of preemption looking back at our worstcase analysis described under fcfs processes w x y and z will still execute in the same order heavily penalizing the short process y shortest remaining time the shortest remaining time srt policy is a preemptive version of spn in this case the scheduler always chooses the process coefficient value a a a age of observation figure exponential smoothing coefficients observed or average value simple average observed value time a increasing function observed or average value simple average observed value time b decreasing function figure use of exponential averaging that has the shortest expected remaining processing time when a new process joins the ready queue it may in fact have a shorter remaining time than the currently running process accordingly the scheduler may preempt the current process when a new process becomes ready as with spn the scheduler must have an estimate of processing time to perform the selection function and there is a risk of starvation of longer processes srt does not have the bias in favor of long processes found in fcfs unlike round robin no additional interrupts are generated reducing overhead on the other hand elapsed service times must be recorded contributing to overhead srt should also give superior turnaround time performance to spn because a short job is given immediate preference to a running longer job note that in our example table the three shortest processes all receive immediate service yielding a normalized turnaround time for each of highest response ratio next in table we have used the normalized turnaround time which is the ratio of turnaround time to actual service time as a figure of merit for each individual process we would like to minimize this ratio and we would like to minimize the average value over all processes in general we can not know ahead of time what the service time is going to be but we can approximate it either based on past history or some input from the user or a configuration manager consider the following ratio r ws s where r response ratio w time spent waiting for the processor s expected service time if the process with this value is dispatched immediately r is equal to the normalized turnaround time note that the minimum value of r is which occurs when a process first enters the system thus our scheduling rule becomes the following when the current process completes or is blocked choose the ready process with the greatest value of r this approach is attractive because it accounts for the age of the process while shorter jobs are favored a smaller denominator yields a larger ratio aging without service increases the ratio so that a longer process will eventually get past competing shorter jobs as with srt and spn the expected service time must be estimated to use highest response ratio next hrrn feedback if we have no indication of the relative length of various processes then none of spn srt and hrrn can be used another way of establishing a preference for shorter jobs is to penalize jobs that have been running longer in other words if we can not focus on the time remaining to execute let us focus on the time spent in execution so far the way to do this is as follows scheduling is done on a preemptive at time quantum basis and a dynamic priority mechanism is used when a process first enters the system it is placed in rq see figure after its first preemption when it returns to the ready state it is placed in rq each subsequent time that it is preempted it is demoted to the next lowerpriority queue a short process will complete quickly without migrating very far down the hierarchy of ready queues a longer process will gradually drift downward thus newer shorter processes are favored over older longer processes within each queue except the lowestpriority queue a simple fcfs mechanism is used once in the lowestpriority queue a rq release admit processor rq release processor rqn release processor figure feedback scheduling process can not go lower but is returned to this queue repeatedly until it completes execution thus this queue is treated in roundrobin fashion figure illustrates the feedback scheduling mechanism by showing the path that a process will follow through the various queues this approach is known as multilevel feedback meaning that the os allocates the processor to a process and when the process blocks or is preempted feeds it back into one of several priority queues there are a number of variations on this scheme a simple version is to perform preemption in the same fashion as for round robin at periodic intervals our example shows this figure and table for a quantum of one time unit note that in this case the behavior is similar to round robin with a time quantum of one problem with the simple scheme just outlined is that the turnaround time of longer processes can stretch out alarmingly indeed it is possible for starvation to occur if new jobs are entering the system frequently to compensate for this we can vary the preemption times according to the queue a process scheduled from rq is allowed to execute for one time unit and then is preempted a process scheduled from rq is allowed to execute two time units and so on in general a process scheduled from rqi is allowed to execute i time units before preemption this scheme is illustrated for our example in figure and table dotted lines are used to emphasize that this is a time sequence diagram rather than a static depiction of possible transitions such as figure even with the allowance for greater time allocation at lower priority a longer process may still suffer starvation a possible remedy is to promote a process to a higherpriority queue after it spends a certain amount of time waiting for service in its current queue performance comparison clearly the performance of various scheduling policies is a critical factor in the choice of a scheduling policy however it is impossible to make definitive comparisons because relative performance will depend on a variety of factors including the probability distribution of service times of the various processes the efficiency of the scheduling and context switching mechanisms and the nature of the io demand and the performance of the io subsystem nevertheless we attempt in what follows to draw some general conclusions queueing analysis in this section we make use of basic queueing formulas with the common assumptions of poisson arrivals and exponential service times first we make the observation that any such scheduling discipline that chooses the next item to be served independent of service time obeys the following relationship tr ts r where tr turnaround time or residence time total time in system waiting plus execution ts average service time average time spent in running state r processor utilization in particular a prioritybased scheduler in which the priority of each process is assigned independent of expected service time provides the same average turnaround time and average normalized turnaround time as a simple fcfs discipline furthermore the presence or absence of preemption makes no differences in these averages with the exception of round robin and fcfs the various scheduling disciplines considered so far do make selections on the basis of expected service time unfortunately it turns out to be quite difficult to develop closed analytic models of these disciplines however we can get an idea of the relative performance of such scheduling algorithms compared to fcfs by considering priority scheduling in which priority is based on service time if scheduling is done on the basis of priority and if processes are assigned to a priority class on the basis of service time then differences do emerge table shows the formulas that result when we assume two priority classes with different service times for each class in the table refers to the arrival rate these results can the queueing terminology used in this chapter is summarized in appendix h poisson arrivals essentially means random arrivals as explained in appendix h table formulas for singleserver queues with two priority categories assumptions poisson arrival rate priority items are serviced before priority items firstcomefirstserved dispatching for items of equal priority no item is interrupted while being served no items leave the queue lost calls delayed a general formulas l l l r lts r lts r r r ts l ts l ts l l tr l tr l tr l l b no interrupts exponential service times c preemptiveresume queueing discipline exponential service times tr ts rts rts tr ts rts r r tr ts tr ts tr ts a rts rts b r r r be generalized to any number of priority classes note that the formulas differ for nonpreemptive versus preemptive scheduling in the latter case it is assumed that a lowerpriority process is immediately interrupted when a higherpriority process becomes ready as an example let us consider the case of two priority classes with an equal number of process arrivals in each class and with the average service time for the lowerpriority class being five times that of the upper priority class thus we wish to give preference to shorter processes figure shows the overall result by giving preference to shorter jobs the average normalized turnaround time is improved at higher levels of utilization as might be expected the improvement is greatest with the use of preemption notice however that overall performance is not much affected however significant differences emerge when we consider the two priority classes separately figure shows the results for the higherpriority shorter processes for comparison the upper line on the graph assumes that priorities are not used but that we are simply looking at the relative performance of that half of all processes that have the shorter processing time the other two lines assume that these processes are assigned a higher priority when the system is run using priority scheduling without preemption the improvements are significant they are even more significant when preemption is used priority classes ts ts normalized response time trts priority priority with preemption no priority utilization figure overall normalized response time priority classes ts ts normalized response time trts no priority priority priority with preemption utilization figure normalized response time for shorter processes priority classes ts ts normalized response time trts priority with preemption priority no priority utilization figure normalized response time for longer processes figure shows the same analysis for the lowerpriority longer processes as expected such processes suffer a performance degradation under priority scheduling simulation modeling some of the difficulties of analytic modeling are overcome by using discreteevent simulation which allows a wide range of policies to be modeled the disadvantage of simulation is that the results for a given run only apply to that particular collection of processes under that particular set of assumptions nevertheless useful insights can be gained the results of one such study are reported in fink the simulation involved processes with an arrival rate of and an average service time of ts thus the assumption is that the processor utilization is r ts note therefore that we are only measuring one utilization point to present the results processes are grouped into servicetime percentiles each of which has processes thus the processes with the shortest service time are in the first percentile with these eliminated the remaining processes with the shortest service time are in the second percentile and so on this allows us to view the effect of various policies on processes as a function of the length of the process figure shows the normalized turnaround time and figure shows the average waiting time looking at the turnaround time we can see that the performance of fcfs is very unfavorable with onethird of the processes having fcfs normalized turnaround time hrrn fb rr q srt rr q spn spn hrrn fb fcfs srt percentile of time required figure simulation result for normalized turnaround time rr q fb spn hrrn wait time fcfs fcfs rr q hrrn spn srt fb percentile of time required figure simulation result for waiting time a normalized turnaround time greater than times the service time furthermore these are the shortest processes on the other hand the absolute waiting time is uniform as is to be expected because scheduling is independent of service time the figures show round robin using a quantum of one time unit except for the shortest processes which execute in less than one quantum round robin yields a normalized turnaround time of about five for all processes treating all fairly shortest process next performs better than round robin except for the shortest processes shortest remaining time the preemptive version of spn performs better than spn except for the longest of all processes we have seen that among nonpreemptive policies fcfs favors long processes and spn favors short ones highest response ratio next is intended to be a compromise between these two effects and this is indeed confirmed in the figures finally the figure shows feedback scheduling with fixed uniform quanta in each priority queue as expected fb performs quite well for short processes fairshare scheduling all of the scheduling algorithms discussed so far treat the collection of ready processes as a single pool of processes from which to select the next running process this pool may be broken down by priority but is otherwise homogeneous however in a multiuser system if individual user applications or jobs may be organized as multiple processes or threads then there is a structure to the collection of processes that is not recognized by a traditional scheduler from the users point of view the concern is not how a particular process performs but rather how his or her set of processes which constitute a single application performs thus it would be attractive to make scheduling decisions on the basis of these process sets this approach is generally known as fairshare scheduling further the concept can be extended to groups of users even if each user is represented by a single process for example in a timesharing system we might wish to consider all of the users from a given department to be members of the same group scheduling decisions could then be made that attempt to give each group similar service thus if a large number of people from one department log onto the system we would like to see response time degradation primarily affect members of that department rather than users from other departments the term fair share indicates the philosophy behind such a scheduler each user is assigned a weighting of some sort that defines that users share of system resources as a fraction of the total usage of those resources in particular each user is assigned a share of the processor such a scheme should operate in a more or less linear fashion so that if user a has twice the weighting of user b then in the long run user a should be able to do twice as much work as user b the objective of a fairshare scheduler is to monitor usage to give fewer resources to users who have had more than their fair share and more to those who have had less than their fair share a number of proposals have been made for fairshare schedulers henr kay wood in this section we describe the scheme proposed in henr and implemented on a number of unix systems the scheme is simply referred to as the fairshare scheduler fss fss considers the execution history of a related group of processes along with the individual execution history of each process in making scheduling decisions the system divides the user community into a set of fairshare groups and allocates a fraction of the processor resource to each group thus there might be four groups each with of the processor usage in effect each fairshare group is provided with a virtual system that runs proportionally slower than a full system scheduling is done on the basis of priority which takes into account the underlying priority of the process its recent processor usage and the recent processor usage of the group to which the process belongs the higher the numerical value of the priority the lower is the priority the following formulas apply for process j in group k cpuji cpuji gcpuki gcpuki pji basej cpuji gcpuki wk where cpuji measure of processor utilization by process j through interval i gcpuki measure of processor utilization of group k through interval i pji priority of process j at beginning of interval i lower values equal higher priorities basej base priority of process j wk weighting assigned to group k with the constraint that wk and a wk k each process is assigned a base priority the priority of a process drops as the process uses the processor and as the group to which the process belongs uses the processor in the case of the group utilization the average is normalized by dividing by the weight of that group the greater the weight assigned to the group the less its utilization will affect its priority figure is an example in which process a is in one group and processes b and c are in a second group with each group having a weighting of assume that all processes are processor bound and are usually ready to run all processes have a base priority of processor utilization is measured as follows the processor is interrupted times per second during each interrupt the processor usage field of the currently running process is incremented as is the corresponding group processor field once per second priorities are recalculated in the figure process a is scheduled first at the end of one second it is preempted processes b and c now have the higher priority and process b is scheduled at the end of the second time unit process a has the highest priority note that the pattern repeats the kernel schedules the processes in order a b a c a b and so on thus of the processor is allocated to process a which constitutes one group and to processes b and c which constitute another group process a process b process c process group process group process group time cpu cpu cpu cpu cpu cpu priority count count priority count count priority count count group group colored rectangle represents executing process figure example of fairshare scheduler three processes two groups traditional unix scheduling in this section we examine traditional unix scheduling which is used in both svr and bsd unix these systems are primarily targeted at the timesharing interactive environment the scheduling algorithm is designed to provide good response time for interactive users while ensuring that lowpriority background jobs do not starve although this algorithm has been replaced in modern unix systems it is worthwhile to examine the approach because it is representative of practical timesharing scheduling algorithms the scheduling scheme for svr includes an accommodation for realtime requirements and so its discussion is deferred to chapter the traditional unix scheduler employs multilevel feedback using round robin within each of the priority queues the system makes use of onesecond preemption that is if a running process does not block or complete within one second it is preempted priority is based on process type and execution history the following formulas apply cpuji cpuji pji basej cpuji nicej where cpuji measure of processor utilization by process j through interval i pji priority of process j at beginning of interval i lower values equal higher priorities basej base priority of process j nicej usercontrollable adjustment factor the priority of each process is recomputed once per second at which time a new scheduling decision is made the purpose of the base priority is to divide all processes into fixed bands of priority levels the cpu and nice components are restricted to prevent a process from migrating out of its assigned band assigned by the base priority level these bands are used to optimize access to block devices eg disk and to allow the os to respond quickly to system calls in decreasing order of priority the bands are swapper block io device control file manipulation character io device control user processes this hierarchy should provide the most efficient use of the io devices within the user process band the use of execution history tends to penalize processorbound processes at the expense of iobound processes again this should improve efficiency coupled with the roundrobin preemption scheme the scheduling strategy is well equipped to satisfy the requirements for generalpurpose time sharing an example of process scheduling is shown in figure processes a b and c are created at the same time with base priorities of we will ignore the nice value the clock interrupts the system times per second and increments a counter for the running process the example assumes that none of the processes block themselves and that no other processes are ready to run compare this with figure time process a process b process c priority cpu count priority cpu count priority cpu count colored rectangle represents executing process figure example of a traditional unix process scheduling summary the os must make three types of scheduling decisions with respect to the execution of processes longterm scheduling determines when new processes are admitted to the system mediumterm scheduling is part of the swapping function and determines when a program is brought partially or fully into main memory so that it may be executed shortterm scheduling determines which ready process will be executed next by the processor this chapter focuses on the issues relating to shortterm scheduling a variety of criteria are used in designing the shortterm scheduler some of these criteria relate to the behavior of the system as perceived by the individual user user oriented while others view the total effectiveness of the system in meeting the needs of all users system oriented some of the criteria relate specifically to quantitative measures of performance while others are more qualitative in nature from a users point of view response time is generally the most important characteristic of a system while from a system point of view throughput or processor utilization is important a variety of algorithms have been developed for making the shortterm scheduling decision among all ready processes firstcomefirstserved select the process that has been waiting the longest for service round robin use time slicing to limit any running process to a short burst of processor time and rotate among all ready processes shortest process next select the process with the shortest expected processing time and do not preempt the process shortest remaining time select the process with the shortest expected remaining process time a process may be preempted when another process becomes ready highest response ratio next base the scheduling decision on an estimate of normalized turnaround time feedback establish a set of scheduling queues and allocate processes to queues based on execution history and other criteria the choice of scheduling algorithm will depend on expected performance and on implementation complexity recommended reading virtually every textbook on operating systems covers scheduling rigorous queueing analyses of various scheduling policies are presented in klei and conw dowd provides an instructive performance analysis of various scheduling algorithms conw conway r maxwell w and miller l theory of scheduling reading ma addisonwesley reprinted by dover publications dowd dowdy l and lowery c ps to operating systems upper saddle river nj prentice hall klei kleinrock l queuing systems volume three computer applications new york wiley key terms review questions and problems key terms arrival rate mediumterm scheduler shortterm scheduler dispatcher multilevel feedback throughput exponential averaging predictability time slicing fairshare scheduling residence time turnaround time fairness response time utilization firstcomefirstserved round robin waiting time firstinfirstout scheduling priority longterm scheduler service time review questions briefly describe the three types of processor scheduling what is usually the critical performance requirement in an interactive operating system what is the difference between turnaround time and response time for process scheduling does a lowpriority value represent a low priority or a high priority what is the difference between preemptive and nonpreemptive scheduling briefly define fcfs scheduling briefly define roundrobin scheduling briefly define shortestprocessnext scheduling briefly define shortestremainingtime scheduling briefly define highestresponserationext scheduling briefly define feedback scheduling problems consider the following workload process burst time priority arrival time p ms ms p ms ms p ms ms p ms ms a show the schedule using shortest remaining time nonpreemptive priority a smaller priority number implies higher priority and round robin with quantum ms use time scale diagram as shown below for the fcfs example to show the schedule for each requested scheduling policy example for fcfs unit ms p p p p p p p p p p p p p p p p p p p p p b what is the average waiting time of the above scheduling policies consider the following set of processes process arrival time processing time a b c d e perform the same analysis as depicted in table and figure for this set prove that among nonpreemptive scheduling algorithms spn provides the minimum average waiting time for a batch of jobs that arrive at the same time assume that the scheduler must always execute a task if one is available assume the following bursttime pattern for a process and assume that the initial guess is produce a plot similar to those of figure consider the following pair of equations as an alternative to equation sn atn asn xn minubound maxlbound bsn where ubound and lbound are prechosen upper and lower bounds on the estimated value of tthe value of xn is used in the shortestprocessnext algorithm instead of the value of sn what functions do a and b perform and what is the effect of higher and lower values on each in the bottom example in figure process a runs for two time units before control is passed to process b another plausible scenario would be that a runs for three time units before control is passed to process b what policy differences in the feedbackscheduling algorithm would account for the two different scenarios in a nonpreemptive uniprocessor system the ready queue contains three jobs at time t immediately after the completion of a job these jobs arrived at times t t and t with estimated execution times of r r and r respectively figure shows the linear increase of their response ratios over time use this example to find a variant of response ratio scheduling known as minimax response ratio scheduling that minimizes the maximum response ratio for a given batch of jobs ignoring further arrivals hint decide first which job to schedule as the last one prove that the minimax response ratio algorithm of the preceding problem minimizes the maximum response ratio for a given batch of jobs hint focus attention on the job that will achieve the highest response ratio and all jobs executed before it consider the same subset of jobs scheduled in any other order and observe the response ratio of the job that is executed as the last one among them notice that this subset may now be mixed with other jobs from the total set define residence time tr as the average total time a process spends waiting and being served show that for fifo with mean service time ts we have tr ts r where r is utilization response ratio r r r t t t t time figure response ratio as a function of time a processor is multiplexed at infinite speed among all processes present in a ready queue with no overhead this is an idealized model of roundrobin scheduling among ready processes using time slices that are very small compared to the mean service time show that for poisson input from an infinite source with exponential service times the mean response time rx of a process with service time x is given by rx x r hint review the basic queueing equations in appendix h or chapter then consider the number of items waiting w in the system upon arrival of the given process consider a variant of the rr scheduling algorithm where the entries in the ready queue are pointers to the pcbs a what would be the effect of putting two pointers to the same process in the ready queue b what would be the major advantage of this scheme c how could you modify the basic rr algorithm to achieve the same effect without the duplicate pointers in a queueing system new jobs must wait for a while before being served while a job waits its priority increases linearly with time from zero at a rate a a job waits until its priority reaches the priority of the jobs in service then it begins to share the processor equally with other jobs in service using round robin while its priority continues to increase at a slower rate b the algorithm is referred to as selfish round robin because the jobs in service try in vain to monopolize the processor by increasing their priority continuously use figure to show that the mean response time rx for a job of service time x is given by rx s xs r r where b ba r ls r ra ab assuming that arrival and service times are exponentially distributed with means and s respectively hint consider the total system and the two subsystems separately an interactive system using roundrobin scheduling and swapping tries to give guaranteed response to trivial requests as follows after completing a roundrobin cycle among all ready processes the system determines the time slice to allocate to each l a l b departures waiting jobs served jobs increasing priority l b a a l time figure selfish round robin ready process for the next cycle by dividing a maximum response time by the number of processes requiring service is this a reasonable policy which type of process is generally favored by a multilevel feedback queueing scheduler a processorbound process or an iobound process briefly explain why in prioritybased process scheduling the scheduler only gives control to a particular process if no other process of higher priority is currently in the ready state assume that no other information is used in making the process scheduling decision also assume that process priorities are established at process creation time and do not change in a system operating with such assumptions why would using dekkers solution see section a to the mutual exclusion problem be dangerous explain this by telling what undesired event could occur and how it could occur five batch jobs a through e arrive at a computer center at essentially the same time they have an estimated running time of and minutes respectively their externally defined priorities are and respectively with a lower value corresponding to a higher priority for each of the following scheduling algorithms determine the turnaround time for each process and the average turnaround for all jobs ignore process switching overhead explain how you arrived at your answers in the last three cases assume that only one job at a time runs until it finishes and that all jobs are completely processor bound a round robin with a time quantum of minute b priority scheduling c fcfs run in order and d shortest job first multiprocessor and realtime scheduling multiprocessor scheduling granularity design issues process scheduling thread scheduling realtime scheduling background characteristics of realtime operating systems realtime scheduling deadline scheduling rate monotonic scheduling priority inversion linux scheduling unix svr scheduling unix freebsd scheduling windows scheduling linux virtual machine process scheduling summary recommended reading key terms review questions and problems bear in mind sir henry one of the phrases in that queer old legend which dr mortimer has read to us and avoid the moor in those hours of darkness when the powers of evil are exalted the hound of the baskervilles arthur conan doyle learning objectives after studying this chapter you should be able to understand the concept of thread granularity discuss the key design issues in multiprocessor thread scheduling and some of the key approaches to scheduling understand the requirements imposed by realtime scheduling explain the scheduling methods used in linux unix svr and windows this chapter continues our survey of process and thread scheduling we begin with an examination of issues raised by the availability of more than one processor a number of design issues are explored this is followed by a look at the scheduling of processes on a multiprocessor system then the somewhat different design considerations for multiprocessor thread scheduling are examined the second section of this chapter covers realtime scheduling the section begins with a discussion of the characteristics of realtime processes and then looks at the nature of the scheduling process two approaches to realtime scheduling deadline scheduling and rate monotonic scheduling are examined multiprocessor scheduling when a computer system contains more than a single processor several new issues are introduced into the design of the scheduling function we begin with a brief overview of multiprocessors and then look at the rather different considerations when scheduling is done at the process level and at the thread level we can classify multiprocessor systems as follows loosely coupled or distributed multiprocessor or cluster consists of a collection of relatively autonomous systems each processor having its own main memory and io channels we address this type of configuration in chapter functionally specialized processors an example is an io processor in this case there is a master generalpurpose processor specialized processors are controlled by the master processor and provide services to it issues relating to io processors are addressed in chapter tightly coupled multiprocessor consists of a set of processors that share a common main memory and are under the integrated control of an operating system table synchronization granularity and processes synchronization grain size description interval instructions fine parallelism inherent in a single instruction stream medium parallel processing or multitasking within a single application coarse multiprocessing of concurrent processes in a multiprogramming environment very coarse distributed processing across network nodes to form a single m computing environment independent multiple unrelated processes not applicable our concern in this section is with the last category and specifically with issues relating to scheduling granularity a good way of characterizing multiprocessors and placing them in context with other architectures is to consider the synchronization granularity or frequency of synchronization between processes in a system we can distinguish five categories of parallelism that differ in the degree of granularity these are summarized in table which is adapted from gehr and wood independent parallelism with independent parallelism there is no explicit synchronization among processes each represents a separate independent application or job a typical use of this type of parallelism is in a timesharing system each user is performing a particular application such as word processing or using a spreadsheet the multiprocessor provides the same service as a multiprogrammed uniprocessor because more than one processor is available average response time to the users will be less it is possible to achieve a similar performance gain by providing each user with a personal computer or workstation if any files or information are to be shared then the individual systems must be hooked together into a distributed system supported by a network this approach is examined in chapter on the other hand a single multiprocessor shared system in many instances is more costeffective than a distributed system allowing economies of scale in disks and other peripherals coarse and very coarsegrained parallelism with coarse and very coarsegrained parallelism there is synchronization among processes but at a very gross level this kind of situation is easily handled as a set of concurrent processes running on a multiprogrammed uniprocessor and can be supported on a multiprocessor with little or no change to user software a simple example of an application that can exploit the existence of a multiprocessor is given in wood the authors have developed a program that takes a specification of files needing recompilation to rebuild a piece of software and determines which of these compiles usually all of them can be run simultaneously the program then spawns one process for each parallel compile the authors report that the speedup on a multiprocessor actually exceeds what would be expected by simply adding up the number of processors in use due to synergies in the disk buffer caches a topic explored in chapter and sharing of compiler code which is loaded into memory only once in general any collection of concurrent processes that need to communicate or synchronize can benefit from the use of a multiprocessor architecture in the case of very infrequent interaction among processes a distributed system can provide good support however if the interaction is somewhat more frequent then the overhead of communication across the network may negate some of the potential speedup in that case the multiprocessor organization provides the most effective support mediumgrained parallelism we saw in chapter that a single application can be effectively implemented as a collection of threads within a single process in this case the programmer must explicitly specify the potential parallelism of an application typically there will need to be rather a high degree of coordination and interaction among the threads of an application leading to a mediumgrain level of synchronization whereas independent very coarse and coarsegrained parallelism can be supported on either a multiprogrammed uniprocessor or a multiprocessor with little or no impact on the scheduling function we need to reexamine scheduling when dealing with the scheduling of threads because the various threads of an application interact so frequently scheduling decisions concerning one thread may affect the performance of the entire application we return to this issue later in this section finegrained parallelism finegrained parallelism represents a much more complex use of parallelism than is found in the use of threads although much work has been done on highly parallel applications this is so far a specialized and fragmented area with many different approaches chapter provides an example of the use of granularity for the valve game software design issues scheduling on a multiprocessor involves three interrelated issues the assignment of processes to processors the use of multiprogramming on individual processors the actual dispatching of a process in looking at these three issues it is important to keep in mind that the approach taken will depend in general on the degree of granularity of the applications and on the number of processors available assignment of processes to processors if we assume that the architecture of the multiprocessor is uniform in the sense that no processor has a particular physical advantage with respect to access to main memory or to io devices then the simplest scheduling approach is to treat the processors as a pooled resource and assign processes to processors on demand the question then arises as to whether the assignment should be static or dynamic if a process is permanently assigned to one processor from activation until its completion then a dedicated shortterm queue is maintained for each processor an advantage of this approach is that there may be less overhead in the scheduling function because the processor assignment is made once and for all also the use of dedicated processors allows a strategy known as group or gang scheduling as discussed later a disadvantage of static assignment is that one processor can be idle with an empty queue while another processor has a backlog to prevent this situation a common queue can be used all processes go into one global queue and are scheduled to any available processor thus over the life of a process the process may be executed on different processors at different times in a tightly coupled sharedmemory architecture the context information for all processes will be available to all processors and therefore the cost of scheduling a process will be independent of the identity of the processor on which it is scheduled yet another option is dynamic load balancing in which threads are moved for a queue for one processor to a queue for another processor linux uses this approach regardless of whether processes are dedicated to processors some means is needed to assign processes to processors two approaches have been used master slave and peer with a masterslave architecture key kernel functions of the operating system always run on a particular processor the other processors may only execute user programs the master is responsible for scheduling jobs once a process is active if the slave needs service eg an io call it must send a request to the master and wait for the service to be performed this approach is quite simple and requires little enhancement to a uniprocessor multiprogramming operating system conflict resolution is simplified because one processor has control of all memory and io resources there are two disadvantages to this approach a failure of the master brings down the whole system and the master can become a performance bottleneck in a peer architecture the kernel can execute on any processor and each processor does selfscheduling from the pool of available processes this approach complicates the operating system the operating system must ensure that two processors do not choose the same process and that the processes are not somehow lost from the queue techniques must be employed to resolve and synchronize competing claims to resources there is of course a spectrum of approaches between these two extremes one approach is to provide a subset of processors dedicated to kernel processing instead of just one another approach is simply to manage the difference between the needs of kernel processes and other processes on the basis of priority and execution history the use of multiprogramming on individual processors when each process is statically assigned to a processor for the duration of its lifetime a new question arises should that processor be multiprogrammed the readers first reaction may be to wonder why the question needs to be asked it would appear particularly wasteful to tie up a processor with a single process when that process may frequently be blocked waiting for io or because of concurrencysynchronization considerations in the traditional multiprocessor which is dealing with coarsegrained or independent synchronization granularity see table it is clear that each individual processor should be able to switch among a number of processes to achieve high utilization and therefore better performance however for mediumgrained applications running on a multiprocessor with many processors the situation is less clear when many processors are available it is no longer paramount that every single processor be busy as much as possible rather we are concerned to provide the best performance on average for the applications an application that consists of a number of threads may run poorly unless all of its threads are available to run simultaneously process dispatching the final design issue related to multiprocessor scheduling is the actual selection of a process to run we have seen that on a multiprogrammed uniprocessor the use of priorities or of sophisticated scheduling algorithms based on past usage may improve performance over a simpleminded firstcomefirstserved strategy when we consider multiprocessors these complexities may be unnecessary or even counterproductive and a simpler approach may be more effective with less overhead in the case of thread scheduling new issues come into play that may be more important than priorities or execution histories we address each of these topics in turn process scheduling in most traditional multiprocessor systems processes are not dedicated to processors rather there is a single queue for all processors or if some sort of priority scheme is used there are multiple queues based on priority all feeding into the common pool of processors in any case we can view the system as being a multiserver queueing architecture consider the case of a dualprocessor system in which each processor of the dualprocessor system has half the processing rate of a processor in the singleprocessor system saue reports a queueing analysis that compares fcfs scheduling to round robin and to shortest remaining time the study is concerned with process service time which measures the amount of processor time a process needs either for a total job or the amount of time needed each time the process is ready to use the processor in the case of round robin it is assumed that the time quantum is large compared to contextswitching overhead and small compared to mean service time the results depend on the variability that is seen in service times a common measure of variability is the coefficient of variation cs a value of cs corresponds to the case where there is no variability the service times of all processes are equal increasing values of cs correspond to increasing variability among the service times that is the larger the value of cs the more widely do the values of the service times vary values of cs of or more are not unusual for processor service time distributions figure a compares roundrobin throughput to fcfs throughput as a function of css note that the difference in scheduling algorithms is much smaller in the dualprocessor case with two processors a single process with long service time is the value of cs is calculated as sts where s is the standard deviation of service time and ts is the mean service time for a further explanation of cs see the discussion in chapter single rr to fcfs throughput ratio processor dual processor coefficient of variation a comparison of rr and fcfs single processor srt to fcfs throughput ratio dual processor coefficient of variation b comparison of srt and fcfs figure comparison of scheduling performance for one and two processors much less disruptive in the fcfs case other processes can use the other processor similar results are shown in figure b the study in saue repeated this analysis under a number of assumptions about degree of multiprogramming mix of iobound versus cpubound processes and the use of priorities the general conclusion is that the specific scheduling discipline is much less important with two processors than with one it should be evident that this conclusion is even stronger as the number of processors increases thus a simple fcfs discipline or the use of fcfs within a static priority scheme may suffice for a multipleprocessor system thread scheduling as we have seen with threads the concept of execution is separated from the rest of the definition of a process an application can be implemented as a set of threads that cooperate and execute concurrently in the same address space on a uniprocessor threads can be used as a program structuring aid and to overlap io with processing because of the minimal penalty in doing a thread switch compared to a process switch these benefits are realized with little cost however the full power of threads becomes evident in a multiprocessor system in this environment threads can be used to exploit true parallelism in an application if the various threads of an application are simultaneously run on separate processors dramatic gains in performance are possible however it can be shown that for applications that require significant interaction among threads mediumgrain parallelism small differences in thread management and scheduling can have a significant performance impact ande among the many proposals for multiprocessor thread scheduling and processor assignment four general approaches stand out load sharing processes are not assigned to a particular processor a global queue of ready threads is maintained and each processor when idle selects a thread from the queue the term load sharing is used to distinguish this strategy from loadbalancing schemes in which work is allocated on a more permanent basis eg see feita gang scheduling a set of related threads is scheduled to run on a set of processors at the same time on a onetoone basis dedicated processor assignment this is the opposite of the loadsharing approach and provides implicit scheduling defined by the assignment of threads to processors each program for the duration of its execution is allocated a number of processors equal to the number of threads in the program when the program terminates the processors return to the general pool for possible allocation to another program dynamic scheduling the number of threads in a process can be altered during the course of execution load sharing load sharing is perhaps the simplest approach and the one that carries over most directly from a uniprocessor environment it has several advantages the load is distributed evenly across the processors assuring that no processor is idle while work is available to do no centralized scheduler is required when a processor is available the scheduling routine of the operating system is run on that processor to select the next thread some of the literature on this topic refers to this approach as selfscheduling because each processor schedules itself without regard to other processors however this term is also used in the literature to refer to programs written in a language that allows the programmer to specify the scheduling eg see fost the global queue can be organized and accessed using any of the schemes discussed in chapter including prioritybased schemes and schemes that consider execution history or anticipated processing demands leut analyzes three different versions of load sharing firstcomefirstserved fcfs when a job arrives each of its threads is placed consecutively at the end of the shared queue when a processor becomes idle it picks the next ready thread which it executes until completion or blocking smallest number of threads first the shared ready queue is organized as a priority queue with highest priority given to threads from jobs with the smallest number of unscheduled threads jobs of equal priority are ordered according to which job arrives first as with fcfs a scheduled thread is run to completion or blocking preemptive smallest number of threads first highest priority is given to jobs with the smallest number of unscheduled threads an arriving job with a smaller number of threads than an executing job will preempt threads belonging to the scheduled job using simulation models the authors report that over a wide range of job characteristics fcfs is superior to the other two policies in the preceding list further the authors find that some form of gang scheduling discussed in the next subsection is generally superior to load sharing there are several disadvantages of load sharing the central queue occupies a region of memory that must be accessed in a manner that enforces mutual exclusion thus it may become a bottleneck if many processors look for work at the same time when there is only a small number of processors this is unlikely to be a noticeable problem however when the multiprocessor consists of dozens or even hundreds of processors the potential for bottleneck is real preempted threads are unlikely to resume execution on the same processor if each processor is equipped with a local cache caching becomes less efficient if all threads are treated as a common pool of threads it is unlikely that all of the threads of a program will gain access to processors at the same time if a high degree of coordination is required between the threads of a program the process switches involved may seriously compromise performance despite the potential disadvantages load sharing is one of the most commonly used schemes in current multiprocessors a refinement of the loadsharing technique is used in the mach operating system blac wend the operating system maintains a local run queue for each processor and a shared global run queue the local run queue is used by threads that have been temporarily bound to a specific processor a processor examines the local run queue first to give bound threads absolute preference over unbound threads as an example of the use of bound threads one or more processors could be dedicated to running processes that are part of the operating system another example is that the threads of a particular application could be distributed among a number of processors with the proper additional software this provides support for gang scheduling discussed next gang scheduling the concept of scheduling a set of processes simultaneously on a set of processors predates the use of threads jone refers to the concept as group scheduling and cites the following benefits if closely related processes execute in parallel synchronization blocking may be reduced less process switching may be necessary and performance will increase scheduling overhead may be reduced because a single decision affects a number of processors and processes at one time on the cm multiprocessor the term coscheduling is used gehr coscheduling is based on the concept of scheduling a related set of tasks called a task force the individual elements of a task force tend to be quite small and are hence close to the idea of a thread the term gang scheduling has been applied to the simultaneous scheduling of the threads that make up a single process feitb gang scheduling is useful for mediumgrained to finegrained parallel applications whose performance severely degrades when any part of the application is not running while other parts are ready to run it is also beneficial for any parallel application even one that is not quite so performance sensitive the need for gang scheduling is widely recognized and implementations exist on a variety of multiprocessor operating systems one obvious way in which gang scheduling improves the performance of a single application is that process switches are minimized suppose one thread of a process is executing and reaches a point at which it must synchronize with another thread of the same process if that other thread is not running but is in a ready queue the first thread is hung up until a process switch can be done on some other processor to bring in the needed thread in an application with tight coordination among threads such switches will dramatically reduce performance the simultaneous scheduling of cooperating threads can also save time in resource allocation for example multiple gangscheduled threads can access a file without the additional overhead of locking during a seek readwrite operation the use of gang scheduling creates a requirement for processor allocation one possibility is the following suppose that we have n processors and m applications each of which has n or fewer threads then each application could be given m of the available time on the n processors using time slicing feita notes that this strategy can be inefficient consider an example in which there are two applications one with four threads and one with one thread using uniform time allocation wastes of the processing resource because when the singlethread application runs three processors are left idle see figure if there are several onethread applications these could all be fit together to increase processor utilization if that option is not available an alternative to uniform scheduling is scheduling that is weighted by the number of threads thus the fourthread application could be given fourfifths of the time and the onethread application given only onefifth of the time reducing the processor waste to uniform division division by weights group group group group pe pe pe idle pe idle pe idle pe idle pe idle pe idle time waste waste figure example of scheduling groups with four and one threads feitb dedicated processor assignment an extreme form of gang scheduling suggested in tuck is to dedicate a group of processors to an application for the duration of the application that is when an application is scheduled each of its threads is assigned a processor that remains dedicated to that thread until the application runs to completion this approach would appear to be extremely wasteful of processor time if a thread of an application is blocked waiting for io or for synchronization with another thread then that threads processor remains idle there is no multiprogramming of processors two observations can be made in defense of this strategy in a highly parallel system with tens or hundreds of processors each of which represents a small fraction of the cost of the system processor utilization is no longer so important as a metric for effectiveness or performance the total avoidance of process switching during the lifetime of a program should result in a substantial speedup of that program both tuck and zaho report analyses that support statement figure shows the results of one experiment tuck the authors ran two applications simultaneously executing concurrently a matrix multiplication and a fast fourier transform fft calculation on a system with processors each application breaks its problem into a number of tasks which are mapped onto the threads executing that application the programs are written in such a way as to allow the number of threads to be used to vary in essence a number of tasks are defined and queued by an application tasks are taken from the queue and mapped onto the available threads by the application if there are fewer threads than tasks then leftover tasks remain queued and are picked up by threads as they complete their assigned tasks clearly not all applications can be structured in this way but many numerical problems and some other applications can be dealt with in this fashion figure shows the speedup for the applications as the number of threads executing the tasks in each application is varied from to for example we see that when both applications are started simultaneously with threads each the speedup obtained compared to using a single thread for each application is for matrix multiplication and for fft the figure shows that the performance of both applications worsens considerably when the number of threads in each application exceeds eight and thus the total number of processes in the system exceeds the number of processors furthermore the larger the number of threads the worse the matrix multiplication fft speedup number of threads per application figure application speedup as a function of number of threads tuck performance gets because there is a greater frequency of thread preemption and rescheduling this excessive preemption results in inefficiency from many sources including time spent waiting for a suspended thread to leave a critical section time wasted in process switching and inefficient cache behavior the authors conclude that an effective strategy is to limit the number of active threads to the number of processors in the system if most of the applications are either single thread or can use the taskqueue structure this will provide an effective and reasonably efficient use of the processor resources both dedicated processor assignment and gang scheduling attack the scheduling problem by addressing the issue of processor allocation one can observe that the processor allocation problem on a multiprocessor more closely resembles the memory allocation problem on a uniprocessor than the scheduling problem on a uniprocessor the issue is how many processors to assign to a program at any given time which is analogous to how many page frames to assign to a given process at any time gehr proposes the term activity working set analogous to a virtual memory working set as the minimum number of activities threads that must be scheduled simultaneously on processors for the application to make acceptable progress as with memory management schemes the failure to schedule all of the elements of an activity working set can lead to processor thrashing this occurs when the scheduling of threads whose services are required induces the descheduling of other threads whose services will soon be needed similarly processor fragmentation refers to a situation in which some processors are left over when others are allocated and the leftover processors are either insufficient in number or unsuitably organized to support the requirements of waiting applications gang scheduling and dedicated processor allocation are meant to avoid these problems dynamic scheduling for some applications it is possible to provide language and system tools that permit the number of threads in the process to be altered dynamically this would allow the operating system to adjust the load to improve utilization zaho proposes an approach in which both the operating system and the application are involved in making scheduling decisions the operating system is responsible for partitioning the processors among the jobs each job uses the processors currently in its partition to execute some subset of its runnable tasks by mapping these tasks to threads an appropriate decision about which subset to run as well as which thread to suspend when a process is preempted is left to the individual applications perhaps through a set of runtime library routines this approach may not be suitable for all applications however some applications could default to a single thread while others could be programmed to take advantage of this particular feature of the operating system in this approach the scheduling responsibility of the operating system is primarily limited to processor allocation and proceeds according to the following policy when a job requests one or more processors either when the job arrives for the first time or because its requirements change if there are idle processors use them to satisfy the request otherwise if the job making the request is a new arrival allocate it a single processor by taking one away from any job currently allocated more than one processor if any portion of the request can not be satisfied it remains outstanding until either a processor becomes available for it or the job rescinds the request eg if there is no longer a need for the extra processors upon release of one or more processors including job departure scan the current queue of unsatisfied requests for processors assign a single processor to each job in the list that currently has no processors ie to all waiting new arrivals then scan the list again allocating the rest of the processors on an fcfs basis analyses reported in zaho and maju suggest that for applications that can take advantage of dynamic scheduling this approach is superior to gang scheduling or dedicated processor assignment however the overhead of this approach may negate this apparent performance advantage experience with actual systems is needed to prove the worth of dynamic scheduling the memory hierarchy the design constraints on a computers memory can be summed up by three questions how much how fast how expensive the question of how much is somewhat open ended if the capacity is there applications will likely be developed to use it the question of how fast is in a sense easier to answer to achieve greatest performance the memory must be able to keep up with the processor that is as the processor is executing instructions we would not want it to have to pause waiting for instructions or operands the final question must also be considered for a practical system the cost of memory must be reasonable in relationship to other components as might be expected there is a tradeoff among the three key characteristics of memory namely capacity access time and cost a variety of technologies are used to implement memory systems and across this spectrum of technologies the following relationships hold faster access time greater cost per bit greater capacity smaller cost per bit greater capacity slower access speed the dilemma facing the designer is clear the designer would like to use memory technologies that provide for largecapacity memory both because the capacity is needed and because the cost per bit is low however to meet performance requirements the designer needs to use expensive relatively lowercapacity memories with fast access times the way out of this dilemma is to not rely on a single memory component or technology but to employ a memory hierarchy a typical hierarchy is illustrated in figure as one goes down the hierarchy the following occur a decreasing cost per bit b increasing capacity c increasing access time d decreasing frequency of access to the memory by the processor thus smaller more expensive faster memories are supplemented by larger cheaper slower memories the key to the success of this organization is the decreasing frequency of access at lower levels we will examine this concept in greater detail later in this chapter when we discuss the cache and when we discuss virtual memory later in this book a brief explanation is provided at this point suppose that the processor has access to two levels of memory level contains bytes and has an access time of s level contains bytes and has an access time of s assume that if a byte to be accessed is in level then the processor accesses it directly if it is in level then the byte is first transferred to level and then accessed by the processor for simplicity we ignore the time required for the processor to determine whether the byte is in level or level riesgters meinmboorayrd cache mmeaminory sotourtabgoeard magcndectdiddrcvvoddrbdimswlkrurwraamy stoorfafgleine magnetic tape figure the memory hierarchy figure shows the general shape of the curve that models this situation the figure shows the average access time to a twolevel memory as a function of the hit ratio h where h is defined as the fraction of all memory accesses that are found in the faster memory eg the cache t is the access time to level and t is the access time to level as can be seen for high percentages of level access the average total access time is much closer to that of level than that of level in our example suppose of the memory accesses are found in the cache h then the average time to access a byte can be expressed as s s s s the result is close to the access time of the faster memory so the strategy of using two memory levels works in principle but only if conditions a through d in the preceding list apply by employing a variety of technologies a spectrum of if the accessed word is found in the faster memory that is defined as a hit a miss occurs if the accessed word is not found in the faster memory t t t average access time t fraction of accesses involving only level hit ratio figure performance of a simple twolevel memory memory systems exists that satisfies conditions a through c fortunately condition d is also generally valid the basis for the validity of condition d is a principle known as locality of reference denn during the course of execution of a program memory references by the processor for both instructions and data tend to cluster programs typically contain a number of iterative loops and subroutines once a loop or subroutine is entered there are repeated references to a small set of instructions similarly operations on tables and arrays involve access to a clustered set of data bytes over a long period of time the clusters in use change but over a short period of time the processor is primarily working with fixed clusters of memory references accordingly it is possible to organize data across the hierarchy such that the percentage of accesses to each successively lower level is substantially less than that of the level above consider the twolevel example already presented let level memory contain all program instructions and data the current clusters can be temporarily placed in level from time to time one of the clusters in level will have to be swapped back to level to make room for a new cluster coming in to level on average however most references will be to instructions and data contained in level this principle can be applied across more than two levels of memory the fastest smallest and most expensive type of memory consists of the registers internal to the processor typically a processor will contain a few dozen such registers although some processors contain hundreds of registers skipping down two levels main memory is the principal internal memory system of the computer each location in main memory has a unique address and most machine instructions refer to one or more main memory addresses main memory is usually extended with a higherspeed smaller cache the cache is not usually visible to the programmer or indeed to the processor it is a device for staging the movement of data between main memory and processor registers to improve performance the three forms of memory just described are typically volatile and employ semiconductor technology the use of three levels exploits the fact that semiconductor memory comes in a variety of types which differ in speed and cost data are stored more permanently on external mass storage devices of which the most common are hard disk and removable media such as removable disk tape and optical storage external nonvolatile memory is also referred to as secondary memory or auxiliary memory these are used to store program and data files and are usually visible to the programmer only in terms of files and records as opposed to individual bytes or words a hard disk is also used to provide an extension to main memory known as virtual memory which is discussed in chapter additional levels can be effectively added to the hierarchy in software for example a portion of main memory can be used as a buffer to temporarily hold data that are to be read out to disk such a technique sometimes referred to as a disk cache examined in detail in chapter improves performance in two ways disk writes are clustered instead of many small transfers of data we have a few large transfers of data this improves disk performance and minimizes processor involvement some data destined for writeout may be referenced by a program before the next dump to disk in that case the data are retrieved rapidly from the software cache rather than slowly from the disk appendix a examines the performance implications of multilevel memory structures realtime scheduling background realtime computing is becoming an increasingly important discipline the operating system and in particular the scheduler is perhaps the most important component of a realtime system examples of current applications of realtime systems include control of laboratory experiments process control in industrial plants robotics air traffic control telecommunications and military command and control systems nextgeneration systems will include the autonomous land rover controllers of robots with elastic joints systems found in intelligent manufacturing the space station and undersea exploration realtime computing may be defined as that type of computing in which the correctness of the system depends not only on the logical result of the computation but also on the time at which the results are produced we can define a realtime system by defining what is meant by a realtime process or task in general in a realtime system some of the tasks are realtime tasks and these have a certain degree of urgency to them such tasks are attempting to control or react to events that take place in the outside world because these events occur in real time a realtime task must be able to keep up with the events with which it is concerned thus it is usually possible to associate a deadline with a particular task where the deadline specifies either a start time or a completion time such a task may be classified as hard or soft a hard realtime task is one that must meet its deadline otherwise it will cause unacceptable damage or a fatal error to the system a soft realtime task has an associated deadline that is desirable but not mandatory it still makes sense to schedule and complete the task even if it has passed its deadline another characteristic of realtime tasks is whether they are periodic or aperiodic an aperiodic task has a deadline by which it must finish or start or it may have a constraint on both start and finish time in the case of a periodic task the requirement may be stated as once per period t or exactly t units apart characteristics of realtime operating systems realtime operating systems can be characterized as having unique requirements in five general areas morg determinism responsiveness user control reliability failsoft operation an operating system is deterministic to the extent that it performs operations at fixed predetermined times or within predetermined time intervals when multiple processes are competing for resources and processor time no system will be fully deterministic in a realtime operating system process requests for service are dictated by external events and timings the extent to which an operating system can deterministically satisfy requests depends first on the speed with which it can as usual terminology poses a problem because various words are used in the literature with varying meanings it is common for a particular process to operate under realtime constraints of a repetitive nature that is the process lasts for a long time and during that time performs some repetitive function in response to realtime events let us for this section refer to an individual function as a task thus the process can be viewed as progressing through a sequence of tasks at any given time the process is engaged in a single task and it is the processtask that must be scheduled respond to interrupts and second on whether the system has sufficient capacity to handle all requests within the required time one useful measure of the ability of an operating system to function deterministically is the maximum delay from the arrival of a highpriority device interrupt to when servicing begins in nonrealtime operating systems this delay may be in the range of tens to hundreds of milliseconds while in realtime operating systems that delay may have an upper bound of anywhere from a few microseconds to a millisecond a related but distinct characteristic is responsiveness determinism is concerned with how long an operating system delays before acknowledging an interrupt responsiveness is concerned with how long after acknowledgment it takes an operating system to service the interrupt aspects of responsiveness include the following the amount of time required to initially handle the interrupt and begin execution of the interrupt service routine isr if execution of the isr requires a process switch then the delay will be longer than if the isr can be executed within the context of the current process the amount of time required to perform the isr this generally is dependent on the hardware platform the effect of interrupt nesting if an isr can be interrupted by the arrival of another interrupt then the service will be delayed determinism and responsiveness together make up the response time to external events response time requirements are critical for realtime systems because such systems must meet timing requirements imposed by individuals devices and data flows external to the system user control is generally much broader in a realtime operating system than in ordinary operating systems in a typical nonrealtime operating system the user either has no control over the scheduling function of the operating system or can only provide broad guidance such as grouping users into more than one priority class in a realtime system however it is essential to allow the user finegrained control over task priority the user should be able to distinguish between hard and soft tasks and to specify relative priorities within each class a realtime system may also allow the user to specify such characteristics as the use of paging or process swapping what processes must always be resident in main memory what disk transfer algorithms are to be used what rights the processes in various priority bands have and so on reliability is typically far more important for realtime systems than nonrealtime systems a transient failure in a nonrealtime system may be solved by simply rebooting the system a processor failure in a multiprocessor nonrealtime system may result in a reduced level of service until the failed processor is repaired or replaced but a realtime system is responding to and controlling events in real time loss or degradation of performance may have catastrophic consequences ranging from financial loss to major equipment damage and even loss of life as in other areas the difference between a realtime and a nonrealtime operating system is one of degree even a realtime system must be designed to respond to various failure modes failsoft operation is a characteristic that refers to the ability of a system to fail in such a way as to preserve as much capability and data as possible for example a typical traditional unix system when it detects a corruption of data within the kernel issues a failure message on the system console dumps the memory contents to disk for later failure analysis and terminates execution of the system in contrast a realtime system will attempt either to correct the problem or minimize its effects while continuing to run typically the system notifies a user or user process that it should attempt corrective action and then continues operation perhaps at a reduced level of service in the event a shutdown is necessary an attempt is made to maintain file and data consistency an important aspect of failsoft operation is referred to as stability a realtime system is stable if in cases where it is impossible to meet all task deadlines the system will meet the deadlines of its most critical highestpriority tasks even if some less critical task deadlines are not always met to meet the foregoing requirements realtime operating systems typically include the following features stan fast process or thread switch small size with its associated minimal functionality ability to respond to external interrupts quickly multitasking with interprocess communication tools such as semaphores signals and events use of special sequential files that can accumulate data at a fast rate preemptive scheduling based on priority minimization of intervals during which interrupts are disabled primitives to delay tasks for a fixed amount of time and to pauseresume tasks special alarms and timeouts the heart of a realtime system is the shortterm task scheduler in designing such a scheduler fairness and minimizing average response time are not paramount what is important is that all hard realtime tasks complete or start by their deadline and that as many as possible soft realtime tasks also complete or start by their deadline most contemporary realtime operating systems are unable to deal directly with deadlines instead they are designed to be as responsive as possible to realtime tasks so that when a deadline approaches a task can be quickly scheduled from this point of view realtime applications typically require deterministic response times in the severalmillisecond to submillisecond span under a broad set of conditions leadingedge applications in simulators for military aircraft for example often have constraints in the range of s atla figure illustrates a spectrum of possibilities in a preemptive scheduler that uses simple roundrobin scheduling a realtime task would be added to the ready queue to await its next timeslice as illustrated in figure a in this case the scheduling time will generally be unacceptable for realtime applications alternatively in a nonpreemptive scheduler we could use a priority scheduling mechanism giving realtime tasks higher priority in this case a realtime task that is ready would be scheduled as soon as the current process blocks or runs to completion figure b this could lead to a delay of several seconds if a slow lowpriority task were request from a realtime process realtime process added to run queue to await its next slice process process process n realtime process clock tick scheduling time a roundrobin preemptive scheduler request from a realtime process realtime process added to head of run queue current process realtime process current process scheduling time blocked or completed b prioritydriven nonpreemptive scheduler request from a realtime process wait for next preemption point current process realtime process preemption scheduling time point c prioritydriven preemptive scheduler on preemption points request from a realtime process realtime process preempts current process and executes immediately current process realtime process scheduling time d immediate preemptive scheduler figure scheduling of realtime process executing at a critical time again this approach is not acceptable a more promising approach is to combine priorities with clockbased interrupts preemption points occur at regular intervals when a preemption point occurs the currently running task is preempted if a higherpriority task is waiting this would include the preemption of tasks that are part of the operating system kernel such a delay may be on the order of several milliseconds figure c while this last approach may be adequate for some realtime applications it will not suffice for more demanding applications in those cases the approach that has been taken is sometimes referred to as immediate preemption in this case the operating system responds to an interrupt almost immediately unless the system is in a criticalcode lockout section scheduling delays for a realtime task can then be reduced to s or less realtime scheduling realtime scheduling is one of the most active areas of research in computer science in this subsection we provide an overview of the various approaches to realtime scheduling and look at two popular classes of scheduling algorithms in a survey of realtime scheduling algorithms rama observes that the various scheduling approaches depend on whether a system performs schedulability analysis if it does whether it is done statically or dynamically and whether the result of the analysis itself produces a schedule or plan according to which tasks are dispatched at run time based on these considerations the authors identify the following classes of algorithms static tabledriven approaches these perform a static analysis of feasible schedules of dispatching the result of the analysis is a schedule that determines at run time when a task must begin execution static prioritydriven preemptive approaches again a static analysis is performed but no schedule is drawn up rather the analysis is used to assign priorities to tasks so that a traditional prioritydriven preemptive scheduler can be used dynamic planningbased approaches feasibility is determined at run time dynamically rather than offline prior to the start of execution statically an arriving task is accepted for execution only if it is feasible to meet its time constraints one of the results of the feasibility analysis is a schedule or plan that is used to decide when to dispatch this task dynamic best effort approaches no feasibility analysis is performed the system tries to meet all deadlines and aborts any started process whose deadline is missed static tabledriven scheduling is applicable to tasks that are periodic input to the analysis consists of the periodic arrival time execution time periodic ending deadline and relative priority of each task the scheduler attempts to develop a schedule that enables it to meet the requirements of all periodic tasks this is a predictable approach but one that is inflexible because any change to any task requirements requires that the schedule be redone earliestdeadlinefirst or other periodic deadline techniques discussed subsequently are typical of this category of scheduling algorithms static prioritydriven preemptive scheduling makes use of the prioritydriven preemptive scheduling mechanism common to most nonrealtime multiprogramming systems in a nonrealtime system a variety of factors might be used to determine priority for example in a timesharing system the priority of a process changes depending on whether it is processor bound or io bound in a realtime system priority assignment is related to the time constraints associated with each task one example of this approach is the rate monotonic algorithm discussed subsequently which assigns static priorities to tasks based on the length of their periods with dynamic planningbased scheduling after a task arrives but before its execution begins an attempt is made to create a schedule that contains the previously scheduled tasks as well as the new arrival if the new arrival can be scheduled in such a way that its deadlines are satisfied and that no currently scheduled task misses a deadline then the schedule is revised to accommodate the new task dynamic best effort scheduling is the approach used by many realtime systems that are currently commercially available when a task arrives the system assigns a priority based on the characteristics of the task some form of deadline scheduling such as earliestdeadline scheduling is typically used typically the tasks are aperiodic and so no static scheduling analysis is possible with this type of scheduling until a deadline arrives or until the task completes we do not know whether a timing constraint will be met this is the major disadvantage of this form of scheduling its advantage is that it is easy to implement deadline scheduling most contemporary realtime operating systems are designed with the objective of starting realtime tasks as rapidly as possible and hence emphasize rapid interrupt handling and task dispatching in fact this is not a particularly useful metric in evaluating realtime operating systems realtime applications are generally not concerned with sheer speed but rather with completing or starting tasks at the most valuable times neither too early nor too late despite dynamic resource demands and conflicts processing overloads and hardware or software faults it follows that priorities provide a crude tool and do not capture the requirement of completion or initiation at the most valuable time there have been a number of proposals for more powerful and appropriate approaches to realtime task scheduling all of these are based on having additional information about each task in its most general form the following information about each task might be used ready time time at which task becomes ready for execution in the case of a repetitive or periodic task this is actually a sequence of times that is known in advance in the case of an aperiodic task this time may be known in advance or the operating system may only be aware when the task is actually ready starting deadline time by which a task must begin completion deadline time by which a task must be completed the typical realtime application will either have starting deadlines or completion deadlines but not both processing time time required to execute the task to completion in some cases this is supplied in others the operating system measures an exponential average as defined in chapter for still other scheduling systems this information is not used resource requirements set of resources other than the processor required by the task while it is executing priority measures relative importance of the task hard realtime tasks may have an absolute priority with the system failing if a deadline is missed if the system is to continue to run no matter what then both hard and soft realtime tasks may be assigned relative priorities as a guide to the scheduler subtask structure a task may be decomposed into a mandatory subtask and an optional subtask only the mandatory subtask possesses a hard deadline there are several dimensions to the realtime scheduling function when deadlines are taken into account which task to schedule next and what sort of preemption is allowed it can be shown for a given preemption strategy and using either starting or completion deadlines that a policy of scheduling the task with the earliest deadline minimizes the fraction of tasks that miss their deadlines butt hong panw this conclusion holds for both singleprocessor and multiprocessor configurations the other critical design issue is that of preemption when starting deadlines are specified then a nonpreemptive scheduler makes sense in this case it would be the responsibility of the realtime task to block itself after completing the mandatory or critical portion of its execution allowing other realtime starting deadlines to be satisfied this fits the pattern of figure b for a system with completion deadlines a preemptive strategy figure c or d is most appropriate for example if task x is running and task y is ready there may be circumstances in which the only way to allow both x and y to meet their completion deadlines is to preempt x execute y to completion and then resume x to completion as an example of scheduling periodic tasks with completion deadlines consider a system that collects and processes data from two sensors a and b the deadline for collecting data from sensor a must be met every ms and that for b every ms it takes ms including operating system overhead to process each sample of data from a and ms to process each sample of data from b table summarizes the execution profile of the two tasks figure compares three scheduling techniques using the execution profile of table the first row of figure repeats the information in table the remaining three rows illustrate three scheduling techniques the computer is capable of making a scheduling decision every ms suppose that under these circumstances we attempted to use a priority scheduling scheme the first two timing diagrams in figure show the result if a has higher priority the first instance of task b is given only ms of processing time in two ms chunks by the time its deadline is reached and thus fails if b is given higher priority then a will miss its first deadline the final timing diagram shows the use of earliestdeadline scheduling at time t both a and b arrive because a has the this need not be on a ms boundary if more than ms has elapsed since the last scheduling decision table execution profile of two periodic tasks process arrival time execution time ending deadline a a a a a b b b b deadline deadline a a a a a deadline deadline deadline deadline deadline arrival times execution a a a a a times and deadlines b b time ms fixedpriority scheduling a b a b a b a b a b a has priority a a b a a a b missed fixedpriority scheduling b a a b a b has priority a a b a a a b missed missed earliestdeadline scheduling a b a b a b a b a using completion deadlines a a b a a a b figure scheduling of periodic realtime tasks with completion deadlines based on table arrival times a b c d e requirements starting deadline b c e d a arrival times a b c d e earliest service a c e d deadline starting deadline b missed c e d a arrival times a b c d e earliest deadline service b c e d a with unforced idle times starting deadline b c e d a arrival times a b c d e firstcome a c d firstserved service fcfs starting deadline b missed c e missed d a figure scheduling of aperiodic realtime tasks with starting deadlines earliest deadline it is scheduled first when a completes b is given the processor at t a arrives because a has an earlier deadline than b b is interrupted so that a can execute to completion then b is resumed at t at t a arrives however b has an earlier ending deadline and is allowed to execute to completion at t a is then given the processor and finishes at t in this example by scheduling to give priority at any preemption point to the task with the nearest deadline all system requirements can be met because the tasks are periodic and predictable a static tabledriven scheduling approach is used now consider a scheme for dealing with aperiodic tasks with starting deadlines the top part of figure shows the arrival times and starting deadlines for an example consisting of five tasks each of which has an execution time of ms table summarizes the execution profile of the five tasks table execution profile of five aperiodic tasks process arrival time execution time starting deadline a b c d e a straightforward scheme is to always schedule the ready task with the earliest deadline and let that task run to completion when this approach is used in the example of figure note that although task b requires immediate service the service is denied this is the risk in dealing with aperiodic tasks especially with starting deadlines a refinement of the policy will improve performance if deadlines can be known in advance of the time that a task is ready this policy referred to as earliest deadline with unforced idle times operates as follows always schedule the eligible task with the earliest deadline and let that task run to completion an eligible task may not be ready and this may result in the processor remaining idle even though there are ready tasks note that in our example the system refrains from scheduling task a even though that is the only ready task the result is that even though the processor is not used to maximum efficiency all scheduling requirements are met finally for comparison the fcfs policy is shown in this case tasks b and e do not meet their deadlines rate monotonic scheduling one of the more promising methods of resolving multitask scheduling conflicts for periodic tasks is rate monotonic scheduling rms the scheme was first proposed in liu but has only recently gained popularity bria sha rms assigns priorities to tasks on the basis of their periods for rms the highestpriority task is the one with the shortest period the second highestpriority task is the one with the second shortest period and so on when more than one task is available for execution the one with the shortest period is serviced first if we plot the priority of tasks as a function of their rate the result is a monotonically increasing function figure hence the name rate monotonic scheduling high highest rate and highestpriority task priority lowest rate and lowestpriority task low rate hz figure a task set with rms warr cycle cycle p processing idle processing task p execution time c time task p period t figure periodic task timing diagram figure illustrates the relevant parameters for periodic tasks the tasks period t is the amount of time between the arrival of one instance of the task and the arrival of the next instance of the task a tasks rate in hertz is simply the inverse of its period in seconds for example a task with a period of ms occurs at a rate of hz typically the end of a tasks period is also the tasks hard deadline although some tasks may have earlier deadlines the execution or computation time c is the amount of processing time required for each occurrence of the task it should be clear that in a uniprocessor system the execution time must be no greater than the period must have c t if a periodic task is always run to completion that is if no instance of the task is ever denied service because of insufficient resources then the utilization of the processor by this task is u ct for example if a task has a period of ms and an execution time of ms its processor utilization is one measure of the effectiveness of a periodic scheduling algorithm is whether or not it guarantees that all hard deadlines are met suppose that we have n tasks each with a fixed period and execution time then for it to be possible to meet all deadlines the following inequality must hold c c g cn t t tn the sum of the processor utilizations of the individual tasks can not exceed a value of which corresponds to total utilization of the processor equation provides a bound on the number of tasks that a perfect scheduling algorithm can successfully schedule for any particular algorithm the bound may be lower for rms it can be shown that the following inequality holds c c g cn nn t t tn table gives some values for this upper bound as the number of tasks increases the scheduling bound converges to ln as an example consider the case of three periodic tasks where ui citi task p c t u task p c t u task p c t u table value of the rms upper bound n nn ln the total utilization of these three tasks is the upper bound for the schedulability of these three tasks using rms is c c c n t t t because the total utilization required for the three tasks is less than the upper bound for rms we know that if rms is used all tasks will be successfully scheduled it can also be shown that the upper bound of equation holds for earliest deadline scheduling thus it is possible to achieve greater overall processor utilization and therefore accommodate more periodic tasks with earliest deadline scheduling nevertheless rms has been widely adopted for use in industrial applications sha offers the following explanation the performance difference is small in practice the upper bound of equation is a conservative one and in practice utilization as high as is often achieved most hard realtime systems also have soft realtime components such as certain noncritical displays and builtin self tests that can execute at lower priority levels to absorb the processor time that is not used with rms scheduling of hard realtime tasks stability is easier to achieve with rms when a system can not meet all deadlines because of overload or transient errors the deadlines of essential tasks need to be guaranteed provided that this subset of tasks is schedulable in a static priority assignment approach one only needs to ensure that essential tasks have relatively high priorities this can be done in rms by structuring essential tasks to have short periods or by modifying the rms priorities to account for essential tasks with earliest deadline scheduling a periodic tasks priority changes from one period to another this makes it more difficult to ensure that essential tasks meet their deadlines priority inversion priority inversion is a phenomenon that can occur in any prioritybased preemptive scheduling scheme but is particularly relevant in the context of realtime scheduling the bestknown instance of priority inversion involved the mars pathfinder mission this rover robot landed on mars on july and began gathering and transmitting voluminous data back to earth but a few days into the mission the lander software began experiencing total system resets each resulting in losses of data after much effort by the jet propulsion laboratory jpl team that built the pathfinder the problem was traced to priority inversion jone in any priority scheduling scheme the system should always be executing the task with the highest priority priority inversion occurs when circumstances within the system force a higherpriority task to wait for a lowerpriority task a simple example of priority inversion occurs if a lowerpriority task has locked a resource such as a device or a binary semaphore and a higherpriority task attempts to lock that same resource the higherpriority task will be put in a blocked state until the resource is available if the lowerpriority task soon finishes with the resource and releases it the higherpriority task may quickly resume and it is possible that no realtime constraints are violated a more serious condition is referred to as an unbounded priority inversion in which the duration of a priority inversion depends not only on the time required to handle a shared resource but also on the unpredictable actions of other unrelated tasks the priority inversion experienced in the pathfinder software was unbounded and serves as a good example of the phenomenon our discussion follows that of time the pathfinder software included the following three tasks in decreasing order of priority t periodically checks the health of the spacecraft systems and software t processes image data t performs an occasional test on equipment status after t executes it reinitializes a timer to its maximum value if this timer ever expires it is assumed that the integrity of the lander software has somehow been compromised the processor is halted all devices are reset the software is completely reloaded the spacecraft systems are tested and the system starts over this recovery sequence does not complete until the next day t and t share a common data structure protected by a binary semaphore s figure a shows the sequence that caused the priority inversion t t begins executing t t locks semaphore s and enters its critical section t t which has a higher priority than t preempts t and begins executing t t attempts to enter its critical section but is blocked because the semaphore is locked by t t resumes execution in its critical section t t which has a higher priority than t preempts t and begins executing t t is suspended for some reason unrelated to t and t t resumes t t leaves its critical section and unlocks the semaphore t preempts t locks the semaphore and enters its critical section blocked by t attempt to lock s s locked t t preempted preempted s unlocked s locked by t by t t t t t t t t t t time a unbounded priority inversion s locked blocked by t by t attempt to lock s s unlocked t t s locked preempted s unlocked by t by t t t t t t t t t b use of priority inheritance normal execution execution in critical section figure priority inversion in this set of circumstances t must wait for both t and t to complete and fails to reset the timer before it expires in practical systems two alternative approaches are used to avoid unbounded priority inversion priority inheritance protocol and priority ceiling protocol the basic idea of priority inheritance is that a lowerpriority task inherits the priority of any higherpriority task pending on a resource they share this priority change takes place as soon as the higherpriority task blocks on the resource it should end when the resource is released by the lowerpriority task figure b shows that priority inheritance resolves the problem of unbounded priority inversion illustrated in figure a the relevant sequence of events is as follows t t begins executing t t locks semaphore s and enters its critical section t t which has a higher priority than t preempts t and begins executing t t attempts to enter its critical section but is blocked because the semaphore is locked by t t is immediately and temporarily assigned the same priority as t t resumes execution in its critical section t t is ready to execute but because t now has a higher priority t is unable to preempt t t t leaves its critical section and unlocks the semaphore its priority level is downgraded to its previous default level t preempts t locks the semaphore and enters its critical section t t is suspended for some reason unrelated to t and t begins executing this was the approach taken to solving the pathfinder problem in the priority ceiling approach a priority is associated with each resource the priority assigned to a resource is one level higher than the priority of its highestpriority user the scheduler then dynamically assigns this priority to any task that accesses the resource once the task finishes with the resource its priority returns to normal linux scheduling for linux and earlier linux provided a realtime scheduling capability coupled with a scheduler for nonrealtime processes that made use of the traditional unix scheduling algorithm described in section linux includes essentially the same realtime scheduling capability as previous releases and a substantially revised scheduler for nonrealtime processes we examine these two areas in turn realtime scheduling the three linux scheduling classes are schedfifo firstinfirstout realtime threads schedrr roundrobin realtime threads schedother other nonrealtime threads within each class multiple priorities may be used with priorities in the realtime classes higher than the priorities for the schedother class the default values are as follows realtime priority classes range from to inclusively and schedother classes range from to a lower number equals a higher priority for fifo threads the following rules apply the system will not interrupt an executing fifo thread except in the following cases a another fifo thread of higher priority becomes ready b the executing fifo thread becomes blocked waiting for an event such as io c the executing fifo thread voluntarily gives up the processor following a call to the primitive schedyield when an executing fifo thread is interrupted it is placed in the queue associated with its priority when a fifo thread becomes ready and if that thread has a higher priority than the currently executing thread then the currently executing thread is preempted and the highestpriority ready fifo thread is executed if more than one thread has that highest priority the thread that has been waiting the longest is chosen the schedrr policy is similar to the schedfifo policy except for the addition of a timeslice associated with each thread when a schedrr thread has executed for its timeslice it is suspended and a realtime thread of equal or higher priority is selected for running figure is an example that illustrates the distinction between fifo and rr scheduling assume a process has four threads with three relative priorities assigned as shown in figure a assume that all waiting threads are ready to execute when the current thread waits or terminates and that no higherpriority thread is awakened while a thread is executing figure b shows a flow in which all of the threads are in the schedfifo class thread d executes until it waits or terminates next although threads b and c have the same priority thread b starts because it has been waiting longer than thread c thread b executes until it waits or terminates then thread c executes until it waits or terminates finally thread a executes figure c shows a sample flow if all of the threads are in the schedrr class thread d executes until it waits or terminates next threads b and c are time sliced because they both have the same priority finally thread a executes the final scheduling class is schedother a thread in this class can only execute if there are no realtime threads ready to execute a minimum b middle d b c a c middle d maximum a relative thread priorities b flow with fifo scheduling d b c b c a c flow with rr scheduling figure example of linux realtime scheduling nonrealtime scheduling the linux scheduler for the schedother class did not scale well with increasing number of processors and increasing number of processes the drawbacks of this scheduler include the following the linux scheduler uses a single runqueue for all processors in a symmetric multiprocessing system smp this means a task can be scheduled on any processor which can be good for load balancing but bad for memory caches for example suppose a task executed on cpu and its data were in that processors cache if the task got rescheduled to cpu its data would need to be invalidated in cpu and brought into cpu the linux scheduler uses a single runqueue lock thus in an smp system the act of choosing a task to execute locks out any other processor from manipulating the runqueues the result is idle processors awaiting release of the runqueue lock and decreased efficiency preemption is not possible in the linux scheduler this means that a lowerpriority task can execute while a higherpriority task waited for it to complete to correct these problems linux uses a completely new priority scheduler known as the o scheduler the scheduler is designed so that the time to select the appropriate process and assign it to a processor is constant regardless of the load on the system or the number of processors the kernel maintains two scheduling data structure for each processor in the system of the following form figure struct prioarray int nractive number of tasks in this array unsigned long bitmapbitmapsize priority bitmap struct listhead queuemaxprio priority queues a separate queue is maintained for each priority level the total number of queues in the structure is maxprio which has a default value of the structure also includes a bitmap array of sufficient size to provide one bit per priority level thus with priority levels and bit words bitmapsize has a value of this creates a bitmap of bits of which bits are ignored the bitmap indicates which queues are not empty finally nractive indicates the total number of tasks present on all queues two structures are maintained an active queues structure and an expired queues structure initially both bitmaps are set to all zeroes and all queues are empty as a process becomes ready it is assigned to the appropriate priority queue in the active queues structure and is assigned the appropriate timeslice if a task is preempted before it completes its timeslice it is returned to an active queue when a task completes its timeslice it goes into the appropriate queue in the expired queues structure and is assigned a new timeslice all scheduling is done from among tasks in the active the term o is an example of the bigo notation used for characterizing the time complexity of algorithms appendix i explains this notation highestpriority bit nonempty priority active queue active queues queues by priority each queue contains ready tasks for that priority bit bit priority array for active queues priority expired queues queues by priority each queue contains ready tasks with expired timeslices for that priority bit priority array for expired queues figure linux scheduling data structures for each processor queues structure when the active queues structure is empty a simple pointer assignment results in a switch of the active and expired queues and scheduling continues scheduling is simple and efficient on a given processor the scheduler picks the highestpriority nonempty queue if multiple tasks are in that queue the tasks are scheduled in roundrobin fashion linux also includes a mechanism for moving tasks from the queue lists of one processor to that of another periodically the scheduler checks to see if there is a substantial imbalance among the number of tasks assigned to each processor to balance the load the schedule can transfer some tasks the highestpriority active tasks are selected for transfer because it is more important to distribute highpriority tasks fairly calculating priorities and timeslices each nonrealtime task is assigned an initial priority in the range of with a default of this is the tasks static priority and is specified by the user as the task executes a dynamic priority is calculated as a function of the tasks static priority and its execution behavior the linux scheduler is designed to favor iobound tasks over processorbound tasks this preference tends to provide good interactive response the technique used by linux to determine the dynamic priority is to keep a running tab on how much time a process sleeps waiting for an event versus how much time the process runs in essence a task that spends most of its time sleeping is given a higher priority timeslices are assigned in the range of ms in general higherpriority tasks are assigned larger timeslices relationship to realtime tasks realtime tasks are handled in a different manner from nonrealtime tasks in the priority queues the following considerations apply all realtime tasks have only a static priority no dynamic priority changes are made schedfifo tasks do not have assigned timeslices such tasks are scheduled in fifo discipline if a shedfifo task is blocked it returns to the same priority queue in the active queue list when it becomes unblocked although schedrr tasks do have assigned timeslices they also are never moved to the expired queue list when a schedrr task exhausts its timeslice it is returned to its priority queue with the same timeslice value timeslice values are never changed the effect of these rules is that the switch between the active queue list and the expired queue list only happens when there are no ready realtime tasks waiting to execute unix svr scheduling the scheduling algorithm used in unix svr is a complete overhaul of the scheduling algorithm used in earlier unix systems described in section the new algorithm is designed to give highest preference to realtime processes nexthighest preference to kernelmode processes and lowest preference to other usermode processes referred to as timeshared processes the two major modifications implemented in svr are as follows the addition of a preemptable static priority scheduler and the introduction of a set of priority levels divided into three priority classes the insertion of preemption points because the basic kernel is not preemptive it can only be split into processing steps that must run to completion without interruption in between the processing steps safe places known as preemption points have been identified where the kernel can safely interrupt processing and schedule a new process a safe place is defined as a region of code where all kernel data structures are either updated and consistent or locked via a semaphore figure illustrates the priority levels defined in svr each process is defined to belong to one of three priority classes and is assigned a priority level within that class the classes are as follows real time processes at these priority levels are guaranteed to be selected to run before any kernel or timesharing process in addition realtime timeshared processes are the processes that correspond to users in a traditional timesharing system priority global scheduling class value sequence first real time kernel time shared last figure svr priority classses processes can make use of preemption points to preempt kernel processes and user processes kernel processes at these priority levels are guaranteed to be selected to run before any timesharing process but must defer to realtime processes timeshared the lowestpriority processes intended for user applications other than realtime applications figure indicates how scheduling is implemented in svr a dispatch queue is associated with each priority level and processes at a given priority level are executed in roundrobin fashion a bitmap vector dqactmap contains one bit for each priority level the bit is set to one for any priority level with a nonempty queue whenever a running process leaves the running state due to a block timeslice expiration or preemption the dispatcher checks dqactmap and dispatches a ready process from the highestpriority nonempty queue in addition whenever a defined preemption point is reached the kernel checks a flag called kprunrun if set this indicates that at least one realtime process is in the ready state and the kernel preempts the current process if it is of lower priority than the highestpriority realtime ready process within the timesharing class the priority of a process is variable the scheduler reduces the priority of a process each time it uses up a time quantum and it raises its dqactmap dispq n p p p p p p p p figure svr dispatch queues priority if it blocks on an event or resource the time quantum allocated to a timesharing process depends on its priority ranging from ms for priority to ms for priority each realtime process has a fixed priority and a fixed time quantum unix freebsd scheduling the unix freebsd scheduler is designed to provide a more efficient operation than previous unix schedulers under heavy load and when used on a multiprocessor or multicore platform the scheduler is quite complex and here we present an overview of the most significant design features for more detail see mcku and robe priority classes the underlying priority mechanism in the freebsd scheduler is similar to that of unix svr for freebsd five priority classes are defined table the first two classes are for kernelmode thread and the remaining classes for usermode threads kernel threads execute code that is complied into the kernels load image and operate with the kernels privileged execution code the highestpriority threads are referred to as bottomhalf kernel threads in this class run in the kernel are scheduled based on interrupt priorities these priorities are set when the corresponding devices are configured and do not change tophalf kernel threads also run in the kernel and execute various kernel functions these priorities are set based on predefined priorities and never change the next lower priority class is referred to as realtime user a thread with a realtime priority is not subject to priority degradation that is a realtime thread maintains the priority it began with and does not drop to a lower priority as a result of using resources next comes the timesharing user priority class for threads in this class priority is periodically recalculated based on a number of parameters including the amount of processor time used the amount of memory resources held and other resource consumption parameters the lowest range of priorities is referred to as the idle user class this class is intended for applications that will only consume processor time when no other threads are ready to execute table freebsd thread scheduling classes priority class thread type description bottomhalf kernel scheduled by interrupts can block to await a resource tophalf kernel runs until blocked or done can block to await a resource realtime user allowed to run until blocked or until a higherpriority thread becomes available preemptive scheduling timesharing user adjusts priorities based on processor usage idle user only run when there are no time sharing or realtime threads to run note lower number corresponds to higher priority smp and multicore support the latest version of the freebsd scheduler introduced with freebsd was designed to provide effective scheduling for a smp or multicore system the new scheduler meets three design goals address the need for processor affinity in smp and multicore systems the term processor affinity refers to a scheduler that only migrates a thread moves thread from one processor to another when necessary to avoid having an idle processor provide better support for multithreading on multicore systems improve the performance of the scheduling algorithm so that it is no longer a function of the number of threads in the system in this subsection we look at three key features of the new scheduler queue structure interactivity scoring and thread migration queue structure the previous version of the freebsd schedule used a single global scheduling queue for all processors that it traverses once per second to recalculate their priorities the use of a single list for all threads means that the performance of the scheduler is dependent on the number of tasks in the system and as the number of tasks grows more processor time must be spent in the scheduler maintaining the list the new scheduler performs scheduling independently for each processor for each processor three queues are maintained each of the queues has the structure shown in figure for svr two runqueues implement the kernel realtime and timesharing scheduling classes priorities through the third queue is only for the idle class priorities through the two runqueues are designated current and next every thread that is granted a timeslice place in the ready state is placed in either the current queue or the next queue as explained subsequently at the appropriate priority for that thread the scheduler for a processor selects threads from the current queue in priority order until the current queue is empty when the current queue is empty the scheduler swaps the current and next queue and begins to schedule threads from the new current queue the use of two runqueues guarantees that each thread will be granted processor time at least once every two queue switches regardless of priority avoiding starvation several rules determine the assignment of a thread to either the current queue or the next queue kernel and realtime threads are always inserted onto the current queue a timesharing thread is assigned to the current queue if it is interactive explained in the next subsection or to the next queue otherwise inserting interactive threads onto the current queue results in a low interactive response time for such threads compared to other timesharing threads that do not exhibit a high degree of interactivity interactivity scoring a thread is considered to be interactive if the ratio of its voluntary sleep time versus its run time is below a certain threshold interactive threads typically have high sleep times as they wait for user input these sleep intervals are followed by bursts of processor activity as the thread processes the users request highest realtime priority classes lowest highest variable priority classes lowest figure windows thread dispatching priorities the interactivity threshold is defined in the scheduler code and is not configurable the scheduler uses two equations to compute the interactivity score of a thread first we define a scaling factor scaling factor maximum interactivity score for threads whose sleep time exceeds their run time the following equation is used interactivity score scaling factor a run b sleep when a threads run time exceeds its sleep time the following equation is used instead interactivity score scaling factor a sleep b run the result is that threads whose sleep time exceeds their run time score in the lower half of the range of interactivity scores and threads whose run time exceeds their sleep time score in the upper half of the range thread migration in general it is desirable to schedule a ready thread onto the last processor that it ran on this is called processor affinity the alternative is to allow a thread to migrate to another processor for its next execution time slice processor affinity is significant because of local caches dedicated to a single processor when a thread is run it may still have data in the cache of its last processor changing to another processor means that the necessary data must be loaded into caches in the new processor and cache lines from the preceding processor must be invalidated on the other hand processor migration may allow a better load balancing and may prevent idle periods on some processors while other processor have more work than they can handle in a timely fashion the freebsd scheduler supports two mechanisms for thread migration to balance load pull and push with the pull mechanism and idle processor steals a thread from a nonidle processor when a processor has no work to do it sets a bit in a global bitmask indicating that it is idle when an active processor is about to add work to its own run queue it first checks for such idle bits and if a set idle bit is found passes the thread to the idle processor it is primarily useful when there is a light or sporadic load or in situations where processes are starting and exiting very frequently the pull mechanism is effective in preventing the waste of a processor due to idleness but it is not effective or indeed relevant in a situation in which every processor has work to do but the load has developed in an uneven fashion with the push mechanism a periodic scheduler task evaluates the current load situation and evens it out twice per second this task picks the mostloaded and leastloaded processors in the system and equalizes their run queues push migration ensures fairness among the runnable threads windows scheduling windows is designed to be as responsive as possible to the needs of a single user in a highly interactive environment or in the role of a server windows implements a preemptive scheduler with a flexible system of priority levels that includes roundrobin scheduling within each level and for some levels dynamic priority variation on the basis of their current thread activity threads are the unit of scheduling in windows rather than processes process and thread priorities priorities in windows are organized into two bands or classes real time and variable each of these bands consists of priority levels threads requiring immediate attention are in the realtime class which includes functions such as communications and realtime tasks overall because windows makes use of a prioritydriven preemptive scheduler threads with realtime priorities have precedence over other threads when a thread becomes ready whose priority is higher than the currently executing thread the lowerpriority thread is preempted and the processor given to the higherpriority thread priorities are handled somewhat differently in the two classes figure in the realtime priority class all threads have a fixed priority that never changes all of the active threads at a given priority level are in a roundrobin queue in the variable priority class a threads priority begins an initial priority value and then may be temporarily boosted raised during the threads lifetime there is a fifo queue at each priority level a thread will change queues among the variable priority classes as its own priority changes however a thread at priority level or below is never boosted to level or any other level in the realtime class the initial priority of a thread in the variable priority class is determined by two quantities process base priority and thread base priority the process base priority is an attribute of the process object and can take on any value from through priority is reserved for the executives perprocessor idle threads each thread object associated with a process object has a thread base priority attribute that indicates the threads base priority relative to that of the process the threads base priority can be equal to that of its process or within two levels above or below that of the process so for example if a process has a base priority of and one of its threads has a base priority of then the initial priority of that thread is once a thread in the variable priority class has been created its actual priority referred to as the threads current priority may fluctuate within given boundaries the current priority may never fall below the threads base priority and it may never exceed figure gives an example the process object has a base priority attribute of each thread object associated with this process object must have an initial priority of between and suppose the base priority for thread is then the current priority for that thread may fluctuate in the range from through depending on what boosts it has been given if a thread is interrupted to wait on an io event the kernel boosts its priority if a boosted thread is interrupted because highest above normal base priority normal below normal lowest process threads base threads dynamic priority priority priority figure example of windows priority relationship it has used up its current time quantum the kernel lowers its priority thus processorbound threads tend toward lower priorities and iobound threads tend toward higher priorities in the case of iobound threads the kernel boosts the priority more for interactive waits eg wait on keyboard or mouse than for other types of io eg disk io thus interactive threads tend to have the highest priorities within the variable priority class multiprocessor scheduling when windows is run on a single processor the highestpriority thread is always active unless it is waiting on an event if there is more than one thread that has the same highest priority then the processor is shared round robin among all the threads at that priority level in a multiprocessor system with n processors the kernel tries to give the n processors to the n highestpriority threads that are ready to run the remaining lower priority threads must wait until the other threads block or have their priority decay lowerpriority threads may also have their priority boosted to for a very short time if they are being starved solely to correct instances of priority inversion the foregoing scheduling discipline is affected by the processor affinity attribute of a thread if a thread is ready to execute but the only available processors are not in its processor affinity set then that thread is forced to wait and the kernel schedules the next available thread cache memory although cache memory is invisible to the os it interacts with other memory management hardware furthermore many of the principles used in virtual memory schemes discussed in chapter are also applied in cache memory motivation on all instruction cycles the processor accesses memory at least once to fetch the instruction and often one or more additional times to fetch operands and or store results the rate at which the processor can execute instructions is clearly limited by the memory cycle time the time it takes to read one word from or write one word to memory this limitation has been a significant problem because of the persistent mismatch between processor and main memory speeds over the years processor speed has consistently increased more rapidly than memory access speed we are faced with a tradeoff among speed cost and size ideally main memory should be built with the same technology as that of the processor registers giving memory cycle times comparable to processor cycle times this has always been too expensive a strategy the solution is to exploit the principle of locality by providing a small fast memory between the processor and main memory namely the cache block transfer word transfer cpu cache main memory fast slow a single cache cpu level level level main l cache l cache l cache memory fastest fast less slow fast b threelevel cache organization figure cache and main memory cache principles cache memory is intended to provide memory access time approaching that of the fastest memories available and at the same time support a large memory size that has the price of less expensive types of semiconductor memories the concept is illustrated in figure a there is a relatively large and slow main memory together with a smaller faster cache memory the cache contains a copy of a portion of main memory when the processor attempts to read a byte or word of memory a check is made to determine if the byte or word is in the cache if so the byte or word is delivered to the processor if not a block of main memory consisting of some fixed number of bytes is read into the cache and then the byte or word is delivered to the processor because of the phenomenon of locality of reference when a block of data is fetched into the cache to satisfy a single memory reference it is likely that many of the nearfuture memory references will be to other bytes in the block figure b depicts the use of multiple levels of cache the l cache is slower and typically larger than the l cache and the l cache is slower and typically larger than the l cache figure depicts the structure of a cachemain memory system main memory consists of up to n addressable words with each word having a unique nbit address for mapping purposes this memory is considered to consist of a number of fixedlength blocks of k words each that is there are m nk blocks cache consists of c slots also referred to as lines of k words each and the number of slots is considerably less than the number of main memory blocks cm some subset of the blocks of main memory resides in the slots of the cache if a word in a block the symbol means much less than similarly the symbol means much greater than line memory number tag block address block k words c block length k words a cache block n word length b main memory figure cachemainmemory structure of memory that is not in the cache is read that block is transferred to one of the slots of the cache because there are more blocks than slots an individual slot can not be uniquely and permanently dedicated to a particular block therefore each slot includes a tag that identifies which particular block is currently being stored the tag is usually some number of higherorder bits of the address and refers to all addresses that begin with that sequence of bits as a simple example suppose that we have a bit address and a bit tag the tag refers to the block of locations with the following addresses figure illustrates the read operation the processor generates the address ra of a word to be read if the word is contained in the cache it is delivered to the processor otherwise the block containing that word is loaded into the cache and the word is delivered to the processor cache design a detailed discussion of cache design is beyond the scope of this book key elements are briefly summarized here we will see that similar design issues must be start ra read address receive address ra from cpu is block no access main containing ra memory for block in cache containing ra yes fetch ra word allocate cache and deliver slot for main to cpu memory block load main deliver ra word memory block to cpu into cache slot done figure cache read operation addressed in dealing with virtual memory and disk cache design they fall into the following categories cache size block size mapping function replacement algorithm write policy number of cache levels we have already dealt with the issue of cache size it turns out that reasonably small caches can have a significant impact on performance another size issue is that of block size the unit of data exchanged between cache and main memory as the block size increases from very small to larger sizes the hit ratio will at first increase because of the principle of locality the high probability that data in the vicinity of a referenced word are likely to be referenced in the near future as the block size increases more useful data are brought into the cache the hit ratio will begin to decrease however as the block becomes even bigger and the probability of using the newly fetched data becomes less than the probability of reusing the data that have to be moved out of the cache to make room for the new block when a new block of data is read into the cache the mapping function determines which cache location the block will occupy two constraints affect the design of the mapping function first when one block is read in another may have to be replaced we would like to do this in such a way as to minimize the probability that we will replace a block that will be needed in the near future the more flexible the mapping function the more scope we have to design a replacement algorithm to maximize the hit ratio second the more flexible the mapping function the more complex is the circuitry required to search the cache to determine if a given block is in the cache the replacement algorithm chooses within the constraints of the mapping function which block to replace when a new block is to be loaded into the cache and the cache already has all slots filled with other blocks we would like to replace the block that is least likely to be needed again in the near future although it is impossible to identify such a block a reasonably effective strategy is to replace the block that has been in the cache longest with no reference to it this policy is referred to as the leastrecentlyused lru algorithm hardware mechanisms are needed to identify the leastrecentlyused block if the contents of a block in the cache are altered then it is necessary to write it back to main memory before replacing it the write policy dictates when the memory write operation takes place at one extreme the writing can occur every time that the block is updated at the other extreme the writing occurs only when the block is replaced the latter policy minimizes memory write operations but leaves main memory in an obsolete state this can interfere with multipleprocessor operation and with direct memory access by io hardware modules finally it is now commonplace to have multiple levels of cache labeled l cache closest to the processor l and in many cases a third level l a discussion of the performance benefits of multiple cache levels is beyond our scope see stal for a discussion linux virtual machine process scheduling the linux vserver virtual machine facility introduced in chapter provides a way of controlling vm use of processor time vserver overlays a token bucket filter tbf on top of the standard linux schedule the purpose of the tbf is to determine how much of the processor execution time single processor multiprocessor or multicore is allocated to each vm if only the underlying linux scheduler is used to globally schedule processes across all vms then resource hunger processes in one vm crowd out processes in other vms figure illustrates the tbf concept for each vm a bucket is defined with a capacity of s tokens tokens are added to the bucket at a rate of r tokens during every time interval of length t when the bucket is full additional incoming tokens are simply discarded when a process is executing on this vm it consumes one token for each timer clock tick if the bucket empties the process is put on hold and can not be restarted until the bucket is refilled to a minimum threshold value of m tokens at that point the process is rescheduled a significant consequence of the tbf approach is that a vm may accumulate tokens during a period of quiescence and then later use the tokens in a burst when required adjusting the values of r and t allows for regulating the percentage of capacity that a vm can claim for a single processor we can define capacity allocation as follows r fraction of processor allocation t token input rate rt tokens per second bucket size current bucket s tokens occupancy minimum threshold m tokens running process consumes tokentimer tick figure linux vserver token bucket scheme this equation denotes the fraction of a single processor in a system thus for example if a system is multicore with four cores and we wish to provide one vm on an average of one dedicated processor then we set r and t the overall system is limited as follows if there are n vms then n ri a ti i the parameters s and m are set so as to penalize a vm after a certain amount of burst time the following parameters must be configured or allocated for a vm following a burst time of b the vm suffers a hold time of h with these parameters it is possible to calculate the desired values of s and m as follows mwh r t s w b a rb t where w is the rate at which the schedule runs makes decisions for example consider a vm with a limit of of processor time and we wish to say that after using the processor for seconds there will be a hold time of seconds the scheduler runs at hz this requirement is met with the following values m tokens s tokens summary with a tightly coupled multiprocessor multiple processors have access to the same main memory in this configuration the scheduling structure is somewhat more complex for example a given process may be assigned to the same processor for its entire life or dispatched to any processor each time it enters the running state performance studies suggest that the differences among various scheduling algorithms are less significant in a multiprocessor system a realtime process or task is one that is executed in connection with some process or function or set of events external to the computer system and that must meet one or more deadlines to interact effectively and correctly with the external environment a realtime operating system is one that is capable of managing realtime processes in this context the traditional criteria for a scheduling algorithm do not apply rather the key factor is the meeting of deadlines algorithms that rely heavily on preemption and on reacting to relative deadlines are appropriate in this context recommended reading wend is an interesting discussion of approaches to multiprocessor scheduling a good treatment of realtime scheduling is contained in liu the following collections of papers all contain important articles on realtime operating systems and scheduling kris stan lee and tilb sha provides a good explanation of priority inversion priority inheritance and priority ceiling zead analyzes the performance of the svr realtime scheduler lind provides an overview of the linux scheduler love contains a more detailed discussion kris krishna c and lee y eds special issue on realtime systems proceedings of the ieee january lee lee y and krishna c eds readings in realtime systems los alamitos ca ieee computer society press lind lindsley r whats new in the scheduler linux journal march liu liu j realtime systems upper saddle river nj prentice hall love love r linux kernel development upper saddle river nj addisonwesley sha sha l rajkumar r and lehoczky j priority inheritance protocols an approach to realtime synchronization ieee transactions on computers september stan stankovic j and ramamritham k eds advances in realtime systems los alamitos ca ieee computer society press tilb tilborg a and koob g eds foundations of realtime computing scheduling and resource management boston kluwer academic publishers wend wendorf j wendorf r and tokuda h scheduling operating system processing on smallscale microprocessors proceedings nd annual hawaii international conference on system science january zead zeadally s an evaluation of the realtime performance of svr and svr operating systems review january key terms review questions and problems key terms aperiodic task hard realtime task realtime scheduling deadline scheduling load sharing responsiveness deterministic operating system periodic task soft realtime task failsoft operation priority inversion thread scheduling gang scheduling rate monotonic scheduling unbounded priority granularity realtime operating system inversion review questions list and briefly define five different categories of synchronization granularity list and briefly define four techniques for thread scheduling list and briefly define three versions of load sharing what is the difference between hard and soft realtime tasks what is the difference between periodic and aperiodic realtime tasks list and briefly define five general areas of requirements for a realtime operating system list and briefly define four classes of realtime scheduling algorithms what items of information about a task might be useful in realtime scheduling problems consider a set of three periodic tasks with the execution profiles of table develop scheduling diagrams similar to those of figure for this set of tasks table execution profile for problem process arrival time execution time ending deadline a a b b c c table execution profile for problem process arrival time execution time starting deadline a b c d e consider a set of five aperiodic tasks with the execution profiles of table develop scheduling diagrams similar to those of figure for this set of tasks least laxity first llf is a realtime scheduling algorithm for periodic tasks slack time or laxity is the amount of time between when a task would complete if it started now and its next deadline this is the size of the available scheduling window laxity can be expressed as laxity deadline time current time processor time needed llf selects the task with the minimum laxity to execute next if two or more tasks have the same minimum laxity value they are serviced on a fcfs basis a suppose a task currently has a laxity of t by how long may the scheduler delay starting this task and still meet its deadline b suppose a task currently has a laxity of what does this mean c what does it mean if a task has negative laxity d consider a set of three periodic tasks with the execution profiles of table a develop scheduling diagrams similar to those of figure for this set of tasks that compare rate monotonic earliest deadline first and llfassume preemption may occur at ms intervals comment on the results repeat problem d for the execution profiles of table b comment on the results maximum urgency first muf is a realtime scheduling algorithm for periodic tasks each task is assigned an urgency that is defined as a combination of two fixed priorities and one dynamic priority one of the fixed priorities the criticality has precedence over the dynamic priority meanwhile the dynamic priority has precedence over the other fixed priority called the user priority the dynamic priority is inversely table execution profiles for problems through a light load task period execution time a b c b heavy load task period execution time a b c proportional to the laxity of a task muf can be explained as follows first tasks are ordered from shortest to longest period define the critical task set as the first n tasks such that worstcase processor utilization does not exceed among critical set tasks that are ready the scheduler selects the task with the least laxity if no critical set tasks are ready the schedule chooses among the noncritical tasks the one with the least laxity ties are broken through an optional user priority and then by fcfs repeat problem d adding muf to the diagramsassume that userdefined priorities are a highest b next c lowest comment on the results repeat problem adding muf to the diagrams comment on the results this problem demonstrates that although equation for rate monotonic scheduling is a sufficient condition for successful scheduling it is not a necessary condition ie sometimes successful scheduling is possible even if equation is not satisfied a consider a task set with the following independent periodic tasks task p c t task p c t can these tasks be successfully scheduled using rate monotonic scheduling b now add the following task to the set task p c t is equation satisfied c suppose that the first instance of the preceding three tasks arrives at time t assume that the first deadline for each task is the following d d d using rate monotonic scheduling will all three deadlines be met what about deadlines for future repetitions of each task draw a diagram similar to that of figure b that shows the sequence events for this same example using priority ceiling chapter io management and disk scheduling io devices organization of the io function the evolution of the io function direct memory access operating system design issues design objectives logical structure of the io function io buffering single buffer double buffer circular buffer the utility of buffering disk scheduling disk performance parameters disk scheduling policies raid disk cache design considerations performance considerations unix svr io linux io windows io summary recommended reading key terms review questions and problems an artifact can be thought of as a meeting point aninterfacein todays terms between an inner environment the substance and organization of the artifact itself and an outer environment the surroundings in which it operates if the inner environment is appropriate to the outer environment or vice versa the artifact will serve its intended purpose the sciences of the artificial herbert simon learning objectives after studying this chapter you should be able to summarize key categories of io devices on computers discuss the organization of the io function explain some of the key issues in the design of os support for io analyze the performance implications of various io buffering alternatives understand the performance issues involved in magnetic disk access explain the concept of raid and describe the various levels understand the performance implications of disk cache describe the io mechanisms in unix linux and windows perhaps the messiest aspect of operating system design is inputoutput because there is such a wide variety of devices and applications of those devices it is difficult to develop a general consistent solution we begin with a brief discussion of io devices and the organization of the io function these topics which generally come within the scope of computer architecture set the stage for an examination of io from the point of view of the os the next section examines operating system design issues including design objectives and the way in which the io function can be structured then io buffering is examined one of the basic io services provided by the operating system is a buffering function which improves overall performance the next sections of the chapter are devoted to magnetic disk io in contemporary systems this form of io is the most important and is key to the performance as perceived by the user we begin by developing a model of disk io performance and then examine several techniques that can be used to enhance performance appendix j summarizes characteristics of secondary storage devices including magnetic disk and optical memory a set of animations that illustrate concepts in this chapter is available online click on the rotating globe at williamstallings comososehtml for access io devices as was mentioned in chapter external devices that engage in io with computer systems can be roughly grouped into three categories human readable suitable for communicating with the computer user examples include printers and terminals the latter consisting of video display keyboard and perhaps other devices such as a mouse machine readable suitable for communicating with electronic equipment examples are disk drives usb keys sensors controllers and actuators communication suitable for communicating with remote devices examples are digital line drivers and modems there are great differences across classes and even substantial differences within each class among the key differences are the following data rate there may be differences of several orders of magnitude between the data transfer rates figure gives some examples application the use to which a device is put has an influence on the software and policies in the os and supporting utilities for example a disk used for files requires the support of file management software a disk used as a backing store for pages in a virtual memory scheme depends on the use of virtual memory hardware and software furthermore these applications have an impact on disk scheduling algorithms discussed later in this chapter as another example a terminal may be used by an ordinary user or a system administrator these uses imply different privilege levels and perhaps different priorities in the os complexity of control a printer requires a relatively simple control interface a disk is much more complex the effect of these differences on the os is filtered to some extent by the complexity of the io module that controls the device as discussed in the next section gigabit ethernet graphics display hard disk ethernet optical disk scanner laser printer floppy disk modem mouse keyboard data rate bps figure typical io device data rates unit of transfer data may be transferred as a stream of bytes or characters eg terminal io or in larger blocks eg disk io data representation different data encoding schemes are used by different devices including differences in character code and parity conventions error conditions the nature of errors the way in which they are reported their consequences and the available range of responses differ widely from one device to another this diversity makes a uniform and consistent approach to io both from the point of view of the operating system and from the point of view of user processes difficult to achieve organization of the io function appendix c summarizes three techniques for performing io programmed io the processor issues an io command on behalf of a process to an io module that process then busy waits for the operation to be completed before proceeding interruptdriven io the processor issues an io command on behalf of a process there are then two possibilities if the io instruction from the process is nonblocking then the processor continues to execute instructions from the process that issued the io command if the io instruction is blocking then the next instruction that the processor executes is from the os which will put the current process in a blocked state and schedule another process direct memory access dma a dma module controls the exchange of data between main memory and an io module the processor sends a request for the transfer of a block of data to the dma module and is interrupted only after the entire block has been transferred table indicates the relationship among these three techniques in most computer systems dma is the dominant form of transfer that must be supported by the operating system the evolution of the io function as computer systems have evolved there has been a pattern of increasing complexity and sophistication of individual components nowhere is this more table io techniques no interrupts use of interrupts iotomemory transfer through programmed io interruptdriven io processor direct iotomemory transfer direct memory access dma evident than in the io function the evolutionary steps can be summarized as follows the processor directly controls a peripheral device this is seen in simple microprocessorcontrolled devices a controller or io module is added the processor uses programmed io without interrupts with this step the processor becomes somewhat divorced from the specific details of external device interfaces the same configuration as step is used but now interrupts are employed the processor need not spend time waiting for an io operation to be performed thus increasing efficiency the io module is given direct control of memory via dma it can now move a block of data to or from memory without involving the processor except at the beginning and end of the transfer the io module is enhanced to become a separate processor with a specialized instruction set tailored for io the central processing unit cpu directs the io processor to execute an io program in main memory the io processor fetches and executes these instructions without processor intervention this allows the processor to specify a sequence of io activities and to be interrupted only when the entire sequence has been performed the io module has a local memory of its own and is in fact a computer in its own right with this architecture a large set of io devices can be controlled with minimal processor involvement a common use for such an architecture has been to control communications with interactive terminals the io processor takes care of most of the tasks involved in controlling the terminals as one proceeds along this evolutionary path more and more of the io function is performed without processor involvement the central processor is increasingly relieved of iorelated tasks improving performance with the last two steps and a major change occurs with the introduction of the concept of an io module capable of executing a program a note about terminology for all of the modules described in steps through the term direct memory access is appropriate because all of these types involve direct control of main memory by the io module also the io module in step is often referred to as an io channel and that in step as an io processor however each term is on occasion applied to both situations in the latter part of this section we will use the term io channel to refer to both types of io modules direct memory access figure indicates in general terms the dma logic the dma unit is capable of mimicking the processor and indeed of taking over control of the system bus just like a processor it needs to do this to transfer data to and from memory over the system bus data count data lines data register address address lines register request to dma acknowledge from dma control interrupt logic read write figure typical dma block diagram the dma technique works as follows when the processor wishes to read or write a block of data it issues a command to the dma module by sending to the dma module the following information whether a read or write is requested using the read or write control line between the processor and the dma module the address of the io device involved communicated on the data lines the starting location in memory to read from or write to communicated on the data lines and stored by the dma module in its address register the number of words to be read or written again communicated via the data lines and stored in the data count register the processor then continues with other work it has delegated this io operation to the dma module the dma module transfers the entire block of data one word at a time directly to or from memory without going through the processor when the transfer is complete the dma module sends an interrupt signal to the processor thus the processor is involved only at the beginning and end of the transfer figure cc the dma mechanism can be configured in a variety of ways some possibilities are shown in figure in the first example all modules share the same system bus the dma module acting as a surrogate processor uses programmed io to exchange data between memory and an io module through the dma module this configuration while it may be inexpensive is clearly inefficient as with processorcontrolled programmed io each transfer of a word consumes two bus cycles transfer request followed by transfer the number of required bus cycles can be cut substantially by integrating the dma and io functions as figure b indicates this means that there is a path between the dma module and one or more io modules that does not include the processor dma io io memory a singlebus detached dma processor dma dma memory io io io b singlebus integrated dmaio system bus processor dma memory io bus io io io c io bus figure alternative dma configurations system bus the dma logic may actually be a part of an io module or it may be a separate module that controls one or more io modules this concept can be taken one step further by connecting io modules to the dma module using an io bus figure c this reduces the number of io interfaces in the dma module to one and provides for an easily expandable configuration in all of these cases figure b and c the system bus that the dma module shares with the processor and main memory is used by the dma module only to exchange data with memory and to exchange control signals with the processor the exchange of data between the dma and io modules takes place off the system bus operating system design issues design objectives two objectives are paramount in designing the io facility efficiency and generality efficiency is important because io operations often form a bottleneck in a computing system looking again at figure we see that most io devices are extremely slow compared with main memory and the processor one way to tackle this problem is multiprogramming which as we have seen allows some processes to be waiting on io operations while another process is executing however even with the vast size of main memory in todays machines it will still often be the case that io is not keeping up with the activities of the processor swapping is used to bring in additional ready processes to keep the processor busy but this in itself is an io operation thus a major effort in io design has been schemes for improving the efficiency of the io the area that has received the most attention because of its importance is disk io and much of this chapter will be devoted to a study of disk io efficiency the other major objective is generality in the interests of simplicity and freedom from error it is desirable to handle all devices in a uniform manner this applies both to the way in which processes view io devices and to the way in which the os manages io devices and operations because of the diversity of device characteristics it is difficult in practice to achieve true generality what can be done is to use a hierarchical modular approach to the design of the io function this approach hides most of the details of device io in lowerlevel routines so that user processes and upper levels of the os see devices in terms of general functions such as read write open close lock and unlock we turn now to a discussion of this approach logical structure of the io function in chapter in the discussion of system structure we emphasized the hierarchical nature of modern operating systems the hierarchical philosophy is that the functions of the os should be separated according to their complexity their characteristic time scale and their level of abstraction following this approach leads to an organization of the os into a series of layers each layer performs a related subset of the functions required of the os it relies on the next lower layer to perform more primitive functions and to conceal the details of those functions it provides services to the next higher layer ideally the layers should be defined so that changes in one layer do not require changes in other layers thus we have decomposed one problem into a number of more manageable subproblems in general lower layers deal with a far shorter time scale some parts of the os must interact directly with the computer hardware where events can have a time scale as brief as a few billionths of a second at the other end of the spectrum parts of the os communicate with the user who issues commands at a much more leisurely pace perhaps one every few seconds the use of a set of layers conforms nicely to this environment applying this philosophy specifically to the io facility leads to the type of organization suggested by figure the details of the organization will depend on the type of device and the application the three most important logical structures are presented in the figure of course a particular operating system may not conform exactly to these structures however the general principles are valid and most operating systems approach io in approximately this way user user user processes processes processes directory management logical communication file system io architecture physical organization device device device io io io scheduling scheduling scheduling control control control hardware hardware hardware a local peripheral device b communications port c file system figure a model of io organization let us consider the simplest case first that of a local peripheral device that communicates in a simple fashion such as a stream of bytes or records figure a the following layers are involved logical io the logical io module deals with the device as a logical resource and is not concerned with the details of actually controlling the device the logical io module is concerned with managing general io functions on behalf of user processes allowing them to deal with the device in terms of a device identifier and simple commands such as open close read and write device io the requested operations and data buffered characters records etc are converted into appropriate sequences of io instructions channel commands and controller orders buffering techniques may be used to improve utilization scheduling and control the actual queueing and scheduling of io operations occurs at this layer as well as the control of the operations thus interrupts are handled at this layer and io status is collected and reported this is the layer of software that actually interacts with the io module and hence the device hardware for a communications device the io structure figure b looks much the same as that just described the principal difference is that the logical io module is replaced by a communications architecture which may itself consist of a number of layers an example is tcpip which is discussed in chapter figure c shows a representative structure for managing io on a secondary storage device that supports a file system the three layers not previously discussed are as follows directory management at this layer symbolic file names are converted to identifiers that either reference the file directly or indirectly through a file descriptor or index table this layer is also concerned with user operations that affect the directory of files such as add delete and reorganize file system this layer deals with the logical structure of files and with the operations that can be specified by users such as open close read and write access rights are also managed at this layer physical organization just as virtual memory addresses must be converted into physical main memory addresses taking into account the segmentation and paging structure logical references to files and records must be converted to physical secondary storage addresses taking into account the physical track and sector structure of the secondary storage device allocation of secondary storage space and main storage buffers is generally treated at this layer as well because of the importance of the file system we will spend some time in this chapter and the next looking at its various components the discussion in this chapter focuses on the lower three layers while the upper two layers are examined in chapter io buffering suppose that a user process wishes to read blocks of data from a disk one at a time with each block having a length of bytes the data are to be read into a data area within the address space of the user process at virtual location to the simplest way would be to execute an io command something like read block disk to the disk unit and then wait for the data to become available the waiting could either be busy waiting continuously test the device status or more practically process suspension on an interrupt there are two problems with this approach first the program is hung up waiting for the relatively slow io to complete the second problem is that this approach to io interferes with swapping decisions by the os virtual locations to must remain in main memory during the course of the block transfer otherwise some of the data may be lost if paging is being used at least the page containing the target locations must be locked into main memory thus although portions of the process may be paged out to disk it is impossible to swap the process out completely even if this is desired by the operating system notice also that there is a risk of singleprocess deadlock if a process issues an io command is suspended awaiting the result and then is swapped out prior to the beginning of the operation the process is blocked waiting on the io event and the io operation is blocked waiting for the process to be swapped in to avoid this deadlock the user memory involved in the io operation must be locked in main memory immediately before the io request is issued even though the io operation is queued and may not be executed for some time the same considerations apply to an output operation if a block is being transferred from a user process area directly to an io module then the process is blocked during the transfer and the process may not be swapped out to avoid these overheads and inefficiencies it is sometimes convenient to perform input transfers in advance of requests being made and to perform output transfers some time after the request is made this technique is known as buffering in this section we look at some of the buffering schemes that are supported by operating systems to improve the performance of the system in discussing the various approaches to buffering it is sometimes important to make a distinction between two types of io devices block oriented and stream oriented a blockoriented device stores information in blocks that are usually of fixed size and transfers are made one block at a time generally it is possible to reference data by its block number disks and usb keys are examples of blockoriented devices a streamoriented device transfers data in and out as a stream of bytes with no block structure terminals printers communications ports mouse and other pointing devices and most other devices that are not secondary storage are stream oriented single buffer the simplest type of support that the os can provide is single buffering figure b when a user process issues an io request the os assigns a buffer in the system portion of main memory to the operation for blockoriented devices the single buffering scheme can be described as follows input transfers are made to the system buffer when the transfer is complete the process moves the block into user space and immediately requests another block this is called reading ahead or anticipated input it is done in the expectation that the block will eventually be needed for many types of computation this is a reasonable assumption most of the time because data are usually accessed sequentially only at the end of a sequence of processing will a block be read in unnecessarily this approach will generally provide a speedup compared to the lack of system buffering the user process can be processing one block of data while the next block is being read in the os is able to swap the process out because the input operation is taking place in system memory rather than user process memory this technique does however complicate the logic in the operating system the os must keep track of the assignment of system buffers to user processes the swapping logic is also affected if the io operation involves the same disk that is used for swapping operating system user process io device in a no buffering operating system user process io device in move b single buffering operating system user process io device in move c double buffering operating system user process io device in move d circular buffering figure io buffering schemes input it hardly makes sense to queue disk writes to the same device for swapping the process out this attempt to swap the process and release main memory will itself not begin until after the io operation finishes at which time swapping the process to disk may no longer be appropriate similar considerations apply to blockoriented output when data are being transmitted to a device they are first copied from the user space into the system buffer from which they will ultimately be written the requesting process is now free to continue or to be swapped as necessary knut suggests a crude but informative performance comparison between single buffering and no buffering suppose that t is the time required to input one block and that c is the computation time that intervenes between input requests without buffering the execution time per block is essentially t c with a single buffer the time is max c t m where m is the time required to move the data from the system buffer to user memory in most cases execution time per block is substantially less with a single buffer compared to no buffer for streamoriented io the single buffering scheme can be used in a lineatatime fashion or a byteatatime fashion lineatatime operation is appropriate for scrollmode terminals sometimes called dumb terminals with this form of terminal user input is one line at a time with a carriage return signaling the end of a line and output to the terminal is similarly one line at a time a line printer is another example of such a device byteatatime operation is used on formsmode terminals when each keystroke is significant and for many other peripherals such as sensors and controllers in the case of lineatatime io the buffer can be used to hold a single line the user process is suspended during input awaiting the arrival of the entire line for output the user process can place a line of output in the buffer and continue processing it need not be suspended unless it has a second line of output to send before the buffer is emptied from the first output operation in the case of byteatatime io the interaction between the os and the user process follows the producer consumer model discussed in chapter double buffer an improvement over single buffering can be had by assigning two system buffers to the operation figure c a process now transfers data to or from one buffer while the operating system empties or fills the other this technique is known as double buffering or buffer swapping for blockoriented transfer we can roughly estimate the execution time as max c t it is therefore possible to keep the blockoriented device going at full speed if c t on the other hand if c t double buffering ensures that the process will not have to wait on io in either case an improvement over single buffering is achieved again this improvement comes at the cost of increased complexity for streamoriented input we again are faced with the two alternative modes of operation for lineatatime io the user process need not be suspended for input or output unless the process runs ahead of the double buffers for byteatatime operation the double buffer offers no particular advantage over a single buffer of twice the length in both cases the producerconsumer model is followed circular buffer a doublebuffer scheme should smooth out the flow of data between an io device and a process if the performance of a particular process is the focus of our concern then we would like for the io operation to be able to keep up with the process double buffering may be inadequate if the process performs rapid bursts of io in this case the problem can often be alleviated by using more than two buffers when more than two buffers are used the collection of buffers is itself referred to as a circular buffer figure d with each individual buffer being one unit in the circular buffer this is simply the boundedbuffer producerconsumer model studied in chapter the utility of buffering buffering is a technique that smoothes out peaks in io demand however no amount of buffering will allow an io device to keep pace with a process indefinitely when the average demand of the process is greater than the io device can service even with multiple buffers all of the buffers will eventually fill up and the process will have to wait after processing each chunk of data however in a multiprogramming environment when there is a variety of io activity and a variety of process activity to service buffering is one tool that can increase the efficiency of the os and the performance of individual processes disk scheduling over the last years the increase in the speed of processors and main memory has far outstripped that for disk access with processor and main memory speeds increasing by about two orders of magnitude compared to one order of magnitude for disk the result is that disks are currently at least four orders of magnitude slower than main memory this gap is expected to continue into the foreseeable future thus the performance of disk storage subsystem is of vital concern and much research has gone into schemes for improving that performance in this section we highlight some of the key issues and look at the most important approaches because the performance of the disk system is tied closely to file system design issues the discussion continues in chapter disk performance parameters the actual details of disk io operation depend on the computer system the operating system and the nature of the io channel and disk controller hardware a general timing diagram of disk io transfer is shown in figure when the disk drive is operating the disk is rotating at constant speed to read or write the head must be positioned at the desired track and at the beginning of the desired sector on that track track selection involves moving the head in a movablehead system or electronically selecting one head on a fixedhead system on a movablehead system the time it takes to position the head at the track is known as seek time in either case once the track is selected the disk controller waits until the appropriate sector rotates to line up with the head the time it takes for the beginning of the sector to reach the head is known as rotational delay or rotational latency the sum of the seek time if any and the rotational delay equals the access time which is the time it takes to get into position to read or write once the head is in position the read or write operation is then performed as the sector moves under the head this is the data transfer portion of the operation the time required for the transfer is the transfer time wait for wait for seek rotational data device channel delay transfer device busy figure timing of a disk io transfer see appendix j for a discussion of disk organization and formatting in addition to the access time and transfer time there are several queueing delays normally associated with a disk io operation when a process issues an io request it must first wait in a queue for the device to be available at that time the device is assigned to the process if the device shares a single io channel or a set of io channels with other disk drives then there may be an additional wait for the channel to be available at that point the seek is performed to begin disk access in some highend systems for servers a technique known as rotational positional sensing rps is used this works as follows when the seek command has been issued the channel is released to handle other io operations when the seek is completed the device determines when the data will rotate under the head as that sector approaches the head the device tries to reestablish the communication path back to the host if either the control unit or the channel is busy with another io then the reconnection attempt fails and the device must rotate one whole revolution before it can attempt to reconnect which is called an rps miss this is an extra delay element that must be added to the time line of figure seek time seek time is the time required to move the disk arm to the required track it turns out that this is a difficult quantity to pin down the seek time consists of two key components the initial startup time and the time taken to traverse the tracks that have to be crossed once the access arm is up to speed unfortunately the traversal time is not a linear function of the number of tracks but includes a settling time time after positioning the head over the target track until track identification is confirmed much improvement comes from smaller and lighter disk components some years ago a typical disk was inches cm in diameter whereas the most common size today is inches cm reducing the distance that the arm has to travel a typical average seek time on contemporary hard disks is under ms rotational delay rotational delay is the time required for the addressed area of the disk to rotate into a position where it is accessible by the readwrite head disks rotate at speeds ranging from rpm for handheld devices such as digital cameras up to as of this writing rpm at this latter speed there is one revolution per ms thus on the average the rotational delay will be ms transfer time the transfer time to or from the disk depends on the rotation speed of the disk in the following fashion t b rn where t transfer time b number of bytes to be transferred n number of bytes on a track r rotation speed in revolutions per second thus the total average access time can be expressed as ta ts b r rn where ts is the average seek time a timing comparison with the foregoing parameters defined let us look at two different io operations that illustrate the danger of relying on average values consider a disk with an advertised average seek time of ms rotation speed of rpm and byte sectors with sectors per track suppose that we wish to read a file consisting of sectors for a total of mbytes we would like to estimate the total time for the transfer first let us assume that the file is stored as compactly as possible on the disk that is the file occupies all of the sectors on adjacent tracks tracks sectorstrack sectors this is known as sequential organization the time to read the first track is as follows average seek ms rotational delay ms read sectors ms ms suppose that the remaining tracks can now be read with essentially no seek time that is the io operation can keep up with the flow from the disk then at most we need to deal with rotational delay for each succeeding track thus each successive track is read in rmms to read the entire file total time ms seconds now let us calculate the time required to read the same data using random access rather than sequential access that is accesses to the sectors are distributed randomly over the disk for each sector we have average seek ms rotational delay ms read sector ms ms total time ms seconds it is clear that the order in which sectors are read from the disk has a tremendous effect on io performance in the case of file access in which multiple sectors are read or written we have some control over the way in which sectors of data are deployed and we shall have something to say on this subject in the next chapter however even in the case of a file access in a multiprogramming environment there will be io requests competing for the same disk thus it is worthwhile to examine ways in which the performance of disk io can be improved over that achieved with purely random access to the disk disk scheduling policies in the example just described the reason for the difference in performance can be traced to seek time if sector access requests involve selection of tracks at random then the performance of the disk io system will be as poor as possible to improve matters we need to reduce the average time spent on seeks consider the typical situation in a multiprogramming environment in which the os maintains a queue of requests for each io device so for a single disk there will be a number of io requests reads and writes from various processes in the queue if we selected items from the queue in random order then we can expect that the tracks to be visited will occur randomly giving poor performance this random scheduling is useful as a benchmark against which to evaluate other techniques figure compares the performance of various scheduling algorithms for an example sequence of io requests the vertical axis corresponds to the tracks on the disk the horizontal axis corresponds to time or equivalently the number of tracks traversed for this figure we assume that the disk head is initially located at track in this example we assume a disk with tracks and that the disk request queue has random requests in it the requested tracks in the order received by the disk scheduler are table a tabulates the results first in first out the simplest form of scheduling is firstinfirstout fifo scheduling which processes items from the queue in sequential order this strategy has the advantage of being fair because every request is honored and the requests are honored in the order received figure a illustrates the disk arm movement with fifo this graph is generated directly from the data in table a as can be seen the disk accesses are in the same order as the requests were originally received with fifo if there are only a few processes that require access and if many of the requests are to clustered file sectors then we can hope for good performance however this technique will often approximate random scheduling in performance if there are many processes competing for the disk thus it may be profitable to consider a more sophisticated scheduling policy a number of these are listed in table and will now be considered priority with a system based on priority pri the control of the scheduling is outside the control of disk management software such an approach is not intended to optimize disk utilization but to meet other objectives within the os often short batch jobs and interactive jobs are given higher priority than longer jobs that require longer computation this allows a lot of short jobs to be flushed through the system quickly and may provide good interactive response time however longer jobs may have to wait excessively long times furthermore such a policy could lead to countermeasures on the part of users who split their jobs into smaller pieces to beat the system this type of policy tends to be poor for database systems shortest service time first the shortestservicetimefirst sstf policy is to select the disk io request that requires the least movement of the disk arm from its current position thus we always choose to incur the minimum seek time of course always choosing the minimum seek time does not guarantee that the track number time a fifo track number time b sstf track number time c scan track number time d cscan figure comparison of disk scheduling algorithms see table table comparison of disk scheduling algorithms c scan starting d cscan starting at track in the at track in the a fifo starting b sstf starting direction of increasing direction of increasing at track at track track number track number number number number next number next track of tracks next track of tracks next track of tracks track of tracks accessed traversed accessed traversed accessed traversed accessed traversed average average average average seek seek seek seek length length length length average seek time over a number of arm movements will be minimum however this should provide better performance than fifo because the arm can move in two directions a random tiebreaking algorithm may be used to resolve cases of equal distances table disk scheduling algorithms name description remarks selection according to requestor rss random scheduling for analysis and simulation fifo firstinfirstout fairest of them all pri priority by process control outside of disk queue management lifo last in first out maximize locality and resource utilization selection according to requested item sstf shortestservicetime first high utilization small queues scan back and forth over disk better service distribution cscan one way with fast return lower service variability nstepscan scan of n records at a time service guarantee fscan nstepscan with n queue size load sensitive at beginning of scan cycle figure b and table b show the performance of sstf on the same example as was used for fifo the first track accessed is because this is the closest requested track to the starting position the next track accessed is because this is the closest of the remaining requested tracks to the current position of subsequent tracks are selected accordingly scan with the exception of fifo all of the policies described so far can leave some request unfulfilled until the entire queue is emptied that is there may always be new requests arriving that will be chosen before an existing request a simple alternative that prevents this sort of starvation is the scan algorithm also known as the elevator algorithm because it operates much the way an elevator does with scan the arm is required to move in one direction only satisfying all outstanding requests en route until it reaches the last track in that direction or until there are no more requests in that direction this latter refinement is sometimes referred to as the look policy the service direction is then reversed and the scan proceeds in the opposite direction again picking up all requests in order figure c and table c illustrate the scan policy assuming that the initial direction is of increasing track number then the first track selected is since this is the closest track to the starting track of in the increasing direction as can be seen the scan policy behaves almost identically with the sstf policy indeed if we had assumed that the arm was moving in the direction of lower track numbers at the beginning of the example then the scheduling pattern would have been identical for sstf and scan however this is a static example in which no new items are added to the queue even when the queue is dynamically changing scan will be similar to sstf unless the request pattern is unusual note that the scan policy is biased against the area most recently traversed thus it does not exploit locality as well as sstf it is not difficult to see that the scan policy favors jobs whose requests are for tracks nearest to both innermost and outermost tracks and favors the latestarriving jobs the first problem can be avoided via the cscan policy while the second problem is addressed by the nstepscan policy cscan the cscan circular scan policy restricts scanning to one direction only thus when the last track has been visited in one direction the arm is returned to the opposite end of the disk and the scan begins again this reduces the maximum delay experienced by new requests with scan if the expected time for a scan from inner track to outer track is t then the expected service interval for sectors at the periphery is t with cscan the interval is on the order of t s max where smax is the maximum seek time figure d and table d illustrate cscan behavior in this case the first three requested tracks encountered are and then the scan begins starting at the lowest track number and the next requested track encountered is nstepscan and fscan with sstf scan and cscan it is possible that the arm may not move for a considerable period of time for example if one or a few processes have high access rates to one track they can monopolize the entire device by repeated requests to that track highdensity multisurface disks are more likely to be affected by this characteristic than lowerdensity disks andor disks with only one or two surfaces to avoid this arm stickiness the disk request queue can be segmented with one segment at a time being processed completely two examples of this approach are nstepscan and fscan the nstepscan policy segments the disk request queue into subqueues of length n subqueues are processed one at a time using scan while a queue is being processed new requests must be added to some other queue if fewer than n requests are available at the end of a scan then all of them are processed with the next scan with large values of n the performance of nstepscan approaches that of scan with a value of n the fifo policy is adopted fscan is a policy that uses two subqueues when a scan begins all of the requests are in one of the queues with the other empty during the scan all new requests are put into the other queue thus service of new requests is deferred until all of the old requests have been processed raid as discussed earlier the rate in improvement in secondary storage performance has been considerably less than the rate for processors and main memory this mismatch has made the disk storage system perhaps the main focus of concern in improving overall computer system performance as in other areas of computer performance disk storage designers recognize that if one component can only be pushed so far additional gains in performance are to be had by using multiple parallel components in the case of disk storage this leads to the development of arrays of disks that operate independently and in parallel with multiple disks separate io requests can be handled in parallel as long as the data required reside on separate disks further a single io request can be executed in parallel if the block of data to be accessed is distributed across multiple disks with the use of multiple disks there is a wide variety of ways in which the data can be organized and in which redundancy can be added to improve reliability this could make it difficult to develop database schemes that are usable on a number of platforms and operating systems fortunately industry has agreed on a standardized scheme for multipledisk database design known as disk cache in section and appendix a we summarized the principles of cache memory the term cache memory is usually used to apply to a memory that is smaller and faster than main memory and that is interposed between main memory and the processor such a cache memory reduces average memory access time by exploiting the principle of locality the same principle can be applied to disk memory specifically a disk cache is a buffer in main memory for disk sectors the cache contains a copy of some of the sectors on the disk when an io request is made for a particular sector a check is made to determine if the sector is in the disk cache if so the request is satisfied via the cache if not the requested sector is read into the disk cache from the disk because of the phenomenon of locality of reference when a block of data is fetched into the cache to satisfy a single io request it is likely that there will be future references to that same block design considerations several design issues are of interest first when an io request is satisfied from the disk cache the data in the disk cache must be delivered to the requesting process this can be done either by transferring the block of data within main memory from the disk cache to memory assigned to the user process or simply by using a shared memory capability and passing a pointer to the appropriate slot in the disk cache the latter approach saves the time of a memorytomemory transfer and also allows shared access by other processes using the readerswriters model described in chapter a second design issue has to do with the replacement strategy when a new sector is brought into the disk cache one of the existing blocks must be replaced this is the identical problem presented in chapter there the requirement was for a page replacement algorithm a number of algorithms have been tried the most commonly used algorithm is least recently used lru replace that block that has been in the cache longest with no reference to it logically the cache consists of a stack of blocks with the most recently referenced block on the top of the stack when a block in the cache is referenced it is moved from its existing position on the stack to the top of the stack when a block is brought in from secondary memory remove the block that is on the bottom of the stack and push the incoming block onto the top of the stack naturally it is not necessary actually to move these blocks around in main memory a stack of pointers can be associated with the cache another possibility is least frequently used lfu replace that block in the set that has experienced the fewest references lfu could be implemented by associating a counter with each block when a block is brought in it is assigned a count of with each reference to the block its count is incremented by when replacement is required the block with the smallest count is selected intuitively it might seem that lfu is more appropriate than lru because lfu makes use of more pertinent information about each block in the selection process a simple lfu algorithm has the following problem it may be that certain blocks are referenced relatively infrequently overall but when they are referenced there are short intervals of repeated references due to locality thus building up high reference counts after such an interval is over the reference count may be misleading and not reflect the probability that the block will soon be referenced again thus the effect of locality may actually cause the lfu algorithm to make poor replacement choices to overcome this difficulty with lfu a technique known as frequencybased replacement is proposed in robi for clarity let us first consider a simplified version illustrated in figure a the blocks are logically organized in a stack as with the lru algorithm a certain portion of the top part of the stack is designated the new section when there is a cache hit the referenced block is moved to the top of the stack if the block was already in the new section its reference count is not incremented otherwise it is incremented by given a sufficiently large new section this results in the reference counts for blocks that are repeatedly rereferenced within a short interval remaining unchanged on a miss the block with the smallest reference count that is not in the new section is chosen for replacement the least recently used such block is chosen in the event of a tie the authors report that this strategy achieved only slight improvement over lru the problem is the following on a cache miss a new block is brought into the new section with a count of the count remains at as long as the block remains in the new section eventually the block ages out of the new section with its count still at if the block is not now rereferenced fairly quickly it is very likely to be replaced because it necessarily has the smallest reference count of those blocks that are not in the new section in other words there does not seem to be a sufficiently long interval for blocks aging out of the new section to build up their reference counts even if they were relatively frequently referenced a further refinement addresses this problem divide the stack into three sections new middle and old figure b as before reference counts are not incremented on blocks in the new section however only blocks in the old section are eligible for replacement assuming a sufficiently large middle section this allows new section old section mru lru rereference count unchanged rereference count count miss new block brought in count a fifo new section middle section old section mru lru b use of three sections figure frequencybased replacement relatively frequently referenced blocks a chance to build up their reference counts before becoming eligible for replacement simulation studies by the authors indicate that this refined policy is significantly better than simple lru or lfu regardless of the particular replacement strategy the replacement can take place on demand or preplanned in the former case a sector is replaced only when the slot is needed in the latter case a number of slots are released at a time the reason for this latter approach is related to the need to write back sectors if a sector is brought into the cache and only read then when it is replaced it is not necessary to write it back out to the disk however if the sector has been updated then it is necessary to write it back out before replacing it in this latter case it makes sense to cluster the writing and to order the writing to minimize seek time performance considerations the same performance considerations discussed in appendix a apply here the issue of cache performance reduces itself to a question of whether a given miss ratio can be achieved this will depend on the locality behavior of the disk references the replacement algorithm and other design factors principally however the miss ratio is a function of the size of the disk cache figure summarizes results from several studies using lru one for a unix system running on a vax oust and one for ibm mainframe operating systems smit figure shows results for simulation studies of the frequencybased replacement algorithm a comparison of the two figures points out one of the risks of this sort of performance assessment vax unix disk cache miss rate ibm mvs ibm svs cache size megabytes figure some disk cache performance results using lru disk cache miss rate ibm vm ibm mvs vax unix cache size megabytes figure disk cache performance using frequencybased replacement the figures appear to show that lru outperforms the frequencybased replacement algorithm however when identical reference patterns using the same cache structure are compared the frequencybased replacement algorithm is superior thus the exact sequence of reference patterns plus related design issues such as block size will have a profound influence on the performance achieved unix svr io in unix each individual io device is associated with a special file these are managed by the file system and are read and written in the same manner as user data files this provides a clean uniform interface to users and processes to read from or write to a device read and write requests are made for the special file associated with the device figure illustrates the logical structure of the io facility the file subsystem manages files on secondary storage devices in addition it serves as the process interface to devices because these are treated as files there are two types of io in unix buffered and unbuffered buffered io passes through system buffers whereas unbuffered io typically involves the dma file subsystem buffer cache character block device drivers figure unix io structure facility with the transfer taking place directly between the io module and the process io area for buffered io two types of buffers are used system buffer caches and character queues buffer cache the buffer cache in unix is essentially a disk cache io operations with disk are handled through the buffer cache the data transfer between the buffer cache and the user process space always occurs using dma because both the buffer cache and the process io area are in main memory the dma facility is used in this case to perform a memorytomemory copy this does not use up any processor cycles but it does consume bus cycles to manage the buffer cache three lists are maintained free list list of all slots in the cache a slot is referred to as a buffer in unix each slot holds one disk sector that are available for allocation device list list of all buffers currently associated with each disk driver io queue list of buffers that are actually undergoing or waiting for io on a particular device all buffers should be on the free list or on the driver io queue list a buffer once associated with a device remains associated with the device even if it is on the free list until is actually reused and becomes associated with another device these lists are maintained as pointers associated with each buffer rather than physically separate lists when a reference is made to a physical block number on a particular device the os first checks to see if the block is in the buffer cache to minimize the search time the device list is organized as a hash table using a technique similar to the overflow with chaining technique discussed in appendix f figure fb figure depicts the general organization of the buffer cache there is a hash table of fixed length that contains pointers into the buffer cache each reference to a device block maps into a particular entry in the hash table the pointer in that entry points to the first buffer in the chain a hash pointer associated with each buffer points to the next buffer in the chain for that hash table entry thus for all device block hash table buffer cache free list pointers hash pointers device block free list pointer figure unix buffer cache organization references that map into the same hash table entry if the corresponding block is in the buffer cache then that buffer will be in the chain for that hash table entry thus the length of the search of the buffer cache is reduced by a factor on the order of n where n is the length of the hash table for block replacement a leastrecentlyused algorithm is used after a buffer has been allocated to a disk block it can not be used for another block until all other buffers have been used more recently the free list preserves this leastrecentlyused order character queue blockoriented devices such as disk and usb keys can be effectively served by the buffer cache a different form of buffering is appropriate for characteroriented devices such as terminals and printers a character queue is either written by the io device and read by the process or written by the process and read by the device in both cases the producerconsumer model introduced in chapter is used thus character queues may only be read once as each character is read it is effectively destroyed this is in contrast to the buffer cache which may be read multiple times and hence follows the readerswriters model also discussed in chapter unbuffered io unbuffered io which is simply dma between device and process space is always the fastest method for a process to perform io a process that is table device io in unix unbuffered io buffer cache character queue disk drive x x tape drive x x terminals x communication lines x printers x x performing unbuffered io is locked in main memory and can not be swapped out this reduces the opportunities for swapping by tying up part of main memory thus reducing the overall system performance also the io device is tied up with the process for the duration of the transfer making it unavailable for other processes unix devices among the categories of devices recognized by unix are the following disk drives tape drives terminals communication lines printers table shows the types of io suited to each type of device disk drives are heavily used in unix are block oriented and have the potential for reasonable high throughput thus io for these devices tends to be unbuffered or via buffer cache tape drives are functionally similar to disk drives and use similar io schemes because terminals involve relatively slow exchange of characters terminal io typically makes use of the character queue similarly communication lines require serial processing of bytes of data for input or output and are best handled by character queues finally the type of io used for a printer will generally depend on its speed slow printers will normally use the character queue while a fast printer might employ unbuffered io a buffer cache could be used for a fast printer however because data going to a printer are never reused the overhead of the buffer cache is unnecessary direct memory access three techniques are possible for io operations programmed io interruptdriven io and direct memory access dma before discussing dma we briefly define the other two techniques see appendix c for more detail when the processor is executing a program and encounters an instruction relating to io it executes that instruction by issuing a command to the appropriate io module in the case of programmed io the io module performs the requested action and then sets the appropriate bits in the io status register but takes no further action to alert the processor in particular it does not interrupt the processor thus after the io instruction is invoked the processor must take some active role in determining when the io instruction is completed for this purpose the processor periodically checks the status of the io module until it finds that the operation is complete with programmed io the processor has to wait a long time for the io module of concern to be ready for either reception or transmission of more data the processor while waiting must repeatedly interrogate the status of the io module as a result the performance level of the entire system is severely degraded an alternative known as interruptdriven io is for the processor to issue an io command to a module and then go on to do some other useful work the io module will then interrupt the processor to request service when it is ready to exchange data with the processor the processor then executes the data transfer as before and then resumes its former processing interruptdriven io though more efficient than simple programmed io still requires the active intervention of the processor to transfer data between memory and an io module and any data transfer must traverse a path through the processor thus both of these forms of io suffer from two inherent drawbacks the io transfer rate is limited by the speed with which the processor can test and service a device the processor is tied up in managing an io transfer a number of instructions must be executed for each io transfer when large volumes of data are to be moved a more efficient technique is required direct memory access dma the dma function can be performed by a separate module on the system bus or it can be incorporated into an io module in either case the technique works as follows when the processor wishes to read or write a block of data it issues a command to the dma module by sending to the dma module the following information whether a read or write is requested the address of the io device involved the starting location in memory to read data from or write data to the number of words to be read or written the processor then continues with other work it has delegated this io operation to the dma module and that module will take care of it the dma module transfers the entire block of data one word at a time directly to or from memory without going through the processor when the transfer is complete the dma module sends an interrupt signal to the processor thus the processor is involved only at the beginning and end of the transfer the dma module needs to take control of the bus to transfer data to and from memory because of this competition for bus usage there may be times when the processor needs the bus and must wait for the dma module note that this is not an interrupt the processor does not save a context and do something else rather the processor pauses for one bus cycle the time it takes to transfer one word across the bus the overall effect is to cause the processor to execute more slowly during a dma transfer when processor access to the bus is required nevertheless for a multipleword io transfer dma is far more efficient than interruptdriven or programmed io linux io in general terms the linux io kernel facility is very similar to that of other unix implementation such as svr the linux kernel associates a special file with each io device driver block character and network devices are recognized in this section we look at several features of the linux io facility disk scheduling the default disk scheduler in linux is known as the linux elevator which is a variation on the look algorithm discussed in section for linux the elevator algorithm has been augmented by two additional algorithms the deadline io scheduler and the anticipatory io scheduler love we examine each of these in turn the elevator scheduler the elevator scheduler maintains a single queue for disk read and write requests and performs both sorting and merging functions on the queue in general terms the elevator scheduler keeps the list of requests sorted by block number thus as the disk requests are handled the drive moves in a single direction satisfying each request as it is encountered this general strategy is refined in the following manner when a new request is added to the queue four operations are considered in order if the request is to the same ondisk sector or an immediately adjacent sector to a pending request in the queue then the existing request and the new request are merged into one request if a request in the queue is sufficiently old the new request is inserted at the tail of the queue if there is a suitable location the new request is inserted in sorted order if there is no suitable location the new request is placed at the tail of the queue deadline scheduler operation in the preceding list is intended to prevent starvation of a request but is not very effective love it does not attempt to service requests in a given time frame but merely stops insertionsorting requests after a suitable delay two problems manifest themselves with the elevator scheme the first problem is that a distant block request can be delayed for a substantial time because the queue is dynamically updated for example consider the following stream of requests for disk blocks the elevator scheduler reorders these so that the requests are placed in the queue as with being the head of the queue if a continuous sequence of lownumbered block requests arrive then the request for continues to be delayed an even more serious problem concerns the distinction between read and write requests typically a write request is issued asynchronously that is once a process issues the write request it need not wait for the request to actually be satisfied when an application issues a write the kernel copies the data into an appropriate buffer to be written out as time permits once the data are captured in the kernels buffer the application can proceed however for many read operations the process must wait until the requested data are delivered to the application before proceeding thus a stream of write requests eg to place a large file on the disk can block a read request for a considerable time and thus block a process to overcome these problems the deadline io scheduler makes use of three queues figure each incoming request is placed in the sorted sorted elevator queue read fifo queue write fifo queue figure the linux deadline io scheduler elevator queue as before in addition the same request is placed at the tail of a read fifo queue for a read request or a write fifo queue for a write request thus the read and write queues maintain a list of requests in the sequence in which the requests were made associated with each request is an expiration time with a default value of seconds for a read request and seconds for a write request ordinarily the scheduler dispatches from the sorted queue when a request is satisfied it is removed from the head of the sorted queue and also from the appropriate fifo queue however when the item at the head of one of the fifo queues becomes older than its expiration time then the scheduler next dispatches from that fifo queue taking the expired request plus the next few requests from the queue as each request is dispatched it is also removed from the sorted queue the deadline io scheduler scheme overcomes the starvation problem and also the read versus write problem anticipatory io scheduler the original elevator scheduler and the deadline scheduler both are designed to dispatch a new request as soon as the existing request is satisfied thus keeping the disk as busy as possible this same policy applies to all of the scheduling algorithms discussed in section however such a policy can be counterproductive if there are numerous synchronous read requests typically an application will wait until a read request is satisfied and the data available before issuing the next request the small delay between receiving the data for the last read and issuing the next read enables the scheduler to turn elsewhere for a pending request and dispatch that request because of the principle of locality it is likely that successive reads from the same process will be to disk blocks that are near one another if the scheduler were to delay a short period of time after satisfying a read request to see if a new nearby read request is made the overall performance of the system could be enhanced this is the philosophy behind the anticipatory scheduler proposed in iyer and implemented in linux in linux the anticipatory scheduler is superimposed on the deadline scheduler when a read request is dispatched the anticipatory scheduler causes the scheduling system to delay for up to ms depending on the configuration during this small delay there is a good chance that the application that issued the last read request will issue another read request to the same region of the disk if so that request will be serviced immediately if no such read request occurs the scheduler resumes using the deadline scheduling algorithm love reports on two tests of the linux scheduling algorithms the first test involved the reading of a mb file while doing a long streaming write in the background the second test involved doing a read of a large file in the background while reading every file in the kernel source tree the results are listed in the following table io scheduler and kernel test test linux elevator on seconds minutes seconds deadline io scheduler on seconds minutes seconds anticipatory io scheduler on seconds seconds as can be seen the performance improvement depends on the nature of the workload but in both cases the anticipatory scheduler provides a dramatic improvement linux page cache in linux and earlier releases the kernel maintained a page cache for reads and writes from regular file system files and for virtual memory pages and a separate buffer cache for block io for linux and later there is a single unified page cache that is involved in all traffic between disk and main memory the page cache confers two benefits first when it is time to write back dirty pages to disk a collection of them can be ordered properly and written out efficiently second because of the principle of temporal locality pages in the page cache are likely to be referenced again before they are flushed from the cache thus saving a disk io operation dirty pages are written back to disk in two situations when free memory falls below a specified threshold the kernel reduces the size of the page cache to release memory to be added to the free memory pool when dirty pages grow older than a specified threshold a number of dirty pages are written back to disk windows io figure shows the key kernelmode components related to the windows io manager the io manager is responsible for all io for the operating system and provides a uniform interface that all types of drivers can call io manager cache manager file system drivers network drivers hardware device drivers figure windows io manager basic io facilities the io manager works closely with four types of kernel components cache manager the cache manager handles file caching for all file systems it can dynamically increase and decrease the size of the cache devoted to a particular file as the amount of available physical memory varies the system records updates in the cache only and not on disk a kernel thread the lazy writer periodically batches the updates together to write to disk writing the updates in batches allows the io to be more efficient the cache manager works by mapping regions of files into kernel virtual memory and then relying on the virtual memory manager to do most of the work to copy pages to and from the files on disk file system drivers the io manager treats a file system driver as just another device driver and routes io requests for file system volumes to the appropriate software driver for that volume the file system in turn sends io requests to the software drivers that manage the hardware device adapter network drivers windows includes integrated networking capabilities and support for remote file systems the facilities are implemented as software drivers rather than part of the windows executive hardware device drivers these software drivers access the hardware registers of the peripheral devices using entry points in the hardware abstraction layer a set of these routines exists for every platform that windows supports because the routine names are the same for all platforms the source code of windows device drivers is portable across different processor types asynchronous and synchronous io windows offers two modes of io operation asynchronous and synchronous the asynchronous mode is used whenever possible to optimize application performance with asynchronous io an application initiates an io operation and then can continue processing while the io request is fulfilled with synchronous io the application is blocked until the io operation completes asynchronous io is more efficient from the point of view of the calling thread because it allows the thread to continue execution while the io operation is queued by the io manager and subsequently performed however the application that invoked the asynchronous io operation needs some way to determine when the operation is complete windows provides five different techniques for signaling io completion signaling the file object with this approach the event associated with a file object is set when an operation on that object is complete the thread that invoked the io operation can continue to execute until it reaches a point where it must stop until the io operation is complete at that point the thread can wait until the operation is complete and then continue this technique is simple and easy to use but is not appropriate for handling multiple io requests for example if a thread needs to perform multiple simultaneous actions on a single file such as reading from one portion and writing to another portion of the file with this technique the thread could not distinguish between the completion of the read and the completion of the write it would simply know that one of the requested io operations on this file had finished signaling an event object this technique allows multiple simultaneous io requests against a single device or file the thread creates an event for each request later the thread can wait on a single one of these requests or on an entire collection of requests asynchronous procedure call this technique makes use of a queue associated with a thread known as the asynchronous procedure call apc queue in this case the thread makes io requests specifying a usermode routine to call when the io completes the io manager places the results of each request in the calling threads apc queue the next time the thread blocks in the kernel the apcs will be delivered each causing the thread to return to user mode and execute the specified routine io completion ports this technique is used on a windows server to optimize the use of threads the application creates a pool of threads for handling the completion of io requests each thread waits on the completion port and the kernel wakes threads to handle each io completion one of the advantages of this approach is that the application can specify a limit for how many of these threads will run at the same time polling asynchronous io requests write a status and transfer count into the process user virtual memory when the operation completes a thread can just check these values to see if the operation has completed software raid windows supports two sorts of raid configurations defined in ms as follows hardware raid separate physical disks combined into one or more logical disks by the disk controller or disk storage cabinet hardware software raid noncontiguous disk space combined into one or more logical partitions by the faulttolerant software disk driver ftdisk in hardware raid the controller interface handles the creation and regeneration of redundant information the software raid available on windows server implements the raid functionality as part of the operating system and can be used with any set of multiple disks the software raid facility implements raid and raid in the case of raid disk mirroring the two disks containing the primary and mirrored partitions may be on the same disk controller or different disk controllers the latter configuration is referred to as disk duplexing volume shadow copies shadow copies are an efficient way of making consistent snapshots of volumes so that they can be backed up they are also useful for archiving files on a pervolume basis if a user deletes a file he or she can retrieve an earlier copy from any available shadow copy made by the system administrator shadow copies are implemented by a software driver that makes copies of data on the volume before it is overwritten volume encryption windows supports the encryption of entire volumes using a feature called bitlocker this is more secure than encrypting individual files as the entire system works to be sure that the data is safe up to three different methods of supplying the cryptographic key can be provided allowing multiple interlocking layers of security summary the computer systems interface to the outside world is its io architecture this architecture is designed to provide a systematic means of controlling interaction with the outside world and to provide the operating system with the information it needs to manage io activity effectively the io function is generally broken up into a number of layers with lower layers dealing with details that are closer to the physical functions to be performed and higher layers dealing with io in a logical and generic fashion the result is that changes in hardware parameters need not affect most of the io software a key aspect of io is the use of buffers that are controlled by io utilities rather than by application processes buffering smoothes out the differences between the internal speeds of the computer system and the speeds of io devices the use of buffers also decouples the actual io transfer from the address space of the application process this allows the operating system more flexibility in performing its memorymanagement function the aspect of io that has the greatest impact on overall system performance is disk io accordingly there has been greater research and design effort in this area than in any other kind of io two of the most widely used approaches to improve disk io performance are disk scheduling and the disk cache at any time there may be a queue of requests for io on the same disk it is the object of disk scheduling to satisfy these requests in a way that minimizes the mechanical seek time of the disk and hence improves performance the physical layout of pending requests plus considerations of locality come into play a disk cache is a buffer usually kept in main memory that functions as a cache of disk blocks between disk memory and the rest of main memory because of the principle of locality the use of a disk cache should substantially reduce the number of block io transfers between main memory and disk recommended reading general discussions of computer io can be found in most books on computer architecture such as stal meea provides a good survey of the underlying recording technology of disk and tape systems meeb focuses on the data storage techniques for disk and tape systems wied contains an excellent discussion of disk performance issues including those relating to disk scheduling ng looks at disk hardware performance issues cao analyzes disk caching and disk scheduling good surveys of disk scheduling algorithms with a performance analysis are wort and selt pai is an instructive description of an integrated operatingsystem scheme for io buffering and caching dell provides a detailed discussion of windows nt device drivers plus a good overview of the entire windows io architecture an excellent survey of raid technology written by the inventors of the raid concept is chen chen analyzes raid performance another good paper is frie dalt describes the windows nt software raid facility in detail leve examines the need to move beyond raid to a tripleparity configuration stai is a good survey of the standard raid levels plus a number of common raid enhancements cao cao p felten e karlin a and li k implementation and performance of integrated applicationcontrolled file caching prefetching and disk scheduling acm transactions on computer systems november chen chen p lee e gibson g katz r and patterson d raid highperformance reliable secondary storage acm computing surveys june chen chen s and towsley d a performance evaluation of raid architectures ieee transactions on computers october dalt dalton w et al windows nt server security troubleshooting and optimization indianapolis in new riders publishing dell dekker e and newcomer j developing windows nt device drivers a programmers handbook reading ma addison wesley frie friedman m raid keeps going and going and ieee spectrum april leve leventhal a tripleparity raid and beyond communications of the acm january meea mee c and daniel e eds magnetic recording technology new york mcgraw hill meeb mee c and daniel e eds magnetic storage handbook new york mcgraw hill ng ng s advances in disk technology performance issues computer may pai pai v druschel p and zwaenepoel w iolite a unified io buffering and caching system acm transactions on computer systems february selt seltzer m chen p and ousterhout j disk scheduling revisited proceedings usenix winter technical conference january stai staimer m alternatives to raid storage magazine may stal stallings w computer organization and architecture th ed upper saddle river nj prentice hall wied wiederhold g file organization for database design new york mcgrawhill wort worthington b ganger g and patt y scheduling algorithms for modern disk drives acm sigmetrics may key terms review questions and problems key terms block inputoutput io redundant array of blockoriented device io buffer independent disks circular buffer io channel removable disk device io io processor rotational delay direct memory access logical io sector disk access time magnetic disk seek time disk cache nonremovable disk streamoriented device gap programmed io track hard disk readwrite head transfer time interruptdriven io review questions list and briefly define three techniques for performing io what is the difference between logical io and device io what is the difference between blockoriented devices and streamoriented devices give a few examples of each why would you expect improved performance using a double buffer rather than a single buffer for io what delay elements are involved in a disk read or write briefly define the disk scheduling policies illustrated in figure briefly define the seven raid levels what is the typical disk sector size problems consider a program that accesses a single io device and compare unbuffered io to the use of a buffer show that the use of the buffer can reduce the running time by at most a factor of two generalize the result of problem to the case in which a program refers to n devices a perform the same type of analysis as that of table for the following sequence of disk track requests assume that the disk head is initially positioned over track and is moving in the direction of decreasing track number b do the same analysis but now assume that the disk head is moving in the direction of increasing track number consider a disk with n tracks numbered from to n and assume that requested sectors are distributed randomly and evenly over the disk we want to calculate the average number of tracks traversed by a seek a calculate the probability of a seek of length j when the head is currently positioned over track t hint this is a matter of determining the total number of combinations recognizing that all track positions for the destination of the seek are equally likely b calculate the probability of a seek of length k for an arbitrary current position of the head hint this involves the summing over all possible combinations of movements of k tracks c calculate the average number of tracks traversed by a seek using the formula for expected value n ex ai pr x i i n nn n nn n hint use the equalities a a i i i d show that for large values of n the average number of tracks traversed by a seek approaches n the following equation was suggested both for cache memory and disk cache memory ts tc m td generalize this equation to a memory hierarchy with n levels instead of just for the frequencybased replacement algorithm figure define fnew fmiddle and fold as the fraction of the cache that comprises the new middle and old sections respectively clearly fnew fmiddle fold characterize the policy when a fold fnew b fold cache size calculate how much disk space in sectors tracks and surfaces will be required to store byte logical records if the disk is fixed sector with bytes sector with sectorstrack tracks per surface and usable surfaces ignore any file header records and track indexes and assume that records can not span two sectors consider the disk system described in problem and assume that the disk rotates at rpm a processor reads one sector from the disk using interruptdriven io with one interrupt per byte if it takes s to process each interrupt what percentage of the time will the processor spend handling io disregard seek time repeat the preceding problem using dma and assume one interrupt per sector a bit computer has two selector channels and one multiplexor channel each selector channel supports two magnetic disk and two magnetic tape units the multiplexor channel has two line printers two card readers and ten vdt terminals connected to it assume the following transfer rates disk drive kbytess magnetic tape drive kbytess line printer kbytess card reader kbytess vdt kbytes estimate the maximum aggregate io transfer rate in this system it should be clear that disk striping can improve the data transfer rate when the strip size is small compared to the io request size it should also be clear that raid provides improved performance relative to a single large disk because multiple io requests can be handled in parallel however in this latter case is disk striping necessary that is does disk striping improve io request rate performance compared to a comparable disk array without striping consider a drive gbperdrive raid array what is the available data storage capacity for each of the raid levels and file management overview files and file systems file structure file management systems file organization and access the pile the sequential file the indexed sequential file the indexed file the direct or hashed file btrees file directories contents structure naming file sharing access rights simultaneous access record blocking secondary storage management file allocation free space management volumes reliability file system security unix file management linux virtual file system windows file system summary recommended reading key terms review questions and problems if there is one singular characteristic that makes squirrels unique among small mammals it is their natural instinct to hoard food squirrels have developed sophisticated capabilities in their hoarding different types of food are stored in different ways to maintain quality mushrooms for instance are usually dried before storing this is done by impaling them on branches or leaving them in the forks of trees for later retrieval pine cones on the other hand are often harvested while green and cached in damp conditions that keep seeds from ripening gray squirrels usually strip outer husks from walnuts before storing squirrels a wildlife handbook kim long learning objectives after studying this chapter you should be able to describe the basic concepts of files and file systems understand the principal techniques for file organization and access define btrees explain file directories understand the requirements for file sharing understand the concept of record blocking describe the principal design issues for secondary storage management understand the design issues for file system security explain the os file systems used in linux unix and windows in most applications the file is the central element with the exception of realtime applications and some other specialized applications the input to the application is by means of a file and in virtually all applications output is saved in a file for longterm storage and for later access by the user and by other programs files have a life outside of any individual application that uses them for input andor output users wish to be able to access files save them and maintain the integrity of their contents to aid in these objectives virtually all operating systems provide file management systems typically a file management system consists of system utility programs that run as privileged applications however at the very least a file management system needs special services from the operating system at the most the entire file management system is considered part of the operating system thus it is appropriate to consider the basic elements of file management in this book we begin with an overview followed by a look at various file organization schemes although file organization is generally beyond the scope of the operating system it is essential to have a general understanding of the common alternatives to appreciate some of the design tradeoffs involved in file management the remainder of this chapter looks at other topics in file management overview files and file systems from the users point of view one of the most important parts of an operating system is the file system the file system provides the resource abstractions typically associated with secondary storage the file system permits users to create data collections called files with desirable properties such as longterm existence files are stored on disk or other secondary storage and do not disappear when a user logs off sharable between processes files have names and can have associated access permissions that permit controlled sharing structure depending on the file system a file can have an internal structure that is convenient for particular applications in addition files can be organized into hierarchical or more complex structure to reflect the relationships among files any file system provides not only a means to store data organized as files but a collection of functions that can be performed on files typical operations include the following create a new file is defined and positioned within the structure of files delete a file is removed from the file structure and destroyed open an existing file is declared to be opened by a process allowing the process to perform functions on the file close the file is closed with respect to a process so that the process no longer may perform functions on the file until the process opens the file again read a process reads all or a portion of the data in a file write a process updates a file either by adding new data that expands the size of the file or by changing the values of existing data items in the file typically a file system maintains a set of attributes associated with the file these include owner creation time time last modified access privileges and so on file structure four terms are in common use when discussing files field record file database a field is the basic element of data an individual field contains a single value such as an employees last name a date or the value of a sensor reading it is characterized by its length and data type eg ascii string decimal depending on the file design fields may be fixed length or variable length in the latter case the field often consists of two or three subfields the actual value to be stored the name of the field and in some cases the length of the field in other cases of variablelength fields the length of the field is indicated by the use of special demarcation symbols between fields a record is a collection of related fields that can be treated as a unit by some application program for example an employee record would contain such fields as name social security number job classification date of hire and so on again depending on design records may be of fixed length or variable length a record will be of variable length if some of its fields are of variable length or if the number of fields may vary in the latter case each field is usually accompanied by a field name in either case the entire record usually includes a length field a file is a collection of similar records the file is treated as a single entity by users and applications and may be referenced by name files have file names and may be created and deleted access control restrictions usually apply at the file level that is in a shared system users and programs are granted or denied access to entire files in some more sophisticated systems such controls are enforced at the record or even the field level some file systems are structured only in terms of fields not records in that case a file is a collection of fields a database is a collection of related data the essential aspects of a database are that the relationships that exist among elements of data are explicit and that the database is designed for use by a number of different applications a database may contain all of the information related to an organization or project such as a business or a scientific study the database itself consists of one or more types of files usually there is a separate database management system that is independent of the operating system although that system may make use of some file management programs users and applications wish to make use of files typical operations that must be supported include the following retrieveall retrieve all the records of a file this will be required for an application that must process all of the information in the file at one time for example an application that produces a summary of the information in the file would need to retrieve all records this operation is often equated with the term sequential processing because all of the records are accessed in sequence retrieveone this requires the retrieval of just a single record interactive transactionoriented applications need this operation retrievenext this requires the retrieval of the record that is next in some logical sequence to the most recently retrieved record some interactive applications such as filling in forms may require such an operation a program that is performing a search may also use this operation retrieveprevious similar to retrievenext but in this case the record that is previous to the currently accessed record is retrieved insertone insert a new record into the file it may be necessary that the new record fit into a particular position to preserve a sequencing of the file deleteone delete an existing record certain linkages or other data structures may need to be updated to preserve the sequencing of the file updateone retrieve a record update one or more of its fields and rewrite the updated record back into the file again it may be necessary to preserve sequencing with this operation if the length of the record has changed the update operation is generally more difficult than if the length is preserved retrievefew retrieve a number of records for example an application or user may wish to retrieve all records that satisfy a certain set of criteria the nature of the operations that are most commonly performed on a file will influence the way the file is organized as discussed in section it should be noted that not all file systems exhibit the sort of structure discussed in this subsection on unix and unixlike systems the basic file structure is just a stream of bytes for example a c program is stored as a file but does not have physical fields records and so on file management systems a file management system is that set of system software that provides services to users and applications in the use of files typically the only way that a user or application may access files is through the file management system this relieves the user or programmer of the necessity of developing specialpurpose software for each application and provides the system with a consistent welldefined means of controlling its most important asset gros suggests the following objectives for a file management system to meet the data management needs and requirements of the user which include storage of data and the ability to perform the aforementioned operations to guarantee to the extent possible that the data in the file are valid to optimize performance both from the system point of view in terms of overall throughput and from the users point of view in terms of response time to provide io support for a variety of storage device types to minimize or eliminate the potential for lost or destroyed data to provide a standardized set of io interface routines to user processes to provide io support for multiple users in the case of multipleuser systems with respect to the first point meeting user requirements the extent of such requirements depends on the variety of applications and the environment in which the computer system will be used for an interactive generalpurpose system the following constitute a minimal set of requirements each user should be able to create delete read write and modify files each user may have controlled access to other users files each user may control what types of accesses are allowed to the users files each user should be able to restructure the users files in a form appropriate to the problem each user should be able to move data between files each user should be able to back up and recover the users files in case of damage each user should be able to access his or her files by name rather than by numeric identifier these objectives and requirements should be kept in mind throughout our discussion of file management systems file system architecture one way of getting a feel for the scope of file management is to look at a depiction of a typical software organization as suggested in figure of course different systems will be organized differently but this organization is reasonably representative at the lowest level device drivers communicate directly with peripheral devices or their controllers or channels a device driver is responsible for starting io operations on a device and processing the completion of an io request for file operations the typical devices controlled are disk and tape drives device drivers are usually considered to be part of the operating system the next level is referred to as the basic file system or the physical io level this is the primary interface with the environment outside of the computer system it deals with blocks of data that are exchanged with disk or tape systems thus it is concerned with the placement of those blocks on the secondary storage device and on the buffering of those blocks in main memory it does not understand the content of the data or the structure of the files involved the basic file system is often considered part of the operating system the basic io supervisor is responsible for all file io initiation and termination at this level control structures are maintained that deal with device io scheduling and file status the basic io supervisor selects the device on which file io is to be performed based on the particular file selected it is also concerned with scheduling disk and tape accesses to optimize performance io buffers are user program pile sequential indexed indexed hashed sequential logical io basic io supervisor basic file system disk device driver tape device driver figure file system software architecture assigned and secondary memory is allocated at this level the basic io supervisor is part of the operating system logical io enables users and applications to access records thus whereas the basic file system deals with blocks of data the logical io module deals with file records logical io provides a generalpurpose record io capability and maintains basic data about files the level of the file system closest to the user is often termed the access method it provides a standard interface between applications and the file systems and devices that hold the data different access methods reflect different file structures and different ways of accessing and processing the data some of the most common access methods are shown in figure and these are briefly described in section file management functions another way of viewing the functions of a file system is shown in figure let us follow this diagram from left to right users and application programs interact with the file system by means of commands for creating and deleting files and for performing operations on files before performing any operation the file system must identify and locate the selected file this requires the use of some sort of directory that serves to describe the location of all files plus their attributes in addition most shared systems enforce user access control only authorized users are allowed to access particular files in particular ways the basic operations that a user or application may perform on a file are performed at the record level the user or application views the file as having some structure that organizes the records such as a sequential structure eg personnel records are stored alphabetically by last name thus to translate user commands into specific physical blocks physical blocks records in main memory in secondary file buffers storage disk directory structure access management method disk blocking scheduling user program commands operation file io free storage file name manipulation management functions file allocation user access control file management concerns operating system concerns figure elements of file management file manipulation commands the access method appropriate to this file structure must be employed whereas users and applications are concerned with records or fields io is done on a block basis thus the records or fields of a file must be organized as a sequence of blocks for output and unblocked after input to support block io of files several functions are needed the secondary storage must be managed this involves allocating files to free blocks on secondary storage and managing free storage so as to know what blocks are available for new files and growth in existing files in addition individual block io requests must be scheduled this issue was dealt with in chapter both disk scheduling and file allocation are concerned with optimizing performance as might be expected these functions therefore need to be considered together furthermore the optimization will depend on the structure of the files and the access patterns accordingly developing an optimum file management system from the point of view of performance is an exceedingly complicated task figure suggests a division between what might be considered the concerns of the file management system as a separate system utility and the concerns of the operating system with the point of intersection being record processing this division is arbitrary various approaches are taken in various systems in the remainder of this chapter we look at some of the design issues suggested in figure we begin with a discussion of file organizations and access methods although this topic is beyond the scope of what is usually considered the concerns of the operating system it is impossible to assess the other filerelated design issues without an appreciation of file organization and access next we look at the concept of file directories these are often managed by the operating system on behalf of the file management system the remaining topics deal with the physical io aspects of file management and are properly treated as aspects of os design one such issue is the way in which logical records are organized into physical blocks finally there are the related issues of file allocation on secondary storage and the management of free secondary storage file organization and access in this section we use the term file organization to refer to the logical structuring of the records as determined by the way in which they are accessed the physical organization of the file on secondary storage depends on the blocking strategy and the file allocation strategy issues dealt with later in this chapter in choosing a file organization several criteria are important short access time ease of update economy of storage simple maintenance reliability the relative priority of these criteria will depend on the applications that will use the file for example if a file is only to be processed in batch mode with all of the records accessed every time then rapid access for retrieval of a single record is of minimal concern a file stored on cdrom will never be updated and so ease of update is not an issue these criteria may conflict for example for economy of storage there should be minimum redundancy in the data on the other hand redundancy is a primary means of increasing the speed of access to data an example of this is the use of indexes the number of alternative file organizations that have been implemented or just proposed is unmanageably large even for a book devoted to file systems in this brief survey we will outline five fundamental organizations most structures used in actual systems either fall into one of these categories or can be implemented with a combination of these organizations the five organizations the first four of which are depicted in figure are as follows variablelength records fixedlength records variable set of fields fixed set of fields in fixed order chronological order sequential order based on key field a pile file b sequential file exhaustive exhaustive partial index index index n index main file levels index overflow file primary file c indexed sequential file variablelength records d indexed file figure common file organizations the pile the sequential file the indexed sequential file the indexed file the direct or hashed file table summarizes relative performance aspects of these five organizations the pile the leastcomplicated form of file organization may be termed the pile data are collected in the order in which they arrive each record consists of one burst of data the purpose of the pile is simply to accumulate the mass of data and save it records may have different fields or similar fields in different orders thus each field should be selfdescribing including a field name as well as a value the length of each field must be implicitly indicated by delimiters explicitly included as a subfield or known as default for that field type because there is no structure to the pile file record access is by exhaustive search that is if we wish to find a record that contains a particular field with a particular value it is necessary to examine each record in the pile until the desired table grades of performance for five basic file organizations wied space update retrieval attributes record size file single method variable fixed equal greater record subset exhaustive pile a b a e e d b sequential f a d f f d a indexed f b b d b d b sequential indexed b c c c a b d hashed f b b f b f e a excellent well suited to this purpose or b good oo r c adequate or log n d requires some extra effort on e possible with extreme effort or n f not reasonable for this purpose on where r size of the result o number of records that overflow n number of records in file the table employs the bigo notation used for characterizing the time complexity of algorithms appendix i explains this notation record is found or the entire file has been searched if we wish to find all records that contain a particular field or contain that field with a particular value then the entire file must be searched pile files are encountered when data are collected and stored prior to processing or when data are not easy to organize this type of file uses space well when the stored data vary in size and structure is perfectly adequate for exhaustive searches and is easy to update however beyond these limited uses this type of file is unsuitable for most applications the sequential file the most common form of file structure is the sequential file in this type of file a fixed format is used for records all records are of the same length consisting of the same number of fixedlength fields in a particular order because the length and position of each field are known only the values of fields need to be stored the field name and length for each field are attributes of the file structure one particular field usually the first field in each record is referred to as the key field the key field uniquely identifies the record thus key values for different records are always different further the records are stored in key sequence alphabetical order for a text key and numerical order for a numerical key sequential files are typically used in batch applications and are generally optimum for such applications if they involve the processing of all the records eg a billing or payroll application the sequential file organization is the only one that is easily stored on tape as well as disk for interactive applications that involve queries andor updates of individual records the sequential file provides poor performance access requires the sequential search of the file for a key match if the entire file or a large portion of the file can be brought into main memory at one time more efficient search techniques are possible nevertheless considerable processing and delay are encountered to access a record in a large sequential file additions to the file also present problems typically a sequential file is stored in simple sequential ordering of the records within blocks that is the physical organization of the file on tape or disk directly matches the logical organization of the file in this case the usual procedure is to place new records in a separate pile file called a log file or transaction file periodically a batch update is performed that merges the log file with the master file to produce a new file in correct key sequence an alternative is to organize the sequential file physically as a linked list one or more records are stored in each physical block each block on disk contains a pointer to the next block the insertion of new records involves pointer manipulation but does not require that the new records occupy a particular physical block position thus some added convenience is obtained at the cost of additional processing and overhead the indexed sequential file a popular approach to overcoming the disadvantages of the sequential file is the indexed sequential file the indexed sequential file maintains the key characteristic of the sequential file records are organized in sequence based on a key field two features are added an index to the file to support random access and an overflow file the index provides a lookup capability to reach quickly the vicinity of a desired record the overflow file is similar to the log file used with a sequential file but is integrated so that a record in the overflow file is located by following a pointer from its predecessor record in the simplest indexed sequential structure a single level of indexing is used the index in this case is a simple sequential file each record in the index file consists of two fields a key field which is the same as the key field in the main file and a pointer into the main file to find a specific field the index is searched to find the highest key value that is equal to or precedes the desired key value the search continues in the main file at the location indicated by the pointer to see the effectiveness of this approach consider a sequential file with million records to search for a particular key value will require on average onehalf million record accesses now suppose that an index containing entries is constructed with the keys in the index more or less evenly distributed over the main file now it will take on average accesses to the index file followed by accesses to the main file to find the record the average search length is reduced from to additions to the file are handled in the following manner each record in the main file contains an additional field not visible to the application which is a pointer to the overflow file when a new record is to be inserted into the file it is added to the overflow file the record in the main file that immediately precedes the new record in logical sequence is updated to contain a pointer to the new record in the overflow file if the immediately preceding record is itself in the overflow file then the pointer in that record is updated as with the sequential file the indexed sequential file is occasionally merged with the overflow file in batch mode the indexed sequential file greatly reduces the time required to access a single record without sacrificing the sequential nature of the file to process the entire file sequentially the records of the main file are processed in sequence until a pointer to the overflow file is found then accessing continues in the overflow file until a null pointer is encountered at which time accessing of the main file is resumed where it left off to provide even greater efficiency in access multiple levels of indexing can be used thus the lowest level of index file is treated as a sequential file and a higherlevel index file is created for that file consider again a file with million records a lowerlevel index with entries is constructed a higherlevel index into the lowerlevel index of entries can then be constructed the search begins at the higherlevel index average length accesses to find an entry point into the lowerlevel index this index is then searched average length to find an entry point into the main file which is then searched average length thus the average length of search has been reduced from to to the indexed file the indexed sequential file retains one limitation of the sequential file effective processing is limited to that which is based on a single field of the file for example when it is necessary to search for a record on the basis of some other attribute than the key field both forms of sequential file are inadequate in some applications the flexibility of efficiently searching by various attributes is desirable to achieve this flexibility a structure is needed that employs multiple indexes one for each type of field that may be the subject of a search in the general indexed file the concept of sequentiality and a single key are abandoned records are accessed only through their indexes the result is that there is now no restriction on the placement of records as long as a pointer in at least one index refers to that record furthermore variablelength records can be employed two types of indexes are used an exhaustive index contains one entry for every record in the main file the index itself is organized as a sequential file for ease of searching a partial index contains entries to records where the field of interest exists with variablelength records some records will not contain all fields when a new record is added to the main file all of the index files must be updated indexed files are used mostly in applications where timeliness of information is critical and where data are rarely processed exhaustively examples are airline reservation systems and inventory control systems the direct or hashed file the direct or hashed file exploits the capability found on disks to access directly any block of a known address as with sequential and indexed sequential files a key field is required in each record however there is no concept of sequential ordering here the direct file makes use of hashing on the key value this function is explained in appendix f figure fb shows the type of hashing organization with an overflow file that is typically used in a hash file direct files are often used where very rapid access is required where fixedlength records are used and where records are always accessed one at a time examples are directories pricing tables schedules and name lists multiprocessor and multicore organization traditionally the computer has been viewed as a sequential machine most computer programming languages require the programmer to specify algorithms as sequences of instructions a processor executes programs by executing machine instructions in sequence and one at a time each instruction is executed in a sequence of operations fetch instruction fetch operands perform operation store results this view of the computer has never been entirely true at the microoperation level multiple control signals are generated at the same time instruction pipelining at least to the extent of overlapping fetch and execute operations has been around for a long time both of these are examples of performing functions in parallel as computer technology has evolved and as the cost of computer hardware has dropped computer designers have sought more and more opportunities for parallelism usually to improve performance and in some cases to improve reliability in this book we examine the three most popular approaches to providing parallelism by replicating processors symmetric multiprocessors smps multicore computers and clusters smps and multicore computers are discussed in this section clusters are examined in chapter symmetric multiprocessors definition an smp can be defined as a standalone computer system with the following characteristics there are two or more similar processors of comparable capability these processors share the same main memory and io facilities and are interconnected by a bus or other internal connection scheme such that memory access time is approximately the same for each processor all processors share access to io devices either through the same channels or through different channels that provide paths to the same device all processors can perform the same functions hence the term symmetric the system is controlled by an integrated operating system that provides interaction between processors and their programs at the job task file and data element levels points to should be selfexplanatory point illustrates one of the contrasts with a loosely coupled multiprocessing system such as a cluster in the latter the physical unit of interaction is usually a message or complete file in an smp individual data elements can constitute the level of interaction and there can be a high degree of cooperation between processes an smp organization has a number of potential advantages over a uniprocessor organization including the following performance if the work to be done by a computer can be organized so that some portions of the work can be done in parallel then a system with multiple processors will yield greater performance than one with a single processor of the same type availability in a symmetric multiprocessor because all processors can perform the same functions the failure of a single processor does not halt the machine instead the system can continue to function at reduced performance incremental growth a user can enhance the performance of a system by adding an additional processor scaling vendors can offer a range of products with different price and performance characteristics based on the number of processors configured in the system it is important to note that these are potential rather than guaranteed benefits the operating system must provide tools and functions to exploit the parallelism in an smp system an attractive feature of an smp is that the existence of multiple processors is transparent to the user the operating system takes care of scheduling of tasks on individual processors and of synchronization among processors organization figure illustrates the general organization of an smp there are multiple processors each of which contains its own control unit arithmeticlogic unit and registers each processor has access to a shared main memory and the io devices through some form of interconnection mechanism a shared bus is a common facility the processors can communicate with each other through memory messages and status information left in shared address spaces it may processor processor processor l cache l cache l cache l cache l cache l cache system bus main io memory io adapter subsystem io adapter io adapter figure symmetric multiprocessor organization also be possible for processors to exchange signals directly the memory is often organized so that multiple simultaneous accesses to separate blocks of memory are possible in modern computers processors generally have at least one level of cache memory that is private to the processor this use of cache introduces some new design considerations because each local cache contains an image of a portion of main memory if a word is altered in one cache it could conceivably invalidate a word in another cache to prevent this the other processors must be alerted that an update has taken place this problem is known as the cache coherence problem and is typically addressed in hardware rather than by the os multicore computers a multicore computer also known as a chip multiprocessor combines two or more processors called cores on a single piece of silicon called a die typically each core consists of all of the components of an independent processor such as registers alu pipeline hardware and control unit plus l instruction and data caches in addition to the multiple cores contemporary multicore chips also include l cache and in some cases l cache the motivation for the development of multicore computers can be summed up as follows for decades microprocessor systems have experienced a steady usually exponential increase in performance this is partly due to hardware trends such as an increase in clock frequency and the ability to put cache memory closer to the processor because of the increasing miniaturization of microcomputer components performance has also been improved by the increased complexity of processor design to exploit parallelism in instruction execution and memory access in brief designers have come up against practical limits in the ability to achieve greater performance by means of more complex processors designers have found that the best way to improve performance to take advantage of advances in hardware is to put multiple processors and a substantial amount of cache memory on a single chip a detailed discussion of the rationale for this trend is beyond our scope but is summarized in appendix c an example of a multicore system is the intel core i which includes four x processors each with a dedicated l cache and with a shared l cache figure one mechanism intel uses to make its caches more effective is prefetching in which the hardware examines memory access patterns and attempts to fill the caches speculatively with data thats likely to be requested soon the core i chip supports two forms of external communications to other chips the ddr memory controller brings the memory controller for the ddr double data rate main memory onto the chip the interface supports three channels that are bytes wide for a total bus width of bits for an aggregate data rate of up to gbs with the memory controller on the chip the front side bus is eliminated the quickpath interconnect qpi is a pointtopoint link electrical interconnect specification it enables highspeed communications among connected processor chips the qpi link operates at gts transfers per second a description of hardwarebased cache coherency schemes is provided in stal core core core core kb id kb id kb id kb id l caches l caches l caches l caches kb kb kb kb l cache l cache l cache l cache mb l cache ddr memory quickpath controllers interconnect b gts b gts figure intel core i block diagram at bits per transfer that adds up to gbs and since qpi links involve dedicated bidirectional pairs the total bandwidth is gbs btrees the preceding section referred to the use of an index file to access individual records in a file or database for a large file or database a single sequential file of indexes on the primary key does not provide for rapid access to provide more efficient access a structured index file is typically used the simplest such structure is a twolevel organization in which the original file is broken into sections and the upper level consists of a sequenced set of pointers to the lowerlevel sections this structure can then be extended to more than two levels resulting in a tree structure unless some discipline is imposed on the construction of the tree index it is likely to end up with an uneven structure with some short branches and some long branches so that the time to search the index is uneven therefore a balanced tree structure with all branches of equal length would appear to give the best average performance such a structure is the btree which has become the standard method of organizing indexes for databases and is commonly used in os file systems including those supported by mac os x windows and several linux file systems the btree structure provides for efficient searching adding and deleting of items key key keyk subtree subtree subtree subtreek subtreek figure a btree node with k children before illustrating the concept of btree let us define a btree and its characteristics more precisely a btree is a tree structure no closed loops with the following characteristics figure the tree consists of a number of nodes and leaves each node contains at least one key which uniquely identifies a file record and more than one pointer to child nodes or leaves the number of keys and pointers contained in a node may vary within limits explained below each node is limited to the same number of maximum keys the keys in a node are stored in nondecreasing order each key has an associated child that is the root of a subtree containing all nodes with keys less than or equal to the key but greater than the preceding key a node also has an additional rightmost child that is the root for a subtree containing all keys greater than any keys in the node thus each node has one more pointer than keys a btree is characterized by its minimum degree d and satisfies the following properties every node has at most d keys and d children or equivalently d pointers every node except for the root has at least d keys and d pointers as a result each internal node except the root is at least half full and has at least d children the root has at least key and children all leaves appear on the same level and contain no information this is a logical construct to terminate the tree the actual implementation may differ for example each bottomlevel node may contain keys alternating with null pointers a nonleaf node with k pointers contains k keys typically a btree has a relatively large branching factor large number of children resulting in a tree of low height figure illustrates two levels of a btree the upper level has k keys and k pointers and satisfies the following relationship key key c keyk some treatments require as stated here that the maximum number of keys in a node is odd eg corm others specify even come still others allow odd or even knut the choice does not fundamentally affect the performance of btrees each pointer points to a node that is the top level of a subtree of this upperlevel node each of these subtree nodes contains some number of keys and pointers unless it is a leaf node the following relationships hold all the keys in subtree are less than key all the keys in subtree are greater than key and are less than key all the keys in subtree are greater than key and are less than key all the keys in subtreek are greater than keyk and are less than keyk all the keys in subtreek are greater than keyk to search for a key you start at the root node if the key you want is in the node youre done if not you go down one level there are three cases the key you want is less then the smallest key in this node take the leftmost pointer down to the next level the key you want is greater than the largest key in this node take the rightmost pointer down to the next level the value of the key is between the values of two adjacent keys in this node take the pointer between these keys down to the next level for example consider the tree in figure d and the desired key is at the root level so you take the rightmost branch down to the next level here we have so you take the pointer between and down to the next level where the key is found associated with this key is a pointer to the desired record an advantage of this tree structure over other tree structures is that it is broad and shallow so that the search terminates quickly furthermore because it is balanced all branches from root to leaf are of equal length there are no long searches compared to other searches the rules for inserting a new key into the btree must maintain a balanced tree this is done as follows search the tree for the key if the key is not in the tree then you have reached a node at the lowest level if this node has fewer than d keys then insert the key into this node in the proper sequence if the node is full having d keys then split this node around its median key into two new nodes with d keys each and promote the median key to the next higher level as described in step if the new key has a value less than the median key insert it into the lefthand new node otherwise insert it into the righthand new node the result is that the original node has been split into two nodes one with d keys and one with d keys the promoted node is inserted into the parent node following the rules of step therefore if the parent node is already full it must be split and its median key promoted to the next highest layer if the process of promotion reaches the root node and the root node is already full then insertion again follows the rules of step however in this case the median key becomes a new root node and the height of the tree increases by a btree of minimum degree d b key inserted this is a simple insertion into a node c key inserted this requires splitting a node into two parts and promoting one key to the root node d key inserted this requires splitting a node into two parts and promoting one key to the root node this then requires the root node to be split and a new root created figure inserting nodes into a btree figure illustrates the insertion process on a btree of degree d in each part of the figure the nodes affected by the insertion process are unshaded file directories contents associated with any file management system and collection of files is a file directory the directory contains information about the files including attributes location and ownership much of this information especially that concerned with storage table information elements of a file directory basic information file name name as chosen by creator user or program must be unique within a specific directory file type for example text binary load module etc file organization for systems that support different organizations address information volume indicates device on which file is stored starting address starting physical address on secondary storage eg cylinder track and block number on disk size used current size of the file in bytes words or blocks size allocated the maximum size of the file access control information owner user who is assigned control of this file the owner may be able to grantdeny access to other users and to change these privileges access information a simple version of this element would include the users name and password for each authorized user permitted actions controls reading writing executing and transmitting over a network usage information date created when file was first placed in directory identity of creator usually but not necessarily the current owner date last read access date of the last time a record was read identity of last reader user who did the reading date last modified date of the last update insertion or deletion identity of last modifier user who did the modifying date of last backup date of the last time the file was backed up on another storage medium current usage information about current activity on the file such as process or processes that have the file open whether it is locked by a process and whether the file has been updated in main memory but not yet on disk is managed by the operating system the directory is itself a file accessible by various file management routines although some of the information in directories is available to users and applications this is generally provided indirectly by system routines table suggests the information typically stored in the directory for each file in the system from the users point of view the directory provides a mapping between file names known to users and applications and the files themselves thus each file entry includes the name of the file virtually all systems deal with different types of files and different file organizations and this information is also provided an important category of information about each file concerns its storage including its location and size in shared systems it is also important to provide information that is used to control access to the file typically one user is the owner of the file and may grant certain access privileges to other users finally usage information is needed to manage the current use of the file and to record the history of its usage structure the way in which the information of table is stored differs widely among various systems some of the information may be stored in a header record associated with the file this reduces the amount of storage required for the directory making it easier to keep all or much of the directory in main memory to improve speed the simplest form of structure for a directory is that of a list of entries one for each file this structure could be represented by a simple sequential file with the name of the file serving as the key in some earlier singleuser systems this technique has been used however it is inadequate when multiple users share a system and even for single users with many files to understand the requirements for a file structure it is helpful to consider the types of operations that may be performed on the directory search when a user or application references a file the directory must be searched to find the entry corresponding to that file create file when a new file is created an entry must be added to the directory delete file when a file is deleted an entry must be removed from the directory list directory all or a portion of the directory may be requested generally this request is made by a user and results in a listing of all files owned by that user plus some of the attributes of each file eg type access control information usage information update directory because some file attributes are stored in the directory a change in one of these attributes requires a change in the corresponding directory entry the simple list is not suited to supporting these operations consider the needs of a single user the user may have many types of files including wordprocessing text files graphic files spreadsheets and so on the user may like to have these organized by project by type or in some other convenient way if the directory is a simple sequential list it provides no help in organizing the files and forces the user to be careful not to use the same name for two different types of files the problem is much worse in a shared system unique naming becomes a serious problem furthermore it is difficult to conceal portions of the overall directory from users when there is no inherent structure in the directory a start in solving these problems would be to go to a twolevel scheme in this case there is one directory for each user and a master directory the master directory has an entry for each user directory providing address and access control information each user directory is a simple list of the files of that user this arrangement means that names must be unique only within the collection of files of a single user and that the file system can easily enforce access restriction on directories however it still provides users with no help in structuring collections of files a more powerful and flexible approach and one that is almost universally adopted is the hierarchical or treestructure approach figure as before there is a master directory which has under it a number of user directories each of master directory subdirectory subdirectory subdirectory subdirectory subdirectory file file file file figure treestructured directory these user directories in turn may have subdirectories and files as entries this is true at any level that is at any level a directory may consist of entries for subdirectories andor entries for files it remains to say how each directory and subdirectory is organized the simplest approach of course is to store each directory as a sequential file when directories may contain a very large number of entries such an organization may lead to unnecessarily long search times in that case a hashed structure is to be preferred naming users need to be able to refer to a file by a symbolic name clearly each file in the system must have a unique name in order that file references be unambiguous on the other hand it is an unacceptable burden on users to require that they provide unique names especially in a shared system the use of a treestructured directory minimizes the difficulty in assigning unique names any file in the system can be located by following a path from the root or master directory down various branches until the file is reached the series of directory names culminating in the file name itself constitutes a pathname for the file as an example the file in the lower lefthand corner of figure has the pathname userbwordunitaabc the slash is used to delimit names in the sequence the name of the master directory is implicit because all paths start at that directory note that it is perfectly acceptable to have several files with the same file name as long as they have unique pathnames which is equivalent to saying that the same file name may be used in different directories in our example there is another file in the system with the file name abc but that has the pathname userbdrawabc master directory system usera userb userc directory directory userc directory userb usera draw word directory word directory draw unita abc directory unita abc file abc file pathname userbdrawabc abc pathname userbwordunitaabc figure example of treestructured directory although the pathname facilitates the selection of file names it would be awkward for a user to have to spell out the entire pathname every time a reference is made to a file typically an interactive user or a process has associated with it a current directory often referred to as the working directory files are then referenced relative to the working directory for example if the working directory for user b is word then the pathname unitaabc is sufficient to identify the file in the lower lefthand corner of figure when an interactive user logs on or when a process is created the default for the working directory is the user home directory during execution the user can navigate up or down in the tree to change to a different working directory file sharing in a multiuser system there is almost always a requirement for allowing files to be shared among a number of users two issues arise access rights and the management of simultaneous access access rights the file system should provide a flexible tool for allowing extensive file sharing among users the file system should provide a number of options so that the way in which a particular file is accessed can be controlled typically users or groups of users are granted certain access rights to a file a wide range of access rights has been used the following list is representative of access rights that can be assigned to a particular user for a particular file none the user may not even learn of the existence of the file much less access it to enforce this restriction the user would not be allowed to read the user directory that includes this file knowledge the user can determine that the file exists and who its owner is the user is then able to petition the owner for additional access rights execution the user can load and execute a program but can not copy it proprietary programs are often made accessible with this restriction reading the user can read the file for any purpose including copying and execution some systems are able to enforce a distinction between viewing and copying in the former case the contents of the file can be displayed to the user but the user has no means for making a copy appending the user can add data to the file often only at the end but can not modify or delete any of the files contents this right is useful in collecting data from a number of sources updating the user can modify delete and add to the files data this normally includes writing the file initially rewriting it completely or in part and removing all or a portion of the data some systems distinguish among different degrees of updating changing protection the user can change the access rights granted to other users typically this right is held only by the owner of the file in some systems the owner can extend this right to others to prevent abuse of this mechanism the file owner will typically be able to specify which rights can be changed by the holder of this right deletion the user can delete the file from the file system these rights can be considered to constitute a hierarchy with each right implying those that precede it thus if a particular user is granted the updating right for a particular file then that user is also granted the following rights knowledge execution reading and appending one user is designated as owner of a given file usually the person who initially created a file the owner has all of the access rights listed previously and may grant rights to others access can be provided to different classes of users specific user individual users who are designated by user id user groups a set of users who are not individually defined the system must have some way of keeping track of the membership of user groups all all users who have access to this system these are public files simultaneous access when access is granted to append or update a file to more than one user the operating system or file management system must enforce discipline a bruteforce approach is to allow a user to lock the entire file when it is to be updated a finer grain of control is to lock individual records during update essentially this is the readerswriters problem discussed in chapter issues of mutual exclusion and deadlock must be addressed in designing the shared access capability record blocking as indicated in figure records are the logical unit of access of a structured file whereas blocks are the unit of io with secondary storage for io to be performed records must be organized as blocks there are several issues to consider first should blocks be of fixed or variable length on most systems blocks are of fixed length this simplifies io buffer allocation in main memory and the organization of blocks on secondary storage second what should the relative size of a block be compared to the average record size the tradeoff is this the larger the block the more records that are passed in one io operation if a file is being processed or searched sequentially this is an advantage because the number of io operations is reduced by using larger blocks thus speeding up processing on the other hand if records are being accessed randomly and no particular locality of reference is observed then larger blocks result in the unnecessary transfer of unused records however combining the frequency of sequential operations with the potential for locality of reference we can say that the io transfer time is reduced by using larger blocks the competing concern is that larger blocks require larger io buffers making buffer management more difficult given the size of a block there are three methods of blocking that can be used fixed blocking fixedlength records are used and an integral number of records are stored in a block there may be unused space at the end of each block this is referred to as internal fragmentation variablelength spanned blocking variablelength records are used and are packed into blocks with no unused space thus some records must span two blocks with the continuation indicated by a pointer to the successor block as opposed to a file that is treated only as a stream of bytes such as in the unix file system variablelength unspanned blocking variablelength records are used but spanning is not employed there is wasted space in most blocks because of the inability to use the remainder of a block if the next record is larger than the remaining unused space figure illustrates these methods assuming that a file is stored in sequential blocks on a disk the figure assumes that the file is large enough to span two tracks the effect would not be changed if some other file allocation scheme were used see section fixed blocking is the common mode for sequential files with fixedlength records variablelength spanned blocking is efficient of storage and does not limit r r r r track r r r r track fixed blocking r r r r r r r track r r r r r r r r r track variable blocking spanned r r r r r track r r r r r track variable blocking unspanned data waste due to record fit to blocksize gaps due to hardware design waste due to blocksize constraint from fixed record size waste due to block fit to track size figure record blocking methods wied recall from appendix j that the organization of data on a disk is in a concentric set of rings called tracks each track is the same width as the readwrite head the size of records however this technique is difficult to implement records that span two blocks require two io operations and files are difficult to update regardless of the organization variablelength unspanned blocking results in wasted space and limits record size to the size of a block the recordblocking technique may interact with the virtual memory hardware if such is employed in a virtual memory environment it is desirable to make the page the basic unit of transfer pages are generally quite small so that it is impractical to treat a page as a block for unspanned blocking accordingly some systems combine multiple pages to create a larger block for file io purposes this approach is used for vsam files on ibm mainframes secondary storage management on secondary storage a file consists of a collection of blocks the operating system or file management system is responsible for allocating blocks to files this raises two management issues first space on secondary storage must be allocated to files and second it is necessary to keep track of the space available for allocation we will see that these two tasks are related that is the approach taken for file allocation may influence the approach taken for free space management further we will see that there is an interaction between file structure and allocation policy we begin this section by looking at alternatives for file allocation on a single disk then we look at the issue of free space management and finally we discuss reliability file allocation several issues are involved in file allocation when a new file is created is the maximum space required for the file allocated at once space is allocated to a file as one or more contiguous units which we shall refer to as portions that is a portion is a contiguous set of allocated blocks the size of a portion can range from a single block to the entire file what size of portion should be used for file allocation what sort of data structure or table is used to keep track of the portions assigned to a file an example of such a structure is a file allocation table fat found on dos and some other systems let us examine these issues in turn preallocation versus dynamic allocation a preallocation policy requires that the maximum size of a file be declared at the time of the file creation request in a number of cases such as program compilations the production of summary data files or the transfer of a file from another system over a communications network this value can be reliably estimated however for many applications it is difficult if not impossible to estimate reliably the maximum potential size of the file in those cases users and application programmers would tend to overestimate file size so as not to run out of space this clearly is wasteful from the point of view of secondary storage allocation thus there are advantages to the use of dynamic allocation which allocates space to a file in portions as needed portion size the second issue listed is that of the size of the portion allocated to a file at one extreme a portion large enough to hold the entire file is allocated at the other extreme space on the disk is allocated one block at a time in choosing a portion size there is a tradeoff between efficiency from the point of view of a single file versus overall system efficiency wied lists four items to be considered in the tradeoff contiguity of space increases performance especially for retrievenext operations and greatly for transactions running in a transactionoriented operating system having a large number of small portions increases the size of tables needed to manage the allocation information having fixedsize portions eg blocks simplifies the reallocation of space having variablesize or small fixedsize portions minimizes waste of unused storage due to overallocation of course these items interact and must be considered together the result is that there are two major alternatives variable large contiguous portions this will provide better performance the variable size avoids waste and the file allocation tables are small however space is hard to reuse blocks small fixed portions provide greater flexibility they may require large tables or complex structures for their allocation contiguity has been abandoned as a primary goal blocks are allocated as needed either option is compatible with preallocation or dynamic allocation in the case of variable large contiguous portions a file is preallocated one contiguous group of blocks this eliminates the need for a file allocation table all that is required is a pointer to the first block and the number of blocks allocated in the case of blocks all of the portions required are allocated at one time this means that the file allocation table for the file will remain of fixed size because the number of blocks allocated is fixed with variablesize portions we need to be concerned with the fragmentation of free space this issue was faced when we considered partitioned main memory in chapter the following are possible alternative strategies first fit choose the first unused contiguous group of blocks of sufficient size from a free block list best fit choose the smallest unused group that is of sufficient size nearest fit choose the unused group of sufficient size that is closest to the previous allocation for the file to increase locality it is not clear which strategy is best the difficulty in modeling alternative strategies is that so many factors interact including types of files pattern of file table file allocation methods contiguous chained indexed preallocation necessary possible possible fixed or variable size portions variable fixed blocks fixed blocks variable portion size large small small medium allocation frequency once low to high high low time to allocate medium long short medium file allocation table size one entry one entry large medium access degree of multiprogramming other performance factors in the system disk caching disk scheduling and so on file allocation methods having looked at the issues of preallocation versus dynamic allocation and portion size we are in a position to consider specific file allocation methods three methods are in common use contiguous chained and indexed summarizes some of the characteristics of each method with contiguous allocation a single contiguous set of blocks is allocated to a file at the time of file creation figure thus this is a preallocation strategy using variablesize portions the file allocation table needs just a single entry for each file showing the starting block and the length of the file contiguous allocation is the best from the point of view of the individual sequential file multiple blocks can be read in at a time to improve io performance for sequential processing it is also easy to retrieve a single block for example if a file starts at block b and the ith block of the file is wanted its location on secondary storage is simply b i contiguous allocation presents some problems external fragmentation will occur making it difficult to find contiguous blocks of space of sufficient length from time file allocation table file a file name start block length file a file b file c file b file d file e file c file e file d figure contiguous file allocation file allocation table file a file name start block length file a file b file b file c file c file d file e file e file d figure contiguous file allocation after compaction to time it will be necessary to perform a compaction algorithm to free up additional space on the disk figure also with preallocation it is necessary to declare the size of the file at the time of creation with the problems mentioned earlier at the opposite extreme from contiguous allocation is chained allocation figure typically allocation is on an individual block basis each block contains a pointer to the next block in the chain again the file allocation table needs just a single entry for each file showing the starting block and the length of the file although preallocation is possible it is more common simply to allocate blocks as needed the selection of blocks is now a simple matter any free block can be added to a chain there is no external fragmentation to worry about because only file allocation table file b file name start block length file b figure chained allocation file allocation table file b file name start block length file b figure chained allocation after consolidation one block at a time is needed this type of physical organization is best suited to sequential files that are to be processed sequentially to select an individual block of a file requires tracing through the chain to the desired block one consequence of chaining as described so far is that there is no accommodation of the principle of locality thus if it is necessary to bring in several blocks of a file at a time as in sequential processing then a series of accesses to different parts of the disk are required this is perhaps a more significant effect on a singleuser system but may also be of concern on a shared system to overcome this problem some systems periodically consolidate files figure indexed allocation addresses many of the problems of contiguous and chained allocation in this case the file allocation table contains a separate onelevel index for each file the index has one entry for each portion allocated to the file typically the file indexes are not physically stored as part of the file allocation table rather the file index for a file is kept in a separate block and the entry for the file in the file allocation table points to that block allocation may be on the basis of either fixedsize blocks figure or variablesize portions figure allocation by blocks eliminates external fragmentation whereas allocation by variablesize portions improves locality in either case file consolidation may be done from time to time file consolidation reduces the size of the index in the case of variablesize portions but not in the case of block allocation indexed allocation supports both sequential and direct access to the file and thus is the most popular form of file allocation free space management just as the space that is allocated to files must be managed so the space that is not currently allocated to any file must be managed to perform any of the file allocation techniques described previously it is necessary to know what blocks on the disk are available thus we need a disk allocation table in addition to a file allocation table we discuss here a number of techniques that have been implemented file allocation table file b file name index block file b figure indexed allocation with block portions bit tables this method uses a vector containing one bit for each block on the disk each entry of a corresponds to a free block and each corresponds to a block in use for example for the disk layout of figure a vector of length is needed and would have the following value a bit table has the advantage that it is relatively easy to find one or a contiguous group of free blocks thus a bit table works well with any of the file allocation methods outlined another advantage is that it is as small as possible file allocation table file b file name index block file b start block length figure indexed allocation with variablelength portions however it can still be sizable the amount of memory in bytes required for a block bitmap is disk size in bytes file system block size thus for a gbyte disk with byte blocks the bit table occupies about mbytes can we spare mbytes of main memory for the bit table if so then the bit table can be searched without the need for disk access but even with todays memory sizes mbytes is a hefty chunk of main memory to devote to a single function the alternative is to put the bit table on disk but a mbyte bit table would require about disk blocks we cant afford to search that amount of disk space every time a block is needed so a bit table resident in memory is indicated even when the bit table is in main memory an exhaustive search of the table can slow file system performance to an unacceptable degree this is especially true when the disk is nearly full and there are few free blocks remaining accordingly most file systems that use bit tables maintain auxiliary data structures that summarize the contents of subranges of the bit table for example the table could be divided logically into a number of equalsize subranges a summary table could include for each subrange the number of free blocks and the maximumsized contiguous number of free blocks when the file system needs a number of contiguous blocks it can scan the summary table to find an appropriate subrange and then search that subrange chained free portions the free portions may be chained together by using a pointer and length value in each free portion this method has negligible space overhead because there is no need for a disk allocation table merely for a pointer to the beginning of the chain and the length of the first portion this method is suited to all of the file allocation methods if allocation is a block at a time simply choose the free block at the head of the chain and adjust the first pointer or length value if allocation is by variablelength portion a firstfit algorithm may be used the headers from the portions are fetched one at a time to determine the next suitable free portion in the chain again pointer and length values are adjusted this method has its own problems after some use the disk will become quite fragmented and many portions will be a single block long also note that every time you allocate a block you need to read the block first to recover the pointer to the new first free block before writing data to that block if many individual blocks need to be allocated at one time for a file operation this greatly slows file creation similarly deleting highly fragmented files is very time consuming indexing the indexing approach treats free space as a file and uses an index table as described under file allocation for efficiency the index should be on the basis of variablesize portions rather than blocks thus there is one entry in the table for every free portion on the disk this approach provides efficient support for all of the file allocation methods free block list in this method each block is assigned a number sequentially and the list of the numbers of all free blocks is maintained in a reserved portion of the disk depending on the size of the disk either or bits will be needed to store a single block number so the size of the free block list is or times the size of the corresponding bit table and thus must be stored on disk rather than in main memory however this is a satisfactory method consider the following points the space on disk devoted to the free block list is less than of the total disk space if a bit block number is used then the space penalty is bytes for every byte block although the free block list is too large to store in main memory there are two effective techniques for storing a small part of the list in main memory a the list can be treated as a pushdown stack appendix p with the first few thousand elements of the stack kept in main memory when a new block is allocated it is popped from the top of the stack which is in main memory similarly when a block is deallocated it is pushed onto the stack there only has to be a transfer between disk and main memory when the inmemory portion of the stack becomes either full or empty thus this technique gives almost zerotime access most of the time b the list can be treated as a fifo queue with a few thousand entries from both the head and the tail of the queue in main memory a block is allocated by taking the first entry from the head of the queue and deallocated by adding it to the end of the tail of the queue there only has to be a transfer between disk and main memory when either the inmemory portion of the head of the queue becomes empty or the inmemory portion of the tail of the queue becomes full in either of the strategies listed in the preceding point stack or fifo queue a background thread can slowly sort the inmemory list or lists to facilitate contiguous allocation volumes the term volume is used somewhat differently by different operating systems and file management systems but in essence a volume is a logical disk carr defines a volume as follows volume a collection of addressable sectors in secondary memory that an os or application can use for data storage the sectors in a volume need not be consecutive on a physical storage device instead they need only appear that way to the os or application a volume may be the result of assembling and merging smaller volumes in the simplest case a single disk equals one volume frequently a disk is divided into partitions with each partition functioning as a separate volume it is also common to treat multiple disks as a single volume or partitions on multiple disks as a single volume reliability consider the following scenario user a requests a file allocation to add to an existing file the request is granted and the disk and file allocation tables are updated in main memory but not yet on disk the system crashes and subsequently restarts user b requests a file allocation and is allocated space on disk that overlaps the last allocation to user a user a accesses the overlapped portion via a reference that is stored inside as file this difficulty arose because the system maintained a copy of the disk allocation table and file allocation table in main memory for efficiency to prevent this type of error the following steps could be performed when a file allocation is requested lock the disk allocation table on disk this prevents another user from causing alterations to the table until this allocation is completed search the disk allocation table for available space this assumes that a copy of the disk allocation table is always kept in main memory if not it must first be read in allocate space update the disk allocation table and update the disk updating the disk involves writing the disk allocation table back onto disk for chained disk allocation it also involves updating some pointers on disk update the file allocation table and update the disk unlock the disk allocation table this technique will prevent errors however when small portions are allocated frequently the impact on performance will be substantial to reduce this overhead a batch storage allocation scheme could be used in this case a batch of free portions on the disk is obtained for allocation the corresponding portions on disk are marked in use allocation using this batch may proceed in main memory when the batch is exhausted the disk allocation table is updated on disk and a new batch may be acquired if a system crash occurs portions on the disk marked in use must be cleaned up in some fashion before they can be reallocated the technique for cleanup will depend on the file systems particular characteristics file system security following successful logon the user has been granted access to one or a set of hosts and applications this is generally not sufficient for a system that includes sensitive data in its database through the useraccess control procedure a user can be identified to the system associated with each user there can be a profile that specifies permissible operations and file accesses the operating system can then enforce rules based on the user profile the database management system however must control access to specific records or even portions of records for example it may be permissible for anyone in administration to obtain a list of company personnel but only selected individuals may have access to salary information the issue is more than just a matter of level of detail whereas the operating system may grant a user permission to access a file or use an application following which there are no further security checks the database management system must make a decision on each individual access attempt that decision will depend not only on the users identity but also on the specific parts of the data being accessed and even on the information already divulged to the user a general model of access control as exercised by a file or database management system is that of an access matrix figure a based on a figure in sand the basic elements of the model are as follows file file file file account account own own inquiry user a r r credit w w own inquiry inquiry user b r r w r debit credit w r own inquiry user c w r r debit w a access matrix file a b c user a file file own r own own r r w r r w w w file b c user b file file file file own own r r r r w r w w file a b user c file file file own r own r w w r r w w file b c c capability lists for files of part a own r r w b access control lists for files of part a figure example of access control structures subject an entity capable of accessing objects generally the concept of subject equates with that of process any user or application actually gains access to an object by means of a process that represents that user or application object anything to which access is controlled examples include files portions of files programs segments of memory and software objects eg java objects access right the way in which an object is accessed by a subject examples are read write execute and functions in software objects one dimension of the matrix consists of identified subjects that may attempt data access typically this list will consist of individual users or user groups although access could be controlled for terminals hosts or applications instead of or in addition to users the other dimension lists the objects that may be accessed at the greatest level of detail objects may be individual data fields more aggregate groupings such as records files or even the entire database may also be objects in the matrix each entry in the matrix indicates the access rights of that subject for that object in practice an access matrix is usually sparse and is implemented by decomposition in one of two ways the matrix may be decomposed by columns yielding access control lists figure b thus for each object an access control list lists users and their permitted access rights the access control list may contain a default or public entry this allows users that are not explicitly listed as having special rights to have a default set of rights elements of the list may include individual users as well as groups of users decomposition by rows yields capability tickets figure c a capability ticket specifies authorized objects and operations for a user each user has a number of tickets and may be authorized to loan or give them to others because tickets may be dispersed around the system they present a greater security problem than access control lists in particular the ticket must be unforgeable one way to accomplish this is to have the operating system hold all tickets on behalf of users these tickets would have to be held in a region of memory inaccessible to users network considerations for dataoriented access control parallel those for useroriented access control if only certain users are permitted to access certain items of data then encryption may be needed to protect those items during transmission to authorized users typically data access control is decentralized that is controlled by hostbased database management systems if a network database server exists on a network then data access control becomes a network function unix file management in the unix file system six types of files are distinguished regular or ordinary contains arbitrary data in zero or more data blocks regular files contain information entered in them by a user an application program or a system utility program the file system does not impose any internal structure to a regular file but treats it as a stream of bytes directory contains a list of file names plus pointers to associated inodes index nodes described later directories are hierarchically organized figure directory files are actually ordinary files with special write protection privileges so that only the file system can write into them while read access is available to user programs special contains no data but provides a mechanism to map physical devices to file names the file names are used to access peripheral devices such as terminals and printers each io device is associated with a special file as discussed in section named pipes as discussed in section a pipe is an interprocess communications facility a pipe file buffers data received in its input so that a process that reads from the pipes output receives the data on a firstinfirstout basis links in essence a link is an alternative file name for an existing file symbolic links this is a data file that contains the name of the file it is linked to in this section we are concerned with the handling of ordinary files which correspond to what most systems treat as files inodes modern unix operating systems support multiple file systems but map all of these into a uniform underlying system for supporting file systems and allocating disk space to files all types of unix files are administered by the os by means of inodes an inode index node is a control structure that contains the key information needed by the operating system for a particular file several file names may be associated with a single inode but an active inode is associated with exactly one file and each file is controlled by exactly one inode the attributes of the file as well as its permissions and other control information are stored in the inode the exact inode structure varies from one unix implementation to another the freebsd inode structure shown in figure includes the following data elements the type and access mode of the file the files owner and groupaccess identifiers the time that the file was created when it was most recently read and written and when its inode was most recently updated by the system the size of the file in bytes a sequence of block pointers explained in the next subsection the number of physical blocks used by the file including blocks used to hold indirect pointers and attributes the number of directory entries that reference the file the kernel and usersettable flags that describe the characteristics of the file the generation number of the file a randomly selected number assigned to the inode each time that the latter is allocated to a new file the generation number is used to detect references to deleted files the blocksize of the data blocks referenced by the inode typically the same as but sometimes larger than the file system blocksize mode data data data owners data timestamps data data data size direct data direct data data pointers pointers data data direct single indirect pointers pointers data double indirect triple indirect pointers block count data reference count pointers pointers flags data generation number pointers blocksize pointers pointers data extended attr size extended pointers attribute data blocks inode figure structure of freebsd inode and file the size of the extended attribute information zero or more extended attribute entries the blocksize value is typically the same as but sometimes larger than the file system blocksize on traditional unix systems a fixed blocksize of bytes was used freebsd has a minimum blocksize of bytes kbytes the blocksize can be any power of greater than or equal to for typical file systems the blocksize is kbytes or kbytes the default freebsd blocksize is kbytes extended attribute entries are variablelength entries used to store auxiliary data that are separate from the contents of the file the first two extended attributes defined for freebsd deal with security the first of these support access control lists this is described in chapter the second defined extended attribute supports the use of security labels which are part of what is known as a mandatory access control scheme also defined in chapter on the disk there is an inode table or inode list that contains the inodes of all the files in the file system when a file is opened its inode is brought into main memory and stored in a memoryresident inode table file allocation file allocation is done on a block basis allocation is dynamic as needed rather than using preallocation hence the blocks of a file on disk are not necessarily contiguous an indexed method is used to keep track of each file with part of the index stored in the inode for the file in all unix implementations the inode includes a number of direct pointers and three indirect pointers single double triple the freebsd inode includes bytes of address information that is organized as fifteen bit addresses or pointers the first addresses point to the first data blocks of the file if the file requires more than data blocks one or more levels of indirection is used as follows the thirteenth address in the inode points to a block on disk that contains the next portion of the index this is referred to as the single indirect block this block contains the pointers to succeeding blocks in the file if the file contains more blocks the fourteenth address in the inode points to a double indirect block this block contains a list of addresses of additional single indirect blocks each of single indirect blocks in turn contains pointers to file blocks if the file contains still more blocks the fifteenth address in the inode points to a triple indirect block that is a third level of indexing this block points to additional double indirect blocks all of this is illustrated in figure the total number of data blocks in a file depends on the capacity of the fixedsize blocks in the system in freebsd the minimum block size is kbytes and each block can hold a total of block addresses thus the maximum size of a file with this block size is over gb table this scheme has several advantages the inode is of fixed size and relatively small and hence may be kept in main memory for long periods table capacity of a freebsd file with kbyte block size level number of blocks number of bytes direct k single indirect m double indirect k g triple indirect k m g smaller files may be accessed with little or no indirection reducing processing and disk access time the theoretical maximum size of a file is large enough to satisfy virtually all applications directories directories are structured in a hierarchical tree each directory can contain files andor other directories a directory that is inside another directory is referred to as a subdirectory as was mentioned a directory is simply a file that contains a list of file names plus pointers to associated inodes figure shows the overall structure each directory entry dentry contains a name for the associated file or subdirectory plus an integer called the inumber index number when the file or directory is accessed its inumber is used as an index into the inode table volume structure a unix file system resides on a single logical disk or disk partition and is laid out with the following elements boot block contains code required to boot the operating system superblock contains attributes and information about the file system such as partition size and inode table size inode table the collection of inodes for each file data blocks storage space available for data files and subdirectories inode table directory i name i name i name i name figure unix directories and inodes traditional unix file access control most unix systems depend on or at least are based on the file access control scheme introduced with the early versions of unix each unix user is assigned a unique user identification number user id a user is also a member of a primary group and possibly a number of other groups each identified by a group id when a file is created it is designated as owned by a particular user and marked with that users id it also belongs to a specific group which initially is either its creators primary group or the group of its parent directory if that directory has setgid permission set associated with each file is a set of protection bits the owner id group id and protection bits are part of the files inode nine of the protection bits specify read write and execute permission for the owner of the file other members of the group to which this file belongs and all other users these form a hierarchy of owner group and all others with the highest relevant set of permissions being used figure a shows an example in which the file owner has read and write access all other members of the files group have read access and users outside the group have no access rights to the file when applied to a directory the read and write bits grant the right to list and to createrenamedelete files in the directory the execute bit grants the right to search the directory for a component of a filename ogotrwhoneureprcclclalasasssss rwr user rwgroup r other a traditional unix approach minimal access control list ogotrwhoneureprcclclalasasssss rwrwuser rwmasked userjoerwentries group r mask rwother b extended access control list figure unix file access control note that the permissions that apply to a directory are distinct from those that apply to any file or directory it contains the fact that a user has the right to write to the directory does not give the user the right to write to a file in that directory that is governed by the permissions of the specific file the user would however have the right to rename the file the remaining three bits define special additional behavior for files or directories two of these are the set user id setuid and set group id setgid permissions if these are set on an executable file the operating system functions as follows when a user with execute privileges for this file executes the file the system temporarily allocates the rights of the users id of the file creator or the files group respectively to those of the user executing the file these are known as the effective user id and effective group id and are used in addition to the real user id and real group id of the executing user when making access control decisions for this program this change is only effective while the program is being executed this feature enables the creation and use of privileged programs that may use files normally inaccessible to other users it enables users to access certain files in a controlled fashion alternatively when applied to a directory the setgid permission indicates that newly created files will inherit the group of this directory the setuid permission is ignored the final permission bit is the sticky bit when set on a file this originally indicated that the system should retain the file contents in memory following execution this is no longer used when applied to a directory though it specifies that only the owner of any file in the directory can rename move or delete that file this is useful for managing files in shared temporary directories one particular user id is designated as superuser the superuser is exempt from the usual file access control constraints and has systemwide access any program that is owned by and setuid to the superuser potentially grants unrestricted access to the system to any user executing that program hence great care is needed when writing such programs this access scheme is adequate when file access requirements align with users and a modest number of groups of users for example suppose a user wants to give read access for file x to users a and b and read access for file y to users b and c we would need at least two user groups and user b would need to belong to both groups in order to access the two files however if there are a large number of different groupings of users requiring a range of access rights to different files then a very large number of groups may be needed to provide this this rapidly becomes unwieldy and difficult to manage even if possible at all one way to overcome this problem is to use access control lists which are provided in most modern unix systems a final point to note is that the traditional unix file access control scheme implements a simple protection domain structure a domain is associated with the user and switching the domain corresponds to changing the user id temporarily access control lists in unix many modern unix and unixbased operating systems support access control lists including freebsd openbsd linux and solaris in this section we describe the freebsd approach but other implementations have essentially the same features and interface the feature is referred to as extended access control list while the traditional unix approach is referred to as minimal access control list most unix systems impose a limit on the maximum number of groups any user may belong to as well as to the total number of groups possible on the system freebsd allows the administrator to assign a list of unix user ids and groups to a file by using the setfacl command any number of users and groups can be associated with a file each with three protection bits read write execute offering a flexible mechanism for assigning access rights a file need not have an acl but may be protected solely by the traditional unix file access mechanism freebsd files include an additional protection bit that indicates whether the file has an extended acl freebsd and most unix implementations that support extended acls use the following strategy eg figure b the owner class and other class entries in the ninebit permission field have the same meaning as in the minimal acl case the group class entry specifies the permissions for the owner group for this file these permissions represent the maximum permissions that can be assigned to named users or named groups other than the owning user in this latter role the group class entry functions as a mask additional named users and named groups may be associated with the file each with a threebit permission field the permissions listed for a named user or named group are compared to the mask field any permission for the named user or named group that is not present in the mask field is disallowed when a process requests access to a file system object two steps are performed step selects the acl entry that most closely matches the requesting process the acl entries are looked at in the following order owner named users owning or named groups and others only a single entry determines access step checks if the matching entry contains sufficient permissions a process can be a member in more than one group so more than one group entry can match if any of these matching group entries contain the requested permissions one that contains the requested permissions is picked the result is the same no matter which entry is picked if none of the matching group entries contains the requested permissions access will be denied no matter which entry is picked recommended reading and web sites stal covers the topics of this chapter in detail in addition there are many other texts on computer organization and architecture among the more worthwhile texts are the following patt is a comprehensive survey henn by the same authors is a more advanced text that emphasizes quantitative aspects of design denn looks at the history of the development and application of the locality principle making for fascinating reading denn denning p the locality principle communications of the acm july henn hennessy j and patterson d computer architecture a quantitative approach san mateo ca morgan kaufmann patt patterson d and hennessy j computer organization and design the hardwaresoftware interface san mateo ca morgan kaufmann stal stallings w computer organization and architecture th ed upper saddle river nj prentice hall recommended web sites www computer architecture home page a comprehensive index to information relevant to computer architecture researchers including architecture groups and projects technical organizations literature employment and commercial information cpu info center information on specific processors including technical papers product information and latest announcements linux virtual file system linux includes a versatile and powerful filehandling facility designed to support a wide variety of file management systems and file structures the approach taken in linux is to make use of a virtual file system vfs which presents a single uniform file system interface to user processes the vfs defines a common file model that is capable of representing any conceivable file systems general feature and behavior the vfs assumes that files are objects in a computers mass storage memory that share basic properties regardless of the target file system or the underlying processor hardware files have symbolic names that allow them to be uniquely identified within a specific directory within the file system a file has an owner protection against unauthorized access or modification and a variety of other properties a file may be created read from written to or deleted for any specific file system a user process system call system calls interface virtual file system vfs linux kernel ibm jfs dos fs ntfs ext fs page cache device drivers io request hardware disk controller figure linux virtual file system context mapping module is needed to transform the characteristics of the real file system to the characteristics expected by the virtual file system figure indicates the key ingredients of the linux file system strategy a user process issues a file system call eg read using the vfs file scheme the vfs converts this into an internal to the kernel file system call that is passed to a mapping function for a specific file system eg ibms journaling file system jfs in most cases the mapping function is simply a mapping of file system functional calls from one scheme to another in some cases the mapping function is more complex for example some file systems use a file allocation table fat which stores the position of each file in the directory tree in these file systems directories are not files for such file systems the mapping function must be able to construct dynamically and when needed the files corresponding to the directories in any case the original user file system call is translated into a call that is native to the target file system the target file system software is then invoked to perform the requested function on a file or directory under its control and secondary storage the results of the operation are then communicated back to the user in a similar fashion figure indicates the role that vfs plays within the linux kernel when a process initiates a fileoriented system call eg read the kernel calls a function in the vfs this function handles the filesystemindependent manipulations and initiates a call to a function in the target file system code this call passes through a mapping function that converts the call from the vfs into a call to the target file system calls system calls vfs using using vfs system filesystem x disk io user interface linux calls mapping interface calls virtual function file file to file system x user system system x process files on secondary storage maintained by file system x figure linux virtual file system concept system the vfs is independent of any file system so the implementation of a mapping function must be part of the implementation of a file system on linux the target file system converts the file system request into deviceoriented instructions that are passed to a device driver by means of page cache functions vfs is an objectoriented scheme because it is written in c rather than a language that supports object programming such as c or java vfs objects are implemented simply as c data structures each object contains both data and pointers to filesystemimplemented functions that operate on data the four primary object types in vfs are as follows superblock object represents a specific mounted file system inode object represents a specific file dentry object represents a specific directory entry file object represents an open file associated with a process this scheme is based on the concepts used in unix file systems as described in section the key concepts of unix file system to remember are the following a file system consists of a hierarchal organization of directories a directory is the same as what is known as a folder on many nonunix platforms and may contain files and or other directories because a directory may contain other directories a tree structure is formed a path through the tree structure from the root consists of a sequence of directory entries ending in either a directory entry dentry or a file name in unix a directory is implemented as a file that lists the files and directories contained within it thus file operations can be performed on either files or directories the superblock object the superblock object stores information describing a specific file system typically the superblock corresponds to the file system superblock or file system control block which is stored in a special sector on disk the superblock object consists of a number of data items examples include the following the device that this file system is mounted on the basic block size of the file system dirty flag to indicate that the superblock has been changed but not written back to disk file system type flags such as a readonly flag pointer to the root of the file system directory list of open files semaphore for controlling access to the file system list of superblock operations the last item on the preceding list refers to an operations object contained within the superblock object the operations object defines the object methods functions that the kernel can invoke against the superblock object the methods defined for the superblock object include the following readinode read a specified inode from a mounted file system writeinode write given inode to disk putinode release inode deleteinode delete inode from disk notifychange called when inode attributes are changed putsuper called by the vfs on unmount to release the given superblock writesuper called when the vfs decides that the superblock needs to be written to disk statfs obtain file system statistics remountfs called by the vfs when the file system is remounted with new mount options clearinode release inode and clear any pages containing related data the inode object an inode is associated with each file the inode object holds all the information about a named file except its name and the actual data contents of the file items contained in an inode object include owner group permissions access times for a file size of data it holds and number of links the inode object also includes an inode operations object that describes the file systems implemented functions that the vfs can invoke on an inode the methods defined for the inode object include the following create creates a new inode for a regular file associated with a dentry object in some directory lookup searches a directory for an inode corresponding to a file name mkdir creates a new inode for a directory associated with a dentry object in some directory the dentry object a dentry directory entry is a specific component in a path the component may be either a directory name or a file name dentry objects facilitate access to files and directories and are used in a dentry cache for that purpose the dentry object includes a pointer to the inode and superblock it also includes a pointer to the parent dentry and pointers to any subordinate dentrys the file object the file object is used to represent a file opened by a process the object is created in response to the open system call and destroyed in response to the close system call the file object consists of a number of items including the following dentry object associated with the file file system containing the file file objects usage counter users user id users group id file pointer which is the current position in the file from which the next operation will take place the file object also includes an inode operations object that describes the file systems implemented functions that the vfs can invoke on a file object the methods defined for the file object include read write open release and lock windows file system the developers of windows nt designed a new file system the new technology file system ntfs which is intended to meet highend requirements for workstations and servers examples of highend applications include the following clientserver applications such as file servers compute servers and database servers resourceintensive engineering and scientific applications network applications for large corporate systems this section provides an overview of ntfs key features of ntfs ntfs is a flexible and powerful file system built as we shall see on an elegantly simple file system model the most noteworthy features of ntfs include the following recoverability high on the list of requirements for the new windows file system was the ability to recover from system crashes and disk failures in the event of such failures ntfs is able to reconstruct disk volumes and return them to a consistent state it does this by using a transactionprocessing model for changes to the file system each significant change is treated as an atomic action that is either entirely performed or not performed at all each transaction that was in process at the time of a failure is subsequently backed out or brought to completion in addition ntfs uses redundant storage for critical file system data so that failure of a disk sector does not cause the loss of data describing the structure and status of the file system security ntfs uses the windows object model to enforce security an open file is implemented as a file object with a security descriptor that defines its security attributes the security descriptor is persisted as an attribute of each file on disk large disks and large files ntfs supports very large disks and very large files more efficiently than other file systems such as fat multiple data streams the actual contents of a file are treated as a stream of bytes in ntfs it is possible to define multiple data streams for a single file an example of the utility of this feature is that it allows windows to be used by remote macintosh systems to store and retrieve files on macintosh each file has two components the file data and a resource fork that contains information about the file ntfs treats these two components as two data streams within a single file journaling ntfs keeps a log of all changes made to files on the volumes programs such as desktop search can read the journal to identify what files have changed compression and encryption entire directories and individual files can be transparently compressed andor encrypted hard and symbolic links in order to support posix windows has always supported hard links which allow a single file to be accessible by multiple path names on the same volume starting with windows vista symbolic links are supported which allow a file or directory to be accessible by multiple path names even if the names are on different volumes windows also supports mount points which allow volumes to appear at junction points on other volumes rather than be named by driver letters such as d ntfs volume and file structure ntfs makes use of the following disk storage concepts sector the smallest physical storage unit on the disk the data size in bytes is a power of and is almost always bytes cluster one or more contiguous next to each other on the disk sectors the cluster size in sectors is a power of volume a logical partition on a disk consisting of one or more clusters and used by a file system to allocate space at any time a volume consists of file system information a collection of files and any additional unallocated space remaining on the volume that can be allocated to files a volume can be all or a portion of a single disk or it can extend across multiple disks if hardware or table windows ntfs partition and cluster sizes volume size sectors per cluster cluster size mbyte bytes mbyte gbyte k gbyte gbyte k gbyte gbyte k gbyte gbyte k gbyte gbyte k gbyte gbyte k gbyte k software raid is employed a volume consists of stripes spanning multiple disks the maximum volume size for ntfs is bytes the cluster is the fundamental unit of allocation in ntfs which does not recognize sectors for example suppose each sector is bytes and the system is configured with two sectors per cluster one cluster k bytes if a user creates a file of bytes two clusters are allocated to the file later if the user updates the file to bytes another two clusters are allocated the clusters allocated to a file need not be contiguous it is permissible to fragment a file on the disk currently the maximum file size supported by ntfs is clusters which is equivalent to a maximum of bytes a cluster can have at most bytes the use of clusters for allocation makes ntfs independent of physical sector size this enables ntfs to support easily nonstandard disks that do not have a byte sector size and to support efficiently very large disks and very large files by using a larger cluster size the efficiency comes from the fact that the file system must keep track of each cluster allocated to each file with larger clusters there are fewer items to manage table shows the default cluster sizes for ntfs the defaults depend on the size of the volume the cluster size that is used for a particular volume is established by ntfs when the user requests that a volume be formatted ntfs volume layout ntfs uses a remarkably simple but powerful approach to organizing information on a disk volume every element on a volume is a file and every file consists of a collection of attributes even the data contents of a file is treated as an attribute with this simple structure a few generalpurpose functions suffice to organize and manage a file system figure shows the layout of an ntfs volume which consists of four regions the first few sectors on any volume are occupied by the partition boot partition system boot master file table files file area sector figure ntfs volume layout sector although it is called a sector it can be up to sectors long which contains information about the volume layout and the file system structures as well as boot startup information and code this is followed by the master file table mft which contains information about all of the files and folders directories on this ntfs volume in essence the mft is a list of all files and their attributes on this ntfs volume organized as a set of rows in a table structure following the mft is a region containing system files among the files in this region are the following mft a mirror of the first few rows of the mft used to guarantee access to the volume in the case of a singlesector failure in the sectors storing the mft log file a list of transaction steps used for ntfs recoverability cluster bit map a representation of the space on the volume showing which clusters are in use attribute definition table defines the attribute types supported on this volume and indicates whether they can be indexed and whether they can be recovered during a system recovery operation master file table the heart of the windows file system is the mft the mft is organized as a table of byte rows called records each row describes a file on this volume including the mft itself which is treated as a file if the contents of a file are small enough then the entire file is located in a row of the mft otherwise the row for that file contains partial information and the remainder of the file spills over into other available clusters on the volume with pointers to those clusters in the mft row of that file each record in the mft consists of a set of attributes that serve to define the file or folder characteristics and the file contents table lists the attributes that may be found in a row with the required attributes indicated by shading table windows ntfs file and directory attribute types attribute type description standard information includes access attributes readonly readwrite etc time stamps including when the file was created or last modified and how many directories point to the file link count attribute list a list of attributes that make up the file and the file reference of the mft file record in which each attribute is located used when all attributes do not fit into a single mft file record file name a file or directory must have one or more names security descriptor specifies who owns the file and who can access it data the contents of the file a file has one default unnamed data attribute and may have one or more named data attributes index root used to implement folders index allocation used to implement folders volume information includes volumerelated information such as the version and name of the volume bitmap provides a map representing records in use on the mft or folder note greencolored rows refer to required file attributes the other attributes are optional recoverability ntfs makes it possible to recover the file system to a consistent state following a system crash or disk failure the key elements that support recoverability are as follows figure io manager includes the ntfs driver which handles the basic open close read and write functions of ntfs in addition the software raid module ftdisk can be configured for use log file service maintains a log of file system metadata changes on disk the log file is used to recover an ntfsformatted volume in the case of a system failure ie without having to run the file system check utility cache manager responsible for caching file reads and writes to enhance performance the cache manager optimizes disk io virtual memory manager the ntfs accesses cached files by mapping file references to virtual memory references and reading and writing virtual memory it is important to note that the recovery procedures used by ntfs are designed to recover file system metadata not file contents thus the user should never lose a volume or the directoryfile structure of an application because of a crash however user data are not guaranteed by the file system providing full recoverability including user data would make for a much more elaborate and resourceconsuming recovery facility the essence of the ntfs recovery capability is logging each operation that alters a file system is treated as a transaction each suboperation of a transaction io manager log the transaction log file ntfs driver readwrite a service readwrite mirrored or the file faulttolerant striped volume flush the write the driver readwrite log file cache the disk disk driver cache load data from manager disk into memory access the mapped file or flush the cache virtual memory manager figure windows ntfs components that alters important file system data structures is recorded in a log file before being recorded on the disk volume using the log a partially completed transaction at the time of a crash can later be redone or undone when the system recovers in general terms these are the steps taken to ensure recoverability as described in russ ntfs first calls the log file system to record in the log file in the cache any transactions that will modify the volume structure ntfs modifies the volume in the cache the cache manager calls the log file system to prompt it to flush the log file to disk once the log file updates are safely on disk the cache manager flushes the volume changes to disk key terms review questions and problems key terms address register instruction register program counter cache memory interrupt programmed io cache slot interruptdriven io reentrant procedure central processing unit io module register data register locality secondary memory direct memory access main memory spatial locality hit ratio multicore stack inputoutput multiprocessor system bus instruction processor temporal locality instruction cycle review questions list and briefly define the four main elements of a computer define the two main categories of processor registers in general terms what are the four distinct actions that a machine instruction can specify what is an interrupt how are multiple interrupts dealt with what characteristics distinguish the various elements of a memory hierarchy what is cache memory what is the difference between a multiprocessor and a multicore system what is the distinction between spatial locality and temporal locality in general what are the strategies for exploiting spatial locality and temporal locality problems suppose the hypothetical processor of figure also has two io instructions load ac from io store ac to io in these cases the bit address identifies a particular external device show the program execution using format of figure for the following program load ac from device add contents of memory location store ac to device assume that the next value retrieved from device is and that location contains a value of the program execution of figure is described in the text using six steps expand this description to show the use of the mar and mbr consider a hypothetical bit microprocessor having bit instructions composed of two fields the first byte contains the opcode and the remainder an immediate operand or an operand address a what is the maximum directly addressable memory capacity in bytes b discuss the impact on the system speed if the microprocessor bus has a bit local address bus and a bit local data bus or a bit local address bus and a bit local data bus c how many bits are needed for the program counter and the instruction register consider a hypothetical microprocessor generating a bit address eg assume that the program counter and the address registers are bits wide and having a bit data bus a what is the maximum memory address space that the processor can access directly if it is connected to a bit memory b what is the maximum memory address space that the processor can access directly if it is connected to an bit memory c what architectural features will allow this microprocessor to access a separate io space d if an input and an output instruction can specify an bit io port number how many bit io ports can the microprocessor support how many bit io ports explain consider a bit microprocessor with a bit external data bus driven by an mhz input clock assume that this microprocessor has a bus cycle whose minimum duration equals four input clock cycles what is the maximum data transfer rate across the bus that this microprocessor can sustain in bytess to increase its performance would it be better to make its external data bus bits or to double the external clock frequency supplied to the microprocessor state any other assumptions you make and explain hint determine the number of bytes that can be transferred per bus cycle consider a computer system that contains an io module controlling a simple keyboardprinter teletype the following registers are contained in the cpu and connected directly to the system bus inpr input register bits outr output register bits fgi input flag bit fgo output flag bit ien interrupt enable bit keystroke input from the teletype and output to the printer are controlled by the io module the teletype is able to encode an alphanumeric symbol to an bit word and decode an bit word into an alphanumeric symbol the input flag is set when an bit word enters the input register from the teletype the output flag is set when a word is printed a describe how the cpu using the first four registers listed in this problem can achieve io with the teletype b describe how the function can be performed more efficiently by also employing ien in virtually all systems that include dma modules dma access to main memory is given higher priority than processor access to main memory why a dma module is transferring characters to main memory from an external device transmitting at bits per second bps the processor can fetch instructions at the rate of million instructions per second by how much will the processor be slowed down due to the dma activity a computer consists of a cpu and an io device d connected to main memory m via a shared bus with a data bus width of one word the cpu can execute a maximum of instructions per second an average instruction requires five processor cycles three of which use the memory bus a memory read or write operation uses one processor cycle suppose that the cpu is continuously executing background programs that require of its instruction execution rate but not any io instructions assume that one processor cycle equals one bus cycle now suppose that very large blocks of data are to be transferred between m and d a if programmed io is used and each oneword io transfer requires the cpu to execute two instructions estimate the maximum io data transfer rate in words per second possible through d b estimate the same rate if dma transfer is used consider the following code for i i i for j j j ai ai j a give one example of the spatial locality in the code b give one example of the temporal locality in the code generalize equations and in appendix a to nlevel memory hierarchies consider a memory system with the following parameters tc ns cc centsbit tm ns cm centsbit a what is the cost of mbyte of main memory b what is the cost of mbyte of main memory using cache memory technology c if the effective access time is greater than the cache access time what is the hit ratio h a computer has a cache main memory and a disk used for virtual memory if a referenced word is in the cache ns are required to access it if it is in main memory but not in the cache ns are needed to load it into the cache this includes the time to originally check the cache and then the reference is started again if the word is not in main memory ms are required to fetch the word from disk followed by ns to copy it to the cache and then the reference is started again the cache hit ratio is and the mainmemory hit ratio is what is the average time in ns required to access a referenced word on this system suppose a stack is to be used by the processor to manage procedure calls and returns can the program counter be eliminated by using the top of the stack as a program counter appendix a performance characteristics of twolevel memories in this chapter reference is made to a cache that acts as a buffer between main memory and processor creating a twolevel internal memory this twolevel architecture exploits a property known as locality to provide improved performance over a comparable onelevel memory the main memory cache mechanism is part of the computer architecture implemented in hardware and typically invisible to the os accordingly this mechanism is not pursued in this book however there are two other instances of a twolevel memory approach that also exploit the property of locality and that are at least partially implemented in the os virtual memory and the disk cache table these two topics are explored in chapters and respectively in this appendix we look at some of the performance characteristics of twolevel memories that are common to all three approaches table characteristics of twolevel memories main memory virtual memory cache paging disk cache typical access time ratios memory management implemented by combination of hardware system software system special hardware and system software typical block size to bytes to bytes to bytes access of processor to direct access indirect access indirect access second level locality the basis for the performance advantage of a twolevel memory is the principle of locality referred to in section this principle states that memory references tend to cluster over a long period of time the clusters in use change but over a short period of time the processor is primarily working with fixed clusters of memory references intuitively the principle of locality makes sense consider the following line of reasoning except for branch and call instructions which constitute only a small fraction of all program instructions program execution is sequential hence in most cases the next instruction to be fetched immediately follows the last instruction fetched it is rare to have a long uninterrupted sequence of procedure calls followed by the corresponding sequence of returns rather a program remains confined to a rather narrow window of procedureinvocation depth thus over a short period of time references to instructions tend to be localized to a few procedures most iterative constructs consist of a relatively small number of instructions repeated many times for the duration of the iteration computation is therefore confined to a small contiguous portion of a program in many programs much of the computation involves processing data structures such as arrays or sequences of records in many cases successive references to these data structures will be to closely located data items this line of reasoning has been confirmed in many studies with reference to point a variety of studies have analyzed the behavior of highlevel language programs table includes key results measuring the appearance of various statement types during execution from the following studies the earliest study of programming language behavior performed by knuth knut examined a collection of fortran programs used as student exercises tanenbaum tane published measurements collected from over procedures used in os programs and written in a language that supports structured programming sal patterson and sequin patt analyzed a set of measurements taken from compilers and programs for typesetting computeraided design cad sorting and file table relative dynamic frequency of highlevel language operations study huck knut patt tane language pascal fortran pascal c sal workload scientific student system system system assign loop call if goto other comparison the programming languages c and pascal were studied huck huck analyzed four programs intended to represent a mix of generalpurpose scientific computing including fast fourier transform and the integration of systems of differential equations there is good agreement in the results of this mixture of languages and applications that branching and call instructions represent only a fraction of statements executed during the lifetime of a program thus these studies confirm assertion from the preceding list with respect to assertion studies reported in patt provide confirmation this is illustrated in figure which shows callreturn behavior each call is represented by the line moving down and to the right and each return by the line moving up and to the right in the figure a window with depth equal to is defined only a sequence of calls and returns with a net movement of in either direction causes the window to move as can be seen the executing program can remain within a stationary window for long periods of time a study by the same analysts of c and pascal programs showed that a window of depth would only need to shift on less than of the calls or returns tami time in units of callsreturns t return call w nesting depth figure example callreturn behavior of a program a distinction is made in the literature between spatial locality and temporal locality spatial locality refers to the tendency of execution to involve a number of memory locations that are clustered this reflects the tendency of a processor to access instructions sequentially spatial location also reflects the tendency of a program to access data locations sequentially such as when processing a table of data temporal locality refers to the tendency for a processor to access memory locations that have been used recently for example when an iteration loop is executed the processor executes the same set of instructions repeatedly traditionally temporal locality is exploited by keeping recently used instruction and data values in cache memory and by exploiting a cache hierarchy spatial locality is generally exploited by using larger cache blocks and by incorporating prefetching mechanisms fetching items whose use is expected into the cache control logic recently there has been considerable research on refining these techniques to achieve greater performance but the basic strategies remain the same operation of twolevel memory the locality property can be exploited in the formation of a twolevel memory the upperlevel memory m is smaller faster and more expensive per bit than the lowerlevel memory m m is used as a temporary store for part of the contents of the larger m when a memory reference is made an attempt is made to access the item in m if this succeeds then a quick access is made if not then a block of memory locations is copied from m to m and the access then takes place via m because of locality once a block is brought into m there should be a number of accesses to locations in that block resulting in fast overall service to express the average time to access an item we must consider not only the speeds of the two levels of memory but also the probability that a given reference can be found in m we have ts h t h t t t h t where ts average system access time t access time of m eg cache disk cache t access time of m eg main memory disk h hit ratio fraction of time reference is found in m figure shows average access time as a function of hit ratio as can be seen for a high percentage of hits the average total access time is much closer to that of m than m performance let us look at some of the parameters relevant to an assessment of a twolevel memory mechanism first consider cost we have cs c s c s s s where cs average cost per bit for the combined twolevel memory c average cost per bit of upperlevel memory m c average cost per bit of lowerlevel memory m s size of m s size of m we would like cs c given that c c this requires s s figure shows the relationship next consider access time for a twolevel memory to provide a significant performance improvement we need to have ts approximately equal to t ts t given that t is much less than t ts t a hit ratio of close to is needed so we would like m to be small to hold down cost and large to improve the hit ratio and therefore the performance is there a size of m that satisfies both requirements to a reasonable extent we can answer this question with a series of subquestions what value of hit ratio is needed to satisfy the performance requirement what size of m will assure the needed hit ratio does this size satisfy the cost requirement cc relative combined cost csc cc cc relative size of two levels ss figure relationship of average memory cost to relative memory size for a twolevel memory note that both axes use a log scale a basic review of log scales is in the math refresher document at the computer science student resource site at computersciencestudentcom to get at this consider the quantity tts which is referred to as the access efficiency it is a measure of how close average access time ts is to m access time t from equation t ts h t t in figure we plot tts as a function of the hit ratio h with the quantity tt as a parameter a hit ratio in the range of to would seem to be needed to satisfy the performance requirement we can now phrase the question about relative memory size more exactly is a hit ratio of or higher reasonable for s s this will depend on a number of factors including the nature of the software being executed and the details of the design of the twolevel memory the main determinant is of course the degree of locality figure suggests the effect of locality on the hit ratio clearly if m is the same size as m then the hit ratio will be all of the items in m are always stored also in m now suppose that there is no locality that is references are completely random in that case the hit ratio should be a strictly linear function of the relative memory size for example if m is half the size of m then at any time half of the items from m are also in m and the hit ratio will be in practice however there is some degree of locality in the references the effects of moderate and strong locality are indicated in the figure so if there is strong locality it is possible to achieve high values of hit ratio even with relatively small upperlevel memory size for example numerous studies r access efficiency tts r r r hit ratio h figure access efficiency as a function of hit ratio r tt strong locality moderate hit ratio locality no locality relative memory size ss figure hit ratio as a function of relative memory size have shown that rather small cache sizes will yield a hit ratio above regardless of the size of main memory eg agar przy stre and smit a cache in the range of k to k words is generally adequate whereas main memory is now typically in the gigabyte range when we consider virtual memory and disk cache we will cite other studies that confirm the same phenomenon namely that a relatively small m yields a high value of hit ratio because of locality this brings us to the last question listed earlier does the relative size of the two memories satisfy the cost requirement the answer is clearly yes if we need only a relatively small upperlevel memory to achieve good performance then the average cost per bit of the two levels of memory will approach that of the cheaper lowerlevel memory summary a file management system is a set of system software that provides services to users and applications in the use of files including file access directory maintenance and access control the file management system is typically viewed as a system service that itself is served by the operating system rather than being part of the operating system itself however in any system at least part of the file management function is performed by the operating system a file consists of a collection of records the way in which these records may be accessed determines its logical organization and to some extent its physical organization on disk if a file is primarily to be processed as a whole then a sequential file organization is the simplest and most appropriate if sequential access is needed but random access to individual file is also desired then an indexed sequential file may give the best performance if access to the file is principally at random then an indexed file or hashed file may be the most appropriate whatever file structure is chosen a directory service is also needed this allows files to be organized in a hierarchical fashion this organization is useful to the user in keeping track of files and is useful to the file management system in providing access control and other services to users file records even when of fixed size generally do not conform to the size of a physical disk block accordingly some sort of blocking strategy is needed a tradeoff among complexity performance and space utilization determines the blocking strategy to be used a key function of any file management scheme is the management of disk space part of this function is the strategy for allocating disk blocks to a file a variety of methods have been employed and a variety of data structures have been used to keep track of the allocation for each file in addition the space on disk that has not been allocated must be managed this latter function primarily consists of maintaining a disk allocation table indicating which blocks are free recommended reading there are a number of good books on file structures and file management the following all focus on file management systems but also address related os issues perhaps the most useful is wied which takes a quantitative approach to file management and deals with all of the issues raised in figure from disk scheduling to file structure venu presents an objectoriented design approach toward file structure implementation liva emphasizes file structures providing a good and lengthy survey with comparative performance analyses gros provides a balanced look at issues relating to both file io and file access methods it also contains general descriptions of all of the control structures needed by a file system these provide a useful checklist in assessing a file system design folk emphasizes the processing of files addressing such issues as maintenance searching and sorting and sharing come provides a thorough discussion of btrees corm and knut also include good treatments the linux file system is examined in detail in love and bove a good overview is rubi cust provides a good overview of the nt file system naga covers the material in more detail bove bovet d and cesati m understanding the linux kernel sebastopol ca oreilly come comer d the ubiquitous btree computing surveys june corm cormen t et al introduction to algorithms cambridge ma mit press cust custer h inside the windows nt file system redmond wa microsoft press folk folk m and zoellick b file structures an objectoriented approach with c reading ma addisonwesley gros grosshans d file systems design and implementation englewood cliffs nj prentice hall knut knuth d the art of computer programming volume sorting and searching reading ma addisonwesley liva livadas p file structures theory and practice englewood cliffs nj prentice hall love love r linux kernel development upper saddle river nj addisonwesley naga nagar r windows nt file system internals sebastopol ca oreilly rubi rubini a the virtual file system in linux linux journal may venu venugopal k files structures using c new york mcgrawhill wied wiederhold g file organization for database design new york mcgrawhill key terms review questions and problems key terms access method file allocation inode bit table file allocation table key field block file directory pathname chained file allocation file management system pile contiguous file allocation file name record database hashed file sequential file disk allocation table indexed file working directory field indexed file allocation file indexed sequential file review questions what is the difference between a field and a record what is the difference between a file and a database what is a file management system what criteria are important in choosing a file organization list and briefly define five file organizations why is the average search time to find a record in a file less for an indexed sequential file than for a sequential file what are typical operations that may be performed on a directory what is the relationship between a pathname and a working directory what are typical access rights that may be granted or denied to a particular user for a particular file list and briefly define three blocking methods list and briefly define three file allocation methods problems define b block size r record size p size of block pointer f blocking factor expected number of records within a block give a formula for f for the three blocking methods depicted in figure one scheme to avoid the problem of preallocation versus waste or lack of contiguity is to allocate portions of increasing size as the file grows for example begin with a portion size of one block and double the portion size for each allocation consider a file of n records with a blocking factor of f and suppose that a simple onelevel index is used as a file allocation table a give an upper limit on the number of entries in the file allocation table as a function of f and n b what is the maximum amount of the allocated file space that is unused at any time what file organization would you choose to maximize efficiency in terms of speed of access use of storage space and ease of updating addingdeletingmodifying when the data are a updated infrequently and accessed frequently in random order b updated frequently and accessed in its entirety relatively frequently c updated frequently and accessed frequently in random order for the btree in figure c show the result of inserting the key an alternative algorithm for insertion into a btree is the following as the insertion algorithm travels down the tree each full node that is encountered is immediately split even though it may turn out that the split was unnecessary a what is the advantage of this technique b what are the disadvantages both the search and the insertion time for a btree are a function of the height of the tree we would like to develop a measure of the worstcase search or insertion time consider a btree of degree d that contains a total of n keys develop an inequality that shows an upper bound on the height h of the tree as a function of d and n ignoring overhead for directories and file descriptors consider a file system in which files are stored in blocks of k bytes for each of the following file sizes calculate the percentage of wasted file space due to incomplete filling of the last block bytes bytes bytes what are the advantages of using directories directories can be implemented either as special files that can only be accessed in limited ways or as ordinary data files what are the advantages and disadvantages of each approach some operating systems have a treestructured file system but limit the depth of the tree to some small number of levels what effect does this limit have on users how does this simplify file system design if it does consider a hierarchical file system in which free disk space is kept in a free space list a suppose the pointer to free space is lost can the system reconstruct the free space list b suggest a scheme to ensure that the pointer is never lost as a result of a single memory failure in unix system v the length of a block is kbyte and each block can hold a total of block addresses using the inode scheme what is the maximum size of a file consider the organization of a unix file as represented by the inode figure assume that there are direct block pointers and a singly doubly and triply indirect pointer in each inode further assume that the system block size and the disk sector size are both k if the disk block pointer is bits with bits to identify the physical disk and bits to identify the physical block then a what is the maximum file size supported by this system b what is the maximum file system partition supported by this system c assuming no information other than that the file inode is already in main memory how many disk accesses are required to access the byte in position chapter embedded operating systems embedded systems characteristics of embedded operating systems adapting an existing commercial operating system purposebuilt embedded operating system ecos configurability ecos components ecos scheduler ecos thread synchronization tinyos wireless sensor networks tinyos goals tinyos components tinyos scheduler example configuration tinyos resource interface recommended reading and web sites key terms review questions and problems in brief the conventional arguments that bird brains are too small or do not have particular structures needed for intelligence are based on ignorance of brains in general and bird brains in particular it is unwarranted to argue that the small brains and small bodies of birds render them less capable of behaving with intelligent awareness than animals with large brains and large bodies the human nature of birds theodore barber learning objectives after studying this chapter you should be able to explain the concept of embedded system understand the characteristics of embedded operating systems describe the architecture and key features of ecos describe the architecture and key features of tinyos in this chapter we examine one of the most important and widely used categories of operating systems embedded operating systems the embedded system environment places unique and demanding requirements on the os and calls for design strategies quite different than that found in ordinary operating systems we begin with an overview of the concept of embedded systems and then turn to an examination of the principles of embedded operating systems finally this chapter surveys two very different approaches to embedded os design embedded systems the term embedded system refers to the use of electronics and software within a product as opposed to a generalpurpose computer such as a laptop or desktop system the following is a good general definition embedded system a combination of computer hardware and software and perhaps additional mechanical or other parts designed to perform a dedicated function in many cases embedded systems are part of a larger system or product as in the case of an antilock braking system in a car embedded systems far outnumber generalpurpose computer systems encompassing a broad range of applications table these systems have widely varying requirements and constraints such as the following grim small to large systems implying very different cost constraints thus different needs for optimization and reuse michael barr embedded systems glossary netrino technical library httpwwwnetrinocomembeddedsystemsglossary table examples of embedded systems and their markets noer market embedded device automotive ignition system engine control brake system consumer electronics cell phones mp players ebook readers digital and analog televisions settop boxes dvds vcrs cable boxes kitchen appliances refrigerators toasters microwave ovens automobiles toysgames telephonescell phonespagers cameras global positioning systems industrial control robotics and controls systems for manufacturing sensors medical infusion pumps dialysis machines prosthetic devices cardiac monitors office automation fax machine photocopier printers monitors scanners relaxed to very strict requirements and combinations of different quality requirements for example with respect to safety reliability realtime flexibility and legislation short to long lifetimes different environmental conditions in terms of for example radiation vibrations and humidity different application characteristics resulting in static versus dynamic loads slow to fast speed compute versus interface intensive tasks andor combinations thereof different models of computation ranging from discreteevent systems to those involving continuous time dynamics usually referred to as hybrid systems often embedded systems are tightly coupled to their environment this can give rise to realtime constraints imposed by the need to interact with the environment constraints such as required speeds of motion required precision of measurement and required time durations dictate the timing of software operations if multiple activities must be managed simultaneously this imposes more complex realtime constraints software auxiliary systems fpga memory power asic cooling human processor diagnostic interface port ad da conversion conversion electromechanical backup and safety sensors actuators external environment figure possible organization of an embedded system figure based on koop shows in general terms an embedded system organization in addition to the processor and memory there are a number of elements that differ from the typical desktop or laptop computer there may be a variety of interfaces that enable the system to measure manipulate and otherwise interact with the external environment the human interface may be as simple as a flashing light or as complicated as realtime robotic vision the diagnostic port may be used for diagnosing the system that is being controlled not just for diagnosing the embedded computer specialpurpose field programmable fpga application specific asic or even nondigital hardware may be used to increase performance or safety software often has a fixed function and is specific to the application characteristics of embedded operating systems a simple embedded system with simple functionality may be controlled by a specialpurpose program or set of programs with no other software typically more complex embedded systems include an os although it is possible in principle to use a generalpurpose os such as linux for an embedded system constraints of memory space power consumption and realtime requirements typically dictate the use of a specialpurpose os designed for the embedded system environment the following are some of the unique characteristics and design requirements for embedded operating systems realtime operation in many embedded systems the correctness of a computation depends in part on the time at which it is delivered often realtime constraints are dictated by external io and control stability requirements reactive operation embedded software may execute in response to external events if these events do not occur periodically or at predictable intervals the embedded software may need to take into account worstcase conditions and set priorities for execution of routines configurability because of the large variety of embedded systems there is a large variation in the requirements both qualitative and quantitative for embedded os functionality thus an embedded os intended for use on a variety of embedded systems must lend itself to flexible configuration so that only the functionality needed for a specific application and hardware suite is provided marw gives the following examples the linking and loading functions can be used to select only the necessary os modules to load conditional compilation can be used if an objectoriented structure is used proper subclasses can be defined however verification is a potential problem for designs with a large number of derived tailored operating systems takada cites this as a potential problem for ecos taka io device flexibility there is virtually no device that needs to be supported by all versions of the os and the range of io devices is large marw suggests that it makes sense to handle relatively slow devices such as disks and network interfaces by using special tasks instead of integrating their drives into the os kernel streamlined protection mechanisms embedded systems are typically designed for a limited welldefined functionality untested programs are rarely added to the software after the software has been configured and tested it can be assumed to be reliable thus apart from security measures embedded systems have limited protection mechanisms for example io instructions need not be privileged instructions that trap to the os tasks can directly perform their own io similarly memory protection mechanisms can be minimized marw provides the following example let switch correspond to the memorymapped io address of a value that needs to be checked as part of an io operation we can allow the io program to use an instruction such as load register switch to determine the current value this approach is preferable to the use of an os service call which would generate overhead for saving and restoring the task context direct use of interrupts generalpurpose operating systems typically do not permit any user process to use interrupts directly marw lists three reasons why it is possible to let interrupts directly start or stop tasks eg by storing the tasks start address in the interrupt vector address table rather than going through os interrupt service routines embedded systems can be considered to be thoroughly tested with infrequent modifications to the os or application code protection is not necessary as discussed in the preceding bullet item and efficient control over a variety of devices is required there are two general approaches to developing an embedded os the first approach is to take an existing os and adapt it for the embedded application the other approach is to design and implement an os intended solely for embedded use adapting an existing commercial operating system an existing commercial os can be used for an embedded system by adding realtime capability streamlining operation and adding necessary functionality this approach typically makes use of linux but freebsd windows and other generalpurpose operating systems have also been used such operating systems are typically slower and less predictable than a specialpurpose embedded os an advantage of this approach is that the embedded os derived from a commercial generalpurpose os is based on a set of familiar interfaces which facilitates portability the disadvantage of using a generalpurpose os is that it is not optimized for realtime and embedded applications thus considerable modification may be required to achieve adequate performance in particular a typical os optimizes for the average case rather than the worst case for scheduling usually assigns resources on demand and ignores most if not all semantic information about an application purposebuilt embedded operating system a significant number of operating systems have been designed from the ground up for embedded applications two prominent examples of this latter approach are ecos and tinyos both of which are discussed in this chapter typical characteristics of a specialized embedded os include the following has a fast and lightweight process or thread switch scheduling policy is real time and dispatcher module is part of scheduler instead of separate component has a small size responds to external interrupts quickly typical requirement is response time of less than s minimizes intervals during which interrupts are disabled provides fixed or variablesized partitions for memory management as well as the ability to lock code and data in memory provides special sequential files that can accumulate data at a fast rate to deal with timing constraints the kernel provides bounded execution time for most primitives maintains a realtime clock much of the discussion in the remainder of section is based on course notes on embedded systems from prof rajesh gupta university of california at san diego provides for special alarms and timeouts supports realtime queuing disciplines such as earliest deadline first and primitives for jamming a message into the front of a queue provides primitives to delay processing by a fixed amount of time and to suspendresume execution the characteristics just listed are common in embedded operating systems with realtime requirements however for complex embedded systems the requirement may emphasize predictable operation over fast operation necessitating different design decisions particularly in the area of task scheduling ecos the embedded configurable operating system ecos is an open source royaltyfree realtime os intended for embedded applications the system is targeted at highperformance small embedded systems for such systems an embedded form of linux or other commercial os would not provide the streamlined software required the ecos software has been implemented on a wide variety of processor platforms including intel ia powerpc sparc arm calmrisc mips and nec vxx it is one of the most widely used embedded operating systems it is implemented in cc configurability an embedded os that is flexible enough to be used in a wide variety of embedded applications and on a wide variety of embedded platforms must provide more functionality than will be needed for any particular application and platform for example many realtime operating systems support task switching concurrency controls and a variety of priority scheduling mechanisms a relatively simple embedded system would not need all these features the challenge is to provide an efficient userfriendly mechanism for configuring selected components and for enabling and disabling particular features within components the ecos configuration tool which runs on windows or linux is used to configure an ecos package to run on a target embedded system the complete ecos package is structured hierarchically making it easy using the configuration tool to assemble a target configuration at a top level ecos consists of a number of components and the configuration user may select only those components needed for the target application for example a system might have a particular serial io device the configuration user would select serial io for this configuration then select one or more specific io devices to be supported the configuration tool would include the minimum necessary software for that support the configuration user can also select specific parameters such as default data rate and the size of io buffers to be used this configuration process can be extended down to finer levels of detail even to the level of individual lines of code for example the configuration tool provides the option of including or omitting a priority inheritance protocol figure shows the top level of the ecos configuration tool as seen by the tool user each of the items on the list in the lefthand window can be selected or deselected when an item is highlighted the lower righthand window provides a description and the upper righthand window provides a link to further documentation plus additional information about the highlighted item items on the list can be expanded to provide a finergrained menu of options figure illustrates an expansion of the ecos kernel option in this figure note that exception handling has been selected for inclusion but smp symmetric multiprocessing has been omitted in general components and individual options can be selected or omitted in some cases individual values can be set for example a minimum acceptable stack size is an integer value that can be set or left to a default value figure shows a typical example of the overall process of creating the binary image to execute in the embedded system this process is run on a source system such as a windows or linux platform and the executable image is destined to execute on a target embedded system such as a sensor in an industrial environment at the highest software level is the application source code for the particular embedded application this code is independent of ecos but makes use of application programming interfaces api to sit on top of the ecos software there may be only one version of the application source code or there may be variations for different versions of the target embedded platform in this example the gnu make utility is used to selectively determine which pieces of a program figure ecos configuration tool top level figure ecos configuration tool kernel details gnu make utility application source code gnu cross compiler ecos kernel libraries target architecture gnu linker libraries executable file figure loading an ecos configuration need to be compiled or recompiled in the case of a modified version of the source code and issues the commands to recompile them the gnu cross compiler executing on the source platform then generates the binary executable code for the target embedded platform the gnu linker links the application object code with the code generated by the ecos configuration tool this latter set of software includes selected portions of the ecos kernel plus selected software for the target embedded system the result can then be loaded into the target system ecos components a key design requirement for ecos is portability to different architectures and platforms with minimal effort to meet this requirement ecos consists of a layered set of components figure hardware abstraction layer at the bottom is the hardware abstraction layer hal the hal is software that presents a consistent api to the upper layers and maps upperlayer operations onto a specific hardware platform thus the hal is different for each hardware platform figure is an example that demonstrates how the hal abstracts hardwarespecific implementations for the same api call on two different platforms as this example shows the call from an upper layer to enable interrupts is the same on both platforms but the c code implementation of the function is specific to each platform the hal is implemented as three separate modules architecture defines the processor family type this module contains the code necessary for processor startup interrupt delivery context switching and other functionality specific to the instruction set architecture of that processor family variant supports the features of the specific processor in the family an example of a supported feature is an onchip module such as a memory management unit mmu platform extends the hal support to tightly coupled peripherals like interrupt controllers and timer devices this module defines the platform or board that includes the selected processor architecture and variant it includes code for startup chip selection configuration interrupt controllers and timer devices user application code standard c library io system device drivers kernel hardware abstraction layer figure ecos layered structure define halenableinterrupts asm volatile mrs r cpsr bic r r xc mrs cpsr r r a arm architecture define halenableinterrupts cygmacrostart cyguint tmp tmp asm volatile mfmsr ori x rwimi mtmsr r tmp r tmp cygmacroend b powerpc architecture figure two implementations of halenableinterrupts macro note that the hal interface can be directly used by any of the upper layers promoting efficient code tinyos the ecos system provides a more streamlined approach for an embedded os than one based on a commercial generalpurpose os such as an embedded version of linux thus ecos and similar systems are better suited for small embedded systems with tight requirements on memory processing time realtime response power consumption and so on tinyos takes the process of streamlining to a much further point resulting in a very minimal os for embedded systems the core os requires bytes of code and data memory combined tinyos represents a significant departure from other embedded operating systems one striking difference is that tinyos is not a realtime os the reason for this is the expected workload which is in the context of a wireless sensor network as described in the next subsection because of power consumption these devices are off most of the time applications tend to be simple with processor contention not much of an issue additionally in tinyos there is no kernel as there is no memory protection and it is a componentbased os there are no processes the os itself does not have a memory allocation system although some rarely used components do introduce one interrupt and exception handling is dependent on the peripheral and it is completely nonblocking so there are few explicit synchronization primitives tinyos has become a popular approach to implementing wireless sensor network software currently over organizations are developing and contributing to an open source standard for tiny os wireless sensor networks tinyos was developed primarily for use with networks of small wireless sensors a number of trends have enabled the development of extremely compact lowpower sensors the wellknown moores law continues to drive down the size of memory and processing logic elements smaller size in turn reduces power consumption low power and small size trends are also evident in wireless communications hardware microelectromechanical sensors mems and transducers as a result it is possible to develop an entire sensor complete with logic in a cubic millimeter the application and system software must be compact enough that sensing communication and computation capabilities can be incorporated into a complete but tiny architecture lowcost smallsize lowpowerconsuming wireless sensors can be used in a host of applications rome figure shows a typical configuration a base wired link internet wireless link sensor sensor and relay base host pc station sensor and relay sensor sensor and relay sensor and relay sensor figure typical wireless sensor network topology station connects the sensor network to a host pc and passes on sensor data from the network to the host pc which can do data analysis andor transmit the data over a corporate network or internet to an analysis server individual sensors collect data and transmit these to the base station either directly or through sensors that act as data relays routing functionality is needed to determine how to relay the data through the sensor network to the base station buon points out that in many applications the user will want to be able to quickly deploy a large number of lowcost devices without having to configure or manage them this means that they must be capable of assembling themselves into an ad hoc network the mobility of individual sensors and the presence of rf interference means that the network will have to be capable of reconfiguring itself in a matter of seconds tinyos goals with the tiny distributed sensor application in mind a group of researchers from uc berkeley hill set the following goals for tinyos allow high concurrency in a typical wireless sensor network application the devices are concurrency intensive several different flows of data must be kept moving simultaneously while sensor data are input in a steady stream processed results must be transmitted in a steady stream in addition external controls from remote sensors or base stations must be managed operate with limited resources the target platform for tinyos will have limited memory and computational resources and run on batteries or solar power a single platform may offer only kilobytes of program memory and hundreds of bytes of ram the software must make efficient use of the available processor and memory resources while enabling lowpower communication adapt to hardware evolution most hardware is in constant evolution applications and most system services must be portable across hardware generations thus it should be possible to upgrade the hardware with little or no software change if the functionality is the same support a wide range of applications applications exhibit a wide range of requirements in terms of lifetime communication sensing and so on a modular generalpurpose embedded os is desired so that a standardized approach leads to economies of scale in developing applications and support software support a diverse set of platforms as with the preceding point a generalpurpose embedded os is desirable be robust once deployed a sensor network must run unattended for months or years ideally there should be redundancy both within a single system and across the network of sensors however both types of redundancy require additional resources one software characteristic that can improve robustness is to use highly modular standardized software components it is worth elaborating on the concurrency requirement in a typical application there will be dozens hundreds or even thousands of sensors networked together usually little buffering is done because of latency issues for example if you are sampling every minutes and want to buffer four samples before sending the average latency is minutes thus information is typically captured processed and streamed onto the network in a continuous flow further if the sensor sampling produces a significant amount of data the limited memory space available limits the number of samples that could be buffered even so in some applications each of the flows may involve a large number of lowlevel events interleaved with higherlevel processing some of the highlevel processing will extend over multiple realtime events further sensors in a network because of the low power of transmission available typically operate over a short physical range thus data from outlying sensors must be relayed to one or more base stations by intermediate nodes tinyos components an embedded software system built using tinyos consists of a set of small modules called components each of which performs a simple task or set of tasks and which interface with each other and with hardware in limited and welldefined ways the only other software module is the scheduler discussed subsequently in fact because there is no kernel there is no actual os but we can take the following view the application area of interest is the wireless sensor network wsn to meet the demanding software requirements of this application a rigid simplified software architecture is dictated consisting of components the tinyos development community has implemented a number of opensource components that provide the basic functions needed for the wsn application examples of such standardized components include singlehop networking adhoc routing power management timers and nonvolatile storage control for specific configurations and applications users build additional specialpurpose components and link and load all of the components needed for the users application tinyos then consists of a suite of standardized components some but not all of these components are used together with applicationspecific userwritten components for any given implementation the os for that implementation is simply the set of standardized components from the tinyos suite all components in a tinyos configuration have the same structure an example of which is shown in figure a the shaded box in the diagram indicates the component which is treated as an object that can only be accessed by defined interfaces indicated by white boxes a component may be hardware or software software components are implemented in nesc which is an extension of c with two distinguishing features a programming model where components interact via interfaces and an eventbased concurrency model with runtocompletion task and interrupt handlers explained subsequently the architecture consists of a layered arrangement of components each component can link to only two other components one below it in the hierarchy and one above it a component issues commands to its lowerlevel component and receives event signals from it similarly the component accepts commands from its upperlevel module timerm stdcontrol timer provides interface stdcontrol timerm interface timer clock uses interface clock as clk a timerm component stdcontrol timer configuration timerc provides interface stdcontrol interface timer stdcontrol timer timerm implementation clock components timerm hwclock stdcontrol timermstdcontrol timer timermtimer timermclk hwclockclock clock hwclock b timerc configuration figure example component and configuration component and issues event signals to it at the bottom of the hierarchy are hardware components and at the top of the hierarchy are application components which may not be part of the standardized tinyos suite but which must conform to the tinyos component structure a software component implements one or more tasks each task in a component is similar to a thread in an ordinary os with certain limitations within a component tasks are atomic once a task has started it runs to completion it can not be preempted by another task in the same component and there is no time slicing however a task can be preempted by an event a task can not block or spin wait these limitations greatly simplify the scheduling and management of tasks within a component there is only a single stack assigned to the currently running task tasks can perform computations call lowerlevel components commands and signal higherlevel events and schedule other tasks commands are nonblocking requests that is a task that issues a command does not block or spin wait for a reply from the lowerlevel component a command is typically a request for the lowerlevel component to perform some service such as initiating a sensor reading the effect on the component that receives the command is specific to the command given and the task required to satisfy the command generally when a command is received a task is scheduled for later execution because a command can not preempt the currently running task the command returns immediately to the calling component at a later time an event will signal completion to the calling component thus a command does not cause a preemption in the called component and does not cause blocking in the calling component events in tinyos may be tied either directly or indirectly to hardware events the lowestlevel software components interface directly to hardware interrupts which may be external interrupts timer events or counter events an event handler in a lowestlevel component may handle the interrupt itself or may propagate event messages up through the component hierarchy a command can post a task that will signal an event in the future in this case there is no tie of any kind to a hardware event a task can be viewed as having three phases a caller posts a command to a module the module then runs the requested task the module then notifies the caller via an event that the task is complete the component depicted in figure a timerm is part of the tinyos timer service this component provides the stdcontrol and timer interface and uses a clock interface providers implement commands ie the logic in this component users implement events ie external to the component many tinyos components use the stdcontrol interface to be initialized started or stopped timerm provides the logic that maps from a hardware clock into tinyoss timer abstraction the timer abstraction can be used for counting down a given time interval figure a also shows the formal specification of the timerm interfaces the interfaces associated with timerm are specified as follows interface stdcontrol command resultt init command resultt start command resultt stop interface timer command resultt startchar type uintt interval command resultt stop event resultt fired interface clock command resultt setratechar interval char scale event resultt fire components are organized into configurations by wiring them together at their interfaces and equating the interfaces of the configuration with some of the interfaces of the components a simple example is shown in figure b the uppercase c stands for component it is used to distinguish between an interface eg timer and a component that provides the interface eg timercthe uppercase m stands for module this naming convention is used when a single logical component has both a configuration and a module the timerc component providing the timer interface is a configuration that links its implementation timerm to clock and led providers otherwise any user of timerc would have to explicitly wire its subcomponents tinyos scheduler the tinyos scheduler operates across all components virtually all embedded systems using tinyos will be uniprocessor systems so that only one task among all the tasks in all the components may execute at a time the scheduler is a separate component it is the one portion of tinyos that must be present in any system the default scheduler in tinyos is a simple fifo firstinfirstout queue a task is posted to the scheduler place in the queue either as a result of an event which triggers the posting or as a result of a specific request by a running task to schedule another task the scheduler is power aware this means that the scheduler puts the processor to sleep when there are no tasks in the queue the peripherals remain operating so that one of them can wake up the system by means of a hardware event signaled to a lowestlevel component once the queue is empty another task can be scheduled only as a result of a direct hardware event this behavior enables efficient battery usage the scheduler has gone through two generations in tinyos x there is a shared task queue for all tasks and a component can post a task to the scheduler multiple times if the task queue is full the post operation fails experience with networking stacks showed this to be problematic as the task might signal completion of a splitphase operation if the post fails the component above might block forever waiting for the completion event in tinyos x every task has its own reserved slot in the task queue and a task can only be posted once a post fails if and only if the task has already been posted if a component needs to post a task multiple times it can set an internal state variable so that when the task executes it reposts itself this slight change in semantics greatly simplifies a lot of component code rather than test to see if a task is posted already before posting it a component can just post the task components do not have to try to recover from failed posts and retry the cost is one byte of state per task a user can replace the default scheduler with one that uses a different dispatching scheme such as a prioritybased scheme or a deadline scheme however preemption and time slicing should not be used because of the overhead such systems generate more importantly they violate the tinyos concurrency model which assumes tasks do not preempt each other example configuration figure shows a configuration assembled from software and hardware components this simplified example called surge and described in gay performs surgem timer sendmsg leds adc photo timer multihop leds clock sendmsg hwclock queuseendd receivemsg genceorimcm a simplified view of the surge application stdcontrol main surgem stdcontrol adc timer sndmsg leds stdcontrol adc stdcontrol timer stdcontrol sndmsg leds photo timerc multihop ledsc b toplevel surge configuration led lightemitting diode adc analogtodigital converter figure examples tinyos application periodic sensor sampling and uses adhoc multihop routing over the wireless network to deliver samples to the base station the upper part of the figure shows the components of surge represented by boxes and the interfaces by which they are wired represented by arrowed lines the surgem component is the applicationlevel component that orchestrates the operation of the configuration figure b shows a portion of the configuration for the surge application the following is a simplified excerpt from the surgem specification module surgem provides interface stdcontrol uses interface adc uses interface timer uses interface sendmsg uses interface leds implementation uintt sensorreading command resultt stdcontrolinit return call timerstarttimerrepeat event resultt timerfired call adcgetdata return success event resultt adcdatareadyuintt data sensorreading data send message with data in it return success this example illustrates the strength of the tinyos approach the software is organized as an interconnected set of simple modules each of which defines one or a few tasks components have simple standardized interfaces to other components be they hardware or software thus components can easily be replaced components can be hardware or software with a boundary change not visible to the application programmer tinyos resource interface tinyos provides a simple but powerful set of conventions for dealing with resources three abstractions for resources are used in tinyos dedicated a resource that a subsystem needs exclusive access to at all times in this class of resources no sharing policy is needed since only a single component ever requires use of the resource examples of dedicated abstractions include interrupts and counters virtualized every client of a virtualized resource interacts with it as if it were a dedicated resource with all virtualized instances being multiplexed on top of a single underlying resource the virtualized abstraction may be used when the underlying resource need not be protected by mutual exclusion an example is a clock or timer shared the shared resource abstraction provides access to a dedicated resource through an arbiter component the arbiter enforces mutual exclusion allowing only one user called a client at a time to have access to a resource and enabling the client to lock the resource in the remainder of this subsection we briefly define the shared resource facility of tinyos the arbiter determines which client has access to the resource at which time while a client holds a resource it has complete and unfettered control arbiters assume that clients are cooperative only acquiring the resource when needed and holding on to it no longer than necessary clients explicitly release resources there is no way for an arbiter to forcibly reclaim it figure shows a simplified view of the shared resource configuration used to provide access to an underlying resource associated with each resource to be shared is an arbiter component the arbiter enforces a policy that enables a client to lock resource resource resource resourcespecific requested configure interfaces resourcespecific interfaces shared resource arbiter resourcespecific info interfaces resource resource resource arbiter resourcespecific requested configure info interfaces arbiter dedicated resource figure shared resource configuration the resource use it and then release the resource the shared resource configuration provides the following interfaces to a client resource the client issues a request at this interface requesting access to the resource if the resource is currently locked the arbiter places the request in a queue when a client is finished with the resource it issues a release command at this interface resource requested this is similar to the resource interface in this case the client is able to hold on to a resource until the client is notified that someone else needs the resource resource configure this interface allows a resource to be automatically configured just before a client is granted access to it components providing the resourceconfigure interface use the interfaces provided by an underlying dedicated resource to configure it into one of its desired modes of operation resourcespecific interfaces once a client has access to a resource it uses resourcespecific interfaces to exchange data and control information with the resource in addition to the dedicated resource the shared resource configuration consists of two components the arbiter accepts requests for access and configuration from a client and enforces the lock on the underlying resource the shared resource component mediates data exchange between the client and the underlying resource arbiter information passed from the arbiter to the shared resource component controls the access of the client to the underlying resource recommended reading and web sites koop provides a systematic discussion of the requirements for embedded systems stan is a useful overview of realtime and embedded systems mass and ecos both provide a detailed description of ecos internals thom provides a brief overview with some code examples from the kernel larm gives a more detailed description of the ecos configuration process hill gives an overview and design rationale for tinyos gay is an interesting discussion of software design strategies using tinyos buon provides a good example of the use of tinyos in building a network or wireless sensors two excellent references for the current version of tinyos are gay and levi buon buonadonna p hill j and culler d active message communication for tiny networked sensors proceedings ieee infocom april ecos ecoscentric limited and red hat inc ecos reference manual http wwwecoscentriccomecosprodochtmlrefecosrefhtml gay gay d et al the nesc language a holistic approach to networked embedded systems proceedings of the acm sigplan conference on programming language design and implementation gay gay d levis p and culler d software design patterns for tinyos proceedings conference on languages compilers and tools for embedded systems hill hill j et al system architecture directions for networked sensors proceedings architectural support for programming languages and operating systems koop koopman p embedded system design issues the rest of the story proceedings international conference on computer design larm larmour j how ecos can be shrunk to fit embedded systems europe may wwwembeddedcomeuropeesemayhtm levi levis p et al t a second generation os for embedded sensor networks technical report tkn telecommunication networks group technische universitat berlin httpcslstanfordedupalpubshtml mass massa a embedded software development with ecos upper saddle river nj prentice hall stan stankovic j et al strategic directions in realtime and embedded systems acm computing surveys december thom thomas g ecos an operating system for embedded systems dr dobbs journal january recommended web sites embeddedcom wide variety of information on embedded systems ecos downloadable software information and links on ecos tinyos community forum downloadable software information and links on tinyos key terms review questions and problems key terms ecos embedded system embedded operating system tinyos review questions what is an embedded system what are some typical requirements or constraints on embedded systems what is an embedded os what are some of the key characteristics of an embedded os explain the relative advantages and disadvantages of an embedded os based on an existing commercial os compared to a purposebuilt embedded os what are the principal objectives that guided the design of the ecos kernel in ecos what is the difference between an interrupt service routine and a deferred service routine what concurrency mechanisms are available in ecos what is the target application for tinyos what are the design goals for tinyos what is a tinyos component what software comprises the tinyos operating system what is the default scheduling discipline for tinyos problems with reference to the device driver interface to the ecos kernel table it is recommended that device drivers should use the intsave variants to claim and release spinlocks rather than the nonintsave variants explain why also in table it is recommended that cygdrvspinlockspin should be used sparingly and in situations where deadlockslivelocks can not occur explain why in table what should be the limitations on the use of cygdrvspinlock destroy explain in table what limitations should be placed in the use of cygdrvmutex destroy why does the ecos bitmap scheduler not support time slicing the implementation of mutexes within the ecos kernel does not support recursive locks if a thread has locked a mutex and then attempts to lock the mutex again typically as a result of some recursive call in a complicated call graph then either an assertion failure will be reported or the thread will deadlock suggest a reason for this policy figure is a listing of code intended for use on the ecos kernel a explain the operation of the code assume thread b begins execution first and thread a begins to execute after some event occurs b what would happen if the mutex unlock and wait code execution in the call to cygcondwait on line were not atomic c why is the while loop on line needed the discussion of ecos spinlocks included an example showing why spinlocks should not be used on a uniprocessor system if two threads of different priorities can compete for the same spinlock explain why the problem still exists even if only threads of the same priority can claim the same spinlock tinyoss scheduler serves tasks in fifo order many other schedulers for tinyos have been proposed but none have caught on what characteristics of the sensornet domain might cause a lack of need for more complex scheduling a the tinyos resource interface does not allow a component that already has a request in the queue for a resource to make a second request suggest a reason b however the tinyos resource interface allows a component holding the resource lock to rerequest the lock this request is enqueued for a later grant suggest a reason for this policy hint what might cause there to be latency between one component releasing a lock and the next requester being granted it unsigned char bufferempty true cygmutext mutcondvar cygcondt condvar void threada cygaddrwordt index while run this thread forever acquire data into the buffer there is data in the buffer now bufferempty false cygmutexlock mutcondvar cygcondsignal condvar cygmutexunlock mutcondvar void threadb cygaddrwordt index while run this thread forever cygmutexlock mutcondvar while bufferempty true cygcondwait condvar get the buffer data set flag to indicate the data in the buffer has been processed bufferempty true cygmutexunlock mutcondvar process the data in the buffer figure condition variable example code chapter computer security threats computer security concepts threats attacks and assets threats and attacks threats and assets intruders intruder behavior patterns intrusion techniques malicious software overview backdoor logic bomb trojan horse mobile code multiplethreat malware viruses worms and bots viruses worms bots rootkits rootkit installation systemlevel call attacks recommended reading and web sites key terms review questions and problems the art of war teaches us to rely not on the likelihood of the enemys not coming but on our own readiness to receive him not on the chance of his not attacking but rather on the fact that we have made our position unassailable the art of war sun tzu learning objectives after studying this chapter you should be able to list and explain the key concepts that comprise computer security understand the spectrum of computer security attacks distinguish among various types of intruder behavior patterns and understand the types of intrusion techniques used to breach computer security summarize the principal types of malicious software present an overview of viruses including typical virus structure and typical virus behavior understand the security threat posed by worms understand the security threat posed by bots explain key aspects of rootkits this chapter provides an overview of security threats we begin with a discussion of what we mean by computer security in essence computer security deals with computerrelated assets that are subject to a variety of threats and for which various measures are taken to protect those assets the remainder of the chapter looks at the two broad categories of computer and network security threats intruders and malicious software cryptographic algorithms such as encryption and hash functions play a role both in computer security threats and computer security techniques appendix k provides an overview of these algorithms computer security concepts the nist computer security handbook nist defines the term computer security as follows computer security the protection afforded to an automated information system in order to attain the applicable objectives of preserving the integrity availability and confidentiality of information system resources includes hardware software firmware informationdata and telecommunications this definition introduces three key objectives that are at the heart of computer security confidentiality this term covers two related concepts data confidentiality assures that private or confidential information is not made available or disclosed to unauthorized individuals privacy assures that individuals control or influence what information related to them may be collected and stored and by whom and to whom that information may be disclosed integrity this term covers two related concepts data integrity assures that information and programs are changed only in a specified and authorized manner system integrity assures that a system performs its intended function in an unimpaired manner free from deliberate or inadvertent unauthorized manipulation of the system availability assures that systems work promptly and service is not denied to authorized users these three concepts form what is often referred to as the cia triad figure the three concepts embody the fundamental security objectives for both data and for information and computing services for example the nist standard fips standards for security categorization of federal information and information systems lists confidentiality integrity and availability as the three security objectives for information and for information systems fips pub provides a useful characterization confidentiality data integrity and services availability figure the security requirements triad rfc internet security glossary defines information as facts and ideas which can be represented encoded as various forms of data and data as information in a specific physical representation usually a sequence of symbols that have meaning especially a representation of information that can be processed or produced by a computer security literature typically does not make much of a distinction nor does this chapter of these three objectives in terms of requirements and the definition of a loss of security in each category confidentiality preserving authorized restrictions on information access and disclosure including means for protecting personal privacy and proprietary information a loss of confidentiality is the unauthorized disclosure of information integrity guarding against improper information modification or destruction including ensuring information nonrepudiation and authenticity a loss of integrity is the unauthorized modification or destruction of information availability ensuring timely and reliable access to and use of information a loss of availability is the disruption of access to or use of information or an information system although the use of the cia triad to define security objectives is well established some in the security field feel that additional concepts are needed to present a complete picture two of the most commonly mentioned are as follows authenticity the property of being genuine and being able to be verified and trusted confidence in the validity of a transmission a message or message originator this means verifying that users are who they say they are and that each input arriving at the system came from a trusted source accountability the security goal that generates the requirement for actions of an entity to be traced uniquely to that entity this supports nonrepudiation deterrence fault isolation intrusion detection and prevention and afteraction recovery and legal action because truly secure systems arent yet an achievable goal we must be able to trace a security breach to a responsible party systems must keep records of their activities to permit later forensic analysis to trace security breaches or to aid in transaction disputes note that fips pub includes authenticity under integrity threats attacks and assets we turn now to a look at threats attacks and assets as related to computer security threats and attacks table based on rfc describes four kinds of threat consequences and lists the kinds of attacks that result in each consequence unauthorized disclosure is a threat to confidentiality the following types of attacks can result in this threat consequence exposure this can be deliberate as when an insider intentionally releases sensitive information such as credit card numbers to an outsider it can also be the result of a human hardware or software error which results in an entity gaining unauthorized knowledge of sensitive data there have been numerous instances table threat consequences and the types of threat actions that cause each consequence based on rfc threat consequence threat action attack unauthorized disclosure exposure sensitive data are directly released to an a circumstance or event whereby an entity unauthorized entity gains access to data for which the entity is not interception an unauthorized entity directly authorized accesses sensitive data traveling between authorized sources and destinations inference a threat action whereby an unauthorized entity indirectly accesses sensitive data but not necessarily the data contained in the communication by reasoning from characteristics or byproducts of communications intrusion an unauthorized entity gains access to sensitive data by circumventing a systems security protections deception masquerade an unauthorized entity gains access to a circumstance or event that may result in a system or performs a malicious act by posing as an an authorized entity receiving false data and authorized entity believing it to be true falsification false data deceive an authorized entity repudiation an entity deceives another by falsely denying responsibility for an act disruption incapacitation prevents or interrupts system operaa circumstance or event that interrupts or tion by disabling a system component prevents the correct operation of system services corruption undesirably alters system operation by and functions adversely modifying system functions or data obstruction a threat action that interrupts delivery of system services by hindering system operation usurpation misappropriation an entity assumes unauthorized a circumstance or event that results in control of logical or physical control of a system resource system services or functions by an unauthorized misuse causes a system component to perform a funcentity tion or service that is detrimental to system security of this such as universities accidentally posting student confidential information on the web interception interception is a common attack in the context of communications on a shared local area network lan such as a wireless lan or a broadcast ethernet any device attached to the lan can receive a copy of packets intended for another device on the internet a determined hacker can gain access to email traffic and other data transfers all of these situations create the potential for unauthorized access to data inference an example of inference is known as traffic analysis in which an adversary is able to gain information from observing the pattern of traffic on a network such as the amount of traffic between particular pairs of hosts on the network another example is the inference of detailed information from a database by a user who has only limited access this is accomplished by repeated queries whose combined results enable inference intrusion an example of intrusion is an adversary gaining unauthorized access to sensitive data by overcoming the systems access control protections deception is a threat to either system integrity or data integrity the following types of attacks can result in this threat consequence masquerade one example of masquerade is an attempt by an unauthorized user to gain access to a system by posing as an authorized user this could happen if the unauthorized user has learned another users logon id and password another example is malicious logic such as a trojan horse that appears to perform a useful or desirable function but actually gains unauthorized access to system resources or tricks a user into executing other malicious logic falsification this refers to the altering or replacing of valid data or the introduction of false data into a file or database for example a student may alter his or her grades on a school database repudiation in this case a user either denies sending data or a user denies receiving or possessing the data disruption is a threat to availability or system integrity the following types of attacks can result in this threat consequence incapacitation this is an attack on system availability this could occur as a result of physical destruction of or damage to system hardware more typically malicious software such as trojan horses viruses or worms could operate in such a way as to disable a system or some of its services corruption this is an attack on system integrity malicious software in this context could operate in such a way that system resources or services function in an unintended manner or a user could gain unauthorized access to a system and modify some of its functions an example of the latter is a user placing backdoor logic in the system to provide subsequent access to a system and its resources by other than the usual procedure obstruction one way to obstruct system operation is to interfere with communications by disabling communication links or altering communication control information another way is to overload the system by placing excess burden on communication traffic or processing resources usurpation is a threat to system integrity the following types of attacks can result in this threat consequence misappropriation this can include theft of service an example is a distributed denial of service attack when malicious software is installed on a number of hosts to be used as platforms to launch traffic at a target host in this case the malicious software makes unauthorized use of processor and operating system resources misuse misuse can occur either by means of malicious logic or a hacker that has gained unauthorized access to a system in either case security functions can be disabled or thwarted threats and assets the assets of a computer system can be categorized as hardware software data and communication lines and networks in this subsection we briefly describe these four categories and relate these to the concepts of integrity confidentiality and availability introduced in section see figure and table hardware a major threat to computer system hardware is the threat to availability hardware is the most vulnerable to attack and the least susceptible to automated controls threats include accidental and deliberate damage to equipment as well as theft the proliferation of personal computers and workstations and the widespread use of lans increase the potential for losses in this area theft of cdroms and dvds can lead to loss of confidentiality physical and administrative security measures are needed to deal with these threats software software includes the operating system utilities and application programs a key threat to software is an attack on availability software especially application software is often easy to delete software can also be altered or damaged to render it useless careful software configuration management which includes making backups of the most recent version of software can maintain high availability a more difficult problem to deal with is software modification that results in a program that still functions but that behaves differently than before which is a threat to integrityauthenticity computer viruses and related attacks fall into this category a final problem is protection against software piracy although computer system computer system sensitive files must be secure data file security data access to the data data must be must be controlled securely transmitted protection through networks network security processes representing users processes representing users guard guard access to the computer facility must be controlled user authentication users making requests figure scope of system security table computer and network assets with examples of threats availability confidentiality integrity hardware equipment is stolen or disabled thus denying service software programs are deleted an unauthorized copy a working program is modidenying access to of software is made fied either to cause it to fail users during execution or to cause it to do some unintended task data files are deleted an unauthorized read existing files are modified or denying access to of data is performed new files are fabricated users an analysis of statistical data reveals underlying data communication lines messages are messages are read messages are modified destroyed or deleted the traffic pattern of delayed reordered or duplicommunication lines messages is observed cated false messages are or networks are renfabricated dered unavailable certain countermeasures are available by and large the problem of unauthorized copying of software has not been solved data hardware and software security are typically concerns of computing center professionals or individual concerns of personal computer users a much more widespread problem is data security which involves files and other forms of data controlled by individuals groups and business organizations security concerns with respect to data are broad encompassing availability secrecy and integrity in the case of availability the concern is with the destruction of data files which can occur either accidentally or maliciously the obvious concern with secrecy is the unauthorized reading of data files or databases and this area has been the subject of perhaps more research and effort than any other area of computer security a less obvious threat to secrecy involves the analysis of data and manifests itself in the use of socalled statistical databases which provide summary or aggregate information presumably the existence of aggregate information does not threaten the privacy of the individuals involved however as the use of statistical databases grows there is an increasing potential for disclosure of personal information in essence characteristics of constituent individuals may be identified through careful analysis for example if one table records the aggregate of the incomes of respondents a b c and d and another records the aggregate of the incomes of a b c d and e the difference between the two aggregates would be the income of e this problem is exacerbated by the increasing desire to combine data sets in many cases matching several sets of data for consistency at different levels of aggregation requires access to individual units thus the individual units which are the subject of privacy concerns are available at various stages in the processing of data sets finally data integrity is a major concern in most installations modifications to data files can have consequences ranging from minor to disastrous communication lines and networks network security attacks can be classified as passive attacks and active attacks a passive attack attempts to learn or make use of information from the system but does not affect system resources an active attack attempts to alter system resources or affect their operation passive attacks are in the nature of eavesdropping on or monitoring of transmissions the goal of the attacker is to obtain information that is being transmitted two types of passive attacks are release of message contents and traffic analysis the concept of release of message contents is easily understood a telephone conversation an electronic mail message and a transferred file may contain sensitive or confidential information we would like to prevent an opponent from learning the contents of these transmissions traffic analysis is a more subtle form of passive attack suppose that we had a way of masking the contents of messages or other information traffic so that opponents even if they captured the message could not extract the information from the message the common technique for masking contents is encryption if we had encryption protection in place an opponent might still be able to observe the pattern of these messages the opponent could determine the location and identity of communicating hosts and could observe the frequency and length of messages being exchanged this information might be useful in guessing the nature of the communication that was taking place passive attacks are very difficult to detect because they do not involve any alteration of the data typically the message traffic is sent and received in an apparently normal fashion and neither the sender nor the receiver is aware that a third party has read the messages or observed the traffic pattern however it is feasible to prevent the success of these attacks usually by means of encryption thus the emphasis in dealing with passive attacks is on prevention rather than detection active attacks involve some modification of the data stream or the creation of a false stream and can be subdivided into four categories replay masquerade modification of messages and denial of service replay involves the passive capture of a data unit and its subsequent retransmission to produce an unauthorized effect a masquerade takes place when one entity pretends to be a different entity a masquerade attack usually includes one of the other forms of active attack for example authentication sequences can be captured and replayed after a valid authentication sequence has taken place thus enabling an authorized entity with few privileges to obtain extra privileges by impersonating an entity that has those privileges modification of messages simply means that some portion of a legitimate message is altered or that messages are delayed or reordered to produce an unauthorized effect for example a message stating allow john smith to read confidential file accounts is modified to say allow fred brown to read confidential file accounts the denial of service prevents or inhibits the normal use or management of communications facilities this attack may have a specific target for example an entity may suppress all messages directed to a particular destination eg the security audit service another form of service denial is the disruption of an entire network either by disabling the network or by overloading it with messages so as to degrade performance active attacks present the opposite characteristics of passive attacks whereas passive attacks are difficult to detect measures are available to prevent their success on the other hand it is quite difficult to prevent active attacks absolutely because to do so would require physical protection of all communications facilities and paths at all times instead the goal is to detect them and to recover from any disruption or delays caused by them because the detection has a deterrent effect it may also contribute to prevention intruders the concept of intruder was introduced in section gran lists the following examples of intrusion performing a remote root compromise of an email server defacing a web server guessing and cracking passwords copying a database containing credit card numbers viewing sensitive data including payroll records and medical information without authorization running a packet sniffer on a workstation to capture usernames and passwords using a permission error on an anonymous ftp server to distribute pirated software and music files dialing into an unsecured modem and gaining internal network access posing as an executive calling the help desk resetting the executives email password and learning the new password using an unattended loggedin workstation without permission intruder behavior patterns the techniques and behavior patterns of intruders are constantly shifting to exploit newly discovered weaknesses and to evade detection and countermeasures even so intruders typically follow one of a number of recognizable behavior patterns and these patterns typically differ from those of ordinary users in the following we look at three broad examples of intruder behavior patterns to give the reader some feel for the challenge facing the security administrator table based on radc summarizes the behavior hackers traditionally those who hack into computers do so for the thrill of it or for status the hacking community is a strong meritocracy in which status is determined by level of competence thus attackers often look for targets of opportunity and then share the information with others a typical example is a table some examples of intruder patterns of behavior a hacker select the target using ip lookup tools such as nslookup dig and others map network for accessible services using tools such as nmap identify potentially vulnerable services in this case pcanywhere brute force guess pcanywhere password install remote administration tool called dameware wait for administrator to log on and capture his or her password use that password to access remainder of network b criminal enterprise act quickly and precisely to make their activities harder to detect exploit perimeter through vulnerable ports use trojan horses hidden software to leave backdoors for reentry use sniffers to capture passwords do not stick around until noticed make few or no mistakes c internal threat create network accounts for themselves and their friends access accounts and applications they wouldnt normally use for their daily jobs email former and prospective employers conduct furtive instantmessaging chats visit web sites that cater to disgruntled employees such as fdcompanycom perform large downloads and file copying access the network during off hours breakin at a large financial institution reported in radc the intruder took advantage of the fact that the corporate network was running unprotected services some of which were not even needed in this case the key to the breakin was the pcanywhere application the manufacturer symantec advertises this program as a remote control solution that enables secure connection to remote devices but the attacker had an easy time gaining access to pcanywhere the administrator used the same threeletter username and password for the program in this case there was no intrusion detection system on the node corporate network the intruder was only discovered when a vice president walked into her office and saw the cursor moving files around on her windows workstation benign intruders might be tolerable although they do consume resources and may slow performance for legitimate users however there is no way in advance to know whether an intruder will be benign or malign consequently even for systems with no particularly sensitive resources there is a motivation to control this problem intrusion detection systems idss and intrusion prevention systems ipss of the type described in this chapter are designed to counter this type of hacker threat in addition to using such systems organizations can consider restricting remote logons to specific ip addresses andor use virtual private network technology one of the results of the growing awareness of the intruder problem has been the establishment of a number of computer emergency response teams certs these cooperative ventures collect information about system vulnerabilities and disseminate it to systems managers hackers also routinely read cert reports thus it is important for system administrators to quickly insert all software patches to discovered vulnerabilities unfortunately given the complexity of many it systems and the rate at which patches are released this is increasingly difficult to achieve without automated updating even then there are problems caused by incompatibilities resulting from the updated software hence the need for multiple layers of defense in managing security threats to it systems criminals organized groups of hackers have become a widespread and common threat to internetbased systems these groups can be in the employ of a corporation or government but often are loosely affiliated gangs of hackers typically these gangs are young often eastern european russian or southeast asian hackers who do business on the web ante they meet in underground forums with names like darkmarketorg and theftservicescom to trade tips and data and coordinate attacks a common target is a credit card file at an ecommerce server attackers attempt to gain root access the card numbers are used by organized crime gangs to purchase expensive items and are then posted to carder sites where others can access and use the account numbers this obscures usage patterns and complicates investigation whereas traditional hackers look for targets of opportunity criminal hackers usually have specific targets or at least classes of targets in mind once a site is penetrated the attacker acts quickly scooping up as much valuable information as possible and exiting idss and ipss can also be used for these types of attackers but may be less effective because of the quick inandout nature of the attack for ecommerce sites database encryption should be used for sensitive customer information especially credit cards for hosted ecommerce sites provided by an outsider service the ecommerce organization should make use of a dedicated server not used to support multiple customers and closely monitor the providers security services insider attacks insider attacks are among the most difficult to detect and prevent employees already have access to and knowledge of the structure and content of corporate databases insider attacks can be motivated by revenge or simply a feeling of entitlement an example of the former is the case of kenneth patterson fired from his position as data communications manager for american eagle outfitters patterson disabled the companys ability to process credit card purchases during days of the holiday season of as for a sense of entitlement there have always been many employees who felt entitled to take extra office supplies for home use but this now extends to corporate data an example is that of a vice president of sales for a stock analysis firm who quit and went to a competitor before she left she copied the customer database to take with her the offender reported feeling no animus toward her former employee she simply wanted the data because it would be useful to her although ids and ips facilities can be useful in countering insider attacks other more direct approaches are of higher priority examples include the following enforce least privilege only allowing access to the resources employees need to do their job set logs to see what users access and what commands they are entering protect sensitive resources with strong authentication upon termination delete employees computer and network access upon termination make a mirror image of employees hard drive before reissuing it that evidence might be needed if your company information turns up at a competitor intrusion techniques the objective of the intruder is to gain access to a system or to increase the range of privileges accessible on a system most initial attacks use system or software vulnerabilities that allow a user to execute code that opens a back door into the system intruders can get access to a system by exploiting attacks such as buffer overflows on a program that runs with certain privileges alternatively the intruder attempts to acquire information that should have been protected in some cases this information is in the form of a user password with knowledge of some other users password an intruder can log in to a system and exercise all the privileges accorded to the legitimate user password guessing and password acquisition techniques are discussed in chapter malicious software overview the concept of malicious software or malware was introduced in section malware is software designed to cause damage to or use up the resources of a target computer it is frequently concealed within or masquerades as legitimate software in some cases it spreads itself to other computers via email or infected discs the terminology in this area presents problems because of a lack of universal agreement on all of the terms and because some of the categories overlap table is a useful guide in this section we briefly survey some of the key categories of malicious software deferring discussion on the key topics of viruses worms bots and rootkits until the following sections backdoor a backdoor also known as a trapdoor is a secret entry point into a program that allows someone who is aware of the backdoor to gain access without going through the usual security access procedures programmers have used backdoors legitimately for many years to debug and test programs such a backdoor is called a maintenance hook this usually is done when the programmer is developing an application that has an authentication procedure or a long setup requiring the user to enter many different values to run the application to debug the program the table terminology of malicious programs name description virus malware that when executed tries to replicate itself into other executable code when it succeeds the code is said to be infected when the infected code is executed the virus also executes worm a computer program that can run independently and can propagate a complete working version of itself onto other hosts on a network logic bomb a program inserted into software by an intruder a logic bomb lies dormant until a predefined condition is met the program then triggers an unauthorized act trojan horse a computer program that appears to have a useful function but also has a hidden and potentially malicious function that evades security mechanisms sometimes by exploiting legitimate authorizations of a system entity that invokes the trojan horse program backdoor trapdoor any mechanism that bypasses a normal security check it may allow unauthorized access to functionality mobile code software eg script macro or other portable instruction that can be shipped unchanged to a heterogeneous collection of platforms and execute with identical semantics exploits code specific to a single vulnerability or set of vulnerabilities downloaders program that installs other items on a machine that is under attack usually a downloader is sent in an email autorooter malicious hacker tools used to break into new machines remotely kit virus generator set of tools for generating new viruses automatically spammer programs used to send large volumes of unwanted email flooders used to attack networked computer systems with a large volume of traffic to carry out a denialofservice dos attack keyloggers captures keystrokes on a compromised system rootkit set of hacker tools used after attacker has broken into a computer system and gained rootlevel access zombie bot program activated on an infected machine that is activated to launch attacks on other machines spyware software that collects information from a computer and transmits it to another system adware advertising that is integrated into software it can result in popup ads or redirection of a browser to a commercial site developer may wish to gain special privileges or to avoid all the necessary setup and authentication the programmer may also want to ensure that there is a method of activating the program should something be wrong with the authentication procedure that is being built into the application the backdoor is code that recognizes some special sequence of input or is triggered by being run from a certain user id or by an unlikely sequence of events backdoors become threats when unscrupulous programmers use them to gain unauthorized access the backdoor was the basic idea for the vulnerability portrayed in the movie war games another example is that during the development of multics penetration tests were conducted by an air force tiger team simulating adversaries one tactic employed was to send a bogus operating system update to a site running multics the update contained a trojan horse described later that could be activated by a backdoor and that allowed the tiger team to gain access the threat was so well implemented that the multics developers could not find it even after they were informed of its presence enge it is difficult to implement operating system controls for backdoors security measures must focus on the program development and software update activities logic bomb one of the oldest types of program threat predating viruses and worms is the logic bomb the logic bomb is code embedded in some legitimate program that is set to explode when certain conditions are met examples of conditions that can be used as triggers for a logic bomb are the presence or absence of certain files a particular day of the week or date or a particular user running the application once triggered a bomb may alter or delete data or entire files cause a machine halt or do some other damage a striking example of how logic bombs can be employed was the case of tim lloyd who was convicted of setting a logic bomb that cost his employer omega engineering more than million derailed its corporate growth strategy and eventually led to the layoff of workers gaud ultimately lloyd was sentenced to months in prison and ordered to pay million in restitution trojan horse a trojan horse is a useful or apparently useful program or command procedure containing hidden code that when invoked performs some unwanted or harmful function trojan horse programs can be used to accomplish functions indirectly that an unauthorized user could not accomplish directly for example to gain access to the files of another user on a shared system a user could create a trojan horse program that when executed changes the invoking users file permissions so that the files are readable by any user the author could then induce users to run the program by placing it in a common directory and naming it such that it appears to be a useful utility program or application an example is a program that ostensibly produces a listing of the users files in a desirable format after another user has run the program the author of the program can then access the information in the users files an example of a trojan horse program that would be difficult to detect is a compiler that has been modified to insert additional code into certain programs as they are compiled such as a system login program thom the code creates a backdoor in the login program that permits the author to log on to the system using a special password this trojan horse can never be discovered by reading the source code of the login program another common motivation for the trojan horse is data destruction the program appears to be performing a useful function eg a calculator program but it may also be quietly deleting the users files for example a cbs executive was victimized by a trojan horse that destroyed all information contained in his computers memory time the trojan horse was implanted in a graphics routine offered on an electronic bulletin board system trojan horses fit into one of three models continuing to perform the function of the original program and additionally performing a separate malicious activity continuing to perform the function of the original program but modifying the function to perform malicious activity eg a trojan horse version of a login program that collects passwords or to disguise other malicious activity eg a trojan horse version of a process listing program that does not display certain processes that are malicious performing a malicious function that completely replaces the function of the original program mobile code mobile code refers to programs eg script macro or other portable instruction that can be shipped unchanged to a heterogeneous collection of platforms and execute with identical semantics jans the term also applies to situations involving a large homogeneous collection of platforms eg microsoft windows mobile code is transmitted from a remote system to a local system and then executed on the local system without the users explicit instruction mobile code often acts as a mechanism for a virus worm or trojan horse to be transmitted to the users workstation in other cases mobile code takes advantage of vulnerabilities to perform its own exploits such as unauthorized data access or root compromise popular vehicles for mobile code include java applets activex javascript and vbscript the most common ways of using mobile code for malicious operations on local system are crosssite scripting interactive and dynamic web sites email attachments and downloads from untrusted sites or of untrusted software multiplethreat malware viruses and other malware may operate in multiple ways the terminology is far from uniform this subsection gives a brief introduction to several related concepts that could be considered multiplethreat malware a multipartite virus infects in multiple ways typically the multipartite virus is capable of infecting multiple types of files so that virus eradication must deal with all of the possible sites of infection a blended attack uses multiple methods of infection or transmission to maximize the speed of contagion and the severity of the attack some writers characterize a blended attack as a package that includes multiple types of malware an example of a blended attack is the nimda attack erroneously referred to as simply a worm nimda uses four distribution methods email a user on a vulnerable host opens an infected email attachment nimda looks for email addresses on the host and then sends copies of itself to those addresses windows shares nimda scans hosts for unsecured windows file shares it can then use netbios as a transport mechanism to infect files on that host in the hopes that a user will run an infected file which will activate nimda on that host web servers nimda scans web servers looking for known vulnerabilities in microsoft iis if it finds a vulnerable server it attempts to transfer a copy of itself to the server and infect it and its files web clients if a vulnerable web client visits a web server that has been infected by nimda the clients workstation will become infected thus nimda has worm virus and mobile code characteristics blended attacks may also spread through other services such as instant messaging and peertopeer file sharing virusesworms and bots viruses a computer virus is a piece of software that can infect other programs by modifying them the modification includes injecting the original program with a routine to make copies of the virus program which can then go on to infect other programs biological viruses are tiny scraps of genetic code dna or rna that can take over the machinery of a living cell and trick it into making thousands of flawless replicas of the original virus like its biological counterpart a computer virus carries in its instructional code the recipe for making perfect copies of itself the typical virus becomes embedded in a program on a computer then whenever the infected computer comes into contact with an uninfected piece of software a fresh copy of the virus passes into the new program thus the infection can be spread from computer to computer by unsuspecting users who either swap disks or send programs to one another over a network in a network environment the ability to access applications and system services on other computers provides a perfect culture for the spread of a virus the nature of viruses a virus can do anything that other programs do the only difference is that it attaches itself to another program and executes secretly when the host program is run once a virus is executing it can perform any function that is allowed by the privileges of the current user such as erasing files and programs a computer virus has three parts ayco infection mechanism the means by which a virus spreads enabling it to replicate the mechanism is also referred to as the infection vector trigger the event or condition that determines when the payload is activated or delivered payload what the virus does besides spreading the payload may involve damage or may involve benign but noticeable activity viruses worms and bots viruses a computer virus is a piece of software that can infect other programs by modifying them the modification includes injecting the original program with a routine to make copies of the virus program which can then go on to infect other programs biological viruses are tiny scraps of genetic code dna or rna that can take over the machinery of a living cell and trick it into making thousands of flawless replicas of the original virus like its biological counterpart a computer virus carries in its instructional code the recipe for making perfect copies of itself the typical virus becomes embedded in a program on a computer then whenever the infected computer comes into contact with an uninfected piece of software a fresh copy of the virus passes into the new program thus the infection can be spread from computer to computer by unsuspecting users who either swap disks or send programs to one another over a network in a network environment the ability to access applications and system services on other computers provides a perfect culture for the spread of a virus the nature of viruses a virus can do anything that other programs do the only difference is that it attaches itself to another program and executes secretly when the host program is run once a virus is executing it can perform any function that is allowed by the privileges of the current user such as erasing files and programs a computer virus has three parts ayco infection mechanism the means by which a virus spreads enabling it to replicate the mechanism is also referred to as the infection vector trigger the event or condition that determines when the payload is activated or delivered payload what the virus does besides spreading the payload may involve damage or may involve benign but noticeable activity during its lifetime a typical virus goes through the following four phases dormant phase the virus is idle the virus will eventually be activated by some event such as a date the presence of another program or file or the capacity of the disk exceeding some limit not all viruses have this stage propagation phase the virus places an identical copy of itself into other programs or into certain system areas on the disk each infected program will now contain a clone of the virus which will itself enter a propagation phase triggering phase the virus is activated to perform the function for which it was intended as with the dormant phase the triggering phase can be caused by a variety of system events including a count of the number of times that this copy of the virus has made copies of itself execution phase the function is performed the function may be harmless such as a message on the screen or damaging such as the destruction of programs and data files most viruses carry out their work in a manner that is specific to a particular operating system and in some cases specific to a particular hardware platform thus they are designed to take advantage of the details and weaknesses of particular systems virus structure a virus can be prepended or postpended to an executable program or it can be embedded in some other fashion the key to its operation is that the infected program when invoked will first execute the virus code and then execute the original code of the program a very general depiction of virus structure is shown in figure based on cohe in this case the virus code v is prepended to infected programs and it is assumed that the entry point to the program when invoked is the first line of the program the infected program begins with the virus code and works as follows the first line of code is a jump to the main virus program the second line is a special marker that is used by the virus to determine whether or not a potential victim program has already been infected with this virus when the program is invoked control is immediately transferred to the main virus program the virus program may first seek out uninfected executable files and infect them next the virus may perform some action usually detrimental to the system this action could be performed every time the program is invoked or it could be a logic bomb that triggers only under certain conditions finally the virus transfers control to the original program if the infection phase of the program is reasonably rapid a user is unlikely to notice any difference between the execution of an infected and an uninfected program a virus such as the one just described is easily detected because an infected version of a program is longer than the corresponding uninfected one a way to thwart such a simple means of detecting a virus is to compress the executable file so that both the infected and uninfected versions are of identical length figure cohe shows in general terms the logic required the important lines in this virus are numbered we assume that program p is infected with the virus cv when program v goto main subroutine infectexecutable loop file getrandomexecutablefile if firstlineoffile then goto loop else prepend v to file subroutine dodamage whatever damage is to be done subroutine triggerpulled return true if some condition holds main mainprogram infectexecutable if triggerpulled then dodamage goto next next figure a simple virus this program is invoked control passes to its virus which performs the following steps for each uninfected file p that is found the virus first compresses that file to produce p which is shorter than the original program by the size of the virus a copy of the virus is prepended to the compressed program program cv goto main subroutine infectexecutable loop file getrandomexecutablefile if firstlineoffile then goto loop compress file prepend cv to file main mainprogram if askpermission then infectexecutable uncompress restoffile run uncompressed file figure logic for a compression virus the compressed version of the original infected program p is uncompressed the uncompressed original program is executed in this example the virus does nothing other than propagate as previously mentioned the virus may include a logic bomb initial infection once a virus has gained entry to a system by infecting a single program it is in a position to potentially infect some or all other executable files on that system when the infected program executes thus viral infection can be completely prevented by preventing the virus from gaining entry in the first place unfortunately prevention is extraordinarily difficult because a virus can be part of any program outside a system thus unless one is content to take an absolutely bare piece of iron and write all ones own system and application programs one is vulnerable many forms of infection can also be blocked by denying normal users the right to modify programs on the system the lack of access controls on early pcs is a key reason that traditional machine code based viruses spread rapidly on these systems in contrast while it is easy enough to write a machine code virus for unix systems they were almost never seen in practice because the existence of access controls on these systems prevented effective propagation of the virus traditional machine code based viruses are now less prevalent because modern pc operating systems have more effective access controls however virus creators have found other avenues such as macro and email viruses as discussed subsequently viruses classification there has been a continuous arms race between virus writers and writers of antivirus software since viruses first appeared as effective countermeasures are developed for existing types of viruses newer types are developed there is no simple or universally agreedupon classification scheme for viruses in this section we follow ayco and classify viruses along two orthogonal axes the type of target the virus tries to infect and the method the virus uses to conceal itself from detection by users and antivirus software a virus classification by target includes the following categories boot sector infector infects a master boot record or boot record and spreads when a system is booted from the disk containing the virus file infector infects files that the operating system or shell considers to be executable macro virus infects files with macro code that is interpreted by an application a virus classification by concealment strategy includes the following categories encrypted virus a typical approach is as follows a portion of the virus creates a random encryption key and encrypts the remainder of the virus the key is stored with the virus when an infected program is invoked the virus uses the stored random key to decrypt the virus when the virus replicates a different random key is selected because the bulk of the virus is encrypted with a different key for each instance there is no constant bit pattern to observe stealth virus a form of virus explicitly designed to hide itself from detection by antivirus software thus the entire virus not just a payload is hidden polymorphic virus a virus that mutates with every infection making detection by the signature of the virus impossible metamorphic virus as with a polymorphic virus a metamorphic virus mutates with every infection the difference is that a metamorphic virus rewrites itself completely at each iteration increasing the difficulty of detection metamorphic viruses may change their behavior as well as their appearance one example of a stealth virus was discussed earlier a virus that uses compression so that the infected program is exactly the same length as an uninfected version far more sophisticated techniques are possible for example a virus can place intercept logic in disk io routines so that when there is an attempt to read suspected portions of the disk using these routines the virus will present back the original uninfected program thus stealth is not a term that applies to a virus as such but rather refers to a technique used by a virus to evade detection a polymorphic virus creates copies during replication that are functionally equivalent but have distinctly different bit patterns as with a stealth virus the purpose is to defeat programs that scan for viruses in this case the signature of the virus will vary with each copy to achieve this variation the virus may randomly insert superfluous instructions or interchange the order of independent instructions a more effective approach is to use encryption the strategy of the encryption virus is followed the portion of the virus that is responsible for generating keys and performing encryptiondecryption is referred to as the mutation engine the mutation engine itself is altered with each use virus kits another weapon in the virus writers armory is the viruscreation toolkit such a toolkit enables a relative novice to quickly create a number of different viruses although viruses created with toolkits tend to be less sophisticated than viruses designed from scratch the sheer number of new viruses that can be generated using a toolkit creates a problem for antivirus schemes macro viruses in the mids macro viruses became by far the most prevalent type of virus macro viruses are particularly threatening for a number of reasons a macro virus is platform independent many macro viruses infect microsoft word documents or other microsoft office documents any hardware platform and operating system that supports these applications can be infected macro viruses infect documents not executable portions of code most of the information introduced onto a computer system is in the form of a document rather than a program macro viruses are easily spread a very common method is by electronic mail because macro viruses infect user documents rather than system programs traditional file system access controls are of limited use in preventing their spread macro viruses take advantage of a feature found in word and other office applications such as microsoft excel namely the macro in essence a macro is an executable program embedded in a word processing document or other type of file typically users employ macros to automate repetitive tasks and thereby save keystrokes the macro language is usually some form of the basic programming language a user might define a sequence of keystrokes in a macro and set it up so that the macro is invoked when a function key or special short combination of keys is input successive releases of ms office products provide increased protection against macro viruses for example microsoft offers an optional macro virus protection tool that detects suspicious word files and alerts the customer to the potential risk of opening a file with macros various antivirus product vendors have also developed tools to detect and correct macro viruses as in other types of viruses the arms race continues in the field of macro viruses but they no longer are the predominant virus threat email viruses a more recent development in malicious software is the email virus the first rapidly spreading email viruses such as melissa made use of a microsoft word macro embedded in an attachment if the recipient opens the email attachment the word macro is activated then the email virus sends itself to everyone on the mailing list in the users email package the virus does local damage on the users system in a more powerful version of the email virus appeared this newer version can be activated merely by opening an email that contains the virus rather than opening an attachment the virus uses the visual basic scripting language supported by the email package thus we see a new generation of malware that arrives via email and uses email software features to replicate itself across the internet the virus propagates itself as soon as it is activated either by opening an email attachment or by opening the email to all of the email addresses known to the infected host as a result whereas viruses used to take months or years to propagate they now do so in hours this makes it very difficult for antivirus software to respond before much damage is done ultimately a greater degree of security must be built into internet utility and application software on pcs to counter the growing threat worms a worm is a program that can replicate itself and send copies from computer to computer across network connections upon arrival the worm may be activated to replicate and propagate again in addition to propagation the worm usually performs some unwanted function an email virus has some of the characteristics of a worm because it propagates itself from system to system however we can still classify it as a virus because it uses a document modified to contain viral macro content and requires human action a worm actively seeks out more machines to infect and each machine that is infected serves as an automated launching pad for attacks on other machines network worm programs use network connections to spread from system to system once active within a system a network worm can behave as a computer virus or bacteria or it could implant trojan horse programs or perform any number of disruptive or destructive actions to replicate itself a network worm uses some sort of network vehicle examples include the following electronic mail facility a worm mails a copy of itself to other systems so that its code is run when the email or an attachment is received or viewed remote execution capability a worm executes a copy of itself on another system either using an explicit remote execution facility or by exploiting a program flaw in a network service to subvert its operations such as buffer overflow described in chapter remote login capability a worm logs on to a remote system as a user and then uses commands to copy itself from one system to the other where it then executes the new copy of the worm program is then run on the remote system where in addition to any functions that it performs at that system it continues to spread in the same fashion a network worm exhibits the same characteristics as a computer virus a dormant phase a propagation phase a triggering phase and an execution phase the propagation phase generally performs the following functions search for other systems to infect by examining host tables or similar repositories of remote system addresses establish a connection with a remote system copy itself to the remote system and cause the copy to be run the network worm may also attempt to determine whether a system has previously been infected before copying itself to the system in a multiprogramming system it may also disguise its presence by naming itself as a system process or using some other name that may not be noticed by a system operator as with viruses network worms are difficult to counter worm propagation model zou describes a model for worm propagation based on an analysis of recent worm attacks the speed of propagation and the total number of hosts infected depend on a number of factors including the mode of propagation the vulnerability or vulnerabilities exploited and the degree of similarity to preceding attacks for the latter factor an attack that is a variation of a recent previous attack may be countered more effectively than a more novel attack figure shows the dynamics for one typical set of parameters propagation proceeds through three phases in the initial phase the number of hosts increases exponentially to see that this is so consider a simplified case in which a worm is launched from a single host and infects two nearby hosts each of these hosts infects two more hosts and so on this results in exponential growth after a time infecting hosts waste some time attacking already infected hosts which reduces the rate of infection during this middle phase growth is approximately linear but the rate of infection is rapid when most vulnerable computers have been infected the attack enters a slow finish phase as the worm seeks out those remaining hosts that are difficult to identify clearly the objective in countering a worm is to catch the worm in its slow start phase at a time when few hosts have been infected slow finish phase number of infected hosts fast spread phase slow start phase time t minutes figure worm propagation model state of worm technology the state of the art in worm technology includes the following multiplatform newer worms are not limited to windows machines but can attack a variety of platforms especially the popular varieties of unix multiexploit new worms penetrate systems in a variety of ways using exploits against web servers browsers email file sharing and other networkbased applications ultrafast spreading one technique to accelerate the spread of a worm is to conduct a prior internet scan to accumulate internet addresses of vulnerable machines polymorphic to evade detection skip past filters and foil realtime analysis worms adopt the virus polymorphic technique each copy of the worm has new code generated on the fly using functionally equivalent instructions and encryption techniques metamorphic in addition to changing their appearance metamorphic worms have a repertoire of behavior patterns that are unleashed at different stages of propagation transport vehicles because worms can rapidly compromise a large number of systems they are ideal for spreading other distributed attack tools such as distributed denial of service bots zeroday exploit to achieve maximum surprise and distribution a worm should exploit an unknown vulnerability that is only discovered by the general network community when the worm is launched bots a bot robot also known as a zombie or drone is a program that secretly takes over another internetattached computer and then uses that computer to launch attacks that are difficult to trace to the bots creator the bot is typically planted on hundreds or thousands of computers belonging to unsuspecting third parties the collection of bots often is capable of acting in a coordinated manner such a collection is referred to as a botnet a botnet exhibits three characteristics the bot functionality a remote control facility and a spreading mechanism to propagate the bots and construct the botnet we examine each of these characteristics in turn uses of bots hone lists the following uses of bots distributed denialofservice ddos attacks a ddos attack is an attack on a computer system or network that causes a loss of service to users spamming with the help of a botnet and thousands of bots an attacker is able to send massive amounts of bulk email spam sniffing traffic bots can also use a packet sniffer to watch for interesting cleartext data passing by a compromised machine the sniffers are mostly used to retrieve sensitive information like usernames and passwords keylogging if the compromised machine uses encrypted communication channels eg https or pops then just sniffing the network packets on the victims computer is useless because the appropriate key to decrypt the packets is missing but by using a keylogger which captures keystrokes on the infected machine an attacker can retrieve sensitive information an implemented filtering mechanism eg i am only interested in key sequences near the keyword paypalcom further helps in stealing secret data spreading new malware botnets are used to spread new bots this is very easy since all bots implement mechanisms to download and execute a file via http or ftp a botnet with hosts that acts as the start base for a worm or mail virus allows very fast spreading and thus causes more harm installing advertisement addons and browser helper objects bhos botnets can also be used to gain financial advantages this works by setting up a fake web site with some advertisements the operator of this web site negotiates a deal with some hosting companies that pay for clicks on ads with the help of a botnet these clicks can be automated so that instantly a few thousand bots click on the popups this process can be further enhanced if the bot hijacks the start page of a compromised machine so that the clicks are executed each time the victim uses the browser attacking irc chat networks botnets are also used for attacks against internet relay chat irc networks popular among attackers is especially the socalled clone attack in this kind of attack the controller orders each bot to connect a large number of clones to the victim irc network the victim is flooded by service requests from thousands of bots or thousands of channeljoins by these cloned bots in this way the victim irc network is brought down similar to a ddos attack manipulating online pollsgames online pollsgames are getting more and more attention and it is rather easy to manipulate them with botnets since every bot has a distinct ip address every vote will have the same credibility as a vote cast by a real person online games can be manipulated in a similar way remote control facility the remote control facility is what distinguishes a bot from a worm a worm propagates itself and activates itself whereas a bot is controlled from some central facility at least initially a typical means of implementing the remote control facility is on an irc server all bots join a specific channel on this server and treat incoming messages as commands more recent botnets tend to avoid irc mechanisms and use covert communication channels via protocols such as http distributed control mechanisms are also used to avoid a single point of failure once a communications path is established between a control module and the bots the control module can activate the bots in its simplest form the control module simply issues command to the bot that causes the bot to execute routines that are already implemented in the bot for greater flexibility the control module can issue update commands that instruct the bots to download a file from some internet location and execute it the bot in this latter case becomes a more generalpurpose tool that can be used for multiple attacks constructing the attack network the first step in a botnet attack is for the attacker to infect a number of machines with bot software that will ultimately be used to carry out the attack the essential ingredients in this phase of the attack are the following software that can carry out the attack the software must be able to run on a large number of machines must be able to conceal its existence must be able to communicate with the attacker or have some sort of timetriggered mechanism and must be able to launch the intended attack toward the target a vulnerability in a large number of systems the attacker must become aware of a vulnerability that many system administrators and individual users have failed to patch and that enables the attacker to install the bot software a strategy for locating and identifying vulnerable machines a process known as scanning or fingerprinting in the scanning process the attacker first seeks out a number of vulnerable machines and infects them then typically the bot software that is installed in the infected machines repeats the same scanning process until a large distributed network of infected machines is created mirk lists the following types of scanning strategies random each compromised host probes random addresses in the ip address space using a different seed this technique produces a high volume of internet traffic which may cause generalized disruption even before the actual attack is launched hit list the attacker first compiles a long list of potential vulnerable machines this can be a slow process done over a long period to avoid detection that an attack is underway once the list is compiled the attacker begins infecting machines on the list each infected machine is provided with a portion of the list to scan this strategy results in a very short scanning period which may make it difficult to detect that infection is taking place topological this method uses information contained on an infected victim machine to find more hosts to scan local subnet if a host can be infected behind a firewall that host then looks for targets in its own local network the host uses the subnet address structure to find other hosts that would otherwise be protected by the firewall rootkits a rootkit is a set of programs installed on a system to maintain administrator or root access to that system root access provides access to all the functions and services of the operating system the rootkit alters the hosts standard functionality in a malicious and stealthy way with root access an attacker has complete control of the system and can add or change programs and files monitor processes send and receive network traffic and get backdoor access on demand a rootkit can make many changes to a system to hide its existence making it difficult for the user to determine that the rootkit is present and to identify what changes have been made in essence a rootkit hides by subverting the mechanisms that monitor and report on the processes files and registries on a computer rootkits can be classified based on whether they can survive a reboot and execution mode a rootkit may be persistent activates each time the system boots the rootkit must store code in a persistent store such as the registry or file system and configure a method by which the code executes without user intervention memory based has no persistent code and therefore can not survive a reboot user mode intercepts calls to apis application program interfaces and modifies returned results for example when an application performs a directory listing the return results dont include entries identifying the files associated with the rootkit kernel mode can intercept calls to native apis in kernel mode the rootkit can also hide the presence of a malware process by removing it from the kernels list of active processes rootkit installation unlike worms or bots rootkits do not directly rely on vulnerabilities or exploits to get on a computer one method of rootkit installation is via a trojan horse program the user is induced to load the trojan horse which then installs the rootkit another means of rootkit installation is by hacker activity the following sequence is representative of a hacker attack to install a rootkit geer the attacker uses a utility to identify open ports or other vulnerabilities the attacker uses password cracking malware or a system vulnerability to gain initial access and eventually root access the attacker uploads the rootkit to the victims machine the attacker can add a virus denial of service or other type of attack to the rootkits payload the attacker then runs the rootkits installation script the rootkit replaces binaries files commands or system utilities to hide its presence the rootkit listens at a port in the target server installs sniffers or keyloggers activates a malicious payload or takes other steps to compromise the victim systemlevel call attacks programs operating at the user level interact with the kernel through system calls thus system calls are a primary target of kernellevel rootkits to achieve concealment as an example of how rootkits operate we look at the implementation of system calls in linux in linux each system call is assigned a unique syscall number when a usermode process executes a system call the process refers to the system call by this number the kernel maintains a system call table with one entry per system call routine each entry contains a pointer to the corresponding routine the syscall number serves as an index into the system call table levi lists three techniques that can be used to change system calls modify the system call table the attacker modifies selected syscall addresses stored in the system call table this enables the rootkit to direct a system call away from the legitimate routine to the rootkits replacement figure shows how the knark rootkit achieves this modify system call table targets the attacker overwrites selected legitimate system call routines with malicious code the system call table is not changed system call table system call table knarkfork knarkread knarkexecve sysfork sysfork sysread sysread sysexecve sysexecve syschdir syschdir a normal kernel memory layout b after knark install figure system call table modification by rootkit redirect the system call table the attacker redirects references to the entire system call table to a new table in a new kernel memory location recommended reading and web sites the topics in this chapter are covered in more detail in stal it is useful to read some of the classic tutorial papers on computer security these provide a historical perspective from which to appreciate current work and thinking the papers to read are ware brow salt shan and summ two more recent short treatments of computer security are andr and lamp nist is an exhaustive pages treatment of the subject another good treatment is nrc also useful is fras andr andrews m and whittaker j computer security ieee security and privacy septemberoctober brow browne p computer security a survey acm sigmis database fall fras fraser b site security handbook rfc september lamp lampson b computer security in the real world computer june nist national institute of standards and technology an introduction to computer security the nist handbook special publication october nrc national research council computers at risk safe computing in the information age washington dc national academy press salt saltzer j and schroeder m the protection of information in computer systems proceedings of the ieee september shan shanker k the total computer security problem an overview computer june stal stallings w and brown l computer security principles and practice upper saddle river nj prentice hall summ summers r an overview of computer security ibm systems journal vol no ware ware w ed security controls for computer systems rand report october httpwwwrandorgpubsreportsrindexhtml recommended web sites computer security resource center maintained by the national institute on standards and technology nist contains a broad range of information on security threats technology and standards cert coordination center the organization that grew from the computer emergency response team formed by the defense advanced research projects agency site provides good information on internet security threats vulnerabilities and attack statistics vmyths dedicated to exposing virus hoaxes and dispelling misconceptions about real viruses key terms review questions and problems key terms accountability email virus passive attack active attack falsification privacy asset hacker replay attack insider attack repudiation authenticity integrity system integrity availability interception threat backdoor intruder traffic analysis confidentiality intrusion trapdoor data integrity logic bomb trojan horse deception macro virus usurpation denial of service malicious software virus disruption malware virus kit exposure masquerade worm review questions define computer security what are the fundamental requirements addressed by computer security what is the difference between passive and active security threats list and briefly define three classes of intruders list and briefly define three intruder behavior patterns what is the role of compression in the operation of a virus what is the role of encryption in the operation of a virus what are typical phases of operation of a virus or worm in general terms how does a worm propagate what is the difference between a bot and a rootkit problems consider an automated teller machine atm in which users provide a personal identification number pin and a card for account access give examples of confidentiality integrity and availability requirements associated with the system and in each case indicate the degree of importance of the requirement repeat the preceding problem for a telephone switching system that routes calls through a switching network based on the telephone number requested by the caller consider a desktop publishing system used to produce documents for various organizations a give an example of a type of publication for which confidentiality of the stored data is the most important requirement b give an example of a type of publication in which data integrity is the most important requirement c give an example in which system availability is the most important requirement for each of the following assets assign a low moderate or high impact level for the loss of confidentiality availability and integrity respectively justify your answers a an organization managing public information on its web server b a law enforcement organization managing extremely sensitive investigative information c a financial organization managing routine administrative information not privacyrelated information d an information system used for large acquisitions in a contracting organization contains both sensitive presolicitation phase contract information and routine administrative information assess the impact for the two data sets separately and the information system as a whole e a power plant contains a scada supervisory control and data acquisition system controlling the distribution of electric power for a large military installation the scada system contains both realtime sensor data and routine administrative information assess the impact for the two data sets separately and the information system as a whole assume that passwords are selected from fourcharacter combinations of alphabetic characters assume that an adversary is able to attempt passwords at a rate of one per second a assuming no feedback to the adversary until each attempt has been completed what is the expected time to discover the correct password b assuming feedback to the adversary flagging an error as each incorrect character is entered what is the expected time to discover the correct password there is a flaw in the virus program of figure what is it the question arises as to whether it is possible to develop a program that can analyze a piece of software to determine if it is a virus consider that we have a program d that is supposed to be able to do that that is for any program p if we run dp the result returned is true p is a virus or false p is not a virus now consider the following program program cv mainprogram if dcv then goto next else infectexecutable next in the preceding program infectexecutable is a module that scans memory for executable programs and replicates itself in those programs determine if d can correctly decide whether cv is a virus the point of this problem is to demonstrate the type of puzzles that must be solved in the design of malicious code and therefore the type of mindset that one wishing to counter such attacks must adopt a consider the following c program begin print begin print end end what do you think the program was intended to do does it work b answer the same questions for the following program char m a i n and so on t main int i printfchar t for i ti ii printfd ti printfs t c what is the specific relevance of this problem to this chapter consider the following fragment legitimate code if data is friday the th crashcomputer legitimate code what type of malicious software is this consider the following fragment in an authentication program username readusername password readpassword if username is t hckr return allowlogin if username and password are valid return allowlogin else return denylogin what type of malicious software is this the following code fragments show a sequence of virus instructions and a polymorphic version of the virus describe the effect produced by the metamorphic code original code metamorphic code mov eax mov eax add eax ebx push ecx call eax pop ecx add eax ebx swap eax ebx swap ebx eax call eax nop computer security techniques authentication passwordbased authentication tokenbased authentication biometric authentication access control discretionary access control rolebased access control intrusion detection basic principles hostbased intrusion detection techniques audit records malware defense antivirus approaches worm countermeasures bot countermeasures rootkit countermeasures dealing with buffer overflow attacks compiletime defenses runtime defenses windows security access control scheme access token security descriptors recommended reading and web sites key terms review questions and problems to guard against the baneful influence exerted by strangers is therefore an elementary dictate of savage prudence hence before strangers are allowed to enter a district or at least before they are permitted to mingle freely with the inhabitants certain ceremonies are often performed by the natives of the country for the purpose of disarming the strangers of their magical powers or of disinfecting so to speak the tainted atmosphere by which they are supposed to be surrounded the golden bough sir james george frazer learning objectives after studying this chapter you should be able to define and compare three methods of user authentication compare and contrast two methods of access control explain the basic principles and techniques of intrusion detection explain the basic principles and techniques of malware defense understand how to defend against buffer overflow attacks explain the file system used in windows this chapter introduces common measures used to counter the security threats discussed in chapter authentication user authentication was introduced in section note that user authentication is distinct from message authentication message authentication is a procedure that allows communicating parties to verify that the contents of a received message have not been altered and that the source is authentic this chapter is concerned solely with user authentication passwordbased authentication a widely used line of defense against intruders is the password system virtually all multiuser systems networkbased servers webbased ecommerce sites and other similar services require that a user provide not only a name or identifier id but also a password the system compares the password to a previously stored password for that user id maintained in a system password file the password serves to authenticate the id of the individual logging on to the system in turn the id provides security in the following ways the id determines whether the user is authorized to gain access to a system in some systems only those who already have an id filed on the system are allowed to gain access the id determines the privileges accorded to the user a few users may have supervisory or superuser status that enables them to read files and perform functions that are especially protected by the operating system some systems have guest or anonymous accounts and users of these accounts have more limited privileges than others the id is used in what is referred to as discretionary access control for example by listing the ids of the other users a user may grant permission to them to read files owned by that user the use of hashed passwords a widely used password security technique is the use of hashed passwords and a salt value this scheme is found on virtually all unix variants as well as on a number of other operating systems the following procedure is employed figure a to load a new password into the system the user selects or is assigned a password this password is combined with a fixedlength salt value morr in older implementations this value is related to the time at which the password is assigned to the user newer implementations use a pseudorandom or random number the password and salt serve as inputs to a hashing algorithm to produce a fixedlength hash code the hash algorithm is designed to be slow to execute to thwart attacks the hashed password is then stored together with a plaintext copy of the salt in the password file for the corresponding user id the hashedpassword method has been shown to be secure against a variety of cryptanalytic attacks wagn when a user attempts to log on to a unix system the user provides an id and a password figure b the operating system uses the id to index into the password file and retrieve the plaintext salt and the encrypted password the salt and usersupplied password are used as input to the encryption routine if the result matches the stored value the password is accepted the salt serves three purposes it prevents duplicate passwords from being visible in the password file even if two users choose the same password those passwords will be assigned different salt values hence the hashed passwords of the two users will differ it greatly increases the difficulty of offline dictionary attacks for a salt of length b bits the number of possible passwords is increased by a factor of b increasing the difficulty of guessing a password in a dictionary attack it becomes nearly impossible to find out whether a person with passwords on two or more systems has used the same password on all of them to see the second point consider the way that an offline dictionary attack would work the attacker obtains a copy of the password file suppose first that the salt is not used the attackers goal is to guess a single password to that end the attacker submits a large number of likely passwords to the hashing function if any of the guesses matches one of the hashes in the file then the attacker has found a password that is in the file but faced with the unix scheme the attacker must take each guess and submit it to the hash function once for each salt value in the dictionary file multiplying the number of guesses that must be checked password password file salt user id salt hash code slow hash load function a loading a new password password file user id user id salt hash code salt select password slow hash function hashed password compare b verifying a password figure unix password scheme there are two threats to the unix password scheme first a user can gain access on a machine using a guest account or by some other means and then run a password guessing program called a password cracker on that machine the attacker should be able to check many thousands of possible passwords with little resource consumption second if an opponent is able to obtain a copy of the password file then a cracker program can be run on another machine at leisure this enables the opponent to run through millions of possible passwords in a reasonable period unix implementations since the original development of unix most implementations have relied on the following password scheme each user selects a password of up to eight printable characters in length this is converted into a bit value using bit ascii that serves as the key input to an encryption routine the hash routine known as crypt is based on des a bit salt value is used the modified des algorithm is executed with a data input consisting of a bit block of zeros the output of the algorithm then serves as input for a second encryption this process is repeated for a total of encryptions the resulting bit output is then translated into an character sequence the modification of the des algorithm converts it into a oneway hash function the crypt routine is designed to discourage guessing attacks software implementations of des are slow compared to hardware versions and the use of iterations multiplies the time required by this particular implementation is now considered woefully inadequate for example perr reports the results of a dictionary attack using a supercomputer the attack was able to process over million password guesses in about minutes further the results showed that for about anyone should be able to do the same in a few months using one uniprocessor machine despite its known weaknesses this unix scheme is still often required for compatibility with existing account management software or in multivendor environments there are other much stronger hashsalt schemes available for unix the recommended hash function for many unix systems including linux solaris and freebsd is based on the md secure hash algorithm which is similar to but not as secure as sha the md crypt routine uses a salt of up to bits and effectively has no limitations on password length it produces a bit hash value it is also far slower than crypt to achieve the slowdown md crypt uses an inner loop with iterations probably the most secure version of the unix hashsalt scheme was developed for openbsd another widely used open source unix this scheme reported in prov uses a hash function based on the blowfish symmetric block cipher the hash function called bcrypt is quite slow to execute bcrypt allows passwords of up to characters in length and requires a random salt value of bits to produce a bit hash value bcrypt also includes a cost variable an increase in the cost variable causes a corresponding increase in the time required to perform a bcyrpt hash the cost assigned to a new password is configurable so that administrators can assign a higher cost to privileged users tokenbased authentication objects that a user possesses for the purpose of user authentication are called tokens in this subsection we examine two types of tokens that are widely used these are cards that have the appearance and size of bank cards memory cards memory cards can store but not process data the most common such card is the bank card with a magnetic stripe on the back a magnetic stripe can store only a simple security code which can be read and unfortunately reprogrammed by an inexpensive card reader there are also memory cards that include an internal electronic memory see appendix k for a discussion of secure hash algorithms memory cards can be used alone for physical access such as a hotel room for computer user authentication such cards are typically used with some form of password or personal identification number pin a typical application is an automatic teller machine atm the memory card when combined with a pin or password provides significantly greater security than a password alone an adversary must gain physical possession of the card or be able to duplicate it plus must gain knowledge of the pin among the potential drawbacks are the following nist requires special reader this increases the cost of using the token and creates the requirement to maintain the security of the readers hardware and software token loss a lost token temporarily prevents its owner from gaining system access thus there is an administrative cost in replacing the lost token in addition if the token is found stolen or forged then an adversary now need only determine the pin to gain unauthorized access user dissatisfaction although users may have no difficulty in accepting the use of a memory card for atm access its use for computer access may be deemed inconvenient smart cards a wide variety of devices qualify as smart tokens these can be categorized along three dimensions that are not mutually exclusive physical characteristics smart tokens include an embedded microprocessor a smart token that looks like a bank card is called a smart card other smart tokens can look like calculators keys or other small portable objects interface manual interfaces include a keypad and display for humantoken interaction smart tokens with an electronic interface communicate with a compatible readerwriter authentication protocol the purpose of a smart token is to provide a means for user authentication we can classify the authentication protocols used with smart tokens into three categories static with a static protocol the user authenticates himself or herself to the token and then the token authenticates the user to the computer the latter half of this protocol is similar to the operation of a memory token dynamic password generator in this case the token generates a unique password periodically eg every minute this password is then entered into the computer system for authentication either manually by the user or electronically via the token the token and the computer system must be initialized and kept synchronized so that the computer knows the password that is current for this token challengeresponse in this case the computer system generates a challenge such as a random string of numbers the smart token generates a response based on the challenge for example publickey cryptography could be used and the token could encrypt the challenge string with the tokens private key for user authentication to computer the most important category of smart token is the smart card which has the appearance of a credit card has an electronic interface and may use any of the type of protocols just described the remainder of this section discusses smart cards a smart card contains within it an entire microprocessor including processor memory and io ports some versions incorporate a special coprocessing circuit for cryptographic operation to speed the task of encoding and decoding messages or generating digital signatures to validate the information transferred in some cards the io ports are directly accessible by a compatible reader by means of exposed electrical contacts other cards rely instead on an embedded antenna for wireless communication with the reader biometric authentication a biometric authentication system attempts to authenticate an individual based on his or her unique physical characteristics these include static characteristics such as fingerprints hand geometry facial characteristics and retinal and iris patterns and dynamic characteristics such as voiceprint and signature in essence biometrics is based on pattern recognition compared to passwords and tokens biometric authentication is both technically complex and expensive while it is used in a number of specific applications biometrics has yet to mature as a standard tool for user authentication to computer systems a number of different types of physical characteristics are either in use or under study for user authentication the most common are the following facial characteristics facial characteristics are the most common means of humantohuman identification thus it is natural to consider them for identification by computer the most common approach is to define characteristics based on relative location and shape of key facial features such as eyes eyebrows nose lips and chin shape an alternative approach is to use an infrared camera to produce a face thermogram that correlates with the underlying vascular system in the human face fingerprints fingerprints have been used as a means of identification for centuries and the process has been systematized and automated particularly for law enforcement purposes a fingerprint is the pattern of ridges and furrows on the surface of the fingertip fingerprints are believed to be unique across the entire human population in practice automated fingerprint recognition and matching system extract a number of features from the fingerprint for storage as a numerical surrogate for the full fingerprint pattern hand geometry hand geometry systems identify features of the hand including shape and lengths and widths of fingers retinal pattern the pattern formed by veins beneath the retinal surface is unique and therefore suitable for identification a retinal biometric system obtains a digital image of the retinal pattern by projecting a lowintensity beam of visual or infrared light into the eye iris another unique physical characteristic is the detailed structure of the iris hand iris signature retina cost face finger voice accuracy figure cost versus accuracy of various biometric characteristics in user authentication schemes signature each individual has a unique style of handwriting and this is reflected especially in the signature which is typically a frequently written sequence however multiple signature samples from a single individual will not be identical this complicates the task of developing a computer representation of the signature that can be matched to future samples voice whereas the signature style of an individual reflects not only the unique physical attributes of the writer but also the writing habit that has developed voice patterns are more closely tied to the physical and anatomical characteristics of the speaker nevertheless there is still a variation from sample to sample over time from the same speaker complicating the biometric recognition task figure gives a rough indication of the relative cost and accuracy of these biometric measures the concept of accuracy does not apply to user authentication schemes using smart cards or passwords for example if a user enters a password it either matches exactly the password expected for that user or not in the case of biometric parameters the system instead must determine how closely a presented biometric characteristic matches a stored characteristic before elaborating on the concept of biometric accuracy we need to have a general idea of how biometric systems work access control an access control policy dictates what types of access are permitted under what circumstances and by whom access control policies are generally grouped into the following categories discretionary access control dac controls access based on the identity of the requestor and on access rules authorizations stating what requestors are or are not allowed to do this policy is termed discretionary because an entity might have access rights that permit the entity by its own volition to enable another entity to access some resource mandatory access control mac controls access based on comparing security labels which indicate how sensitive or critical system resources are with security clearances which indicate system entities are eligible to access certain resources this policy is termed mandatory because an entity that has clearance to access a resource may not just by its own volition enable another entity to access that resource rolebased access control rbac controls access based on the roles that users have within the system and on rules stating what accesses are allowed to users in given roles dac is the traditional method of implementing access control this method was introduced in chapter we provide more detail in this section mac is a concept that evolved out of requirements for military information security and is beyond the scope of this book rbac has become increasingly popular and is introduced later in this section these three policies are not mutually exclusive figure an access control mechanism can employ two or even all three of these policies to cover different classes of system resources discretionary access control this section introduces a general model for dac developed by lampson graham and denning lamp grah denn the model assumes a set of subjects a set of objects and a set of rules that govern the access of subjects to objects let us define the protection state of a system to be the set of information at a given point in time that specifies the access rights for each subject with respect to each object we can identify three requirements representing the protection state enforcing access rights and allowing subjects to alter the protection state in certain ways the model addresses all three requirements giving a general logical description of a dac system discretionary mandatory access control access control policy policy rolebased access control policy figure access control policies before continuing the reader should review section and the discussion of unix file access control in section to represent the protection state we extend the universe of objects in the access control matrix to include the following processes access rights include the ability to delete a process stop block and wake up a process devices access rights include the ability to readwrite the device to control its operation eg a disk seek and to blockunblock the device for use memory locations or regions access rights include the ability to readwrite certain locations of regions of memory that are protected so that the default is that access is not allowed subjects access rights with respect to a subject have to do with the ability to grant or delete access rights of that subject to other objects as explained subsequently figure is an example compare figure a for an access control matrix a each entry as x contains strings called access attributes that specify the access rights of subject s to object x for example in figure s may read file f because read appears in as f from a logical or functional point of view a separate access control module is associated with each type of object figure the module evaluates each request by a subject to access an object to determine if the access right exists an access attempt triggers the following steps a subject s issues a request of type a for object x the request causes the system the operating system or an access control interface module of some sort to generate a message of the form s a x to the controller for x the controller interrogates the access matrix a to determine if a is in as x if so the access is allowed if not the access is denied and a protection violation occurs the violation should trigger a warning and an appropriate action figure suggests that every access by a subject to an object is mediated by the controller for that object and that the controllers decision is based on the objects subjects files processes disk drives s s s f f p p d d s control owner owner read read wakeup wakeup seek owner control owner subjects s control write execute owner seek s control write stop copy flag set figure extended access control matrix system intervention subjects access control mechanisms objects si read f si read f filesystem files memory segments addressing pages hardware sj wakeup p sj wakeup p process manager processes terminal terminal device devices manager instruction decoding instructions hardware sk grant a to sn x sk grant a sn x sm delete b from sp y sm delete b sp y access matrix monitor access write matrix read figure an organization of the access control function current contents of the matrix in addition certain subjects have the authority to make specific changes to the access matrix a request to modify the access matrix is treated as an access to the matrix with the individual entries in the matrix treated as objects such accesses are mediated by an access matrix controller which controls updates to the matrix the model also includes a set of rules that govern modifications to the access matrix shown in table for this purpose we introduce the access rights owner and control and the concept of a copy flag explained in the subsequent paragraphs the first three rules deal with transferring granting and deleting access rights suppose that the entry exists in as x this means that s has access right to subject x and because of the presence of the copy flag can transfer this right with table access control system commands rule command by s authorization operation r transfer e a f to s x in aso x store e a f in as x a a r grant e a f to s x owner in aso x store e a f in as x a a r delete from s x control in aso s delete from as x or owner in aso x r w read s x control in aso s copy as x into w or owner in aso x r create object x none add column for x to a store owner in aso x r destroy object x owner in aso x delete column for x from a r create subject s none add row for s to a execute create object s store control in as s r destroy subject s owner in aso s delete row for s from a execute destroy object s or without copy flag to another subject rule r expresses this capability a subject would transfer the access right without the copy flag if there were a concern that the new subject would maliciously transfer the right to another subject that should not have that access right for example s may place read or read in any matrix entry in the f column rule r states that if s is designated as the owner of object x then s can grant an access right to that object for any other subject rule states that s can add any access right to as x for any s if s has owner access to x rule r permits s to delete any access right from any matrix entry in a row for which s controls the subject and for any matrix entry in a column for which s owns the object rule r permits a subject to read that portion of the matrix that it owns or controls the remaining rules in table govern the creation and deletion of subjects and objects rule r states that any subject can create a new object which it owns and can then grant and delete access to the object under rule r the owner of an object can destroy the object resulting in the deletion of the corresponding column of the access matrix rule r enables any subject to create a new subject the creator owns the new subject and the new subject has control access to itself rule r permits the owner of a subject to delete the row and column if there are subject columns of the access matrix designated by that subject the set of rules in table is an example of the rule set that could be defined for an access control system the following are examples of additional or alternative rules that could be included a transferonly right could be defined which results in the transferred right being added to the target subject and deleted from the transferring subject the number of owners of an object or a subject could be limited to one by not allowing the copy flag to accompany the owner right the ability of one subject to create another subject and to have owner access right to that subject can be used to define a hierarchy of subjects for example in figure s owns s and s so that s and s are subordinate to s by the rules of table s can grant and delete to s access rights that s already has thus a subject can create another subject with a subset of its own access rights this might be useful for example if a subject is invoking an application that is not fully trusted and does not want that application to be able to transfer access rights to other subjects rolebased access control traditional dac systems define the access rights of individual users and groups of users in contrast rbac is based on the roles that users assume in a system rather than the users identity typically rbac models define a role as a job function within an organization rbac systems assign access rights to roles instead of individual users in turn users are assigned to different roles either statically or dynamically according to their responsibilities rbac now enjoys widespread commercial use and remains an area of active research the national institute of standards and technology nist has issued a standard security requirements for cryptographic modules fips pub may that requires support for access control and administration through roles the relationship of users to roles is many to many as is the relationship of roles to resources or system objects figure the set of users changes in some users roles resources role role role figure users roles and resources environments frequently and the assignment of a user to one or more roles may also be dynamic the set of roles in the system in most environments is likely to be static with only occasional additions or deletions each role will have specific access rights to one or more resources the set of resources and the specific access rights associated with a particular role are also likely to change infrequently we can use the access matrix representation to depict the key elements of an rbac system in simple terms as shown in figure the upper matrix relates individual users to roles typically there are many more users than roles each matrix entry is either blank or marked the latter indicating that this user is assigned r r rn u u u u u u um objects r r rn f f p p d d r control owner owner read read wakeup wakeup seek owner control owner r control write execute owner seek roles rn control write stop figure access control matrix representation of rbac to this role note that a single user may be assigned multiple roles more than one mark in a row and that multiple users may be assigned to a single role more than one mark in a column the lower matrix has the same structure as the dac matrix with roles as subjects typically there are few roles and many objects or resources in this matrix the entries are the specific access rights enjoyed by the roles note that a role can be treated as an object allowing the definition of role hierarchies rbac lends itself to an effective implementation of the principle of least privilege that is each role should contain the minimum set of access rights needed for that role a user is assigned to a role that enables him or her to perform only what is required for that role multiple users assigned to the same role enjoy the same minimal set of access rights intrusion detection intrusion detection systems were introduced in section basic principles authentication facilities access control facilities and firewalls all play a role in countering intrusions another line of defense is intrusion detection and this has been the focus of much research in recent years this interest is motivated by a number of considerations including the following if an intrusion is detected quickly enough the intruder can be identified and ejected from the system before any damage is done or any data are compromised even if the detection is not sufficiently timely to preempt the intruder the sooner that the intrusion is detected the less the amount of damage and the more quickly that recovery can be achieved an effective ids can serve as a deterrent thus acting to prevent intrusions intrusion detection enables the collection of information about intrusion techniques that can be used to strengthen intrusion prevention measures intrusion detection is based on the assumption that the behavior of the intruder differs from that of a legitimate user in ways that can be quantified of course we can not expect that there will be a crisp exact distinction between an attack by an intruder and the normal use of resources by an authorized user rather we must expect that there will be some overlap figure suggests in abstract terms the nature of the task confronting the designer of an ids although the typical behavior of an intruder differs from the typical behavior of an authorized user there is an overlap in these behaviors thus a loose interpretation of intruder behavior which will catch more intruders will also lead to a number of false positives or authorized users identified as intruders on the other hand an attempt to limit false positives by a tight interpretation of intruder behavior will lead to an increase in false negatives or intruders not identified as intruders thus there is an element of compromise and art in the practice of intrusion detection probability density function profile of profile of authorized user intruder behavior behavior overlap in observed or expected behavior average behavior average behavior measurable behavior of intruder of authorized user parameter figure profiles of behavior of intruders and authorized users in andersons study ande it was postulated that one could with reasonable confidence distinguish between a masquerader and a legitimate user patterns of legitimate user behavior can be established by observing past history and significant deviation from such patterns can be detected anderson suggests that the task of detecting a misfeasor legitimate user performing in an unauthorized fashion is more difficult in that the distinction between abnormal and normal behavior may be small anderson concluded that such violations would be undetectable solely through the search for anomalous behavior however misfeasor behavior might nevertheless be detectable by intelligent definition of the class of conditions that suggest unauthorized use finally the detection of the clandestine user was felt to be beyond the scope of purely automated techniques these observations which were made in remain true today for the remainder of this section we concentrate on hostbased intrusion detection hostbased intrusion detection techniques hostbased idss add a specialized layer of security software to vulnerable or sensitive systems examples include database servers and administrative systems the hostbased ids monitors activity on the system in a variety of ways to detect suspicious behavior in some cases an ids can halt an attack before any damage is done but its primary purpose is to detect intrusions log suspicious events and send alerts the primary benefit of a hostbased ids is that it can detect both external and internal intrusions something that is not possible either with networkbased idss or firewalls hostbased idss follow one of two general approaches to intrusion detection anomaly detection involves the collection of data relating to the behavior of legitimate users over a period of time then statistical tests are applied to observed behavior to determine with a high level of confidence whether that behavior is not legitimate user behavior the following are two approaches to statistical anomaly detection a threshold detection this approach involves defining thresholds independent of user for the frequency of occurrence of various events b profile based a profile of the activity of each user is developed and used to detect changes in the behavior of individual accounts signature detection involves an attempt to define a set of rules or attack patterns that can be used to decide that a given behavior is that of an intruder in essence anomaly approaches attempt to define normal or expected behavior whereas signaturebased approaches attempt to define proper behavior in terms of the types of attackers listed earlier anomaly detection is effective against masqueraders who are unlikely to mimic the behavior patterns of the accounts they appropriate on the other hand such techniques may be unable to deal with misfeasors for such attacks signaturebased approaches may be able to recognize events and sequences that in context reveal penetration in practice a system may employ a combination of both approaches to be effective against a broad range of attacks audit records a fundamental tool for intrusion detection is the audit record some record of ongoing activity by users must be maintained as input to an ids basically two plans are used native audit records virtually all multiuser operating systems include accounting software that collects information on user activity the advantage of using this information is that no additional collection software is needed the disadvantage is that the native audit records may not contain the needed information or may not contain it in a convenient form detectionspecific audit records a collection facility can be implemented that generates audit records containing only that information required by the ids one advantage of such an approach is that it could be made vendor independent and ported to a variety of systems the disadvantage is the extra overhead involved in having in effect two accounting packages running on a machine a good example of detectionspecific audit records is one developed by dorothy denning denn each audit record contains the following fields subject initiators of actions a subject is typically a terminal user but might also be a process acting on behalf of users or groups of users all activity arises through commands issued by subjects subjects may be grouped into different access classes and these classes may overlap action operation performed by the subject on or with an object for example login read perform io and execute object receptors of actions examples include files programs messages records terminals printers and useror programcreated structures when a subject is the recipient of an action such as electronic mail then that subject is considered an object objects may be grouped by type object granularity may vary by object type and by environment for example database actions may be audited for the database as a whole or at the record level exceptioncondition denotes which if any exception condition is raised on return resourceusage a list of quantitative elements in which each element gives the amount used of some resource eg number of lines printed or displayed number of records read or written processor time io units used session elapsed time timestamp unique timeanddate stamp identifying when the action took place most user operations are made up of a number of elementary actions for example a file copy involves the execution of the user command which includes doing access validation and setting up the copy plus the read from one file plus the write to another file consider the command copy gameexe to library gameexe issued by smith to copy an executable file game from the current directory to the library directory the following audit records may be generated smith execute librarycopyexe cpu smith read smithgameexe records smith execute librarycopyexe writeviol records in this case the copy is aborted because smith does not have write permission to library the decomposition of a user operation into elementary actions has three advantages because objects are the protectable entities in a system the use of elementary actions enables an audit of all behavior affecting an object thus the system can detect attempted subversions of access controls by noting an abnormality in the number of exception conditions returned and can detect successful subversions by noting an abnormality in the set of objects accessible to the subject singleobject singleaction audit records simplify the model and the implementation because of the simple uniform structure of the detectionspecific audit records it may be relatively easy to obtain this information or at least part of it by a straightforward mapping from existing native audit records to the detectionspecific audit records malware defense antivirus approaches the ideal solution to the threat of viruses is prevention do not allow a virus to get into the system in the first place this goal is in general impossible to achieve although prevention can reduce the number of successful viral attacks the next best approach is to be able to do the following detection once the infection has occurred determine that it has occurred and locate the virus identification once detection has been achieved identify the specific virus that has infected a program removal once the specific virus has been identified remove all traces of the virus from the infected program and restore it to its original state remove the virus from all infected systems so that the disease can not spread further if detection succeeds but either identification or removal is not possible then the alternative is to discard the infected program and reload a clean backup version advances in virus and antivirus technology go hand in hand early viruses were relatively simple code fragments and could be identified and purged with relatively simple antivirus software packages as the virus arms race has evolved both viruses and necessarily antivirus software have grown more complex and sophisticated increasingly sophisticated antivirus approaches and products continue to appear in this subsection we highlight two of the most important generic decryption generic decryption gd technology enables the antivirus program to easily detect even the most complex polymorphic viruses while maintaining fast scanning speeds nach recall that when a file containing a polymorphic virus is executed the virus must decrypt itself to activate in order to detect such a structure executable files are run through a gd scanner which contains the following elements cpu emulator a softwarebased virtual computer instructions in an executable file are interpreted by the emulator rather than executed on the underlying processor the emulator includes software versions of all registers and other processor hardware so that the underlying processor is unaffected by programs interpreted on the emulator virus signature scanner a module that scans the target code looking for known virus signatures emulation control module controls the execution of the target code at the start of each simulation the emulator begins interpreting instructions in the target code one at a time thus if the code includes a decryption routine that decrypts and hence exposes the virus that code is interpreted in effect the virus does the work for the antivirus program by exposing the virus periodically the control module interrupts interpretation to scan the target code for virus signatures during interpretation the target code can cause no damage to the actual personal computer environment because it is being interpreted in a completely controlled environment the most difficult design issue with a gd scanner is to determine how long to run each interpretation typically virus elements are activated soon after a program begins executing but this need not be the case the longer the scanner emulates a particular program the more likely it is to catch any hidden viruses however the antivirus program can take up only a limited amount of time and resources before users complain of degraded system performance digital immune system the digital immune system is a comprehensive approach to virus protection developed by ibm kepha kephb whit and subsequently refined by symantec syma the motivation for this development has been the rising threat of internetbased virus propagation we first say a few words about this threat and then summarize ibms approach traditionally the virus threat was characterized by the relatively slow spread of new viruses and new mutations antivirus software was typically updated on a monthly basis and this was sufficient to control the problem also traditionally the internet played a comparatively small role in the spread of viruses but as ches points out two major trends in internet technology have had an increasing impact on the rate of virus propagation in recent years integrated mail systems systems such as lotus notes and microsoft outlook make it very simple to send anything to anyone and to work with objects that are received mobileprogram systems capabilities such as java and activex allow programs to move on their own from one system to another in response to the threat posed by these internetbased capabilities ibm has developed a prototype digital immune system this system expands on the use of program emulation discussed in the preceding subsection and provides a generalpurpose emulation and virusdetection system the objective of this system is to provide rapid response time so that viruses can be stamped out almost as soon as they are introduced when a new virus enters an organization the immune system automatically captures it analyzes it adds detection and shielding for it removes it and passes information about that virus to systems running ibm antivirus so that it can be detected before it is allowed to run elsewhere figure illustrates the typical steps in digital immune system operation a monitoring program on each pc uses a variety of heuristics based on system behavior suspicious changes to programs or family signature to infer that a virus may be present the monitoring program forwards a copy of any program thought to be infected to an administrative machine within the organization virusinfected virus client client administrative machine machine analyze virus analysis machine behavior and machine structure private client extract network client machine signature machine derive prescription administrative client machine other client individual client private user network figure digital immune system the administrative machine encrypts the sample and sends it to a central virus analysis machine this machine creates an environment in which the infected program can be safely run for analysis techniques used for this purpose include emulation or the creation of a protected environment within which the suspect program can be executed and monitored the virus analysis machine then produces a prescription for identifying and removing the virus the resulting prescription is sent back to the administrative machine the administrative machine forwards the prescription to the infected client the prescription is also forwarded to other clients in the organization subscribers around the world receive regular antivirus updates that protect them from the new virus the success of the digital immune system depends on the ability of the virus analysis machine to detect new and innovative virus strains by constantly analyzing and monitoring the viruses found in the wild it should be possible to continually update the digital immune software to keep up with the threat behaviorblocking software unlike heuristics or fingerprintbased scanners behavior blocking software integrates with the operating system of a host computer and monitors program behavior in real time for malicious actions conr nach the behavior blocking software then blocks potentially malicious actions before they have a chance to affect the system monitored behaviors can include attempts to open view delete andor modify files attempts to format disk drives and other unrecoverable disk operations behaviorblocking software at server flags suspicious code the blocker sandboxes the suspicious software to administrator sets prevent it from proceeding acceptable software behavior policies and uploads them to a server policies can also be internet uploaded to desktops sandbox malicious software manages to make it administrator through the firewall firewall server running server alerts administrator behaviorblocking that suspicious code has been software identified and sandboxed awaiting administrators decision on whether the code should be removed or allowed to run figure behaviorblocking software operation modifications to the logic of executable files or macros modification of critical system settings such as startup settings scripting of email and instant messaging clients to send executable content and initiation of network communications figure illustrates the operation of a behavior blocker behaviorblocking software runs on server and desktop computers and is instructed through policies set by the network administrator to let benign actions take place but to intercede when unauthorized or suspicious actions occur the module blocks any suspicious software from executing a blocker isolates the code in a sandbox which restricts the codes access to various os resources and applications the blocker then sends an alert because a behavior blocker can block suspicious software in real time it has an advantage over such established antivirus detection techniques as fingerprinting or heuristics while there are literally trillions of different ways to obfuscate and rearrange the instructions of a virus or worm many of which will evade detection by a fingerprint scanner or heuristic eventually malicious code must make a welldefined request to the operating system given that the behavior blocker can intercept all such requests it can identify and block malicious actions regardless of how obfuscated the program logic appears to be behavior blocking alone has limitations because the malicious code must run on the target machine before all its behaviors can be identified it can cause harm before it has been detected and blocked for example a new virus might shuffle a number of seemingly unimportant files around the hard drive before infecting a single file and being blocked even though the actual infection was blocked the user may be unable to locate his or her files causing a loss to productivity or possibly worse worm countermeasures there is considerable overlap in techniques for dealing with viruses and worms once a worm is resident on a machine antivirus software can be used to detect it in addition because worm propagation generates considerable network activity network activity and usage monitoring can form the basis of a worm defense to begin let us consider the requirements for an effective worm countermeasure scheme generality the approach taken should be able to handle a wide variety of worm attacks including polymorphic worms timeliness the approach should respond quickly so as to limit the number infected systems and the number of generated transmissions from infected systems resiliency the approach should be resistant to evasion techniques employed by attackers to evade worm countermeasures minimal denialofservice costs the approach should result in minimal reduction in capacity or service due to the actions of the countermeasure software that is in an attempt to contain worm propagation the countermeasure should not significantly disrupt normal operation transparency the countermeasure software and devices should not require modification to existing legacy oss application software and hardware global and local coverage the approach should be able to deal with attack sources both from outside and inside the enterprise network no existing worm countermeasure scheme appears to satisfy all these requirements thus administrators typically need to use multiple approaches in defending against worm attacks following jhi we list six classes of worm defense a signaturebased worm scan filtering this type of approach generates a worm signature which is then used to prevent worm scans from enteringleaving a networkhost typically this approach involves identifying suspicious flows and generating a worm signature this approach is vulnerable to the use of polymorphic worms either the detection software misses the worm or if it is sufficiently sophisticated to deal with polymorphic worms the scheme may take a long time to react news is an example of this approach b filterbased worm containment this approach is similar to class a but focuses on worm content rather than a scan signature the filter checks a message to determine if it contains worm code an example is vigilante cost which relies on collaborative worm detection at end hosts this approach can be quite effective but requires efficient detection algorithms and rapid alert dissemination c payloadclassificationbased worm containment these networkbased techniques examine packets to see if they contain a worm various anomaly detection techniques can be used but care is needed to avoid high levels of false positives or negatives an example of this approach is reported in chin which looks for exploit code in network flows this approach does not generate signatures based on byte patterns but rather looks for control and data flow structures that suggest an exploit d threshold random walk trw scan detection trw exploits randomness in picking destinations to connect to as a way of detecting if a scanner is in operation jung trw is suitable for deployment in highspeed lowcost network devices it is effective against the common behavior seen in worm scans e rate limiting this class limits the rate of scanlike traffic from an infected host various strategies can be used including limiting the number of new machines a host can connect to in a window of time detecting a high connection failure rate and limiting the number of unique ip addresses a host can scan in a window of time chen is an example this class of countermeasures may introduce longer delays for normal traffic this class is also not suited for slow stealthy worms that spread slowly to avoid detection based on activity level f rate halting this approach immediately blocks outgoing traffic when a threshold is exceeded either in outgoing connection rate or diversity of connection attempts jhi the approach must include measures to quickly unblock mistakenly blocked hosts in a transparent way rate halting can integrate with a signatureor filterbased approach so that once a signature or filter is generated every blocked host can be unblocked rate halting appears to offer a very effective countermeasure as with rate limiting rate halting techniques are not suitable for slow stealthy worms bot countermeasures a number of the countermeasures discussed in this chapter make sense against bots including idss and digital immune systems once bots are activated and an attack is underway these countermeasures can be used to detect the attack but the primary objective is to try to detect and disable the botnet during its construction phase rootkit countermeasures rootkits can be extraordinarily difficult to detect and neutralize particularly so for kernellevel rootkits many of the administrative tools that could be used to detect a rootkit or its traces can be compromised by the rootkit precisely so that it is undetectable countering rootkits requires a variety of networkand computerlevel security tools both networkbased and hostbased intrusion detection systems can look for the code signatures of known rootkit attacks in incoming traffic hostbased antivirus software can also be used to recognize the known signatures of course there are always new rootkits and modified versions of existing rootkits that display novel signatures for these cases a system needs to look for behaviors that could indicate the presence of a rootkit such as the interception of system calls or a keylogger interacting with a keyboard driver such behavior detection is far from straightforward for example antivirus software typically intercepts system calls another approach is to do some sort of file integrity check an example of this is rootkitrevealer a freeware package from sysinternals the package compares the results of a system scan using apis with the actual view of storage using instructions that do not go through an api because a rootkit conceals itself by modifying the view of storage seen by administrator calls rootkitrevealer catches the discrepancy if a kernellevel rootkit is detected by any means the only secure and reliable way to recover is to do an entire new os install on the infected machine dealing with buffer overflow attacks finding and exploiting a stack buffer overflow is not difficult the large number of exploits over the previous couple of decades clearly illustrates this there is consequently a need to defend systems against such attacks by either preventing them or at least detecting and aborting such attacks this section discusses possible approaches to implementing such protections these can be broadly classified into two categories compiletime defenses which aim to harden programs to resist attacks in new programs runtime defenses which aim to detect and abort attacks in existing programs while suitable defenses have been known for a couple of decades the very large existing base of vulnerable software and systems hinders their deployment hence the interest in runtime defenses which can be deployed in operating systems and updates and can provide some protection for existing vulnerable programs most of these techniques are mentioned in lhee compiletime defenses compiletime defenses aim to prevent or detect buffer overflows by instrumenting programs when they are compiled the possibilities for doing this range from choosing a highlevel language that does not permit buffer overflows to encouraging safe coding standards using safe standard libraries or including additional code to detect corruption of the stack frame choice of programming language one possibility is to write the program using a modern highlevel programming language one that has a strong notion of variable type and what constitutes permissible operations on them such languages are not vulnerable to buffer overflow attacks because their compilers include additional code to enforce range checks automatically removing the need for the programmer to explicitly code them the flexibility and safety provided by these languages the material in this section was developed by lawrie brown of the australian defence force academy does come at a cost in resource use both at compile time and also in additional code that must execute at runtime to impose checks such as that on buffer limits these disadvantages are much less significant than they used to be due to the rapid increase in processor performance increasingly programs are being written in these languages and hence should be immune to buffer overflows in their code though if they use existing system libraries or runtime execution environments written in less safe languages they may still be vulnerable the distance from the underlying machine language and architecture also means that access to some instructions and hardware resources is lost this limits their usefulness in writing code such as device drivers that must interact with such resources for these reasons there is still likely to be at least some code written in less safe languages such as c safe coding techniques if languages such as c are being used programmers need to be aware that their ability to manipulate pointer addresses and access memory directly comes at a cost c was designed as a systems programming language running on systems that were vastly smaller and more constrained than we now use this meant that cs designers placed much more emphasis on space efficiency and performance considerations than on type safety they assumed that programmers would exercise due care in writing code using these languages and take responsibility for ensuring the safe use of all data structures and variables unfortunately as several decades of experience has shown this has not been the case this may be seen in large legacy body of potentially unsafe code in the unix and linux operating systems and applications some of which are potentially vulnerable to buffer overflows azx in order to harden these systems the programmer needs to inspect the code and rewrite any unsafe coding constructs in a safe manner given the rapid uptake of buffer overflow exploits this process has begun in some cases a good example is the openbsd project which produces a free multiplatform bsdbased unixlike operating system among other technology changes programmers have undertaken an extensive audit of the existing code base including the operating system standard libraries and common utilities this has resulted in what is widely regarded as one of the safest operating systems in widespread use the openbsd project claims as of mid that there has only been one remote hole discovered in the default install in more than years this is a clearly enviable record microsoft have also undertaken a major project in reviewing their code base partly in response to continuing bad publicity over the number of vulnerabilities including many buffer overflow issues that have been found in their operating systems and applications code this has clearly been a difficult process though they claim that their new vista operating system will benefit greatly from this process language extensions and use of safe libraries given the problems that can occur in c with unsafe array and pointer references there have been a number of proposals to augment compilers to automatically insert range checks on such references while this is fairly easy for statically allocated arrays handling dynamically allocated memory is more problematic because the size information is not available at compiletime handling this requires an extension to the semantics of a pointer to include bounds information and the use of library routines to ensure that these values are set correctly several such approaches are listed in lhee however there is generally a performance penalty with the use of such techniques that may or may not be acceptable these techniques also require all programs and libraries that require these safety features to be recompiled with the modified compiler while this can be feasible for a new release of an operating system and its associated utilities there will still likely be problems with thirdparty applications a common concern with c comes from the use of unsafe standard library routines especially some of the string manipulation routines one approach to improving the safety of systems has been to replace these with safer variants this can include the provision of new functions such as strlcpy in the bsd family of systems including openbsd using these requires rewriting the source to conform to the new safer semantics alternatively it involves replacement of the standard string library with a safer variant libsafe is a wellknown example of this it implements the standard semantics but includes additional checks to ensure that the copy operations do not extend beyond the local variable space in the stack frame so while it can not prevent corruption of adjacent local variables it can prevent any modification of the old stack frame and return address values and thus prevent the classic stack buffer overflow types of attack we examined previously this library is implemented as a dynamic library arranged to load before the existing standard libraries and can thus provide protection for existing programs without requiring them to be recompiled provided they dynamically access the standard library routines as most programs do the modified library code has been found to typically be at least as efficient as the standard libraries and thus its use is an easy way of protecting existing programs against some forms of buffer overflow attacks stack protection mechanisms an effective method for protecting programs against classic stack overflow attacks is to instrument the function entry and exit code to setup and then check its stack frame for any evidence of corruption if any modification is found the program is aborted rather than allowing the attack to proceed there are several approaches to providing this protection which we discuss next stackguard is one of the best known protection mechanisms it is a gcc gnu compiler collection compiler extension that inserts additional function entry and exit code the added function entry code writes a canary value below the old frame pointer address before the allocation of space for local variables the added function exit code checks that the canary value has not changed before continuing with the usual function exit operations of restoring the old frame pointer and transferring control back to the return address any attempt at a classic stack buffer overflow would have to alter this value in order to change the old frame pointer and return addresses and would thus be detected resulting in the program being aborted for this defense to function successfully it is critical that the canary named after the miners canary used to detect poisonous air in a mine and thus warn the miners in time for them to escape value be unpredictable and should be different on different systems if this were not the case the attacker would simply ensure the shellcode included the correct canary value in the required location typically a random value is chosen as the canary value on process creation and saved as part of the processes state the code added to the function entry and exit then uses this value there are some issues with using this approach first it requires that all programs needing protection be recompiled second because the structure of the stack frame has changed it can cause problems with programs such as debuggers which analyze stack frames however the canary technique has been used to recompile an entire linux distribution and provide it with a high level of resistance to stack overflow attacks similar functionality is available for windows programs by compiling them using microsofts gs visual c compiler option runtime defenses as has been noted most of the compiletime approaches require recompilation of existing programs hence there is interest in runtime defenses that can be deployed as operating systems updates to provide some protection for existing vulnerable programs these defenses involve changes to the memory management of the virtual address space of processes these changes act either to alter the properties of regions of memory or to make predicting the location of targeted buffers sufficiently difficult to thwart many types of attacks executable address space protection many of the buffer overflow attacks involve copying machine code into the targeted buffer and then transferring execution to it a possible defense is to block the execution of code on the stack on the assumption that executable code should only be found elsewhere in the processes address space to support this feature efficiently requires support from the processors memory management unit mmu to tag pages of virtual memory as being nonexecutable some processors such as the sparc used by solaris have had support for this for some time enabling its use in solaris requires a simple kernel parameter change other processors such as the x family have not had this support until recently with the relatively recent addition of the noexecute bit in its mmu extensions have been made available to linux bsd and other unixstyle systems to support the use of this feature some indeed are also capable of protecting the heap as well as the stack which also is the target of attacks support for enabling noexecute protection is also included in recent windows systems making the stack and heap nonexecutable provides a high degree of protection against many types of buffer overflow attacks for existing programs hence the inclusion of this practice is standard in a number of recent operating systems releases however one issue is support for programs that do need to place executable code on the stack this can occur for example in justintime compilers such as is used in the java runtime system executable code on the stack is also used to implement nested functions in c a gcc extension and also linux signal handlers special provisions are needed to support these requirements nonetheless this is regarded as one of the best methods for protecting existing programs and hardening systems against some attacks address space randomization another runtime technique that can be used to thwart attacks involves manipulation of the location of key data structures in a processes address space in particular recall that in order to implement the classic stack overflow attack the attacker needs to be able to predict the approximate location of the targeted buffer the attacker uses this predicted address to determine a suitable return address to use in the attack to transfer control to the shellcode one technique to greatly increase the difficulty of this prediction is to change the address at which the stack is located in a random manner for each process the range of addresses available on modern processors is large bits and most programs only need a small fraction of that therefore moving the stack memory region around by a megabyte or so has minimal impact on most programs but makes predicting the targeted buffers address almost impossible another target of attack is the location of standard library routines in an attempt to bypass protections such as nonexecutable stacks some buffer overflow variants exploit existing code in standard libraries these are typically loaded at the same address by the same program to counter this form of attack we can use a security extension that randomizes the order of loading standard libraries by a program and their virtual memory address locations this makes the address of any specific function sufficiently unpredictable as to render the chance of a given attack correctly predicting its address very low the openbsd system includes versions of these extensions in its technological support for a secure system guard pages a final runtime technique that can be used places guard pages between critical regions of memory in a processes address space again this exploits the fact that a process has much more virtual memory available than it typically needs gaps are placed between the ranges of addresses used for each of the components of the address space these gaps or guard pages are flagged in the mmu as illegal addresses and any attempt to access them results in the process being aborted this can prevent buffer overflow attacks typically of global data which attempt to overwrite adjacent regions in the processes address space a further extension places guard pages between stack frames or between different allocations on the heap this can provide further protection against stack and heap overflow attacks but at cost in execution time supporting the large number of page mappings necessary operating system overview operating system objectives and functions the operating system as a usercomputer interface the operating system as resource manager ease of evolution of an operating system the evolution of operating systems serial processing simple batch systems multiprogrammed batch systems timesharing systems major achievements the process memory management information protection and security scheduling and resource management developments leading to modern operating systems virtual machines virtual machines and virtualizing virtual machine architecture os design considerations for multiprocessor and multicore symmetric multiprocessor os considerations multicore os considerations microsoft windows overview history the modern os architecture clientserver model threads and smp windows objects what is new in windows traditional unix systems history description modern unix systems system v release svr bsd solaris linux history modular structure kernel components linux vserver virtual machine architecture recommended reading and web sites key terms review questions and problems operating systems are those programs that interface the machine with the applications programs the main function of these systems is to dynamically allocate the shared system resources to the executing programs as such research in this area is clearly concerned with the management and scheduling of memory processes and other devices but the interface with adjacent levels continues to shift with time functions that were originally part of the operating system have migrated to the hardware on the other side programmed functions extraneous to the problems being solved by the application programs are included in the operating system what can be automated the computer science and engineering research study mit press learning objectives after studying this chapter you should be able to summarize at a top level the key functions of an operating system os discuss the evolution of operating systems for early simple batch systems to modern complex systems give a brief explanation of each of the major achievements in os research as defined in section discuss the key design areas that have been instrumental in the development of modern operating systems define and discuss virtual machines and virtualization understand the os design issues raised by the introduction of multiprocessor and multicore organization understand the basic structure of windows describe the essential elements of a traditional unix system explain the new features found in modern unix systems discuss linux and its relationship to unix we begin our study of operating systems oss with a brief history this history is itself interesting and also serves the purpose of providing an overview of os principles the first section examines the objectives and functions of operating systems then we look at how operating systems have evolved from primitive batch systems to sophisticated multitasking multiuser systems the remainder of the chapter looks at the history and general characteristics of the two operating systems that serve as examples throughout this book all of the material in this chapter is covered in greater depth later in the book windows security a good example of the access control concepts we have been discussing is the windows access control facility which exploits objectoriented concepts to provide a powerful and flexible access control capability windows provides a uniform access control facility that applies to processes threads files semaphores windows and other objects access control is governed by two entities an access token associated with each process and a security descriptor associated with each object for which interprocess access is possible access control scheme when a user logs on to a windows system windows uses a namepassword scheme to authenticate the user if the logon is accepted a process is created for the user and an access token is associated with that process object the access token whose details are described later include a security id sid which is the identifier by which this user is known to the system for purposes of security the token also contains sids for the security groups to which the user belongs if the initial user process spawns a new process the new process object inherits the same access token the access token serves two purposes it keeps all necessary security information together to speed access validation when any process associated with a user attempts access the security subsystem can make use of the token associated with that process to determine the users access privileges it allows each process to modify its security characteristics in limited ways without affecting other processes running on behalf of the user the chief significance of the second point has to do with privileges that may be associated with a user the access token indicates which privileges a user may have generally the token is initialized with each of these privileges in a disabled state subsequently if one of the users processes needs to perform a privileged operation the process may enable the appropriate privilege and attempt access it would be undesirable to share the same token among all of the users processes because in that case enabling a privilege for one process enables it for all of them associated with each object for which interprocess access is possible is a security descriptor the chief component of the security descriptor is an access control list that specifies access rights for various users and user groups for this object when a process attempts to access this object the sids in the process token are matched against the access control list of the object to determine if access will be allowed or denied when an application opens a reference to a securable object windows verifies that the objects security descriptor grants the process the requested access if the check succeeds windows caches the resulting granted access rights an important aspect of windows security is the concept of impersonation which simplifies the use of security in a clientserver environment if client and server talk through a rpc connection the server can temporarily assume the identity of the client so that it can evaluate a request for access relative to that clients rights after the access the server reverts to its own identity access token figure a shows the general structure of an access token which includes the following parameters security id identifies a user uniquely across all of the machines on the network this generally corresponds to a users logon name special user sids were added in windows for use by processes and services these specially managed sids are designed for secure management they do not use the ordinary password polices human accounts do security id sid flags acl header group sids owner ace header privileges system access access mask default owner control list sid default acl discretionary ace header access control list access mask sid a access token b security descriptor c access control list figure windows security structures group sids a list of the groups to which this user belongs a group is simply a set of user ids that are identified as a group for purposes of access control each group has a unique group sid access to an object can be defined on the basis of group sids individual sids or a combination there is also a sid which reflects the process integrity level low medium high or system privileges a list of securitysensitive system services that this user may call for example createtoken another example is the sebackupprivilege users with this privilege are allowed to use a backup tool to back up files that they normally would not be able to read default owner if this process creates another object this field specifies the owner of the new object generally the owner of a new object is the same as the owner of the spawning process however a user may specify that the default owner of any processes spawned by this process is a group sid to which this user belongs default acl this is an initial list of protections applied to the objects that the user creates the user may subsequently alter the acl for any object that it owns or that one of its groups owns security descriptors figure b shows the general structure of a security descriptor which includes the following parameters flags define the type and contents of a security descriptor the flags indicate whether or not the sacl and dacl are present whether or not they were placed on the object by a defaulting mechanism and whether the pointers in the descriptor use absolute or relative addressing relative descriptors are required for objects that are transmitted over a network such as information transmitted in a rpc owner the owner of the object can generally perform any action on the security descriptor the owner can be an individual or a group sid the owner has the authority to change the contents of the dacl system access control list sacl specifies what kinds of operations on the object should generate audit messages an application must have the corresponding privilege in its access token to read or write the sacl of any object this is to prevent unauthorized applications from reading sacls thereby learning what not to do to avoid generating audits or writing them to generate many audits to cause an illicit operation to go unnoticed the sacl also specifies the object integrity level processes can not modify an object unless the process integrity level meets or exceeds the level on the object discretionary access control list dacl determines which users and groups can access this object for which operations it consists of a list of access control entries aces when an object is created the creating process can assign as owner its own sid or any group sid in its access token the creating process can not assign an owner that is not in the current access token subsequently any process that has been granted the right to change the owner of an object may do so but again with the same restriction the reason for the restriction is to prevent a user from covering his or her tracks after attempting some unauthorized action let us look in more detail at the structure of access control lists because these are at the heart of the windows access control facility figure c each list consists of an overall header and a variable number of access control entries each entry specifies an individual or group sid and an access mask that defines the rights to be granted to this sid when a process attempts to access an object the object manager in the windows executive reads the sid and group sids from the access token along with the integrity level sid if the access requested includes modifying the object the integrity level is checked against the object integrity level in the sacl if that test passes the object manager then scans down the objects dacl if a match is found that is if an ace is found with a sid that matches one of the sids from the access token then the process can have the access rights specified by the access mask in that ace this also may include denying access in which case the access request fails the first matching ace determines the result of the access check figure shows the contents of the access mask the least significant bits specify access rights that apply to a particular type of object for example bit for a file object is filereaddata access and bit for an event object is eventquerystate access the most significant bits of the mask contains bits that apply to all types of objects five of these are referred to as standard access types synchronize gives permission to synchronize execution with some event associated with this object in particular this object can be used in a wait function writeowner allows a program to modify the owner of the object this is useful because the owner of an object can always change the protection on the object the owner may not be denied write dac access delete read control write dac write owner generic synchronize access types standard specific access types access types access system security maximum allowed generic all generic execute generic write generic read figure access mask writedac allows the application to modify the dacl and hence the protection on this object readcontrol allows the application to query the owner and dacl fields of the security descriptor of this object delete allows the application to delete this object the highorder half of the access mask also contains the four generic access types these bits provide a convenient way to set specific access types in a number of different object types for example suppose an application wishes to create several types of objects and ensure that users have read access to the objects even though read has a somewhat different meaning for each object type to protect each object of each type without the generic access bits the application would have to construct a different ace for each type of object and be careful to pass the correct ace when creating each object it is more convenient to create a single ace that expresses the generic concept allow read and simply apply this ace to each object that is created and have the right thing happen that is the purpose of the generic access bits which are genericall allow all access genericexecute allow execution if executable genericwrite allow write access genericread allow readonly access the generic bits also affect the standard access types for example for a file object the genericread bit maps to the standard bits readcontrol and synchronize and to the objectspecific bits filereaddata filereadattributes and filereadea placing an ace on a file object that grants some sid generic read grants those five access rights as if they had been specified individually in the access mask the remaining two bits in the access mask have special meanings the access systemsecurity bit allows modifying audit and alarm control for this object however not only must this bit be set in the ace for a sid but the access token for the process with that sid must have the corresponding privilege enabled finally the maximumallowed bit is not really an access bit but a bit that modifies the algorithm for scanning the dacl for this sid normally windows will scan through the dacl until it reaches an ace that specifically grants bit set or denies bit not set the access requested by the requesting process or until it reaches the end of the dacl in which latter case access is denied the maximum allowed bit allows the objects owner to define a set of access rights that is the maximum that will be allowed to a given user with this in mind suppose that an application does not know all of the operations that it is going to be asked to perform on an object during a session there are three options for requesting access attempt to open the object for all possible accesses the disadvantage of this approach is that the access may be denied even though the application may have all of the access rights actually required for this session only open the object when a specific access is requested and open a new handle to the object for each different type of request this is generally the preferred method because it will not unnecessarily deny access nor will it allow more access than necessary in many cases the object itself does not need to be referenced a second time but the duplicatehandle function can be used to make a copy of the handle with a lower level of access attempt to open the object for as much access as the object will allow this sid the advantage is that the user will not be artificially denied access but the application may have more access than it needs this latter situation may mask bugs in the application an important feature of windows security is that applications can make use of the windows security framework for userdefined objects for example a database server might create its own security descriptors and attach them to portions of a database in addition to normal readwrite access constraints the server could secure databasespecific operations such as scrolling within a result set or performing a join it would be the servers responsibility to define the meaning of special rights and perform access checks but the checks would occur in a standard context using systemwide usergroup accounts and audit logs the extensible security model should also prove useful to implementers of nonmicrosoft file systems recommended reading and web sites the topics in this chapter are covered in more detail in stal ogor is the paper to read for an authoritative survey of user authentication burr is also a worthwhile survey sand is an excellent overview of access control sand is a comprehensive overview of rbac saun compares rbac and dac scar is a detailed and worthwhile treatment of intrusion detection two short but useful survey articles on the subject are kent and mchu ning surveys recent advances in intrusion detection techniques good overview articles on antivirus approaches and malware defense generally are cass forr kepha and nach lhee surveys a range of alternative buffer overflow techniques including a number not mentioned in this chapter along with possible defensive techniques the original published description of buffer overflow attacks is given in levy kupe is a good overview burr burr w dodson d and polk w electronic authentication guideline gaithersburg md national institute of standards and technology special publication september cass cass s anatomy of malice ieee spectrum november forr forrest s hofmeyr s and somayaji a computer immunology communications of the acm october kent kent s on the trail of intrusions into information systems ieee spectrum december kepha kephart j sorkin g chess d and white s fighting computer viruses scientific american november kupe kuperman b et al detection and prevention of stack buffer overflow attacks communications of the acm november levy levy e smashing the stack for fun and profit phrack magazine file issue november lhee lhee k and chapin s buffer overflow and format string overflow vulnerabilities software practice and experience vol mchu mchugh j christie a and allen j the role of intrusion detection systems ieee software septemberoctober nach nachenberg c computer virusantivirus coevolution communications of the acm january ning ning p et al techniques and tools for analyzing intrusion alerts acm transactions on information and system security may ogor ogorman l comparing passwords tokens and biometrics for user authentication proceedings of the ieee december sand sandhu r and samarati p access control principles and practice ieee communications magazine february sand sandhu r et al rolebased access control models computer september saun saunders g hitchens m and varadharajan v rolebased access control and the access control matrix operating systems review october scar scarfone k and mell p guide to intrusion detection and prevention systems nist special publication sp february stal stallings w and brown l computer security principles and practice upper saddle river nj prentice hall recommended web sites password usage and generation nist documents on this topic biometrics consortium governmentsponsored site for the research testing and evaluation of biometric technology nist rbac site includes numerous documents standards and software on rbac stat project a research and open source project that focuses on signaturebased intrusion detection tools for hosts applications and networks snort web site for snort an open source network intrusion prevention and detection system antivirus online ibms site on virus information viruslist site maintained by commercial antivirus software provider good collection of useful information key terms review questions and problems key terms access control discretionary access control malware antivirus dac memory cards audit records hashed passwords rolebased access control authentication hostbased ids rbac bot intrusion detection rootkit buffer overflow intrusion detections system smart cards digital immune system ids worm review questions in general terms what are four means of authenticating a users identity explain the purpose of the salt in figure explain the difference between a simple memory card and a smart card list and briefly describe the principal physical characteristics used for biometric identification briefly describe the difference between dac and rbac explain the difference between anomaly intrusion detection and signature intrusion detection what is a digital immune system how does behaviorblocking software work describe some worm countermeasures what types of programming languages are vulnerable to buffer overflows what are the two broad categories of defenses against buffer overflows list and briefly describe some of the defenses against buffer overflows that can be used when compiling new programs list and briefly describe some of the defenses against buffer overflows that can be implemented when running existing vulnerable programs problems explain the suitability or unsuitability of the following passwords a yk e aristotle b mfmitm for my favorite f tvstove movie is tender mercies g c natalie h dribgib d washington an early attempt to force users to use less predictable passwords involved computersupplied passwords the passwords were eight characters long and were taken from the character set consisting of lowercase letters and digits they were generated by a pseudorandom number generator with possible starting values using the technology of the time the time required to search through all character strings of length from a character alphabet was years unfortunately this is not a true reflection of the actual security of the system explain the problem assume that passwords are selected from fourcharacter combinations of alphabetic characters assume that an adversary is able to attempt passwords at a rate of one per second a assuming no feedback to the adversary until each attempt has been completed what is the expected time to discover the correct password b assuming feedback to the adversary flagging an error as each incorrect character is entered what is the expected time to discover the correct password assume that source elements of length k are mapped in some uniform fashion into a target elements of length p if each digit can take on one of r values then the number of source elements is rk and the number of target elements is the smaller number rp a particular source element xi is mapped to a particular target element yj a what is the probability that the correct source element can be selected by an adversary on one try b what is the probability that a different source element xk xi xk that results in the same target element yj could be produced by an adversary c what is the probability that the correct target element can be produced by an adversary on one try assume that passwords are limited to the use of the printable ascii characters and that all passwords are characters in length assume a password cracker with an encryption rate of million encryptions per second how long will it take to test exhaustively all possible passwords on a unix system because of the known risks of the unix password system the sunos documentation recommends that the password file be removed and replaced with a publicly readable file called etcpublickey an entry in the file for user a consists of a users identifier ida the users public key pua and the corresponding private key pra this private key is encrypted using des with a key derived from the users login password pa when a logs in the system decrypts epa pra to obtain pra a the system then verifies that pa was correctly supplied how b how can an opponent attack this system it was stated that the inclusion of the salt in the unix password scheme increases the difficulty of guessing by a factor of but the salt is stored in plaintext in the same entry as the corresponding ciphertext password therefore those two characters are known to the attacker and need not be guessed why is it asserted that the salt increases security assuming that you have successfully answered the preceding problem and understand the significance of the salt here is another question wouldnt it be possible to thwart completely all password crackers by dramatically increasing the salt size to say or bits for the dac model discussed in section an alternative representation of the protection state is a directed graph each subject and each object in the protection state is represented by a node a single node is used for an entity that is both subject and object a directed line from a subject to an object indicates an access right and the label on the link defines the access right a draw a directed graph that corresponds to the access matrix of figure a b draw a directed graph that corresponds to the access matrix of figure c is there a onetoone correspondence between the directed graph representation and the access matrix representation explain unix treats file directories in the same fashion as files that is both are defined by the same type of data structure called an inode as with files directories include a bit protection string if care is not taken this can create access control problems for example consider a file with protection mode octal contained in a directory with protection mode how might the file be compromised in this case in the traditional unix file access model unix systems provide a default setting for newly created files and directories which the owner may later changethe default is typically full access for the owner combined with one of the following no access for group and other readexecute access for group and none for other or readexecute access for both group and other briefly discuss the advantages and disadvantages of each of these cases including an example of a type of organization where each would be appropriate consider user accounts on a system with a web server configured to provide access to user web areas in general this scheme uses a standard directory name such as public html in a users home directory this acts as the users web area if it exists however to allow the web server to access the pages in this directory it must have at least search execute access to the users home directory readexecute access to the web directory and read access to any web pages in it consider the interaction of this requirement with the cases you discussed for the preceding problem what consequences does this requirement have note that a web server typically executes as a special user and in a group that is not shared with most users on the system are there some circumstances when running such a web service is simply not appropriate explain assume a system with n job positions for job position i the number of individual users in that position is ui and the number of permissions required for the job position is pi a for a traditional dac scheme how many relationships between users and permissions must be defined b for a rbac scheme how many relationships between users and permissions must be defined in the context of an ids we define a false positive to be an alarm generated by an ids in which the ids alerts to a condition that is actually benign a false negative occurs when an ids fails to generate an alarm when an alertworthy condition is in effect using the following diagram depict two curves that roughly indicate false positives and false negatives respectively frequency of alerts less specific conservativeness more specific or looser of signatures or stricter rewrite the function shown in figure a so that it is no longer vulnerable to a stack buffer overflow chapter distributed processing clientserver and clusters clientserver computing what is clientserver computing clientserver applications middleware serviceoriented architecture distributed message passing reliability versus unreliability blocking versus nonblocking remote procedure calls parameter passing parameter representation clientserver binding synchronous versus asynchronous objectoriented mechanisms clusters cluster configurations operating system design issues cluster computer architecture clusters compared to smp windows cluster server beowulf and linux clusters beowulf features beowulf software summary recommended reading and web sites key terms review questions and problems the reader who has persevered thus far in this account will realize the difficulties that were coped with the hazards that were encountered the mistakes that were made and the work that was done the world crisis winston churchill learning objectives after studying this chapter you should be able to present a summary of the key aspects of clientserver computing define serviceoriented architecture understand the principle design issues for distributed message passing understand the principle design issues for remote procedure calls understand the principle design issues for clusters describe the cluster mechanisms in windows and beowulf in this chapter we begin with an examination of some of the key concepts in distributed software including clientserver architecture message passing and remote procedure calls then we examine the increasingly important cluster architecture chapters and complete our discussion of distributed systems operating system objectives and functions an os is a program that controls the execution of application programs and acts as an interface between applications and the computer hardware it can be thought of as having three objectives convenience an os makes a computer more convenient to use efficiency an os allows the computer system resources to be used in an efficient manner ability to evolve an os should be constructed in such a way as to permit the effective development testing and introduction of new system functions without interfering with service let us examine these three aspects of an os in turn the operating system as a usercomputer interface the hardware and software used in providing applications to a user can be viewed in a layered or hierarchical fashion as depicted in figure the user of those applications the end user generally is not concerned with the details of computer hardware thus the end user views a computer system in terms of a set of applications an application can be expressed in a programming language and is developed by an application programmer if one were to develop an application program as a set of machine instructions that is completely responsible for controlling the computer hardware one would be faced with an overwhelmingly complex undertaking to ease this chore a set of system programs is provided some of these programs are referred to as utilities or library programs these implement frequently used functions that assist in program creation the management of files and the control of application application programs programming interface application librariesutilities software binary interface operating system instruction set architecture execution hardware system interconnect memory bus translation hardware io devices main and memory networking figure computer hardware and software structure io devices a programmer will make use of these facilities in developing an application and the application while it is running will invoke the utilities to perform certain functions the most important collection of system programs comprises the os the os masks the details of the hardware from the programmer and provides the programmer with a convenient interface for using the system it acts as mediator making it easier for the programmer and for application programs to access and use those facilities and services briefly the os typically provides services in the following areas program development the os provides a variety of facilities and services such as editors and debuggers to assist the programmer in creating programs typically these services are in the form of utility programs that while not strictly part of the core of the os are supplied with the os and are referred to as application program development tools program execution a number of steps need to be performed to execute a program instructions and data must be loaded into main memory io devices and files must be initialized and other resources must be prepared the os handles these scheduling duties for the user access to io devices each io device requires its own peculiar set of instructions or control signals for operation the os provides a uniform interface that hides these details so that programmers can access such devices using simple reads and writes controlled access to files for file access the os must reflect a detailed understanding of not only the nature of the io device disk drive tape drive but also the structure of the data contained in the files on the storage medium in the case of a system with multiple users the os may provide protection mechanisms to control access to the files system access for shared or public systems the os controls access to the system as a whole and to specific system resources the access function must provide protection of resources and data from unauthorized users and must resolve conflicts for resource contention error detection and response a variety of errors can occur while a computer system is running these include internal and external hardware errors such as a memory error or a device failure or malfunction and various software errors such as division by zero attempt to access forbidden memory location and inability of the os to grant the request of an application in each case the os must provide a response that clears the error condition with the least impact on running applications the response may range from ending the program that caused the error to retrying the operation to simply reporting the error to the application accounting a good os will collect usage statistics for various resources and monitor performance parameters such as response time on any system this information is useful in anticipating the need for future enhancements and in tuning the system to improve performance on a multiuser system the information can be used for billing purposes figure also indicates three key interfaces in a typical computer system instruction set architecture isa the isa defines the repertoire of machine language instructions that a computer can follow this interface is the boundary between hardware and software note that both application programs and utilities may access the isa directly for these programs a subset of the instruction repertoire is available user isa the os has access to additional machine language instructions that deal with managing system resources system isa application binary interface abi the abi defines a standard for binary portability across programs the abi defines the system call interface to the operating system and the hardware resources and services available in a system through the user isa application programming interface api the api gives a program access to the hardware resources and services available in a system through the user isa supplemented with highlevel language hll library calls any system calls are usually performed through libraries using an api enables application software to be ported easily through recompilation to other systems that support the same api the operating system as resource manager a computer is a set of resources for the movement storage and processing of data and for the control of these functions the os is responsible for managing these resources can we say that it is the os that controls the movement storage and processing of data from one point of view the answer is yes by managing the computers resources the os is in control of the computers basic functions but this control is exercised in a curious way normally we think of a control mechanism as something external to that which is controlled or at least as something that is a distinct and separate part of that which is controlled for example a residential heating system is controlled by a thermostat which is separate from the heatgeneration and heatdistribution apparatus this is not the case with the os which as a control mechanism is unusual in two respects the os functions in the same way as ordinary computer software that is it is a program or suite of programs executed by the processor the os frequently relinquishes control and must depend on the processor to allow it to regain control like other computer programs the os provides instructions for the processor the key difference is in the intent of the program the os directs the processor in the use of the other system resources and in the timing of its execution of other programs but in order for the processor to do any of these things it must cease executing the os program and execute other programs thus the os relinquishes control for the processor to do some useful work and then resumes control long enough to prepare the processor to do the next piece of work the mechanisms involved in all this should become clear as the chapter proceeds computer system memory io devices operating io controller printers system keyboards software digital camera io controller etc programs and data io controller processor processor storage os programs data figure the operating system as resource manager figure suggests the main resources that are managed by the os a portion of the os is in main memory this includes the kernel or nucleus which contains the most frequently used functions in the os and at a given time other portions of the os currently in use the remainder of main memory contains user programs and data the memory management hardware in the processor and the os jointly control the allocation of main memory as we shall see the os decides when an io device can be used by a program in execution and controls access to and use of files the processor itself is a resource and the os must determine how much processor time is to be devoted to the execution of a particular user program in the case of a multipleprocessor system this decision must span all of the processors ease of evolution of an operating system a major os will evolve over time for a number of reasons hardware upgrades plus new types of hardware for example early versions of unix and the macintosh os did not employ a paging mechanism because they were run on processors without paging hardware subsequent versions of these operating systems were modified to exploit paging capabilities also paging is introduced briefly later in this chapter and is discussed in detail in chapter the use of graphics terminals and pagemode terminals instead of lineatatime scroll mode terminals affects os design for example a graphics terminal typically allows the user to view several applications at the same time through windows on the screen this requires more sophisticated support in the os new services in response to user demand or in response to the needs of system managers the os expands to offer new services for example if it is found to be difficult to maintain good performance for users with existing tools new measurement and control tools may be added to the os fixes any os has faults these are discovered over the course of time and fixes are made of course the fix may introduce new faults the need to change an os regularly places certain requirements on its design an obvious statement is that the system should be modular in construction with clearly defined interfaces between the modules and that it should be well documented for large programs such as the typical contemporary os what might be referred to as straightforward modularization is inadequate denna that is much more must be done than simply partitioning a program into modules we return to this topic later in this chapter clientserver computing the concept of clientserver computing and related concepts has become increasingly important in information technology systems this section begins with a description of the general nature of clientserver computing this is followed by a discussion of alternative ways of organizing the clientserver functions the issue of file cache consistency raised by the use of file servers is then examined finally this section introduces the concept of middleware what is clientserver computing as with other new waves in the computer field clientserver computing comes with its own set of jargon words table lists some of the terms that are commonly found in descriptions of clientserver products and applications figure attempts to capture the essence of the clientserver concept as the term suggests a clientserver environment is populated by clients and servers the client machines are generally singleuser pcs or workstations that provide a highly userfriendly interface to the end user the clientbased station generally presents the type of graphical interface that is most comfortable to users including the use of windows and a mouse microsoft windows and macintosh os provide examples of such interfaces clientbased applications are tailored for ease of use and include such familiar tools as the spreadsheet each server in the clientserver environment provides a set of shared services to the clients the most common type of server currently is the database server table clientserver terminology applications programming interface api a set of function and call programs that allow clients and servers to intercommunicate client a networked information requester usually a pc or workstation that can query database andor other information from a server middleware a set of drivers apis or other software that improves connectivity between a client application and a server relational database a database in which information access is limited to the selection of rows that satisfy all search criteria server a computer usually a highpowered workstation a minicomputer or a mainframe that houses information for manipulation by networked clients structured query language sql a language developed by ibm and standardized by ansi for addressing creating updating or querying relational databases usually controlling a relational database the server enables many clients to share access to the same database and enables the use of a highperformance computer system to manage the database in addition to clients and servers the third essential ingredient of the client server environment is the network clientserver computing is typically distributed computing users applications and resources are distributed in response to business requirements and linked by a single lan or wan or by an internet of networks lan or wan or internet server workstation client figure generic clientserver environment how does a clientserver configuration differ from any other distributed processing solution there are a number of characteristics that stand out and that together make clientserver distinct from other types of distributed processing there is a heavy reliance on bringing userfriendly applications to the user on his or her own system this gives the user a great deal of control over the timing and style of computer usage and gives departmentlevel managers the ability to be responsive to their local needs although applications are dispersed there is an emphasis on centralizing corporate databases and many network management and utility functions this enables corporate management to maintain overall control of the total capital investment in computing and information systems and to provide interoperability so that systems are tied together at the same time it relieves individual departments and divisions of much of the overhead of maintaining sophisticated computerbased facilities but enables them to choose just about any type of machine and interface they need to access data and information there is a commitment both by user organizations and vendors to open and modular systems this means that the user has more choice in selecting products and in mixing equipment from a number of vendors networking is fundamental to the operation thus network management and network security have a high priority in organizing and operating information systems clientserver applications the key feature of a clientserver architecture is the allocation of applicationlevel tasks between clients and servers figure illustrates the general case in both client and server of course the basic software is an operating system running on the hardware platform the platforms and the operating systems of client and server may differ indeed there may be a number of different types of client platforms and operating client workstation presentation services server request application logic application logic client portion response server portion communications communications software protocol software interaction client server operating system operating system hardware platform hardware platform figure generic clientserver architecture systems and a number of different types of server platforms in a single environment as long as a particular client and server share the same communications protocols and support the same applications these lowerlevel differences are irrelevant it is the communications software that enables client and server to interoperate the principal example of such software is tcpip of course the point of all of this support software communications and operating system is to provide a base for distributed applications ideally the actual functions performed by the application can be split up between client and server in a way that optimizes the use of resources in some cases depending on the application needs the bulk of the applications software executes at the server while in other cases most of the application logic is located at the client an essential factor in the success of a clientserver environment is the way in which the user interacts with the system as a whole thus the design of the user interface on the client machine is critical in most clientserver systems there is heavy emphasis on providing a graphical user interface gui that is easy to use easy to learn yet powerful and flexible thus we can think of a presentation services module in the client workstation that is responsible for providing a userfriendly interface to the distributed applications available in the environment database applications as an example that illustrates the concept of splitting application logic between client and server let us consider one of the most common families of clientserver applications those that use relational databases in this environment the server is essentially a database server interaction between client and server is in the form of transactions in which the client makes a database request and receives a database response figure illustrates in general terms the architecture of such a system the server is responsible for maintaining the database for which purpose a complex client workstation presentation services application logic server request database logic database logic response communications communications database management software protocol software system interaction client server operating system operating system hardware platform hardware platform figure clientserver architecture for database applications database management system software module is required a variety of different applications that make use of the database can be housed on client machines the glue that ties client and server together is software that enables the client to make requests for access to the servers database a popular example of such logic is the structured query language sql figure suggests that all of the application logic the software for number crunching or other types of data analysis is on the client side while the server is only concerned with managing the database whether such a configuration is appropriate depends on the style and intent of the application for example suppose that the primary purpose is to provide online access for record lookup figure a suggests how this might work suppose that the server is maintaining a database of million records called rows in relational database terminology and the user wants to perform a lookup that should result in zero one or at most a few records the user could search for these records using a number of search criteria eg records older than records referring to individuals in ohio records referring to a specific event or characteristic etc an initial client query may yield a server response that there are records that satisfy the search criteria the user then adds additional qualifiers and issues a new query this time a response indicating that there are possible records is returned finally the client issues a third request with additional qualifiers the resulting search criteria yield a single match and the record is returned to the client initial query client possible records server next query possible records final query one record returned record database a desirable clientserver use client query server records returned record database b misused clientserver figure clientserver database usage the preceding application is well suited to a clientserver architecture for two reasons there is a massive job of sorting and searching the database this requires a large disk or bank of disks a highspeed cpu and a highspeed io architecture such capacity and power is not needed and is too expensive for a singleuser workstation or pc it would place too great a traffic burden on the network to move the entire million record file to the client for searching therefore it is not enough for the server just to be able to retrieve records on behalf of a client the server needs to have database logic that enables it to perform searches on behalf of a client now consider the scenario of figure b which has the same millionrecord database in this case a single query results in the transmission of records over the network this might happen if for example the user wishes to find the grand total or mean value of some field across many records or even the entire database clearly this latter scenario is unacceptable one solution to this problem which maintains the clientserver architecture with all of its benefits is to move part of the application logic over to the server that is the server can be equipped with application logic for performing data analysis as well as data retrieval and data searching classes of clientserver applications within the general framework of clientserver there is a spectrum of implementations that divide the work between client and server differently figure illustrates in general terms some of the major options for database applications other splits are possible and the options may have a different characterization for other types of applications in any case it is useful to examine this figure to get a feel for the kind of tradeoffs possible figure depicts four classes hostbased processing hostbased processing is not true clientserver computing as the term is generally used rather hostbased processing refers to the traditional mainframe environment in which all or virtually all of the processing is done on a central host often the user interface is via a dumb terminal even if the user is employing a microcomputer the users station is generally limited to the role of a terminal emulator serverbased processing the most basic class of clientserver configuration is one in which the client is principally responsible for providing a graphical user interface while virtually all of the processing is done on the server this configuration is typical of early clientserver efforts especially departmentallevel systems the rationale behind such configurations is that the user workstation is best suited to providing a userfriendly interface and that databases and applications can easily be maintained on central systems although the user gains the advantage of a better interface this type of configuration does not generally lend itself to any significant gains in productivity or to any fundamental changes in the actual business functions that the system supports client server presentation logic application logic database logic dbms a hostbased processing presentation logic application logic database logic dbms b serverbased processing presentation logic application logic application logic database logic dbms c cooperative processing presentation logic application logic database logic database logic dbms d clientbased processing figure classes of clientserver applications clientbased processing at the other extreme virtually all application processing may be done at the client with the exception of data validation routines and other database logic functions that are best performed at the server generally some of the more sophisticated database logic functions are housed on the client side this architecture is perhaps the most common clientserver approach in current use it enables the user to employ applications tailored to local needs cooperative processing in a cooperative processing configuration the application processing is performed in an optimized fashion taking advantage of the strengths of both client and server machines and of the distribution of data such a configuration is more complex to set up and maintain but in the long run this type of configuration may offer greater user productivity gains and greater network efficiency than other clientserver approaches figures c and d correspond to configurations in which a considerable fraction of the load is on the client this socalled fat client model has been popularized by application development tools such as sybase incs powerbuilder and gupta corps sql windows applications developed with these tools are typically departmental in scope supporting between and users ecke the main benefit of the fat client model is that it takes advantage of desktop power offloading application processing from servers and making them more efficient and less likely to be bottlenecks there are however several disadvantages to the fat client strategy the addition of more functions rapidly overloads the capacity of desktop machines forcing companies to upgrade if the model extends beyond the department to incorporate many users the company must install highcapacity lans to support the large volumes of transmission between the thin servers and the fat clients finally it is difficult to maintain upgrade or replace applications distributed across tens or hundreds of desktops figure b is representative of a thin client approach this approach more nearly mimics the traditional hostcentered approach and is often the migration path for evolving corporatewide applications from the mainframe to a distributed environment threetier clientserver architecture the traditional clientserver architecture involves two levels or tiers a client tier and a server tier a threetier architecture is also common figure in this architecture the application software is distributed among three types of machines a user machine a middletier server and a backend server the user machine is the client machine we have been discussing and in the threetier model is typically a thin client the middletier machines are essentially gateways between the thin user clients and a variety of client middletier server application server backend servers data servers figure threetier clientserver architecture backend database servers the middletier machines can convert protocols and map from one type of database query to another in addition the middletier machine can mergeintegrate results from different data sources finally the middletier machine can serve as a gateway between the desktop applications and the backend legacy applications by mediating between the two worlds the interaction between the middletier server and the backend server also follows the clientserver model thus the middletier system acts as both a client and a server file cache consistency when a file server is used performance of file io can be noticeably degraded relative to local file access because of the delays imposed by the network to reduce this performance penalty individual systems can use file caches to hold recently accessed file records because of the principle of locality use of a local file cache should reduce the number of remote server accesses that must be made figure illustrates a typical distributed mechanism for caching files among a networked collection of workstations when a process makes a file access the request is presented first to the cache of the processs workstation file traffic if not satisfied there the request is passed either to the local disk if the file is stored there disk traffic or to a file server where the file is stored server traffic at the server the servers cache is first interrogated and if there is a miss then the servers disk is accessed the dual caching approach is used to reduce communications traffic client cache and disk io server cache when caches always contain exact copies of remote data we say that the caches are consistent it is possible for caches to become inconsistent when the remote data are changed and the corresponding obsolete local cache copies are not discarded this can happen if one client modifies a file that is also cached by other clients the difficulty is actually at two levels if a client adopts a policy of immediately writing network file server server file traffic client traffic server traffic client traffic cache cache cache disk disk traffic traffic server client disk disk figure distributed file cacheing in sprite any changes to a file back to the server then any other client that has a cache copy of the relevant portion of the file will have obsolete data the problem is made even worse if the client delays writing back changes to the server in that case the server itself has an obsolete version of the file and new file read requests to the server might obtain obsolete data the problem of keeping local cache copies up to date to changes in remote data is known as the cache consistency problem the simplest approach to cache consistency is to use filelocking techniques to prevent simultaneous access to a file by more than one client this guarantees consistency at the expense of performance and flexibility a more powerful approach is provided with the facility in sprite nels oust any number of remote processes may open a file for read and create their own client cache but when an open file request to a server requests write access and other processes have the file open for read access the server takes two actions first it notifies the writing process that although it may maintain a cache it must write back all altered blocks immediately upon update there can be at most one such client second the server notifies all reading processes that have the file open that the file is no longer cacheable middleware the development and deployment of clientserver products has far outstripped efforts to standardize all aspects of distributed computing from the physical layer up to the application layer this lack of standards makes it difficult to implement an integrated multivendor enterprisewide clientserver configuration because much of the benefit of the clientserver approach is tied up with its modularity and the ability to mix and match platforms and applications to provide a business solution this interoperability problem must be solved to achieve the true benefits of the clientserver approach developers must have a set of tools that provide a uniform means and style of access to system resources across all platforms this will enable programmers to build applications that not only look and feel the same on various pcs and workstations but that use the same method to access data regardless of the location of that data the most common way to meet this requirement is by the use of standard programming interfaces and protocols that sit between the application above and communications software and operating system below such standardized interfaces and protocols have come to be referred to as middleware with standard programming interfaces it is easy to implement the same application on a variety of server types and workstation types this obviously benefits the customer but vendors are also motivated to provide such interfaces the reason is that customers buy applications not servers customers will only choose among those server products that run the applications they want the standardized protocols are needed to link these various server interfaces back to the clients that need access to them there is a variety of middleware packages ranging from the very simple to the very complex what they all have in common is the capability to hide the complexities and disparities of different network protocols and operating systems client and server vendors generally provide a number of the more popular middleware packages as options thus a user can settle on a particular middleware strategy and then assemble equipment from various vendors that support that strategy middleware architecture figure suggests the role of middleware in a clientserver architecture the exact role of the middleware component will depend on the style of clientserver computing being used referring back to figure recall that there are a number of different clientserver approaches depending on the way in which application functions are split up in any case figure gives a good general idea of the architecture involved note that there is both a client and server component of middleware the basic purpose of middleware is to enable an application or user at a client to access a variety of services on servers without being concerned about differences among servers to look at one specific application area the structured query language sql is supposed to provide a standardized means for access to a relational database by either a local or remote user or application however many relational database vendors although they support sql have added their own proprietary extensions to sql this enables vendors to differentiate their products but also creates potential incompatibilities as an example consider a distributed system used to support among other things the personnel department the basic employee data such as employee name and address might be stored on a gupta database whereas salary information might be contained on an oracle database when a user in the personnel department requires access to particular records that user does not want to be concerned with which vendors database contains the records needed middleware provides a layer of software that enables uniform access to these differing systems it is instructive to look at the role of middleware from a logical rather than an implementation point of view this viewpoint is illustrated in figure middleware enables the realization of the promise of distributed clientserver computing the entire distributed system can be viewed as a set of applications and resources available to users users need not be concerned with the location of data client workstation presentation services application logic server middleware middleware middleware interaction communications communications application software protocol software services interaction client server operating system operating system hardware platform hardware platform figure the role of middleware in clientserver architecture application application apis middleware distributed system services platform interfaces platform platform os os hardware hardware figure logical view of middleware or indeed the location of applications all applications operate over a uniform applications programming interface api the middleware which cuts across all client and server platforms is responsible for routing client requests to the appropriate server although there is a wide variety of middleware products these products are typically based on one of two underlying mechanisms message passing or remote procedure calls these two methods are examined in the next two sections serviceoriented architecture the serviceoriented architecture soa is a form of clientserver architecture that now enjoys widespread use in enterprise systems an soa organizes business functions into a modular structure rather than as monolithic applications for each department as a result common functions can be used by different departments internally and by external business partners as well the more finegrained the modules the more they can be reused in general an soa consists of a set of services and a set of client applications that use these services a client request may involve a single service or may involve two or more services to coordinating some activity requiring communication of services with each other the services are available through published and discoverable interfaces standardized interfaces are used to enable service modules to communicate with one another and to enable client applications to communicate with service modules the most popular interface is the use of xml extensible markup language over http hypertext transfer protocol known as web services soas are also implemented using other standards such as corba common object request broker architecture at a top level an soa contains three types of architectural elements bih illustrated in figure service provider a network node that provides a service interface for a software asset that manages a specific set of tasks a service provider node can represent the services of a business entity or it can simply represent the service interface for a reusable subsystem service requestor a network node that discovers and invokes other software services to provide a business solution service requestor nodes will often represent a business application component that performs remote procedure calls to a distributed object the service provider in some cases the provider node may reside locally within an intranet or in other cases it could reside remotely over the internet the conceptual nature of soa leaves the networking transport protocol and security details to the specific implementation service broker a specific kind of service provider that acts as a registry and allows for the lookup of service provider interfaces and service locations the service broker can pass on service requests to one or more additional service providers brow lists the following as key characteristics for effective use of services coarsegrained operations on services are frequently implemented to encompass more functionality and operate on larger data sets compared with componentinterface design interfacebased design services implement separately defined interfaces the benefit of this is that multiple services can implement a common interface and a service can implement multiple interfaces provisedrervice sepruvibcliesh bintodscelrivenicte broskeerrvice find requessetrevrice service figure soa model discoverable services need to be found at both design time and run time not only by unique identity but also by interface identity and by service kind single instance unlike componentbased development which instantiates components as needed each service is a single always running instance that a number of clients communicate with loosely coupled services are connected to other services and clients using standard dependencyreducing decoupled messagebased methods such as xml document exchanges asynchronous in general services use an asynchronous messagepassing approach however this is not required in fact many services will use synchronous message passing at times to give the reader some feel for the use of soa we look at an example figure a shows a common approach to building applications targeted at specific user categories for each specific application a single selfcontained application module is built what ties together the various applications in the enterprise is an applicationindependent database management system that supports a number of databases multiple applications may have access to a single database for example in this configuration all three applications require access to a customer information database the advantages of this arrangement are clear by separating the data from the applications and providing a uniform database interface multiple applications can be developed and revised independently from one another this typical approach of a variety of applications using a common set of databases has some drawbacks the addition of a new feature or user service such as atm generally requires building a new application independent of existing applications this is despite the fact that much of the necessary logic has already been implemented in related applications we can achieve greater efficiency and flexibility by migrating to an soa as shown in figure b here the strategy is to isolate services that may be of common use to multiple applications and implement these as separate service modules in this particular example of the soa there are some core applications that deal with the functionality of individual databases these applications are accessible by application programming interfaces by service modules that implement common services finally the specific applications visible to users deal primarily with presentation issues and with specific business logic distributed message passing it is usually the case in a distributed processing systems that the computers do not share main memory each is an isolated computer system thus interprocessor communication techniques that rely on shared memory such as semaphores can not be used instead techniques that rely on message passing are used in this section and the next we look at the two most common approaches the first is the straightforward application of messages as they are used in a single system the second is a separate technique that relies on message passing as a basic function the remote procedure call internet bank loan customer teller officer checking checking loan account web account management application application application credit account customer loan risk analysis info info info external a typical application structure bank atm internet loan teller customer customer officer teller atm customer web loan manager application application application application checking loan account loan request customer management service service vault service service credit risk checking loan service account management proxy application application credit risk account customer loan analysis info info info external b an architecture reflecting soa principles figure example use of soa figure a shows the use of message passing to implement clientserver functionality a client process requires some service eg read a file print and sends a message containing a request for service to a server process the server process honors the request and sends a message containing a reply in its simplest form only two functions are needed send and receive the send function specifies a destination and includes the message content the receive function tells from whom a message is desired including all and provides a buffer where the incoming message is to be stored figure suggests an implementation for message passing processes make use of the services of a messagepassing module service requests can be expressed in terms of primitives and parameters a primitive specifies the function to be performed and the parameters are used to pass data and control information the actual form of a primitive depends on the messagepassing software it may be a procedure call or it may itself be a message to a process that is part of the operating system client server application application messageoriented messageoriented middleware middleware with message queue applicationspecific with message queue transport messages transport network network a messageoriented middleware client server application rpc rpc application stub applicationspecific stub program procedure invocations program transport and returns transport network network b remote procedure calls client server application object rpc request object stub broker server program object requests object requests and responses and responses transport transport transport network network network c object request broker figure middleware mechanisms sending receiving process process messagepassing messagepassing module module processid message figure basic messagepassing primitives the send primitive is used by the process that desires to send the message its parameters are the identifier of the destination process and the contents of the message the messagepassing module constructs a data unit that includes these two elements this data unit is sent to the machine that hosts the destination process using some sort of communications facility such as tcpip when the data unit is received in the target system it is routed by the communications facility to the messagepassing module this module examines the process id field and stores the message in the buffer for that process in this scenario the receiving process must announce its willingness to receive messages by designating a buffer area and informing the messagepassing module by a receive primitive an alternative approach does not require such an announcement instead when the messagepassing module receives a message it signals the destination process with some sort of receive signal and then makes the received message available in a shared buffer several design issues are associated with distributed message passing and these are addressed in the remainder of this section reliability versus unreliability a reliable messagepassing facility is one that guarantees delivery if possible such a facility makes use of a reliable transport protocol or similar logic and performs error checking acknowledgment retransmission and reordering of misordered messages because delivery is guaranteed it is not necessary to let the sending process know that the message was delivered however it might be useful to provide an acknowledgment back to the sending process so that it knows that delivery has already taken place in either case if the facility fails to achieve delivery eg persistent network failure crash of destination system the sending process is notified of the failure at the other extreme the messagepassing facility may simply send the message out into the communications network but will report neither success nor failure this alternative greatly reduces the complexity and processing and communications overhead of the messagepassing facility for those applications that require confirmation that a message has been delivered the applications themselves may use request and reply messages to satisfy the requirement blocking versus nonblocking with nonblocking or asynchronous primitives a process is not suspended as a result of issuing a send or receive thus when a process issues a send primitive the operating system returns control to the process as soon as the message has been queued for transmission or a copy has been made if no copy is made any changes made to the message by the sending process before or even while it is being transmitted are made at the risk of the process when the message has been transmitted or copied to a safe place for subsequent transmission the sending process is interrupted to be informed that the message buffer may be reused similarly a nonblocking receive is issued by a process that then proceeds to run when a message arrives the process is informed by interrupt or it can poll for status periodically nonblocking primitives provide for efficient flexible use of the messagepassing facility by processes the disadvantage of this approach is that it is difficult to test and debug programs that use these primitives irreproducible timingdependent sequences can create subtle and difficult problems the alternative is to use blocking or synchronous primitives a blocking send does not return control to the sending process until the message has been transmitted unreliable service or until the message has been sent and an acknowledgment received reliable service a blocking receive does not return control until a message has been placed in the allocated buffer remote procedure calls a variation on the basic messagepassing model is the remote procedure call this is now a widely accepted and common method for encapsulating communication in a distributed system the essence of the technique is to allow programs on different machines to interact using simple procedure callreturn semantics just as if the two programs were on the same machine that is the procedure call is used for access to remote services the popularity of this approach is due to the following advantages the procedure call is a widely accepted used and understood abstraction the use of remote procedure calls enables remote interfaces to be specified as a set of named operations with designated types thus the interface can be clearly documented and distributed programs can be statically checked for type errors because a standardized and precisely defined interface is specified the communication code for an application can be generated automatically because a standardized and precisely defined interface is specified developers can write client and server modules that can be moved among computers and operating systems with little modification and recoding the remote procedure call mechanism can be viewed as a refinement of reliable blocking message passing figure b illustrates the general architecture and figure provides a more detailed look the calling program makes a normal procedure call with parameters on its machine for example call pxy where p procedure name x passed arguments y returned values it may or may not be transparent to the user that the intention is to invoke a remote procedure on some other machine a dummy or stub procedure p must be included in the callers address space or be dynamically linked to it at call time this procedure creates a message that identifies the procedure being called and includes the parameters it then sends this message to a remote system and waits for a reply when a reply is received the stub procedure returns to the calling program providing the returned values at the remote machine another stub program is associated with the called procedure when a message comes in it is examined and a local call px y is generated this remote procedure is thus called locally so its normal assumptions about where to find parameters the state of the stack and so on are identical to the case of a purely local procedure call several design issues are associated with remote procedure calls and these are addressed in the remainder of this section client remote server application application local local response response local local procedure local procedure calls response call local application local stub local stub or remote procedure call operating system rpc rpc mechanism mechanism remote procedure call figure remote procedure call mechanism parameter passing most programming languages allow parameters to be passed as values call by value or as pointers to a location that contains the value call by reference call by value is simple for a remote procedure call the parameters are simply copied into the message and sent to the remote system it is more difficult to implement call by reference a unique systemwide pointer is needed for each object the overhead for this capability may not be worth the effort parameter representation another issue is how to represent parameters and results in messages if the called and calling programs are in identical programming languages on the same type of machines with the same operating system then the representation requirement may present no problems if there are differences in these areas then there will probably be differences in the ways in which numbers and even text are represented if a fullblown communications architecture is used then this issue is handled by the presentation layer however the overhead of such an architecture has led to the design of remote procedure call facilities that bypass most of the communications architecture and provide their own basic communications facility in that case the conversion responsibility falls on the remote procedure call facility eg see gibb the best approach to this problem is to provide a standardized format for common objects such as integers floatingpoint numbers characters and character strings then the native parameters on any machine can be converted to and from the standardized representation clientserver binding binding specifies how the relationship between a remote procedure and the calling program will be established a binding is formed when two applications have made a logical connection and are prepared to exchange commands and data nonpersistent binding means that a logical connection is established between the two processes at the time of the remote procedure call and that as soon as the values are returned the connection is dismantled because a connection requires the maintenance of state information on both ends it consumes resources the nonpersistent style is used to conserve those resources on the other hand the overhead involved in establishing connections makes nonpersistent binding inappropriate for remote procedures that are called frequently by the same caller with persistent binding a connection that is set up for a remote procedure call is sustained after the procedure return the connection can then be used for future remote procedure calls if a specified period of time passes with no activity on the connection then the connection is terminated for applications that make many repeated calls to remote procedures persistent binding maintains the logical connection and allows a sequence of calls and returns to use the same connection synchronous versus asynchronous the concepts of synchronous and asynchronous remote procedure calls are analogous to the concepts of blocking and nonblocking messages the traditional remote procedure call is synchronous which requires that the calling process wait until the called process returns a value thus the synchronous rpc behaves much like a subroutine call the synchronous rpc is easy to understand and program because its behavior is predictable however it fails to exploit fully the parallelism inherent in distributed applications this limits the kind of interaction the distributed application can have resulting in lower performance to provide greater flexibility various asynchronous rpc facilities have been implemented to achieve a greater degree of parallelism while retaining the familiarity and simplicity of the rpc anan asynchronous rpcs do not block the caller the replies can be received as and when they are needed thus allowing client execution to proceed locally in parallel with the server invocation a typical asynchronous rpc use is to enable a client to invoke a server repeatedly so that the client has a number of requests in the pipeline at one time each with its own set of data synchronization of client and server can be achieved in one of two ways a higherlayer application in the client and server can initiate the exchange and then check at the end that all requested actions have been performed a client can issue a string of asynchronous rpcs followed by a final synchronous rpc the server will respond to the synchronous rpc only after completing all of the work requested in the preceding asynchronous rpcs in some schemes asynchronous rpcs require no reply from the server and the server can not send a reply message other schemes either require or allow a reply but the caller does not wait for the reply objectoriented mechanisms as objectoriented technology becomes more prevalent in operating system design clientserver designers have begun to embrace this approach in this approach clients and servers ship messages back and forth between objects object communications may rely on an underlying message or rpc structure or be developed directly on top of objectoriented capabilities in the operating system a client that needs a service sends a request to an object request broker which acts as a directory of all the remote service available on the network figure c the broker calls the appropriate object and passes along any relevant data then the remote object services the request and replies to the broker which returns the response to the client the success of the objectoriented approach depends on standardization of the object mechanism unfortunately there are several competing designs in this area one is microsofts component object model com the basis for object linking and embedding ole a competing approach developed by the object management group is the common object request broker architecture corba which has wide industry support ibm apple sun and many other vendors support the corba approach clusters clustering is an alternative to symmetric multiprocessing smp as an approach to providing high performance and high availability and is particularly attractive for server applications we can define a cluster as a group of interconnected whole computers working together as a unified computing resource that can create the illusion of being one machine the term whole computer means a system that can run on its own apart from the cluster in the literature each computer in a cluster is typically referred to as a node brew lists four benefits that can be achieved with clustering these can also be thought of as objectives or design requirements absolute scalability it is possible to create large clusters that far surpass the power of even the largest standalone machines a cluster can have dozens or even hundreds of machines each of which is a multiprocessor incremental scalability a cluster is configured in such a way that it is possible to add new systems to the cluster in small increments thus a user can start out with a modest system and expand it as needs grow without having to go through a major upgrade in which an existing small system is replaced with a larger system high availability because each node in a cluster is a standalone computer the failure of one node does not mean loss of service in many products fault tolerance is handled automatically in software superior priceperformance by using commodity building blocks it is possible to put together a cluster with equal or greater computing power than a single large machine at much lower cost cluster configurations in the literature clusters are classified in a number of different ways perhaps the simplest classification is based on whether the computers in a cluster share access to the same disks figure a shows a twonode cluster in which the only interconnection is by means of a highspeed link that can be used for message exchange to coordinate cluster activity the link can be a lan that is shared with other computers that are not part of the cluster or the link can be a dedicated interconnection facility in the latter case one or more of the computers in the cluster will have a link to a lan or wan so that there is a connection between the server cluster and remote client systems note that in the figure each computer is depicted as being a multiprocessor this is not necessary but does enhance both performance and availability in the simple classification depicted in figure the other alternative is a shareddisk cluster in this case there generally is still a message link between nodes in addition there is a disk subsystem that is directly linked to multiple computers within the cluster in figure b the common disk subsystem is a raid system the use of raid or some similar redundant disk technology is common in clusters so that the high availability achieved by the presence of multiple computers is not compromised by a shared disk that is a single point of failure p p p p m io io highspeed message link io io m a standby server with no shared disk highspeed message link p p io io p p m io io io io m raid b shared disk figure cluster configurations a clearer picture of the range of clustering approaches can be gained by looking at functional alternatives a white paper from hewlett packard hp provides a useful classification along functional lines table which we now discuss a common older method known as passive standby is simply to have one computer handle all of the processing load while the other computer remains inactive standing by to take over in the event of a failure of the primary to coordinate the machines the active or primary system periodically sends a heartbeat message to the standby machine should these messages stop arriving the standby assumes that the primary server has failed and puts itself into operation this approach increases availability but does not improve performance further if the only information that is exchanged between the two systems is a heartbeat message and if the two systems do not share common disks then the standby provides a functional backup but has no access to the databases managed by the primary the passive standby is generally not referred to as a cluster the term cluster is reserved for multiple interconnected computers that are all actively doing processing while maintaining the image of a single system to the outside world the term active secondary is often used in referring to this configuration three classifications of clustering can be identified separate servers shared nothing and shared memory table clustering methods benefits and limitations clustering method description benefits limitations passive standby a secondary server easy to implement high cost because the takes over in case of prisecondary server is mary server failure unavailable for other processing tasks active secondary the secondary server is reduced cost because increased complexity also used for processing secondary servers can tasks be used for processing separate servers separate servers have high availability high network and server their own disks data overhead due to copying are continuously copied operations from primary to secondary server servers connected to servers are cabled to reduced network and usually requires disk disks the same disks but each server overhead due to mirroring or raid techserver owns its disks if elimination of copying nology to compensate one server fails its disks operations for risk of disk failure are taken over by the other server servers share disks multiple servers simullow network and server requires lock manager taneously share access overhead reduced risk software usually used to disks of downtime caused by with disk mirroring or disk failure raid technology in one approach to clustering each computer is a separate server with its own disks and there are no disks shared between systems figure a this arrangement provides high performance as well as high availability in this case some type of management or scheduling software is needed to assign incoming client requests to servers so that the load is balanced and high utilization is achieved it is desirable to have a failover capability which means that if a computer fails while executing an application another computer in the cluster can pick up and complete the application for this to happen data must constantly be copied among systems so that each system has access to the current data of the other systems the overhead of this data exchange ensures high availability at the cost of a performance penalty to reduce the communications overhead most clusters now consist of servers connected to common disks figure b in one variation of this approach called shared nothing the common disks are partitioned into volumes and each volume is owned by a single computer if that computer fails the cluster must be reconfigured so that some other computer has ownership of the volumes of the failed computer it is also possible to have multiple computers share the same disks at the same time called the shared disk approach so that each computer has access to all of the volumes on all of the disks this approach requires the use of some type of locking facility to ensure that data can only be accessed by one computer at a time operating system design issues full exploitation of a cluster hardware configuration requires some enhancements to a singlesystem operating system failure management how failures are managed by a cluster depends on the clustering method used table in general two approaches can be taken to dealing with failures highly available clusters and faulttolerant clusters a highly available cluster offers a high probability that all resources will be in service if a failure occurs such as a node goes down or a disk volume is lost then the queries in progress are lost any lost query if retried will be serviced by a different computer in the cluster however the cluster operating system makes no guarantee about the state of partially executed transactions this would need to be handled at the application level a faulttolerant cluster ensures that all resources are always available this is achieved by the use of redundant shared disks and mechanisms for backing out uncommitted transactions and committing completed transactions the function of switching an application and data resources over from a failed system to an alternative system in the cluster is referred to as failover a related function is the restoration of applications and data resources to the original system once it has been fixed this is referred to as failback failback can be automated but this is desirable only if the problem is truly fixed and unlikely to recur if not automatic failback can cause subsequently failed resources to bounce back and forth between computers resulting in performance and recovery problems load balancing a cluster requires an effective capability for balancing the load among available computers this includes the requirement that the cluster be incrementally scalable when a new computer is added to the cluster the loadbalancing facility should automatically include this computer in scheduling applications middleware mechanisms need to recognize that services can appear on different members of the cluster and may migrate from one member to another parallelizing computation in some cases effective use of a cluster requires executing software from a single application in parallel kapp lists three general approaches to the problem parallelizing compiler a parallelizing compiler determines at compile time which parts of an application can be executed in parallel these are then split off to be assigned to different computers in the cluster performance depends on the nature of the problem and how well the compiler is designed parallelized application in this approach the programmer writes the application from the outset to run on a cluster and uses message passing to move data as required between cluster nodes this places a high burden on the programmer but may be the best approach for exploiting clusters for some applications parametric computing this approach can be used if the essence of the application is an algorithm or program that must be executed a large number of times each time with a different set of starting conditions or parameters a good example is a simulation model which will run a large number of different scenarios and then develop statistical summaries of the results for this approach to be effective parametric processing tools are needed to organize run and manage the jobs in an orderly manner cluster computer architecture figure shows a typical cluster architecture the individual computers are connected by some highspeed lan or switch hardware each computer is capable of operating independently in addition a middleware layer of software is installed in each computer to enable cluster operation the cluster middleware provides a unified system image to the user known as a singlesystem image the middleware may also be responsible for providing high availability by means of load balancing and responding to failures in individual components hwan lists the following as desirable cluster middleware services and functions single entry point a user logs on to the cluster rather than to an individual computer single file hierarchy the user sees a single hierarchy of file directories under the same root directory single control point there is a default node used for cluster management and control single virtual networking any node can access any other point in the cluster even though the actual cluster configuration may consist of multiple interconnected networks there is a single virtual network operation single memory space distributed shared memory enables programs to share variables parallel applications sequential applications parallel programming environment cluster middleware single system image and availability infrastructure pcworkstation pcworkstation pcworkstation pcworkstation pcworkstation comm sw comm sw comm sw comm sw comm sw net interface hw net interface hw net interface hw net interface hw net interface hw highspeednetworkswitch figure cluster computer architecture single jobmanagement system under a cluster job scheduler a user can submit a job without specifying the host computer to execute the job single user interface a common graphic interface supports all users regardless of the workstation from which they enter the cluster single io space any node can remotely access any io peripheral or disk device without knowledge of its physical location single process space a uniform processidentification scheme is used a process on any node can create or communicate with any other process on a remote node checkpointing this function periodically saves the process state and intermediate computing results to allow rollback recovery after a failure process migration this function enables load balancing the last four items on the preceding list enhance the availability of the cluster the remaining items are concerned with providing a single system image returning to figure a cluster will also include software tools for enabling the efficient execution of programs that are capable of parallel execution clusters compared to smp both clusters and symmetric multiprocessors provide a configuration with multiple processors to support highdemand applications both solutions are commercially available although smp has been around far longer the main strength of the smp approach is that an smp is easier to manage and configure than a cluster the smp is much closer to the original singleprocessor model for which nearly all applications are written the principal change required in going from a uniprocessor to an smp is to the scheduler function another benefit of the smp is that it usually takes up less physical space and draws less power than a comparable cluster a final important benefit is that the smp products are well established and stable over the long run however the advantages of the cluster approach are likely to result in clusters dominating the highperformance server market clusters are far superior to smps in terms of incremental and absolute scalability clusters are also superior in terms of availability because all components of the system can readily be made highly redundant windows cluster server windows failover clustering is a sharednothing cluster in which each disk volume and other resources are owned by a single system at a time the windows cluster design makes use of the following concepts cluster service the collection of software on each node that manages all clusterspecific activity resource an item managed by the cluster service all resources are objects representing actual resources in the system including hardware devices such as disk drives and network cards and logical items such as logical disk volumes tcpip addresses entire applications and databases online a resource is said to be online at a node when it is providing service on that specific node group a collection of resources managed as a single unit usually a group contains all of the elements needed to run a specific application and for client systems to connect to the service provided by that application the concept of group is of particular importance a group combines resources into larger units that are easily managed both for failover and load balancing operations performed on a group such as transferring the group to another node automatically affect all of the resources in that group resources are implemented as dynamically linked libraries dlls and managed by a resource monitor the resource monitor interacts with the cluster service via remote procedure calls and responds to cluster service commands to configure and move resource groups figure depicts the windows clustering components and their relationships in a single system of a cluster the node manager is responsible for maintaining this nodes membership in the cluster periodically it sends heartbeat messages to the node managers on other nodes in the cluster in the event that one node manager detects a loss of heartbeat messages from another cluster node it broadcasts a cluster management tools cluster api dll rpc cluster global update service database manager manager node event processor manager app failover mgr communication other resource resource mgr manager nodes dll resource monitors resource management interface physical logical app nonaware resource resource resource app dll dll dll clusteraware app figure windows cluster server block diagram message to the entire cluster causing all members to exchange messages to verify their view of current cluster membership if a node manager does not respond it is removed from the cluster and its active groups are transferred to one or more other active nodes in the cluster the configuration database manager maintains the cluster configuration database the database contains information about resources and groups and node ownership of groups the database managers on each of the cluster nodes cooperate to maintain a consistent picture of configuration information faulttolerant transaction software is used to assure that changes in the overall cluster configuration are performed consistently and correctly the resource managerfailover manager makes all decisions regarding resource groups and initiates appropriate actions such as startup reset and failover when failover is required the failover managers on the active node cooperate to negotiate a distribution of resource groups from the failed system to the remaining active systems when a system restarts after a failure the failover manager can decide to move some groups back to this system in particular any group may be configured with a preferred owner if that owner fails and then restarts the group is moved back to the node in a rollback operation the event processor connects all of the components of the cluster service handles common operations and controls cluster service initialization the communications manager manages message exchange with all other nodes of the cluster the global update manager provides a service used by other components within the cluster service microsoft is continuing to ship their cluster product but they have also developed virtualization solutions based on efficient live migration of virtual machines between hypervisors running on different computer systems as part of windows server r for new applications live migration offers many benefits over the cluster approach such as simpler management and improved flexibility beowulf and linux clusters in the beowulf project was initiated under the sponsorship of the nasa high performance computing and communications hpcc project its goal was to investigate the potential of clustered pcs for performing important computation tasks beyond the capabilities of contemporary workstations at minimum cost today the beowulf approach is widely implemented and is perhaps the most important cluster technology available beowulf features key features of beowulf include the following ridg mass market commodity components dedicated processors rather than scavenging cycles from idle workstations a dedicated private network lan or wan or internetted combination no custom components distributed shared storage linux workstations ethernet or interconnected ethernets figure generic beowulf configuration easy replication from multiple vendors scalable io a freely available software base use of freely available distribution computing tools with minimal changes return of the design and improvements to the community although elements of beowulf software have been implemented on a number of different platforms the most obvious choice for a base is linux and most beowulf implementations use a cluster of linux workstations andor pcs figure depicts a representative configuration the cluster consists of a number of workstations perhaps of differing hardware platforms all running the linux operating system secondary storage at each workstation may be made available for distributed access for distributed file sharing distributed virtual memory or other uses the cluster nodes the linux systems are interconnected with a commodity networking approach typically ethernet the ethernet support may be in the form of a single ethernet switch or an interconnected set of switches commodity ethernet products at the standard data rates mbps mbps gbps are used beowulf software the beowulf software environment is implemented as an addon to commercially available royaltyfree base linux distributions the principal source of opensource beowulf software is the beowulf site at wwwbeowulforg but numerous other organizations also offer free beowulf tools and utilities each node in the beowulf cluster runs its own copy of the linux kernel and can function as an autonomous linux system to support the beowulf cluster concept extensions are made to the linux kernel to allow the individual nodes to participate in a number of global namespaces the following are examples of beowulf system software beowulf distributed process space bproc this package allows a process id space to span multiple nodes in a cluster environment and also provides mechanisms for starting processes on other nodes the goal of this package is to provide key elements needed for a single system image on beowulf cluster bproc provides a mechanism to start processes on remote nodes without ever logging into another node and by making all the remote processes visible in the process table of the clusters frontend node beowulf ethernet channel bonding this is a mechanism that joins multiple lowcost networks into a single logical network with higher bandwidth the only additional work over using single network interface is the computationally simple task of distributing the packets over the available device transmit queues this approach allows load balancing over multiple ethernets connected to linux workstations pvmsync this is a programming environment that provides synchronization mechanisms and shared data objects for processes in a beowulf cluster enfuzion enfuzion consists of a set of tools for doing parametric computing as described in section parametric computing involves the execution of a program as a large number of jobs each with different parameters or starting conditions enfusion emulates a set of robot users on a single root node machine each of which will log into one of the many clients that form a cluster each job is set up to run with a unique programmed scenario with an appropriate set of starting conditions kapp summary clientserver computing is the key to realizing the potential of information systems and networks to improve productivity significantly in organizations with client server computing applications are distributed to users on singleuser workstations and personal computers at the same time resources that can and should be shared are maintained on server systems that are available to all clients thus the client server architecture is a blend of decentralized and centralized computing typically the client system provides a graphical user interface gui that enables a user to exploit a variety of applications with minimal training and relative ease servers support shared utilities such as database management systems the actual application is divided between client and server in a way intended to optimize ease of use and performance the key mechanism required in any distributed system is interprocess communication two techniques are in common use a messagepassing facility generalizes the use of messages within a single system the same sorts of conventions and synchronization rules apply another approach is the use of the remote procedure call this is a technique by which two programs on different machines interact using procedure callreturn syntax and semantics both the called and calling program behave as if the partner program were running on the same machine a cluster is a group of interconnected whole computers working together as a unified computing resource that can create the illusion of being one machine the term whole computer means a system that can run on its own apart from the cluster recommended reading and web sites bers provides a good technical discussion of the design issues involved in allocating applications to client and server and in middleware approaches the book also discusses products and standardization efforts reaga and reagb cover clientserver computing and network design approaches for supporting client server computing a good overview of middleware technology and products is brit mena provides a performance comparison of remote procedure calls and distributed message passing two worthwhile surveys of soa are brow and bih cher and bieb analyze the impact of migrating to a soa hutc discusses strategies for migrating to an soa care looks at the role of data services in an soa tane is a survey of distributed operating systems that covers both distributed process communication and distributed process management chan provides an overview of distributed message passing operating systems tay is a survey of the approach taken by various operating systems in implementing remote procedure calls a thorough treatment of clusters can be found in buyya and buyyb the former has a good treatment of beowulf which is also nicely covered in ridg a more detailed treatment of beowulf is sterwindows cluster server is described in shor raja provides a more detailed treatment lai provides a close examination of thin client architecture bers berson a clientserver architecture new york mcgrawhill bieb bieberstein n et al impact of serviceoriented architecture on enterprise systems organizational structures and individuals ibm systems journal vol no bih bih j service oriented architecture soa a new paradigm to implement dynamic ebusiness solutions acm ubiquity august acmorgubiquity viewsvisoahtml brit britton c it architectures and middleware reading ma addisonwesley brow brown a johnston s and kelly k using serviceoriented architecture and componentbased development to build web service applications ibm rational software technical report ibmcomdeveloperworksrational libraryhtml buyya buyya r high performance cluster computing architectures and systems upper saddle river nj prentice hall buyyb buyya r high performance cluster computing programming and applications upper saddle river nj prentice hall care carey m soa what ieee computer march chan chandras r distributed message passing operating systems operating systems review january cher cherbacko l et al impact of service orientation at the business level ibm systems journal vol no hutc hutchinson j et al migrating to soas by way of hybrid systems it pro januaryfebruary lai lai a and nieh j on the performance of widearea thinclient computing acm transactions on computer systems may mena menasce d mom vs rpc communication models for distributed applications ieee internet computing marchapril raja rajagopal r introduction to microsoft windows nt cluster server boca raton fl crc press reaga reagan p clientserver computing upper saddle river nj prentice hall reagb reagan p clientserver network design operation and management upper saddle river nj prentice hall ridg ridge d et al beowulf harnessing the power of parallelism in a pileofpcs proceedings ieee aerospace conference shor short r gamache r vert j and massa m windows nt clusters for availability and scalability proceedings compcon spring february ster sterling t et al how to build a beowulf cambridge ma mit press tane tanenbaum a and renesse r distributed operating systems computing surveys december tay tay b and ananda a a survey of remote procedure calls operating systems review july recommended web sites sql standards a central source of information about the sql standards process and its current documents ieee computer society task force on cluster computing an international forum to promote cluster computing research and education beowulf an international forum to promote cluster computing research and education key terms review questions and problems key terms applications programming client distributed message passing interface clientserver failback beowulf cluster failover fat client message server file cache consistency middleware thin client graphical user interface remote procedure call rpc review questions what is clientserver computing what distinguishes clientserver computing from any other form of distributed data processing what is the role of a communications architecture such as tcpip in a clientserver environment discuss the rationale for locating applications on the client the server or split between client and server what are fat clients and thin clients and what are the differences in philosophy of the two approaches suggest pros and cons for fat client and thin client strategies explain the rationale behind the threetier clientserver architecture what is middleware because we have standards such as tcpip why is middleware needed list some benefits and disadvantages of blocking and nonblocking primitives for message passing list some benefits and disadvantages of nonpersistent and persistent binding for rpcs list some benefits and disadvantages of synchronous and asynchronous rpcs list and briefly define four different clustering methods problems let be the percentage of program code that can be executed simultaneously by n computers in a cluster each computer using a different set of parameters or initial conditions assume that the remaining code must be executed sequentially by a single processor each processor has an execution rate of x mips a derive an expression for the effective mips rate when using the system for exclusive execution of this program in terms of n and x b if n and x mips determine the value of that will yield a system performance of mips an application program is executed on a ninecomputer cluster a benchmark program takes time t on this cluster further of t is time in which the application is running simultaneously on all nine computers the remaining time the application has to run on a single computer a calculate the effective speedup under the aforementioned condition as compared to executing the program on a single computer also calculate the percentage of code that has been parallelized programmed or compiled so as to use the cluster mode in the preceding program b suppose that we are able to effectively use computers rather than computers on the parallelized portion of the code calculate the effective speedup that is achieved the following fortran program is to be executed on a computer and a parallel version is to be executed on a computer cluster l do i l sumi l do j i l sumi sumi i l continue suppose lines and each take two machine cycle times including all processor and memoryaccess activities ignore the overhead caused by the software loop control statements lines and all other system overhead and resource conflicts a what is the total execution time in machine cycle times of the program on a single computer b divide the iloop iterations among the computers as follows computer executes the first iterations i to processor executes the next iterations and so on what are the execution time and speedup factor compared with part a note that the computational workload dictated by the jloop is unbalanced among the computers c explain how to modify the parallelizing to facilitate a balanced parallel execution of all the computational workload over computers a balanced load means an equal number of additions assigned to each computer with respect to both loops d what is the minimum execution time resulting from the parallel execution on computers what is the resulting speedup over a single computer the evolution of operating systems in attempting to understand the key requirements for an os and the significance of the major features of a contemporary os it is useful to consider how operating systems have evolved over the years serial processing with the earliest computers from the late s to the mids the programmer interacted directly with the computer hardware there was no os these computers were run from a console consisting of display lights toggle switches some form of input device and a printer programs in machine code were loaded via the input device eg a card reader if an error halted the program the error condition was indicated by the lights if the program proceeded to a normal completion the output appeared on the printer these early systems presented two main problems scheduling most installations used a hardcopy signup sheet to reserve computer time typically a user could sign up for a block of time in multiples of a half hour or so a user might sign up for an hour and finish in minutes this would result in wasted computer processing time on the other hand the user might run into problems not finish in the allotted time and be forced to stop before resolving the problem setup time a single program called a job could involve loading the compiler plus the highlevel language program source program into memory saving the compiled program object program and then loading and linking together the object program and common functions each of these steps could involve mounting or dismounting tapes or setting up card decks if an error occurred the hapless user typically had to go back to the beginning of the setup sequence thus a considerable amount of time was spent just in setting up the program to run this mode of operation could be termed serial processing reflecting the fact that users have access to the computer in series over time various system software tools were developed to attempt to make serial processing more efficient these include libraries of common functions linkers loaders debuggers and io driver routines that were available as common software for all users simple batch systems early computers were very expensive and therefore it was important to maximize processor utilization the wasted time due to scheduling and setup time was unacceptable to improve utilization the concept of a batch os was developed it appears that the first batch os and the first os of any kind was developed in the mids by general motors for use on an ibm weiz the concept was subsequently refined and implemented on the ibm by a number of ibm customers by the early s a number of vendors had developed batch operating systems for their computer systems ibsys the ibm os for the computers is particularly notable because of its widespread influence on other systems the central idea behind the simple batchprocessing scheme is the use of a piece of software known as the monitor with this type of os the user no longer has direct access to the processor instead the user submits the job on cards or tape to a computer operator who batches the jobs together sequentially and places the entire batch on an input device for use by the monitor each program is constructed to branch back to the monitor when it completes processing at which point the monitor automatically begins loading the next program to understand how this scheme works let us look at it from two points of view that of the monitor and that of the processor monitor point of view the monitor controls the sequence of events for this to be so much of the monitor must always be in main memory and available for execution figure that portion is referred to as the resident monitor the rest of the monitor consists of utilities and common functions that are loaded as subroutines to the user program at the beginning of any job that requires them the monitor reads in jobs one at a time from the input device typically a card reader or magnetic tape drive as it is read in the current job is placed in the user program area and control is passed to this job when the job is completed it returns control to the monitor which immediately reads in the next job the results of each job are sent to an output device such as a printer for delivery to the user processor point of view at a certain point the processor is executing instructions from the portion of main memory containing the monitor these instructions cause the next job to be read into another portion of main interrupt processing device drivers monitor job sequencing control language interpreter boundary user program area figure memory layout for a resident monitor memory once a job has been read in the processor will encounter a branch instruction in the monitor that instructs the processor to continue execution at the start of the user program the processor will then execute the instructions in the user program until it encounters an ending or error condition either event causes the processor to fetch its next instruction from the monitor program thus the phrase control is passed to a job simply means that the processor is now fetching and executing instructions in a user program and control is returned to the monitor means that the processor is now fetching and executing instructions from the monitor program the monitor performs a scheduling function a batch of jobs is queued up and jobs are executed as rapidly as possible with no intervening idle time the monitor improves job setup time as well with each job instructions are included in a primitive form of job control language jcl this is a special type of programming language used to provide instructions to the monitor a simple example is that of a user submitting a program written in the programming language fortran plus some data to be used by the program all fortran instructions and data are on a separate punched card or a separate record on tape in addition to fortran and data lines the job includes job control instructions which are denoted by the beginning the overall format of the job looks like this job ftn fortran instructions load run data end to execute this job the monitor reads the ftn line and loads the appropriate language compiler from its mass storage usually tape the compiler translates the users program into object code which is stored in memory or mass storage if it is stored in memory the operation is referred to as compile load and go if it is stored on tape then the load instruction is required this instruction is read by the monitor which regains control after the compile operation the monitor invokes the loader which loads the object program into memory in place of the compiler and transfers control to it in this manner a large segment of main memory can be shared among different subsystems although only one such subsystem could be executing at a time during the execution of the user program any input instruction causes one line of data to be read the input instruction in the user program causes an input routine that is part of the os to be invoked the input routine checks to make sure that the program does not accidentally read in a jcl line if this happens an error occurs and control transfers to the monitor at the completion of the user job the monitor will scan the input lines until it encounters the next jcl instruction thus the system is protected against a program with too many or too few data lines the monitor or batch os is simply a computer program it relies on the ability of the processor to fetch instructions from various portions of main memory to alternately seize and relinquish control certain other hardware features are also desirable memory protection while the user program is executing it must not alter the memory area containing the monitor if such an attempt is made the processor hardware should detect an error and transfer control to the monitor the monitor would then abort the job print out an error message and load in the next job timer a timer is used to prevent a single job from monopolizing the system the timer is set at the beginning of each job if the timer expires the user program is stopped and control returns to the monitor privileged instructions certain machine level instructions are designated privileged and can be executed only by the monitor if the processor encounters such an instruction while executing a user program an error occurs causing control to be transferred to the monitor among the privileged instructions are io instructions so that the monitor retains control of all io devices this prevents for example a user program from accidentally reading job control instructions from the next job if a user program wishes to perform io it must request that the monitor perform the operation for it interrupts early computer models did not have this capability this feature gives the os more flexibility in relinquishing control to and regaining control from user programs considerations of memory protection and privileged instructions lead to the concept of modes of operation a user program executes in a user mode in which certain areas of memory are protected from the users use and in which certain instructions may not be executed the monitor executes in a system mode or what has come to be called kernel mode in which privileged instructions may be executed and in which protected areas of memory may be accessed of course an os can be built without these features but computer vendors quickly learned that the results were chaos and so even relatively primitive batch operating systems were provided with these hardware features with a batch os processor time alternates between execution of user programs and execution of the monitor there have been two sacrifices some main memory is now given over to the monitor and some processor time is consumed by the monitor both of these are forms of overhead despite this overhead the simple batch system improves utilization of the computer multiprogrammed batch systems even with the automatic job sequencing provided by a simple batch os the processor is often idle the problem is that io devices are slow compared to the processor figure details a representative calculation the calculation concerns a program that processes a file of records and performs on average machine instructions per record in this example the computer spends over of its time waiting for io devices to finish transferring data to and from the file figure a illustrates this situation where we have a single program referred to as uniprogramming the processor spends a certain amount of time executing until it reaches an io instruction it must then wait until that io instruction concludes before proceeding this inefficiency is not necessary we know that there must be enough memory to hold the os resident monitor and one user program suppose that there is room for the os and two user programs when one job needs to wait for io the processor can switch to the other job which is likely not waiting for io figure b furthermore we might expand memory to hold three four or more programs and switch among all of them figure c the approach is known as multiprogramming or multitasking it is the central theme of modern operating systems read one record from file ms execute instructions ms write one record to file ms total ms percent cpu utilization figure system utilization example program a run wait run wait time a uniprogramming program a run wait run wait program b wait run wait run wait combined run run wait run run wait a b a b time b multiprogramming with two programs program a run wait run wait program b wait run wait run wait program c wait run wait run wait combined run run run wait run run run wait a b c a b c time c multiprogramming with three programs figure multiprogramming example to illustrate the benefit of multiprogramming we give a simple example consider a computer with mbytes of available memory not used by the os a disk a terminal and a printer three programs job job and job are submitted for execution at the same time with the attributes listed in table we assume minimal processor requirements for job and job and continuous disk and printer use by job for a simple batch environment these jobs will be executed in sequence thus job completes in minutes job must wait until table sample program execution attributes job job job type of job heavy compute heavy io heavy io duration min min min memory required m m m need disk no no yes need terminal no yes no need printer no no yes table effects of multiprogramming on resource utilization uniprogramming multiprogramming processor use memory use disk use printer use elapsed time min min throughput jobshr jobshr mean response time min min the minutes are over and then completes minutes after that job begins after minutes and completes at minutes from the time it was initially submitted the average resource utilization throughput and response times are shown in the uniprogramming column of table devicebydevice utilization is illustrated in figure a it is evident that there is gross underutilization for all resources when averaged over the required minute time period now suppose that the jobs are run concurrently under a multiprogramming os because there is little resource contention between the jobs all three can run in nearly minimum time while coexisting with the others in the computer assuming that job and job are allotted enough processor time to keep their input and output operations active job will still require minutes to complete but at the end of that time job will be onethird finished and job half finished all three jobs will have finished within minutes the improvement is evident when examining the multiprogramming column of table obtained from the histogram shown in figure b as with a simple batch system a multiprogramming batch system must rely on certain computer hardware features the most notable additional feature that is useful for multiprogramming is the hardware that supports io interrupts and dma direct memory access with interruptdriven io or dma the processor can issue an io command for one job and proceed with the execution of another job while the io is carried out by the device controller when the io operation is complete the processor is interrupted and control is passed to an interrupthandling program in the os the os will then pass control to another job multiprogramming operating systems are fairly sophisticated compared to singleprogram or uniprogramming systems to have several jobs ready to run they must be kept in main memory requiring some form of memory management in addition if several jobs are ready to run the processor must decide which one to run this decision requires an algorithm for scheduling these concepts are discussed later in this chapter timesharing systems with the use of multiprogramming batch processing can be quite efficient however for many jobs it is desirable to provide a mode in which the user interacts directly with the computer indeed for some jobs such as transaction processing an interactive mode is essential cpu cpu memory memory disk disk terminal terminal printer printer job history job job job job history job job job minutes time minutes time a uniprogramming b multiprogramming figure utilization histograms today the requirement for an interactive computing facility can be and often is met by the use of a dedicated personal computer or workstation that option was not available in the s when most computers were big and costly instead time sharing was developed just as multiprogramming allows the processor to handle multiple batch jobs at a time multiprogramming can also be used to handle multiple interactive jobs in this latter case the technique is referred to as time sharing because processor time is shared among multiple users in a timesharing system multiple users simultaneously access the system through terminals with the os interleaving the execution of each user program in a short burst or quantum of computation thus if there are n users actively requesting service at one time each user will only see on the average n of the effective computer capacity not counting os overhead however given the relatively slow human reaction time the response time on a properly designed system should be similar to that on a dedicated computer both batch processing and time sharing use multiprogramming the key differences are listed in table one of the first timesharing operating systems to be developed was the compatible timesharing system ctss corb developed at mit by a group known as project mac machineaided cognition or multipleaccess computers the system was first developed for the ibm in and later transferred to an ibm compared to later systems ctss is primitive the system ran on a computer with bit words of main memory with the resident monitor consuming of that when control was to be assigned to an interactive user the users program and data were loaded into the remaining words of main memory a program was always loaded to start at the location of the th word this simplified both the monitor and memory management a system clock generated interrupts at a rate of approximately one every seconds at each clock interrupt the os regained control and could assign the processor to another user this technique is known as time slicing thus at regular time intervals the current user would be preempted and another user loaded in to preserve the old user program status for later resumption the old user programs and data were written out to disk before the new user programs and data were read in subsequently the old user program code and data were restored in main memory when that program was next given a turn to minimize disk traffic user memory was only written out when the incoming program would overwrite it this principle is illustrated in figure assume that there are four interactive users with the following memory requirements in words job job table batch multiprogramming versus time sharing batch multiprogramming time sharing principal objective maximize processor use minimize response time source of directives to job control language commands commands entered at the operating system provided with the job terminal monitor monitor monitor job job job job free free free a b c monitor monitor monitor job job job job job job free free free d e f figure ctss operation job job initially the monitor loads job and transfers control to it a later the monitor decides to transfer control to job because job requires more memory than job job must be written out first and then job can be loaded b next job is loaded in to be run however because job is smaller than job a portion of job can remain in memory reducing disk write time c later the monitor decides to transfer control back to job an additional portion of job must be written out when job is loaded back into memory d when job is loaded part of job and the portion of job remaining in memory are retained e at this point if either job or job is activated only a partial load will be required in this example it is job that runs next this requires that job and the remaining resident portion of job be written out and that the missing portion of job be read in f the ctss approach is primitive compared to presentday time sharing but it was effective it was extremely simple which minimized the size of the monitor because a job was always loaded into the same locations in memory there was no need for relocation techniques at load time discussed subsequently the technique of only writing out what was necessary minimized disk activity running on the ctss supported a maximum of users time sharing and multiprogramming raise a host of new problems for the os if multiple jobs are in memory then they must be protected from interfering with each other by for example modifying each others data with multiple interactive users the file system must be protected so that only authorized users have access to a particular file the contention for resources such as printers and mass storage devices must be handled these and other problems with possible solutions will be encountered throughout this text major achievements operating systems are among the most complex pieces of software ever developed this reflects the challenge of trying to meet the difficult and in some cases competing objectives of convenience efficiency and ability to evolve denna proposes that there have been four major theoretical advances in the development of operating systems processes memory management information protection and security scheduling and resource management each advance is characterized by principles or abstractions developed to meet difficult practical problems taken together these five areas span many of the key design and implementation issues of modern operating systems the brief review of these five areas in this section serves as an overview of much of the rest of the text the process central to the design of operating systems is the concept of process this term was first used by the designers of multics in the s dale it is a somewhat more general term than job many definitions have been given for the term process including a program in execution an instance of a program running on a computer the entity that can be assigned to and executed on a processor a unit of activity characterized by a single sequential thread of execution a current state and an associated set of system resources this concept should become clearer as we proceed three major lines of computer system development created problems in timing and synchronization that contributed to the development of the concept of the process multiprogramming batch operation time sharing and realtime transaction systems as we have seen multiprogramming was designed to keep the processor and io devices including storage devices simultaneously busy to achieve maximum efficiency the key mechanism is this in response to signals indicating the completion of io transactions the processor is switched among the various programs residing in main memory a second line of development was generalpurpose time sharing here the key design objective is to be responsive to the needs of the individual user and yet for cost reasons be able to support many users simultaneously these goals are compatible because of the relatively slow reaction time of the user for example if a typical user needs an average of seconds of processing time per minute then close to such users should be able to share the same system without noticeable interference of course os overhead must be factored into such calculations a third important line of development has been realtime transaction processing systems in this case a number of users are entering queries or updates against a database an example is an airline reservation system the key difference between the transaction processing system and the timesharing system is that the former is limited to one or a few applications whereas users of a timesharing system can engage in program development job execution and the use of various applications in both cases system response time is paramount the principal tool available to system programmers in developing the early multiprogramming and multiuser interactive systems was the interrupt the activity of any job could be suspended by the occurrence of a defined event such as an io completion the processor would save some sort of context eg program counter and other registers and branch to an interrupthandling routine which would determine the nature of the interrupt process the interrupt and then resume user processing with the interrupted job or some other job the design of the system software to coordinate these various activities turned out to be remarkably difficult with many jobs in progress at any one time each of which involved numerous steps to be performed in sequence it became impossible to analyze all of the possible combinations of sequences of events in the absence of some systematic means of coordination and cooperation among activities programmers resorted to ad hoc methods based on their understanding of the environment that the os had to control these efforts were vulnerable to subtle programming errors whose effects could be observed only when certain relatively rare sequences of actions occurred these errors were difficult to diagnose because they needed to be distinguished from application software errors and hardware errors even when the error was detected it was difficult to determine the cause because the precise conditions under which the errors appeared were very hard to reproduce in general terms there are four main causes of such errors denna improper synchronization it is often the case that a routine must be suspended awaiting an event elsewhere in the system for example a program that initiates an io read must wait until the data are available in a buffer before proceeding in such cases a signal from some other routine is required improper design of the signaling mechanism can result in signals being lost or duplicate signals being received failed mutual exclusion it is often the case that more than one user or program will attempt to make use of a shared resource at the same time for example two users may attempt to edit the same file at the same time if these accesses are not controlled an error can occur there must be some sort of mutual exclusion mechanism that permits only one routine at a time to perform an update against the file the implementation of such mutual exclusion is difficult to verify as being correct under all possible sequences of events nondeterminate program operation the results of a particular program normally should depend only on the input to that program and not on the activities of other programs in a shared system but when programs share memory and their execution is interleaved by the processor they may interfere with each other by overwriting common memory areas in unpredictable ways thus the order in which various programs are scheduled may affect the outcome of any particular program deadlocks it is possible for two or more programs to be hung up waiting for each other for example two programs may each require two io devices to perform some operation eg disk to tape copy one of the programs has seized control of one of the devices and the other program has control of the other device each is waiting for the other program to release the desired resource such a deadlock may depend on the chance timing of resource allocation and release what is needed to tackle these problems is a systematic way to monitor and control the various programs executing on the processor the concept of the process provides the foundation we can think of a process as consisting of three components an executable program the associated data needed by the program variables work space buffers etc the execution context of the program this last element is essential the execution context or process state is the internal data by which the os is able to supervise and control the process this internal information is separated from the process because the os has information not permitted to the process the context includes all of the information that the os needs to manage the process and that the processor needs to execute the process properly the context includes the contents of the various processor registers such as the program counter and data registers it also includes information of use to the os such as the priority of the process and whether the process is waiting for the completion of a particular io event figure indicates a way in which processes may be managed two processes a and b exist in portions of main memory that is a block of memory is allocated to each process that contains the program data and context information each process is recorded in a process list built and maintained by the os the process list contains one entry for each process which includes a pointer to the location of the block of memory that contains the process the entry may also include part or all of the execution context of the process the remainder of the execution context is stored elsewhere perhaps with the process itself as indicated in figure or frequently in a separate region of memory the process index register contains the index into the process list of the process currently controlling the processor the program counter points to the next instruction in that process to be executed the base and limit registers define the region in memory occupied main processor memory registers process index i pc i process base b list limit h j other registers context process data a program code b context process h data b program code figure typical process implementation by the process the base register is the starting address of the region of memory and the limit is the size of the region in bytes or words the program counter and all data references are interpreted relative to the base register and must not exceed the value in the limit register this prevents interprocess interference in figure the process index register indicates that process b is executing process a was previously executing but has been temporarily interrupted the contents of all the registers at the moment of as interruption were recorded in its execution context later the os can perform a process switch and resume execution of process a the process switch consists of storing the context of b and restoring the context of a when the program counter is loaded with a value pointing into as program area process a will automatically resume execution thus the process is realized as a data structure a process can either be executing or awaiting execution the entire state of the process at any instant is contained in its context this structure allows the development of powerful techniques for ensuring coordination and cooperation among processes new features can be designed and incorporated into the os eg priority by expanding the context to include any new information needed to support the feature throughout this book we will see a number of examples where this process structure is employed to solve the problems raised by multiprogramming and resource sharing a final point which we introduce briefly here is the concept of thread in essence a single process which is assigned certain resources can be broken up into multiple concurrent threads that execute cooperatively to perform the work of the process this introduces a new level of parallel activity to be managed by the hardware and software memory management the needs of users can be met best by a computing environment that supports modular programming and the flexible use of data system managers need efficient and orderly control of storage allocation the os to satisfy these requirements has five principal storage management responsibilities process isolation the os must prevent independent processes from interfering with each others memory both data and instructions automatic allocation and management programs should be dynamically allocated across the memory hierarchy as required allocation should be transparent to the programmer thus the programmer is relieved of concerns relating to memory limitations and the os can achieve efficiency by assigning memory to jobs only as needed support of modular programming programmers should be able to define program modules and to create destroy and alter the size of modules dynamically protection and access control sharing of memory at any level of the memory hierarchy creates the potential for one program to address the memory space of another this is desirable when sharing is needed by particular applications at other times it threatens the integrity of programs and even of the os itself the os must allow portions of memory to be accessible in various ways by various users longterm storage many application programs require means for storing information for extended periods of time after the computer has been powered down typically operating systems meet these requirements with virtual memory and file system facilities the file system implements a longterm store with information stored in named objects called files the file is a convenient concept for the programmer and is a useful unit of access control and protection for the os virtual memory is a facility that allows programs to address memory from a logical point of view without regard to the amount of main memory physically available virtual memory was conceived to meet the requirement of having multiple user jobs reside in main memory concurrently so that there would not be a hiatus between the execution of successive processes while one process was written out to secondary store and the successor process was read in because processes vary in size if the processor switches among a number of processes it is difficult to pack them compactly into main memory paging systems were introduced which allow processes to be comprised of a number of fixedsize blocks called pages a program references a word by means of a virtual address consisting of a page number and an offset within the page each page of a process may be located anywhere in main memory the paging system provides for a dynamic mapping between the virtual address used in the program and a real address or physical address in main memory with dynamic mapping hardware available the next logical step was to eliminate the requirement that all pages of a process reside in main memory simultaneously all the pages of a process are maintained on disk when a process is executing some of its pages are in main memory if reference is made to a page that is not in main memory the memory management hardware detects this and arranges for the missing page to be loaded such a scheme is referred to as virtual memory and is depicted in figure a a a a b b b b a a user program b a user program a b b main memory disk main memory consists of a secondary memory disk can number of fixedlength frames hold many fixedlength pages a each equal to the size of a page user program consists of some for a program to execute some number of pages pages for all or all of its pages must be in programs plus the operating system main memory are on disk as are files figure virtual memory concepts real memoryaddress processor management virtual unit address main memory disk address secondary memory figure virtual memory addressing the processor hardware together with the os provides the user with a virtual processor that has access to a virtual memory this memory may be a linear address space or a collection of segments which are variablelength blocks of contiguous addresses in either case programming language instructions can reference program and data locations in the virtual memory area process isolation can be achieved by giving each process a unique nonoverlapping virtual memory memory sharing can be achieved by overlapping portions of two virtual memory spaces files are maintained in a longterm store files and portions of files may be copied into the virtual memory for manipulation by programs figure highlights the addressing concerns in a virtual memory scheme storage consists of directly addressable by machine instructions main memory and lowerspeed auxiliary memory that is accessed indirectly by loading blocks into main memory address translation hardware memory management unit is interposed between the processor and memory programs reference locations using virtual addresses which are mapped into real main memory addresses if a reference is made to a virtual address not in real memory then a portion of the contents of real memory is swapped out to auxiliary memory and the desired block of data is swapped in during this activity the process that generated the address reference must be suspended the os designer needs to develop an address translation mechanism that generates little overhead and a storage allocation policy that minimizes the traffic between memory levels information protection and security the growth in the use of timesharing systems and more recently computer networks has brought with it a growth in concern for the protection of information the nature of the threat that concerns an organization will vary greatly depending on the circumstances however there are some generalpurpose tools that can be built into computers and operating systems that support a variety of protection and security mechanisms in general we are concerned with the problem of controlling access to computer systems and the information stored in them much of the work in security and protection as it relates to operating systems can be roughly grouped into four categories availability concerned with protecting the system against interruption confidentiality assures that users can not read data for which access is unauthorized data integrity protection of data from unauthorized modification authenticity concerned with the proper verification of the identity of users and the validity of messages or data scheduling and resource management a key responsibility of the os is to manage the various resources available to it main memory space io devices processors and to schedule their use by the various active processes any resource allocation and scheduling policy must consider three factors fairness typically we would like all processes that are competing for the use of a particular resource to be given approximately equal and fair access to that resource this is especially so for jobs of the same class that is jobs of similar demands differential responsiveness on the other hand the os may need to discriminate among different classes of jobs with different service requirements the os should attempt to make allocation and scheduling decisions to meet the total set of requirements the os should also make these decisions dynamically for example if a process is waiting for the use of an io device the os may wish to schedule that process for execution as soon as possible to free up the device for later demands from other processes efficiency the os should attempt to maximize throughput minimize response time and in the case of time sharing accommodate as many users as possible these criteria conflict finding the right balance for a particular situation is an ongoing problem for os research scheduling and resource management are essentially operationsresearch problems and the mathematical results of that discipline can be applied in addition measurement of system activity is important to be able to monitor performance and make adjustments figure suggests the major elements of the os involved in the scheduling of processes and the allocation of resources in a multiprogramming environment the os maintains a number of queues each of which is simply a list of processes waiting for some resource the shortterm queue consists of processes that are in main memory or at least an essential minimum portion of each is in main memory and are ready to run as soon as the processor is made available any one of these processes could use the processor next it is up to the shortterm scheduler or operating system service call service from process call handler code interrupt longshortio from process interrupt term term queues handler code queue queue interrupt from io shortterm scheduler code pass control to process figure key elements of an operating system for multiprogramming dispatcher to pick one a common strategy is to give each process in the queue some time in turn this is referred to as a roundrobin technique in effect the roundrobin technique employs a circular queue another strategy is to assign priority levels to the various processes with the scheduler selecting processes in priority order the longterm queue is a list of new jobs waiting to use the processor the os adds jobs to the system by transferring a process from the longterm queue to the shortterm queue at that time a portion of main memory must be allocated to the incoming process thus the os must be sure that it does not overcommit memory or processing time by admitting too many processes to the system there is an io queue for each io device more than one process may request the use of the same io device all processes waiting to use each device are lined up in that devices queue again the os must determine which process to assign to an available io device the os receives control of the processor at the interrupt handler if an interrupt occurs a process may specifically invoke some os service such as an io device handler by means of a service call in this case a service call handler is the entry point into the os in any case once the interrupt or service call is handled the shortterm scheduler is invoked to pick a process for execution the foregoing is a functional description details and modular design of this portion of the os will differ in various systems much of the research and development effort in operating systems has been directed at picking algorithms and data structures for this function that provide fairness differential responsiveness and efficiency developments leading to modern operating systems over the years there has been a gradual evolution of os structure and capabilities however in recent years a number of new design elements have been introduced into both new operating systems and new releases of existing operating systems that create a major change in the nature of operating systems these modern operating systems respond to new developments in hardware new applications and new security threats among the key hardware drivers are multiprocessor systems greatly increased processor speed highspeed network attachments and increasing size and variety of memory storage devices in the application arena multimedia applications internet and web access and clientserver computing have influenced os design with respect to security internet access to computers has greatly increased the potential threat and increasingly sophisticated attacks such as viruses worms and hacking techniques have had a profound impact on os design the rate of change in the demands on operating systems requires not just modifications and enhancements to existing architectures but new ways of organizing the os a wide range of different approaches and design elements has been tried in both experimental and commercial operating systems but much of the work fits into the following categories microkernel architecture multithreading symmetric multiprocessing distributed operating systems objectoriented design most operating systems until recently featured a large monolithic kernel most of what is thought of as os functionality is provided in these large kernels including scheduling file system networking device drivers memory management and more typically a monolithic kernel is implemented as a single process with all elements sharing the same address space a microkernel architecture assigns only a few essential functions to the kernel including address spaces interprocess communication ipc and basic scheduling other os services are provided by processes sometimes called servers that run in user mode and are treated like any other application by the microkernel this approach decouples kernel and server development servers may be customized to specific application or environment requirements the microkernel approach simplifies implementation provides flexibility and is well suited to a distributed environment in essence a microkernel interacts with local and remote server processes in the same way facilitating construction of distributed systems multithreading is a technique in which a process executing an application is divided into threads that can run concurrently we can make the following distinction thread a dispatchable unit of work it includes a processor context which includes the program counter and stack pointer and its own data area for a stack to enable subroutine branching a thread executes sequentially and is interruptable so that the processor can turn to another thread process a collection of one or more threads and associated system resources such as memory containing both code and data open files and devices this corresponds closely to the concept of a program in execution by breaking a single application into multiple threads the programmer has great control over the modularity of the application and the timing of applicationrelated events multithreading is useful for applications that perform a number of essentially independent tasks that do not need to be serialized an example is a database server that listens for and processes numerous client requests with multiple threads running within the same process switching back and forth among threads involves less processor overhead than a major process switch between different processes threads are also useful for structuring processes that are part of the os kernel as described in subsequent chapters symmetric multiprocessing smp is a term that refers to a computer hardware architecture described in chapter and also to the os behavior that exploits that architecture the os of an smp schedules processes or threads across all of the processors smp has a number of potential advantages over uniprocessor architecture including the following performance if the work to be done by a computer can be organized so that some portions of the work can be done in parallel then a system with multiple processors will yield greater performance than one with a single processor of the same type this is illustrated in figure with multiprogramming only one process can execute at a time meanwhile all other processes are waiting for the processor with multiprocessing more than one process can be running simultaneously each on a different processor availability in a symmetric multiprocessor because all processors can perform the same functions the failure of a single processor does not halt the system instead the system can continue to function at reduced performance incremental growth a user can enhance the performance of a system by adding an additional processor scaling vendors can offer a range of products with different price and performance characteristics based on the number of processors configured in the system it is important to note that these are potential rather than guaranteed benefits the os must provide tools and functions to exploit the parallelism in an smp system multithreading and smp are often discussed together but the two are independent facilities even on a uniprocessor system multithreading is useful for structuring applications and kernel processes an smp system is useful even for nonthreaded processes because several processes can run in parallel however the two facilities complement each other and can be used effectively together an attractive feature of an smp is that the existence of multiple processors is transparent to the user the os takes care of scheduling of threads or processes on time process process process a interleaving multiprogramming one processor process process process b interleaving and overlapping multiprocessing two processors blocked running figure multiprogramming and multiprocessing individual processors and of synchronization among processors this book discusses the scheduling and synchronization mechanisms used to provide the singlesystem appearance to the user a different problem is to provide the appearance of a single system for a cluster of separate computers a multicomputer system in this case we are dealing with a collection of entities computers each with its own main memory secondary memory and other io modules a distributed operating system provides the illusion of a single main memory space and a single secondary memory space plus other unified access facilities such as a distributed file system although clusters are becoming increasingly popular and there are many cluster products on the market the state of the art for distributed operating systems lags that of uniprocessor and smp operating systems we examine such systems in part eight another innovation in os design is the use of objectoriented technologies objectoriented design lends discipline to the process of adding modular extensions to a small kernel at the os level an objectbased structure enables programmers to customize an os without disrupting system integrity object orientation also eases the development of distributed tools and fullblown distributed operating systems virtual machines virtual machines and virtualizing traditionally applications have run directly on an os on a pc or a server each pc or server would run only one os at a time thus the vendor had to rewrite parts of its applications for each osplatform they would run on an effective strategy for dealing with this problem is known as virtualization virtualization technology enables a single pc or server to simultaneously run multiple operating systems or multiple sessions of a single os a machine with virtualization can host numerous applications including those that run on different operating systems on a single platform in essence the host operating system can support a number of virtual machines vm each of which has the characteristics of a particular os and in some versions of virtualization the characteristics of a particular hardware platform the vm approach is becoming a common way for businesses and individuals to deal with legacy applications and to optimize their hardware usage by maximizing the number of kinds of applications that a single computer can handle geer commercial vm offerings by companies such as vmware and microsoft are widely used with millions of copies having been sold in addition to their use in server environments these vm technologies also are used in desktop environments to run multiple operating systems typically windows and linux the specific architecture of the vm approach varies among vendors figure shows a typical arrangement the virtual machine monitor vmm or hypervisor runs on top of or is incorporated into the host os the vmm supports vms which are emulated hardware devices each vm runs a separate os the vmm handles each operating systems communications with the processor the storage medium and the network to execute programs the vmm hands off the processor control to a virtual os on a vm most vms use virtualized network applications applications applications and and and processes processes processes virtual virtual virtual machine machine machine n virtual machine monitor host operating system shared hardware figure virtual memory concept connections to communicate with one another when such communication is needed key to the success of this approach is that the vmm provides a layer between software environments and the underlying hardware and host os that is programmable transparent to the software above it and makes efficient use of the hardware below it virtual machine architecture recall from section see figure the discussion of the application programming interface the application binary interface and the instruction set architecture let us use these interface concepts to clarify the meaning of machine in the term virtual machine consider a process executing a compiled application program from the perspective of the process the machine on which it executes consists of the virtual memory space assigned to the process the processor registers it may use the userlevel machine instructions it may execute and the os system calls it may invoke for io thus the abi defines the machine as seen by a process from the perspective of an application the machine characteristics are specified by highlevel language capabilities and os and system library calls thus the api defines the machine for an application for the operating system the machine hardware defines the system that supports the operation of the os and the numerous processes that execute concurrently these processes share a file system and other io resources the system allocates real memory and io resources to the processes and allows the processes to interact with their resources from the os perspective therefore it is the isa that provides the interface between the system and machine with these considerations in mind we can consider two architectural approaches to implementing virtual machines process vms and system vms process virtual machine in essence a process vm presents an abi to an application process translates a set of os and userlevel instructions composing one platform to those of another figure a a process vm is a virtual platform for executing a single process as such the process vm is created when the process is created and terminated when the process is terminated in order to provide crossplatform portability a common implementation of the process vm architecture is as part of an overall hll application environment the resulting abi does not correspond to any specific machine instead the abi specification is designed to easily support a given hll or set of hlls and to be easily portable to a variety of isas the hll vm includes a frontend compiler that generates a virtual binary code for execution or interpretation this code can then be executed on any machine that has the process vm implemented two widely used examples of this approach are the java vm architecture and the microsoft common language infrastructure which is the foundation of the net framework much of the discussion that follows is based on smit virtualizing application architecture view guest application application process process abi vm virtualizing software software process virtual os machine host abi hardware a process vm applications applications guest os os api vmm virtualizing software system isa virtual host hardware machine b system vm figure process and system virtual machines system virtual machine in a system vm virtualizing software translates the isa used by one hardware platform to that of another note in figure a that the virtualizing software in the process vm approach makes use of the services of the host os while in the system vm approach there is logically no separate host os rather the host system os incorporates the vm capability in the system vm case the virtualizing software is host to a number of guest operating systems with each vm including its own os the vmm emulates the hardware isa so that the guest software can potentially execute a different isa from the one implemented on the host with the system vm approach a single hardware platform can support multiple isolated guest os environments simultaneously this approach provides a number of benefits including application portability support of legacy systems without the need to maintain legacy hardware and security by means of isolation of each guest os environment from the other guest environments a variant on the architecture shown in figure b is referred to as a hosted vm in this case the vmm is built on top of an existing host os the vmm relies on the host os to provide device drivers and other lowerlevel services an example of a hosted vm is the vmware gsx server os design considerations for multiprocessor and multicore symmetric multiprocessor os considerations in an smp system the kernel can execute on any processor and typically each processor does selfscheduling from the pool of available processes or threads the kernel can be constructed as multiple processes or multiple threads allowing portions of the kernel to execute in parallel the smp approach complicates the os the os designer must deal with the complexity due to sharing resources like data structures and coordinating actions like accessing devices from multiple parts of the os executing at the same time techniques must be employed to resolve and synchronize claims to resources an smp operating system manages processor and other computer resources so that the user may view the system in the same fashion as a multiprogramming uniprocessor system a user may construct applications that use multiple processes or multiple threads within processes without regard to whether a single processor or multiple processors will be available thus a multiprocessor os must provide all the functionality of a multiprogramming system plus additional features to accommodate multiple processors the key design issues include the following simultaneous concurrent processes or threads kernel routines need to be reentrant to allow several processors to execute the same kernel code simultaneously with multiple processors executing the same or different parts of the kernel kernel tables and management structures must be managed properly to avoid data corruption or invalid operations scheduling any processor may perform scheduling which complicates the task of enforcing a scheduling policy and assuring that corruption of the scheduler data structures is avoided if kernellevel multithreading is used then the opportunity exists to schedule multiple threads from the same process simultaneously on multiple processors multiprocessor scheduling is examined in chapter synchronization with multiple active processes having potential access to shared address spaces or shared io resources care must be taken to provide effective synchronization synchronization is a facility that enforces mutual exclusion and event ordering a common synchronization mechanism used in multiprocessor operating systems is locks described in chapter memory management memory management on a multiprocessor must deal with all of the issues found on uniprocessor computers and is discussed in part three in addition the os needs to exploit the available hardware parallelism to achieve the best performance the paging mechanisms on different processors must be coordinated to enforce consistency when several processors share a page or segment and to decide on page replacement the reuse of physical pages is the biggest problem of concern that is it must be guaranteed that a physical page can no longer be accessed with its old contents before the page is put to a new use reliability and fault tolerance the os should provide graceful degradation in the face of processor failure the scheduler and other portions of the os must recognize the loss of a processor and restructure management tables accordingly because multiprocessor os design issues generally involve extensions to solutions to multiprogramming uniprocessor design problems we do not treat multiprocessor operating systems separately rather specific multiprocessor issues are addressed in the proper context throughout this book multicore os considerations the considerations for multicore systems include all the design issues discussed so far in this section for smp systems but additional concerns arise the issue is one of the scale of the potential parallelism current multicore vendors offer systems with up to eight cores on a single chip with each succeeding processor technology generation the number of cores and the amount of shared and dedicated cache memory increases so that we are now entering the era of manycore systems the design challenge for a manycore multicore system is to efficiently harness the multicore processing power and intelligently manage the substantial onchip resources efficiently a central concern is how to match the inherent parallelism of a manycore system with the performance requirements of applications the potential for parallelism in fact exists at three levels in contemporary multicore system first there is hardware parallelism within each core processor known as instruction level parallelism which may or may not be exploited by application programmers and compilers second there is the potential for multiprogramming and multithreaded execution within each processor finally there is the potential for a single application to execute in concurrent processes or threads across multiple cores without strong and effective os support for the last two types of parallelism just mentioned hardware resources will not be efficiently used in essence then since the advent of multicore technology os designers have been struggling with the problem of how best to extract parallelism from computing workloads a variety of approaches are being explored for nextgeneration operating systems we introduce two general strategies in this section and consider some details in later chapters parallelism within applications most applications can in principle be subdivided into multiple tasks that can execute in parallel with these tasks then being implemented as multiple processes perhaps each with multiple threads the difficulty is that the developer must decide how to split up the application work into independently executable tasks that is the developer must decide what pieces can or should be executed asynchronously or in parallel it is primarily the compiler and the programming language features that support the parallel programming design process but the os can support this design process at minimum by efficiently allocating resources among parallel tasks as defined by the developer perhaps the most effective initiative to support developers is implemented in the latest release of the unixbased mac os x operating system mac os x includes a multicore support capability known as grand central dispatch gcd gcd does not help the developer decide how to break up a task or application into separate concurrent parts but once a developer has identified something that can be split off into a separate task gcd makes it as easy and noninvasive as possible to actually do so in essence gcd is a thread pool mechanism in which the os maps tasks onto threads representing an available degree of concurrency plus threads for blocking on io windows also has a thread pool mechanism since and thread pools have been heavily used in server applications for years what is new in gcd is the extension to programming languages to allow anonymous functions called blocks as a way of specifying tasks gcd is hence not a major evolutionary step nevertheless it is a new and valuable tool for exploiting the available parallelism of a multicore system one of apples slogans for gcd is islands of serialization in a sea of concurrency that captures the practical reality of adding more concurrency to runofthemill desktop applications those islands are what isolate developers from the thorny problems of simultaneous data access deadlock and other pitfalls of multithreading developers are encouraged to identify functions of their applications that would be better executed off the main thread even if they are made up of several sequential or otherwise partially interdependent tasks gcd makes it easy to break off the entire unit of work while maintaining the existing order and dependencies between subtasks in later chapters we look at some of the details of gcd virtual machine approach an alternative approach is to recognize that with the everincreasing number of cores on a chip the attempt to multiprogram individual cores to support multiple applications may be a misplaced use of resources jack if instead we allow one or more cores to be dedicated to a particular process and then leave the processor alone to devote its efforts to that process we avoid much of the overhead of task switching and scheduling decisions the multicore os could then act as a hypervisor that makes a highlevel decision to allocate cores to applications but does little in the way of resource allocation beyond that the reasoning behind this approach is as follows in the early days of computing one program was run on a single processor with multiprogramming each application is given the illusion that it is running on a dedicated processor multiprogramming is based on the concept of a process which is an abstraction of an execution environment to manage processes the os requires protected space free from user and program interference for this purpose the distinction between kernel mode and user mode was developed in effect kernel mode and user mode abstracted the processor into two processors with all these virtual processors however come struggles over who gets the attention of the real processor the overhead of switching between all these processors starts to grow to the point where responsiveness suffers especially when multiple cores are introduced but with manycore systems we can consider dropping the distinction between kernel and user mode in this approach the os acts more like a hypervisor the programs themselves take on many of the duties of resource management the os assigns an application a processor and some memory and the program itself using metadata generated by the compiler would best know how to use these resources summary kernel mode in windows is structured in the hal the kernel and executive layers of ntos and a large number of device drivers implementing everything from device services to file systems and networking to graphics the hal hides certain differences in hardware from the other components the kernel layer manages the cpus to support multithreading and synchronization and the executive implements most kernelmode services the executive is based on kernelmode objects that represent the key executive data structures including processes threads memory sections drivers devices and synchronization objects to mention a few user processes create objects by calling system services and get back handle references which can be used in subsequent system calls to the executive components the operating system also creates objects internally the object manager maintains a namespace into which objects can be inserted for subsequent lookup the most important objects in windows are processes threads and sections processes have virtual address spaces and are containers for resources threads are the unit of execution and are scheduled by the kernel layer using a priority algorithm in which the highestpriority ready thread always runs preempting lowerpriority threads as necessary sections represent memory objects like files that can be mapped into the address spaces of processes exe and dll program images are represented as sections as is shared memory windows supports demandpaged virtual memory the paging algorithm is based on the workingset concept the system maintains several types of page lists to optimize the use of memory the various page lists are fed by trimming the working sets using complex formulas that try to reuse physical pages that have not been referenced in a long time the cache manager manages virtual addresses in the kernel that can be used to map files into memory dramatically improving case study windows chap io performance for many applications because read operations can be satisfied without accessing the disk io is performed by device drivers which follow the windows driver model each driver starts out by initializing a driver object that contains the addresses of the procedures that the system can call to manipulate devices the actual devices are represented by device objects which are created from the configuration description of the system or by the plugandplay manager as it discovers devices when enumerating the system buses devices are stacked and io request packets are passed down the stack and serviced by the drivers for each device in the device stack io is inherently asynchronous and drivers commonly queue requests for further work and return back to their caller filesystem volumes are implemented as devices in the io system the ntfs file system is based on a master file table which has one record per file or directory all the metadata in an ntfs file system is itself part of an ntfs file each file has multiple attributes which can be either in the mft record or nonresident stored in blocks outside the mft ntfs supports unicode compression journaling and encryption among many other features finally windows has a sophisticated security system based on access control lists and integrity levels each process has an authentication token that tells the identity of the user and what special privileges the process has if any each object has a security descriptor associated with it the security descriptor points to a discretionary access control list that contains access control entries that can allow or deny access to individuals or groups windows has added numerous security features in recent releases including bitlocker for encrypting entire volumes and addressspace randomization nonexecutable stacks and other measures to make successful attacks more difficult problems give one advantage and one disadvantage of the registry vs having individual ini files a mouse can have one two or three buttons all three types are in use does the hal hide this difference from the rest of the operating system why or why not the hal keeps track of time starting in the year give an example of an application where this feature is useful in sec we described the problems caused by multithreaded applications closing handles in one thread while still using them in another one possibility for fixing this would be to insert a sequence field how could this help what changes to the system would be required many components of the executive fig call other components of the executive give three examples of one component calling another one but use six different components in all chap problems win does not have signals if they were to be introduced they could be per process per thread both or neither make a proposal and explain why it is a good idea an alternative to using dlls is to statically link each program with precisely those library procedures it actually calls no more and no less if this scheme were to be introduced would it make more sense on client machines or on server machines the discussion of windows usermode scheduling mentioned that usermode and kernelmode threads had different stacks what are some reasons why separate stacks are needed windows uses mb large pages because it improves the effectiveness of the tlb which can have a profound impact on performance why is this why are mb large pages not used all the time is there any limit on the number of different operations that can be defined on an executive object if so where does this limit come from if not why not the win api call waitformultipleobjects allows a thread to block on a set of synchronization objects whose handles are passed as parameters as soon as any one of them is signaled the calling thread is released is it possible to have the set of synchronization objects include two semaphores one mutex and one critical section why or why not hint this is not a trick question but it does require some careful thought when initializing a global variable in a multithreaded program a common programming error is to allow a race condition where the variable can be initialized twice why could this be a problem windows provides the initonceexecuteonce api to prevent such races how might it be implemented name three reasons why a desktop process might be terminated what additional reason might cause a process running a modern application to be terminated modern applications must save their state to disk every time the user switches away from the application this seems inefficient as users may switch back to an application many times and the application simply resumes running why does the operating system require applications to save their state so often rather than just giving them a chance at the point the application is actually going to be terminated as described in sec there is a special handle table used to allocate ids for processes and threads the algorithms for handle tables normally allocate the first available handle maintaining the free list in lifo order in recent releases of windows this was changed so that the id table always keeps the free list in fifo order what is the problem that the lifo ordering potentially causes for allocating process ids and why does not unix have this problem suppose that the quantum is set to msec and the current thread at priority has just started a quantum suddenly an io operation completes and a priority thread is made ready about how long does it have to wait to get to run on the cpu in windows the current priority is always greater than or equal to the base priority are there any circumstances in which it would make sense to have the current priority be lower than the base priority if so give an example if not why not case study windows chap windows uses a facility called autoboost to temporarily raise the priority of a thread that holds the resource that is required by a higherpriority thread how do you think this works in windows it is easy to implement a facility where threads running in the kernel can temporarily attach to the address space of a different process why is this so much harder to implement in user mode why might it be interesting to do so name two ways to give better response time to the threads in important processes even when there is plenty of free memory available and the memory manager does not need to trim working sets the paging system can still frequently be writing to disk why windows swaps the processes for modern applications rather than reducing their working set and paging them why would this be more efficient hint it makes much less of a difference when the disk is an ssd why does the selfmap used to access the physical pages of the page directory and page tables for a process always occupy the same mb of kernel virtual addresses on the x the x can use either bit or bit page table entries windows uses bit ptes so the system can access more than gb of memory with bit ptes the selfmap uses only one pde in the page directory and thus occupies only mb of addresses rather than mb why is this if a region of virtual address space is reserved but not committed do you think a vad is created for it defend your answer which of the transitions shown in fig are policy decisions as opposed to required moves forced by system events eg a process exiting and freeing its pages suppose that a page is shared and in two working sets at once if it is evicted from one of the working sets where does it go in fig what happens when it is evicted from the second working set when a process unmaps a clean stack page it makes the transition in fig where does a dirty stack page go when unmapped why is there no transition to the modified list when a dirty stack page is unmapped suppose that a dispatcher object representing some type of exclusive lock like a mutex is marked to use a notification event instead of a synchronization event to announce that the lock has been released why would this be bad how much would the answer depend on lock hold times the length of quantum and whether the system was a multiprocessor to support posix the native ntcreateprocess api supports duplicating a process in order to support fork in unix fork is shortly followed by an exec most of the time one example where this was used historically was in the berkeley dumps program which would backup disks to magnetic tape fork was used as a way of checkpointing the dump program so it could be restarted if there was an error with the tape device chap problems give an example of how windows might do something similar using ntcreateprocess hint consider processes that host dlls to implement functionality provided by a third party a file has the following mapping give the mft run entries offset disk address consider the mft record of fig suppose that the file grew and a th block was assigned to the end of the file the number of this block is what would the mft record look like now in fig b the first two runs are each of length blocks is it just an accident that they are equal or does this have to do with the way compression works explain your answer suppose that you wanted to build windows lite which of the fields of fig could be removed without weakening the security of the system the mitigation strategy for improving security despite the continuing presence of vulnerabilities has been very successful modern attacks are very sophisticated often requiring the presence of multiple vulnerabilities to build a reliable exploit one of the vulnerabilities that is usually required is an information leak explain how an information leak can be used to defeat addressspace randomization in order to launch an attack based on returnoriented programming an extension model used by many programs web browsers office com servers involves hosting dlls to hook and extend their underlying functionality is this a reasonable model for an rpcbased service to use as long as it is careful to impersonate clients before loading the dll why not when running on a numa machine whenever the windows memory manager needs to allocate a physical page to handle a page fault it attempts to use a page from the numa node for the current threads ideal processor why what if the thread is currently running on a different processor give a couple of examples where an application might be able to recover easily from a backup based on a volume shadow copy rather the state of the disk after a system crash in sec providing new memory to the process heap was mentioned as one of the scenarios that require a supply of zeroed pages in order to satisfy security requirements give one or more other examples of virtual memory operations that require zeroed pages windows contains a hypervisor which allows multiple operating systems to run simultaneously this is available on clients but is far more important in cloud computing when a security update is applied to a guest operating system it is not much different than patching a server however when a security update is applied to the root operating system this can be a big problem for the users of cloud computing what is the nature of the problem what can be done about it case study windows chap the regedit command can be used to export part or all of the registry to a text file under all current versions of windows save the registry several times during a work session and see what changes if you have access to a windows computer on which you can install software or hardware find out what changes when a program or device is added or removed write a unix program that simulates writing an ntfs file with multiple streams it should accept a list of one or more files as arguments and write an output file that contains one stream with the attributes of all arguments and additional streams with the contents of each of the arguments now write a second program for reporting on the attributes and streams and extracting all the components operating system design in the past chapters we have covered a lot of ground and taken a look at many concepts and examples relating to operating systems but studying existing operating systems is different from designing a new one in this chapter we will take a quick look at some of the issues and tradeoffs that operating systems designers have to consider when designing and implementing a new system there is a certain amount of folklore about what is good and what is bad floating around in the operating systems community but surprisingly little has been written down probably the most important book is fred brooks classic the mythical man month in which he relates his experiences in designing and implementing ibms os the th anniversary edition revises some of that material and adds four new chapters brooks three classic papers on operating system design are hints for computer system design lampson on building systems that will fail corbato and endtoend arguments in system design saltzer et al like brooks book all three papers have survived the years extremely well most of their insights are still as valid now as when they were first published this chapter draws upon these sources as well as on personal experience as designer or codesigner of two operating systems amoeba tanenbaum et al and minix tanenbaum and woodhull since no consensus exists among operating system designers about the best way to design an operating system this chapter will thus be more personal speculative and undoubtedly more controversial than the previous ones the nature of the design problem operating system design is more of an engineering project than an exact science it is hard to set clear goals and meet them let us start with these points goals in order to design a successful operating system the designers must have a clear idea of what they want lack of a goal makes it very hard to make subsequent decisions to make this point clearer it is instructive to take a look at two programming languages pli and c pli was designed by ibm in the s because it was a nuisance to have to support both fortran and cobol and embarrassing to have academics yapping in the background that algol was better than both of them so a committee was set up to produce a language that would be all things to all people pli it had a little bit of fortran a little bit of cobol and a little bit of algol it failed because it lacked any unifying vision it was simply a collection of features at war with one another and too cumbersome to be compiled efficiently to boot now consider c it was designed by one person dennis ritchie for one purpose system programming it was a huge success in no small part because ritchie knew what he wanted and did not want as a result it is still in widespread use more than three decades after its appearance having a clear vision of what you want is crucial what do operating system designers want it obviously varies from system to system being different for embedded systems than for server systems however for generalpurpose operating systems four main items come to mind define abstractions provide primitive operations ensure isolation manage the hardware each of these items will be discussed below the most important but probably hardest task of an operating system is to define the right abstractions some of them such as processes address spaces and files have been around so long that they may seem obvious others such as threads are newer and are less mature for example if a multithreaded process that has one thread blocked waiting for keyboard input forks is there a thread in the new process also waiting for keyboard input other abstractions relate to synchronization signals the memory model modeling of io and many other areas each of the abstractions can be instantiated in the form of concrete data structures users can create processes files pipes and more the primitive operations sec interface design it should be clear by now that writing a modern operating system is not easy but where does one begin probably the best place to begin is to think about the interfaces it provides an operating system provides a set of abstractions mostly implemented by data types eg files and operations on them eg read together these form the interface to its users note that in this context the users of the operating system are programmers who write code that use system calls not people running application programs in addition to the main systemcall interface most operating systems have additional interfaces for example some programmers need to write device drivers to insert into the operating system these drivers see certain features and can make certain procedure calls these features and calls also define an interface but a very different one from one application programmers see all of these interfaces must be carefully designed if the system is to succeed guiding principles are there any principles that can guide interface design we believe there are briefly summarized they are simplicity completeness and the ability to be implemented efficiently principle simplicity a simple interface is easier to understand and implement in a bugfree way all system designers should memorize this famous quote from the pioneer french aviator and writer antoine de st exupery perfection is reached not when there is no longer anything to add but when there is no longer anything to take away operating system design chap if you want to get really picky he didnt say that he said il semble que la perfection soit atteinte non quand il ny a plus rien a ajouter mais quand il ny a plus rien a retrancher but you get the idea memorize it either way this principle says that less is better than more at least in the operating system itself another way to say this is the kiss principle keep it simple stupid principle completeness of course the interface must make it possible to do everything that the users need to do that is it must be complete this brings us to another famous quote this one from albert einstein everything should be as simple as possible but no simpler in other words the operating system should do exactly what is needed of it and no more if users need to store data it must provide some mechanism for storing data if users need to communicate with each other the operating system has to provide a communication mechanism and so on in his turing award lecture fernando corbato one of the designers of ctss and multics combined the concepts of simplicity and completeness and said first it is important to emphasize the value of simplicity and elegance for complexity has a way of compounding difficulties and as we have seen creating mistakes my definition of elegance is the achievement of a given functionality with a minimum of mechanism and a maximum of clarity the key idea here is minimum of mechanism in other words every feature function and system call should carry its own weight it should do one thing and do it well when a member of the design team proposes extending a system call or adding some new feature the others should ask whether something awful would happen if it were left out if the answer is no but somebody might find this feature useful some day put it in a userlevel library not in the operating system even if it is slower that way not every feature has to be faster than a speeding bullet the goal is to preserve what corbato called minimum of mechanism let us briefly consider two examples from our own experience minix tanenbaum and woodhull and amoeba tanenbaum et al for all intents and purposes minix until very recently had only three kernel calls send receive and sendrec the system is structured as a collection of processes with the memory manager the file system and each device driver being a separate schedulable process to a first approximation all the kernel does is schedule processes and handle message passing between them consequently only two system calls were needed send to send a message and receive to receive one the third call sendrec is simply an optimization for efficiency reasons to allow a message sec implementation turning away from the user and systemcall interfaces let us now look at how to implement an operating system in the following sections we will examine some general conceptual issues relating to implementation strategies after that we will look at some lowlevel techniques that are often helpful system structure probably the first decision the implementers have to make is what the system structure should be we examined the main possibilities in sec but will review them here an unstructured monolithic design is not a good idea except maybe for a tiny operating system in say a toaster but even there it is arguable layered systems a reasonable approach that has been well established over the years is a layered system dijkstras the system fig was the first layered operating system unix and windows also have a layered structure but the layering in both operating system design chap of them is more a way of trying to describe the system than a real guiding principle that was used in building the system for a new system designers choosing to go this route should first very carefully choose the layers and define the functionality of each one the bottom layer should always try to hide the worst idiosyncracies of the hardware as the hal does in fig probably the next layer should handle interrupts context switching and the mmu so above this level the code is mostly machine independent above this different designers will have different tastes and biases one possibility is to have layer manage threads including scheduling and interthread synchronization as shown in fig the idea here is that starting at layer we have proper threads that are scheduled normally and synchronize using a standard mechanism eg mutexes layer system call handler file system file system m virtual memory driver driver driver n threads thread scheduling thread synchronization interrupt handling context switching mmu hide the lowlevel hardware figure one possible design for a modern layered operating system in layer we might find the device drivers each one running as a separate thread with its own state program counter registers and so on possibly but not necessarily within the kernel address space such a design can greatly simplify the io structure because when an interrupt occurs it can be converted into an unlock on a mutex and a call to the scheduler to potentially schedule the newly readied thread that was blocked on the mutex minix uses this approach but in unix linux and windows the interrupt handlers run in a kind of nomans land rather than as proper threads like other threads that can be scheduled suspended and the like since a huge amount of the complexity of any operating system is in the io any technique for making it more tractable and encapsulated is worth considering above layer we would expect to find virtual memory one or more file systems and the systemcall handlers these layers are focused on providing services to applications if the virtual memory is at a lower level than the file systems then the block cache can be paged out allowing the virtual memory manager to dynamically determine how the real memory should be divided among user pages and kernel pages including the cache windows works this way sec the world according to c operating systems are normally large c or sometimes c programs consisting of many pieces written by many programmers the environment used for developing operating systems is very different from what individuals such as students are used to when writing small java programs this section is an attempt to give a very brief introduction to the world of writing an operating system for smalltime java or python programmers the c language this is not a guide to c but a short summary of some of the key differences between c and languages like python and especially java java is based on c so there are many similarities between the two python is somewhat different but still fairly similar for convenience we focus on java java python and c are all imperative languages with data types variables and control statements for example the primitive data types in c are integers including short and long ones characters and floatingpoint numbers composite data types can be constructed using arrays structures and unions the control statements in c are similar to those in java including if switch for and while statements functions and parameters are roughly the same in both languages introduction chap one feature c has that java and python do not is explicit pointers a pointer is a variable that points to ie contains the address of a variable or data structure consider the statements char c c p c c p c c p which declare c and c to be character variables and p to be a variable that points to ie contains the address of a character the first assignment stores the ascii code for the character c in the variable c the second one assigns the address of c to the pointer variable p the third one assigns the contents of the variable pointed to by p to the variable c so after these statements are executed c also contains the ascii code for c in theory pointers are typed so you are not supposed to assign the address of a floatingpoint number to a character pointer but in practice compilers accept such assignments albeit sometimes with a warning pointers are a very powerful construct but also a great source of errors when used carelessly some things that c does not have include builtin strings threads packages classes objects type safety and garbage collection the last one is a show stopper for operating systems all storage in c is either static or explicitly allocated and released by the programmer usually with the library functions malloc and free it is the latter property total programmer control over memory along with explicit pointers that makes c attractive for writing operating systems operating systems are basically realtime systems to some extent even generalpurpose ones when an interrupt occurs the operating system may have only a few microseconds to perform some action or lose critical information having the garbage collector kick in at an arbitrary moment is intolerable header files an operating system project generally consists of some number of directories each containing many c files containing the code for some part of the system along with some h header files that contain declarations and definitions used by one or more code files header files can also include simple macros such as define buffer size which allows the programmer to name constants so that when buffer size is used in the code it is replaced during compilation by the number good c programming practice is to name every constant except and and sometimes even them macros can have parameters such as define maxa b a b a b which allows the programmer to write sec performance all things being equal a fast operating system is better than a slow one however a fast unreliable operating system is not as good as a reliable slow one since complex optimizations often lead to bugs it is important to use them sparingly this notwithstanding there are places where performance is critical and optimizations are worth the effort in the following sections we will look at some techniques that can be used to improve performance in places where that is called for sec project management programmers are perpetual optimists most of them think that the way to write a program is to run to the keyboard and start typing shortly thereafter the fully debugged program is finished for very large programs it does not quite work like that in the following sections we have a bit to say about managing large software projects especially large operating system projects the mythical man month in his classic book the mythical man month fred brooks one of the designers of os who later moved to academia addresses the question of why it is so hard to build big operating systems brooks when most programmers see his claim that programmers can produce only lines of debugged code per year on large projects they wonder whether prof brooks is living in outer space perhaps on planet bug after all most of them can remember an all nighter when they produced a line program in one night how could this be the annual output of anybody with an iq what brooks pointed out is that large projects with hundreds of programmers are completely different than small projects and that the results obtained from small projects do not scale to large ones in a large project a huge amount of time is consumed planning how to divide the work into modules carefully specifying the modules and their interfaces and trying to imagine how the modules will interact even before coding begins then the modules have to be coded and debugged in isolation finally the modules have to be integrated and the system as a whole has to be tested the normal case is that each module works perfectly when tested by itself but the system crashes instantly when all the pieces are put together brooks estimated the work as being planning coding module testing system testing in other words writing the code is the easy part the hard part is figuring out what the modules should be and making module a correctly talk to module b in a small program written by a single programmer all that is left over is the easy part the title of brooks book comes from his assertion that people and time are not interchangeable there is no such unit as a manmonth or a personmonth if sec trends in operating system design in the head of the us patent office charles h duell asked thenpresident mckinley to abolish the patent office and his job because as he put it everything that can be invented has been invented cerf and navasky nevertheless thomas edison showed up on his doorstep within a few years with a couple of new items including the electric light the phonograph and the movie projector the point is that the world is constantly changing and operating systems must adapt to the new reality all the time in this section we mention a few trends that are relevant for operating system designers today to avoid confusion the hardware developments mentioned below are here already what is not here is the operating system software to use them effectively sec summary suggestions for further reading in this section we give some suggestions for further reading unlike the papers cited in the sections entitled research on in the text which are about current research these references are mostly introductory or tutorial in nature they can however serve to present material in this book from a different perspective or with a different emphasis reading list and bibliography chap introduction silberschatz et al operating system concepts th ed a general textbook on operating systems it covers processes memory management storage management protection and security distributed systems and some specialpurpose systems two case studies are given linux and windows the cover is full of dinosaurs these are legacy animals to empahsize that operating systems also carry a lot of legacy stallings operating systems th ed still another textbook on operating systems it covers all the traditional topics and also includes a small amount of material on distributed systems stevens and rago advanced programming in the unix environment this book tells how to write c programs that use the unix system call interface and the standard c library examples are based on the system v release and the bsd versions of unix the relationship of these implementations to posix is described in detail tanenbaum and woodhull operating systems design and implementation a handson way to learn about operating systems this book discusses the usual principles but in addition discusses an actual operating system minix in great detail and contains a listing of that system as an appendix processes and threads arpacidusseau and arpacidusseau operating systems three easy pieces the entire first part of this book is dedicated to virtualization of the cpu to share it with multiple processes what is nice about this book besides the fact that there is a free online version is that it introduces not only the concepts of processing and scheduling techniques but also the apis and systems calls like fork and exec in some detail andrews and schneider concepts and notations for concurrent programming a tutorial and survey of processes and interprocess communication including busy waiting semaphores monitors message passing and other techniques the article also shows how these concepts are embedded in various programming languages the article is old but it has stood the test of time very well benari principles of concurrent programming this little book is entirely devoted to the problems of interprocess communication there are chapters on mutual exclusion semaphores monitors and the dining philosophers problem among others it too has stood up very well over the years sec research on operating systems computer science is a rapidly advancing field and it is hard to predict where it is going researchers at universities and industrial research labs are constantly thinking up new ideas some of which go nowhere but some of which become the cornerstone of future products and have massive impact on the industry and users telling which is which turns out to be easier to do in hindsight than in real time separating the wheat from the chaff is especially difficult because it often takes to years from idea to impact for example when president eisenhower set up the dept of defenses advanced research projects agency arpa in he was trying to keep the army from killing the navy and the air force over the pentagons research budget he was not trying to invent the internet but one of the things arpa did was fund some university research on the thenobscure concept of packet switching which led to the first experimental packetswitched network the arpanet it went live in before long other arpafunded research networks were connected to the arpanet and the internet was born the internet was then happily used by academic researchers for sending email to each other for years in the early s tim bernerslee invented the world wide web at the cern research lab in geneva and marc andreesen wrote a graphical browser for it at the university of illinois all of a sudden the internet was full of twittering teenagers president eisenhower is probably rolling over in his grave research in operating systems has also led to dramatic changes in practical systems as we discussed earlier the first commercial computer systems were all batch systems until mit invented generalpurpose timesharing in the early s computers were all textbased until doug engelbart invented the mouse and the graphical user interface at stanford research institute in the late s who knows what will come next in this section and in comparable sections throughout the book we will take a brief look at some of the research in operating systems that has taken place during outline of the rest of this book we have now completed our introduction and birdseye view of the operating system it is time to get down to the details as mentioned already from the programmers point of view the primary purpose of an operating system is to provide metric units to avoid any confusion it is worth stating explicitly that in this book as in computer science in general metric units are used instead of traditional english units the furlongstonefortnight system the principal metric prefixes are listed in fig the prefixes are typically abbreviated by their first letters with the units greater than capitalized thus a tb database occupies bytes of storage and a psec or ps clock ticks every seconds since milli and micro both begin with the letter m a choice had to be made normally m is for milli and the greek letter mu is for micro summary operating systems can be viewed from two viewpoints resource managers and extended machines in the resourcemanager view the operating systems job is to manage the different parts of the system efficiently in the extendedmachine view the job of the system is to provide the users with abstractions that are more convenient to use than the actual machine these include processes address spaces and files operating systems have a long history starting from the days when they replaced the operator to modern multiprogramming systems highlights include early batch systems multiprogramming systems and personal computer systems since operating systems interact closely with the hardware some knowledge of computer hardware is useful to understanding them computers are built up of processors memories and io devices these parts are connected by buses the basic concepts on which all operating systems are built are processes memory management io management the file system and security each of these will be treated in a subsequent chapter sec processes all modern computers often do several things at the same time people used to working with computers may not be fully aware of this fact so a few examples may make the point clearer first consider a web server requests come in from all over asking for web pages when a request comes in the server checks to see if the page needed is in the cache if it is it is sent back if it is not a disk request is started to fetch it however from the cpus perspective disk requests take eternity while waiting for a disk request to complete many more requests may come chap in if there are multiple disks present some or all of the newer ones may be fired off to other disks long before the first request is satisfied clearly some way is needed to model and control this concurrency processes and especially threads can help here now consider a user pc when the system is booted many processes are secretly started often unknown to the user for example a process may be started up to wait for incoming email another process may run on behalf of the antivirus program to check periodically if any new virus definitions are available in addition explicit user processes may be running printing files and backing up the users photos on a usb stick all while the user is surfing the web all this activity has to be managed and a multiprogramming system supporting multiple processes comes in very handy here in any multiprogramming system the cpu switches from process to process quickly running each for tens or hundreds of milliseconds while strictly speaking at any one instant the cpu is running only one process in the course of second it may work on several of them giving the illusion of parallelism sometimes people speak of pseudoparallelism in this context to contrast it with the true hardware parallelism of multiprocessor systems which have two or more cpus sharing the same physical memory keeping track of multiple parallel activities is hard for people to do therefore operating system designers over the years have evolved a conceptual model sequential processes that makes parallelism easier to deal with that model its uses and some of its consequences form the subject of this chapter the process model in this model all the runnable software on the computer sometimes including the operating system is organized into a number of sequential processes or just processes for short a process is just an instance of an executing program including the current values of the program counter registers and variables conceptually each process has its own virtual cpu in reality of course the real cpu switches back and forth from process to process but to understand the system it is much easier to think about a collection of processes running in pseudo parallel than to try to keep track of how the cpu switches from program to program this rapid switching back and forth is called multiprogramming as we saw in chap in fig a we see a computer multiprogramming four programs in memory in fig b we see four processes each with its own flow of control ie its own logical program counter and each one running independently of the other ones of course there is only one physical program counter so when each process runs its logical program counter is loaded into the real program counter when it is finished for the time being the physical program counter is saved in the process stored logical program counter in memory in fig c we see that viewed over sec threads in traditional operating systems each process has an address space and a single thread of control in fact that is almost the definition of a process nevertheless in many situations it is desirable to have multiple threads of control in the same address space running in quasiparallel as though they were almost separate processes except for the shared address space in the following sections we will discuss these situations and their implications thread usage why would anyone want to have a kind of process within a process it turns out there are several reasons for having these miniprocesses called threads let us now examine some of them the main reason for having threads is that in many applications multiple activities are going on at once some of these may block from time to time by decomposing such an application into multiple sequential threads that run in quasiparallel the programming model becomes simpler processes and interprocess communication processes frequently need to communicate with other processes for example in a shell pipeline the output of the first process must be passed to the second process and so on down the line thus there is a need for communication between processes preferably in a wellstructured way not using interrupts in the following sections we will look at some of the issues related to this interprocess communication or ipc very briefly there are three issues here the first was alluded to above how one process can pass information to another the second has to do with making sure two or more processes do not get in each others way for example two processes in an airline reservation system each trying to grab the last seat on a plane for a different customer the third concerns proper sequencing when dependencies are present if process a produces data and process b prints them b has to wait until a has produced some data before starting to print we will examine all three of these issues starting in the next section it is also important to mention that two of these issues apply equally well to threads the first one passing information is easy for threads since they share a common address space threads in different address spaces that need to communicate fall under the heading of communicating processes however the other two keeping out of each others hair and proper sequencing apply equally well to threads the same problems exist and the same solutions apply below we will discuss the problem in the context of processes but please keep in mind that the same problems and solutions also apply to threads race conditions in some operating systems processes that are working together may share some common storage that each one can read and write the shared storage may be in main memory possibly in a kernel data structure or it may be a shared file the location of the shared memory does not change the nature of the communication or the problems that arise to see how interprocess communication works in practice let us now consider a simple but common example a print spooler when a process processes and threads chap wants to print a file it enters the file name in a special spooler directory another process the printer daemon periodically checks to see if there are any files to be printed and if there are it prints them and then removes their names from the directory imagine that our spooler directory has a very large number of slots numbered each one capable of holding a file name also imagine that there are two shared variables out which points to the next file to be printed and in which points to the next free slot in the directory these two variables might well be kept in a twoword file available to all processes at a certain instant slots to are empty the files have already been printed and slots to are full with the names of files queued for printing more or less simultaneously processes a and b decide they want to queue a file for printing this situation is shown in fig spooler directory abc out process a progc progn in process b figure two processes want to access shared memory at the same time in jurisdictions where murphys law is applicable the following could happen process a reads in and stores the value in a local variable called next free slot just then a clock interrupt occurs and the cpu decides that process a has run long enough so it switches to process b process b also reads in and also gets a it too stores it in its local variable next free slot at this instant both processes think that the next available slot is process b now continues to run it stores the name of its file in slot and updates in to be an then it goes off and does other things eventually process a runs again starting from the place it left off it looks at next free slot finds a there and writes its file name in slot erasing the name that process b just put there then it computes next free slot which is and sets in to the spooler directory is now internally consistent so the printer daemon will not notice anything wrong but process b will never receive any output user b will hang around the printer for years wistfully hoping for output that if something can go wrong it will sec scheduling adding a node a a a b b x b x c d e c d e c d e a original tree b initialize node x and c when x is completely initialized connect e to x any readers connect x to a readers currently in a and e are not affected in e will have read the old version while readers in a will pick up the new version of the tree removing nodes a a a b x b x x c d e c d e c e d decouple b from a note e wait until we are sure f now we can safely that there may still be readers that all readers have left b remove b and d in b all readers in b will see and c these nodes can not the old version of the tree be accessed any more while all readers currently in a will see the new version figure readcopyupdate inserting a node in the tree and then removing a branch all without locks classical ipc problems the operating systems literature is full of interesting problems that have been widely discussed and analyzed using a variety of synchronization methods in the following sections we will examine three of the betterknown problems the dining philosophers problem in dijkstra posed and then solved a synchronization problem he called the dining philosophers problem since that time everyone inventing yet another synchronization primitive has felt obligated to demonstrate how wonderful the new processes and threads chap primitive is by showing how elegantly it solves the dining philosophers problem the problem can be stated quite simply as follows five philosophers are seated around a circular table each philosopher has a plate of spaghetti the spaghetti is so slippery that a philosopher needs two forks to eat it between each pair of plates is one fork the layout of the table is illustrated in fig figure lunch time in the philosophy department the life of a philosopher consists of alternating periods of eating and thinking this is something of an abstraction even for philosophers but the other activities are irrelevant here when a philosopher gets sufficiently hungry she tries to acquire her left and right forks one at a time in either order if successful in acquiring two forks she eats for a while then puts down the forks and continues to think the key question is can you write a program for each philosopher that does what it is supposed to do and never gets stuck it has been pointed out that the twofork requirement is somewhat artificial perhaps we should switch from italian food to chinese food substituting rice for spaghetti and chopsticks for forks figure shows the obvious solution the procedure take fork waits until the specified fork is available and then seizes it unfortunately the obvious solution is wrong suppose that all five philosophers take their left forks simultaneously none will be able to take their right forks and there will be a deadlock we could easily modify the program so that after taking the left fork the program checks to see if the right fork is available if it is not the philosopher puts down the left one waits for some time and then repeats the whole process this proposal too fails although for a different reason with a little bit of bad luck all the philosophers could start the algorithm simultaneously picking up their left forks seeing that their right forks were not available putting down their left forks sec research on processes and threads in chap we looked at some of the current research in operating system structure in this and subsequent chapters we will look at more narrowly focused research starting with processes as will become clear in time some subjects are much more settled than others most of the research tends to be on the new topics rather than ones that have been around for decades the concept of a process is an example of something that is fairly well settled almost every system has some notion of a process as a container for grouping together related resources such as an address space threads open files protection permissions and so on different systems do the grouping slightly differently but these are just engineering differences the basic idea is not very controversial any more and there is little new research on the subject of processes threads are a newer idea than processes but they too have been chewed over quite a bit still the occasional paper about threads appears from time to time for example about thread clustering on multiprocessors tam et al or on how well modern operating systems like linux scale with many threads and many cores boydwickizer one particularly active research area deals with recording and replaying a process execution viennot et al replaying helps developers track down hardtofind bugs and security experts to investigate incidents summary to hide the effects of interrupts operating systems provide a conceptual model consisting of sequential processes running in parallel processes can be created and terminated dynamically each process has its own address space for some applications it is useful to have multiple threads of control within a single process these threads are scheduled independently and each one has its own stack but all the threads in a process share a common address space threads can be implemented in user space or in the kernel processes can communicate with one another using interprocess communication primitives for example semaphores monitors or messages these primitives are used to ensure that no two processes are ever in their critical regions at the same time a situation that leads to chaos a process can be running runnable or blocked and can change state when it or another process executes one of the interprocess communication primitives interthread communication is similar interprocess communication primitives can be used to solve such problems as the producerconsumer dining philosophers and readerwriter even with these primitives care has to be taken to avoid errors and deadlocks a great many scheduling algorithms have been studied some of these are primarily used for batch systems such as shortestjobfirst scheduling others are common in both batch systems and interactive systems these algorithms include round robin priority scheduling multilevel queues guaranteed scheduling lottery scheduling and fairshare scheduling some systems make a clean separation between the scheduling mechanism and the scheduling policy which allows users to have control of the scheduling algorithm processes and threads chap problems in fig three process states are shown in theory with three states there could be six transitions two out of each state however only four transitions are shown are there any circumstances in which either or both of the missing transitions might occur suppose that you were to design an advanced computer architecture that did process switching in hardware instead of having interrupts what information would the cpu need describe how the hardware process switching might work on all current computers at least part of the interrupt handlers are written in assembly language why when an interrupt or a system call transfers control to the operating system a kernel stack area separate from the stack of the interrupted process is generally used why a computer system has enough room to hold five programs in its main memory these programs are idle waiting for io half the time what fraction of the cpu time is wasted a computer has gb of ram of which the operating system occupies mb the processes are all mb for simplicity and have the same characteristics if the goal is cpu utilization what is the maximum io wait that can be tolerated multiple jobs can run in parallel and finish faster than if they had run sequentially suppose that two jobs each needing minutes of cpu time start simultaneously how long will the last one take to complete if they run sequentially how long if they run in parallel assume io wait consider a multiprogrammed system with degree of ie six programs in memory at the same time assume that each process spends of its time waiting for io what will be the cpu utilization assume that you are trying to download a large gb file from the internet the file is available from a set of mirror servers each of which can deliver a subset of the files bytes assume that a given request specifies the starting and ending bytes of the file explain how you might use threads to improve the download time in the text it was stated that the model of fig a was not suited to a file server using a cache in memory why not could each process have its own cache if a multithreaded process forks a problem occurs if the child gets copies of all the parents threads suppose that one of the original threads was waiting for keyboard input now two threads are waiting for keyboard input one in each process does this problem ever occur in singlethreaded processes in fig a multithreaded web server is shown if the only way to read from a file is the normal blocking read system call do you think userlevel threads or kernellevel threads are being used for the web server why in the text we described a multithreaded web server showing why it is better than a singlethreaded server and a finitestate machine server are there any circumstances in which a singlethreaded server might be better give an example chap problems in fig the register set is listed as a perthread rather than a perprocess item why after all the machine has only one set of registers why would a thread ever voluntarily give up the cpu by calling thread yield after all since there is no periodic clock interrupt it may never get the cpu back can a thread ever be preempted by a clock interrupt if so under what circumstances if not why not in this problem you are to compare reading a file using a singlethreaded file server and a multithreaded server it takes msec to get a request for work dispatch it and do the rest of the necessary processing assuming that the data needed are in the block cache if a disk operation is needed as is the case onethird of the time an additional msec is required during which time the thread sleeps how many requestssec can the server handle if it is single threaded if it is multithreaded what is the biggest advantage of implementing threads in user space what is the biggest disadvantage in fig the thread creations and messages printed by the threads are interleaved at random is there a way to force the order to be strictly thread created thread prints message thread exits thread created thread prints message thread exists and so on if so how if not why not in the discussion on global variables in threads we used a procedure create global to allocate storage for a pointer to the variable rather than the variable itself is this essential or could the procedures work with the values themselves just as well consider a system in which threads are implemented entirely in user space with the runtime system getting a clock interrupt once a second suppose that a clock interrupt occurs while some thread is executing in the runtime system what problem might occur can you suggest a way to solve it suppose that an operating system does not have anything like the select system call to see in advance if it is safe to read from a file pipe or device but it does allow alarm clocks to be set that interrupt blocked system calls is it possible to implement a threads package in user space under these conditions discuss does the busy waiting solution using the turn variable fig work when the two processes are running on a sharedmemory multiprocessor that is two cpus sharing a common memory does petersons solution to the mutualexclusion problem shown in fig work when process scheduling is preemptive how about when it is nonpreemptive can the priority inversion problem discussed in sec happen with userlevel threads why or why not in sec a situation with a highpriority process h and a lowpriority process l was described which led to h looping forever does the same problem occur if roundrobin scheduling is used instead of priority scheduling discuss in a system with threads is there one stack per thread or one stack per process when userlevel threads are used what about when kernellevel threads are used explain processes and threads chap when a computer is being developed it is usually first simulated by a program that runs one instruction at a time even multiprocessors are simulated strictly sequentially like this is it possible for a race condition to occur when there are no simultaneous events like this the producerconsumer problem can be extended to a system with multiple producers and consumers that write or read to from one shared buffer assume that each producer and consumer runs in its own thread will the solution presented in fig using semaphores work for this system consider the following solution to the mutualexclusion problem involving two processes p and p assume that the variable turn is initialized to process ps code is presented below other code while turn do nothing and wait critical section turn other code for process p replace by in above code determine if the solution meets all the required conditions for a correct mutualexclusion solution how could an operating system that can disable interrupts implement semaphores show how counting semaphores ie semaphores that can hold an arbitrary value can be implemented using only binary semaphores and ordinary machine instructions if a system has only two processes does it make sense to use a barrier to synchronize them why or why not can two threads in the same process synchronize using a kernel semaphore if the threads are implemented by the kernel what if they are implemented in user space assume that no threads in any other processes have access to the semaphore discuss your answers synchronization within monitors uses condition variables and two special operations wait and signal a more general form of synchronization would be to have a single primitive waituntil that had an arbitrary boolean predicate as parameter thus one could say for example waituntil x or y z n the signal primitive would no longer be needed this scheme is clearly more general than that of hoare or brinch hansen but it is not used why not hint think about the implementation a fastfood restaurant has four kinds of employees order takers who take customers orders cooks who prepare the food packaging specialists who stuff the food into bags and cashiers who give the bags to customers and take their money each employee can be regarded as a communicating sequential process what form of interprocess communication do they use relate this model to processes in unix chap problems suppose that we have a messagepassing system using mailboxes when sending to a full mailbox or trying to receive from an empty one a process does not block instead it gets an error code back the process responds to the error code by just trying again over and over until it succeeds does this scheme lead to race conditions the cdc computers could handle up to io processes simultaneously using an interesting form of roundrobin scheduling called processor sharing a process switch occurred after each instruction so instruction came from process instruction came from process etc the process switching was done by special hardware and the overhead was zero if a process needed t sec to complete in the absence of competition how much time would it need if processor sharing was used with n processes consider the following piece of c code void main fork fork exit how many child processes are created upon execution of this program roundrobin schedulers normally maintain a list of all runnable processes with each process occurring exactly once in the list what would happen if a process occurred twice in the list can you think of any reason for allowing this can a measure of whether a process is likely to be cpu bound or io bound be determined by analyzing source code how can this be determined at run time explain how time quantum value and context switching time affect each other in a roundrobin scheduling algorithm measurements of a certain system have shown that the average process runs for a time t before blocking on io a process switch requires a time s which is effectively wasted overhead for roundrobin scheduling with quantum q give a formula for the cpu efficiency for each of the following a q b q t c s q t d q s e q nearly five jobs are waiting to be run their expected run times are and x in what order should they be run to minimize average response time your answer will depend on x five batch jobs a through e arrive at a computer center at almost the same time they have estimated running times of and minutes their externally determined priorities are and respectively with being the highest priority for each of the following scheduling algorithms determine the mean process turnaround time ignore process switching overhead processes and threads chap a round robin b priority scheduling c firstcome firstserved run in order d shortest job first for a assume that the system is multiprogrammed and that each job gets its fair share of the cpu for b through d assume that only one job at a time runs until it finishes all jobs are completely cpu bound a process running on ctss needs quanta to complete how many times must it be swapped in including the very first time before it has run at all consider a realtime system with two voice calls of periodicity msec each with cpu time per call of msec and one video stream of periodicity ms with cpu time per call of msec is this system schedulable for the above problem can another video stream be added and have the system still be schedulable the aging algorithm with a is being used to predict run times the previous four runs from oldest to most recent are and msec what is the prediction of the next time a soft realtime system has four periodic events with periods of and msec each suppose that the four events require and x msec of cpu time respectively what is the largest value of x for which the system is schedulable in the dining philosophers problem let the following protocol be used an evennumbered philosopher always picks up his left fork before picking up his right fork an oddnumbered philosopher always picks up his right fork before picking up his left fork will this protocol guarantee deadlockfree operation a realtime system needs to handle two voice calls that each run every msec and consume msec of cpu time per burst plus one video at framessec with each frame requiring msec of cpu time is this system schedulable consider a system in which it is desired to separate policy and mechanism for the scheduling of kernel threads propose a means of achieving this goal in the solution to the dining philosophers problem fig why is the state variable set to hungry in the procedure take forks consider the procedure put forks in fig suppose that the variable statei was set to thinking after the two calls to test rather than before how would this change affect the solution the readers and writers problem can be formulated in several ways with regard to which category of processes can be started when carefully describe three different variations of the problem each one favoring or not favoring some category of processes for each variation specify what happens when a reader or a writer becomes ready to access the database and what happens when a process is finished write a shell script that produces a file of sequential numbers by reading the last number in the file adding to it and then appending it to the file run one instance of the chap problems script in the background and one in the foreground each accessing the same file how long does it take before a race condition manifests itself what is the critical region modify the script to prevent the race hint use ln file filelock to lock the data file assume that you have an operating system that provides semaphores implement a message system write the procedures for sending and receiving messages solve the dining philosophers problem using monitors instead of semaphores suppose that a university wants to show off how politically correct it is by applying the us supreme courts separate but equal is inherently unequal doctrine to gender as well as race ending its longstanding practice of gendersegregated bathrooms on campus however as a concession to tradition it decrees that when a woman is in a bathroom other women may enter but no men and vice versa a sign with a sliding marker on the door of each bathroom indicates which of three possible states it is currently in empty women present men present in some programming language you like write the following procedures woman wants to enter man wants to enter woman leaves man leaves you may use whatever counters and synchronization techniques you like rewrite the program of fig to handle more than two processes write a producerconsumer problem that uses threads and shares a common buffer however do not use semaphores or any other synchronization primitives to guard the shared data structures just let each thread access them when it wants to use sleep and wakeup to handle the full and empty conditions see how long it takes for a fatal race condition to occur for example you might have the producer print a number once in a while do not print more than one number every minute because the io could affect the race conditions a process can be put into a roundrobin queue more than once to give it a higher priority running multiple instances of a program each working on a different part of a data pool can have the same effect first write a program that tests a list of numbers for primality then devise a method to allow multiple instances of the program to run at once in such a way that no two instances of the program will work on the same number can you in fact get through the list faster by running multiple copies of the program note that your results will depend upon what else your computer is doing on a personal computer running only instances of this program you would not expect an improvement but on a system with other processes you should be able to grab a bigger share of the cpu this way the objective of this exercise is to implement a multithreaded solution to find if a given number is a perfect number n is a perfect number if the sum of all its factors excluding itself is n examples are and the input is an integer n the output is processes and threads chap true if the number is a perfect number and false otherwise the main program will read the numbers n and p from the command line the main process will spawn a set of p threads the numbers from to n will be partitioned among these threads so that two threads do not work on the name number for each number in this set the thread will determine if the number is a factor of n if it is it adds the number to a shared buffer that stores factors of n the parent process waits till all the threads complete use the appropriate synchronization primitive here the parent will then determine if the input number is perfect that is if n is a sum of all its factors and then report accordingly note you can make the computation faster by restricting the numbers searched from to the square root of n implement a program to count the frequency of words in a text file the text file is partitioned into n segments each segment is processed by a separate thread that outputs the intermediate frequency count for its segment the main process waits until all the threads complete then it computes the consolidated wordfrequency data based on the individual threads output memory management main memory ram is an important resource that must be very carefully managed while the average home computer nowadays has times more memory than the ibm the largest computer in the world in the early s programs are getting bigger faster than memories to paraphrase parkinsons law programs expand to fill the memory available to hold them in this chapter we will study how operating systems create abstractions from memory and how they manage them what every programmer would like is a private infinitely large infinitely fast memory that is also nonvolatile that is does not lose its contents when the electric power is switched off while we are at it why not make it inexpensive too unfortunately technology does not provide such memories at present maybe you will discover how to do it what is the second choice over the years people discovered the concept of a memory hierarchy in which computers have a few megabytes of very fast expensive volatile cache memory a few gigabytes of mediumspeed mediumpriced volatile main memory and a few terabytes of slow cheap nonvolatile magnetic or solidstate disk storage not to mention removable storage such as dvds and usb sticks it is the job of the operating system to abstract this hierarchy into a useful model and then manage the abstraction the part of the operating system that manages part of the memory hierarchy is called the memory manager its job is to efficiently manage memory keep track of which parts of memory are in use allocate memory to processes when they need it and deallocate it when they are done no memory abstraction the simplest memory abstraction is to have no abstraction at all early mainframe computers before early minicomputers before and early personal computers before had no memory abstraction every program simply saw the physical memory when a program executed an instruction like mov register the computer just moved the contents of physical memory location to register thus the model of memory presented to the programmer was simply physical memory a set of addresses from to some maximum each address corresponding to a cell containing some number of bits commonly eight under these conditions it was not possible to have two running programs in memory at the same time if the first program wrote a new value to say location this would erase whatever value the second program was storing there nothing would work and both programs would crash almost immediately even with the model of memory being just physical memory several options are possible three variations are shown in fig the operating system may be at the bottom of memory in ram random access memory as shown in fig a or it may be in rom readonly memory at the top of memory as shown in fig b or the device drivers may be at the top of memory in a rom and the rest of the system in ram down below as shown in fig c the first model was formerly used on mainframes and minicomputers but is rarely used any more the second model is used on some handheld computers and embedded systems the third model was used by early personal computers eg running msdos where the portion of the system in the rom is called the bios basic input output system models a and c have the disadvantage that a bug in the user program can wipe out the operating system possibly with disastrous results when the system is organized in this way generally only one process at a time can be running as soon as the user types a command the operating system copies the requested program from disk to memory and executes it when the process finishes the operating system displays a prompt character and waits for a user new command when the operating system receives the command it loads a new program into memory overwriting the first one sec a memory abstraction address spaces all in all exposing physical memory to processes has several major drawbacks first if user programs can address every byte of memory they can easily trash the operating system intentionally or by accident bringing the system to a grinding halt unless there is special hardware like the ibm s lockandkey scheme this problem exists even if only one user program application is running second with this model it is difficult to have multiple programs running at once taking turns if there is only one cpu on personal computers it is common to have several programs open at once a word processor an email program a web browser one of them having the current focus but the others being reactivated at the click of a mouse since this situation is difficult to achieve when there is no abstraction from physical memory something had to be done memory management chap the notion of an address space two problems have to be solved to allow multiple applications to be in memory at the same time without interfering with each other protection and relocation we looked at a primitive solution to the former used on the ibm label chunks of memory with a protection key and compare the key of the executing process to that of every memory word fetched however this approach by itself does not solve the latter problem although it can be solved by relocating programs as they are loaded but this is a slow and complicated solution a better solution is to invent a new abstraction for memory the address space just as the process concept creates a kind of abstract cpu to run programs the address space creates a kind of abstract memory for programs to live in an address space is the set of addresses that a process can use to address memory each process has its own address space independent of those belonging to other processes except in some special circumstances where processes want to share their address spaces the concept of an address space is very general and occurs in many contexts consider telephone numbers in the united states and many other countries a local telephone number is usually a digit number the address space for telephone numbers thus runs from to although some numbers such as those beginning with are not used with the growth of smartphones modems and fax machines this space is becoming too small in which case more digits have to be used the address space for io ports on the x runs from to ipv addresses are bit numbers so their address space runs from to again with some reserved numbers address spaces do not have to be numeric the set of com internet domains is also an address space this address space consists of all the strings of length to characters that can be made using letters numbers and hyphens followed by com by now you should get the idea it is fairly simple somewhat harder is how to give each program its own address space so address in one program means a different physical location than address in another program below we will discuss a simple way that used to be common but has fallen into disuse due to the ability to put much more complicated and better schemes on modern cpu chips base and limit registers this simple solution uses a particularly simple version of dynamic relocation what it does is map each process address space onto a different part of physical memory in a simple way the classical solution which was used on machines ranging from the cdc the worlds first supercomputer to the intel the heart of the original ibm pc is to equip each cpu with two special hardware registers usually called the base and limit registers when these registers are used sec virtual memory while base and limit registers can be used to create the abstraction of address spaces there is another problem that has to be solved managing bloatware while memory sizes are increasing rapidly software sizes are increasing much faster in the s many universities ran a timesharing system with dozens of moreorless satisfied users running simultaneously on a mb vax now microsoft recommends having at least gb for bit windows the trend toward multimedia puts even more demands on memory as a consequence of these developments there is a need to run programs that are too large to fit in memory and there is certainly a need to have systems that can support multiple programs running simultaneously each of which fits in memory but all of which collectively exceed memory swapping is not an attractive option since a typical sata disk has a peak transfer rate of several hundreds of mbsec which means it takes seconds to swap out a gb program and the same to swap in a gb program the problem of programs larger than memory has been around since the beginning of computing albeit in limited areas such as science and engineering simulating the creation of the universe or even simulating a new aircraft takes a lot of memory a solution adopted in the s was to split programs into little pieces called overlays when a program started all that was loaded into memory was the overlay manager which immediately loaded and ran overlay when it was done it would tell the overlay manager to load overlay either above overlay in memory if there was space for it or on top of overlay if there was no space some overlay systems were highly complex allowing many overlays in memory at once the overlays were kept on the disk and swapped in and out of memory by the overlay manager although the actual work of swapping overlays in and out was done by the operating system the work of splitting the program into pieces had to be done manually by the programmer splitting large programs up into small modular pieces was time consuming boring and error prone few programmers were good at this it did not take long before someone thought of a way to turn the whole job over to the computer the method that was devised fotheringham has come to be known as virtual memory the basic idea behind virtual memory is that each program has its own address space which is broken up into chunks called pages each page is a contiguous range of addresses these pages are mapped onto physical memory but not all pages have to be in physical memory at the same time to run the program when the program references a part of its address space that is in physical sec page replacement algorithms design issues for paging systems in the previous sections we have explained how paging works and have given a few of the basic page replacement algorithms but knowing the bare mechanics is not enough to design a system and make it work well you have to know a lot more it is like the difference between knowing how to move the rook knight bishop and other pieces in chess and being a good player in the following sections we will look at other issues that operating system designers must consider carefully in order to get good performance from a paging system local versus global allocation policies in the preceding sections we have discussed several algorithms for choosing a page to replace when a fault occurs a major issue associated with this choice which we have carefully swept under the rug until now is how memory should be allocated among the competing runnable processes take a look at fig a in this figure three processes a b and c make up the set of runnable processes suppose a gets a page fault should the page replacement algorithm try to find the least recently used page considering only the six pages currently allocated to a or should it consider all the pages in memory if it looks only at as pages the page with the lowest age value is a so we get the situation of fig b on the other hand if the page with the lowest age value is removed without regard to whose page it is page b will be chosen and we will get the situation of fig c the algorithm of fig b is said to be a local page replacement sec implementation issues implementers of virtual memory systems have to make choices among the major theoretical algorithms such as second chance versus aging local versus global page allocation and demand paging versus prepaging but they also have to be aware of a number of practical implementation issues as well in this section we will take a look at a few of the common problems and some solutions operating system involvement with paging there are four times when the operating system has pagingrelated work to do process creation time process execution time page fault time and process termination time we will now briefly examine each of these to see what has to be done when a new process is created in a paging system the operating system has to determine how large the program and data will be initially and create a page table memory management chap for them space has to be allocated in memory for the page table and it has to be initialized the page table need not be resident when the process is swapped out but has to be in memory when the process is running in addition space has to be allocated in the swap area on disk so that when a page is swapped out it has somewhere to go the swap area also has to be initialized with program text and data so that when the new process starts getting page faults the pages can be brought in some systems page the program text directly from the executable file thus saving disk space and initialization time finally information about the page table and swap area on disk must be recorded in the process table when a process is scheduled for execution the mmu has to be reset for the new process and the tlb flushed to get rid of traces of the previously executing process the new process page table has to be made current usually by copying it or a pointer to it to some hardware registers optionally some or all of the process pages can be brought into memory to reduce the number of page faults initially eg it is certain that the page pointed to by the program counter will be needed when a page fault occurs the operating system has to read out hardware registers to determine which virtual address caused the fault from this information it must compute which page is needed and locate that page on disk it must then find an available page frame in which to put the new page evicting some old page if need be then it must read the needed page into the page frame finally it must back up the program counter to have it point to the faulting instruction and let that instruction execute again when a process exits the operating system must release its page table its pages and the disk space that the pages occupy when they are on disk if some of the pages are shared with other processes the pages in memory and on disk can be released only when the last process using them has terminated page fault handling we are finally in a position to describe in detail what happens on a page fault the sequence of events is as follows the hardware traps to the kernel saving the program counter on the stack on most machines some information about the state of the current instruction is saved in special cpu registers an assemblycode routine is started to save the general registers and other volatile information to keep the operating system from destroying it this routine calls the operating system as a procedure the operating system discovers that a page fault has occurred and tries to discover which virtual page is needed often one of the hardware registers contains this information if not the operating system sec segmentation the virtual memory discussed so far is onedimensional because the virtual addresses go from to some maximum address one address after another for many problems having two or more separate virtual address spaces may be much better than having only one for example a compiler has many tables that are built up as compilation proceeds possibly including sec research on memory management traditional memory management especially paging algorithms for uniprocessor cpus was once a fruitful area for research but most of that seems to have largely died off at least for generalpurpose systems although there are some people who never say die moruz et al or are focused on some application such as online transaction processing that has specialized requirements stoica and ailamaki even on uniprocessors paging to ssds rather than to hard disks brings up new issues and requires new algorithms chen et al paging to the upandcoming nonvolatile phasechange memories also requires rethinking summary in this chapter we have examined memory management we saw that the simplest systems do not swap or page at all once a program is loaded into memory it remains there in place until it finishes some operating systems allow only one process at a time in memory while others support multiprogramming this model is still common in small embedded realtime systems the next step up is swapping when swapping is used the system can handle more processes than it has room for in memory processes for which there is no room are swapped out to the disk free space in memory and on disk can be kept track of with a bitmap or a hole list modern computers often have some form of virtual memory in the simplest form each process address space is divided up into uniformsized blocks called pages which can be placed into any available page frame in memory there are many page replacement algorithms two of the better algorithms are aging and wsclock to make paging systems work well choosing an algorithm is not enough attention to such issues as determining the working set memory allocation policy and page size is required segmentation helps in handling data structures that can change size during execution and simplifies linking and sharing it also facilitates providing different memory management chap protection for different segments sometimes segmentation and paging are combined to provide a twodimensional virtual memory the multics system and the bit intel x support segmentation and paging still it is clear that few operating system developers care deeply about segmentation because they are married to a different memory model consequently it seems to be going out of fashion fast today even the bit version of the x no longer supports real segmentation problems the ibm had a scheme of locking kb blocks by assigning each one a bit key and having the cpu compare the key on every memory reference to the bit key in the psw name two drawbacks of this scheme not mentioned in the text in fig the base and limit registers contain the same value is this just an accident or are they always the same it is just an accident why are they the same in this example a swapping system eliminates holes by compaction assuming a random distribution of many holes and many data segments and a time to read or write a bit memory word of nsec about how long does it take to compact gb for simplicity assume that word is part of a hole and that the highest word in memory contains valid data consider a swapping system in which memory consists of the following hole sizes in memory order mb mb mb mb mb mb mb and mb which hole is taken for successive segment requests of a mb b mb c mb for first fit now repeat the question for best fit worst fit and next fit what is the difference between a physical address and a virtual address for each of the following decimal virtual addresses compute the virtual page number and offset for a kb page and for an kb page using the page table of fig give the physical address corresponding to each of the following virtual addresses a b c the intel processor did not have an mmu or support virtual memory nevertheless some companies sold systems that contained an unmodified cpu and did paging make an educated guess as to how they did it hint think about the logical location of the mmu chap problems what kind of hardware support is needed for a paged virtual memory to work copy on write is an interesting idea used on server systems does it make any sense on a smartphone consider the following c program int xn int step m m is some predefined constant for int i i n i step xi xi a if this program is run on a machine with a kb page size and entry tlb what values of m and n will cause a tlb miss for every execution of the inner loop b would your answer in part a be different if the loop were repeated many times explain the amount of disk space that must be available for page storage is related to the maximum number of processes n the number of bytes in the virtual address space v and the number of bytes of ram r give an expression for the worstcase diskspace requirements how realistic is this amount if an instruction takes nsec and a page fault takes an additional n nsec give a formula for the effective instruction time if page faults occur every k instructions a machine has a bit address space and an kb page the page table is entirely in hardware with one bit word per entry when a process starts the page table is copied to the hardware from memory at one word every nsec if each process runs for msec including the time to load the page table what fraction of the cpu time is devoted to loading the page tables suppose that a machine has bit virtual addresses and bit physical addresses a if pages are kb how many entries are in the page table if it has only a single level explain b suppose this same system has a tlb translation lookaside buffer with entries furthermore suppose that a program contains instructions that fit into one page and it sequentially reads long integer elements from an array that spans thousands of pages how effective will the tlb be for this case you are given the following data about a virtual memory system athe tlb can hold entries and can be accessed in clock cycle nsec b a page table entry can be found in clock cycles or nsec c the average page replacement time is msec if page references are handled by the tlb of the time and only lead to a page fault what is the effective addresstranslation time suppose that a machine has bit virtual addresses and bit physical addresses a what is the main advantage of a multilevel page table over a singlelevel one b with a twolevel page table kb pages and byte entries how many bits should be allocated for the toplevel page table field and how many for the nextlevel page table field explain memory management chap section states that the pentium pro extended each entry in the page table hierarchy to bits but still could only address only gb of memory explain how this statement can be true when page table entries have bits a computer with a bit address uses a twolevel page table virtual addresses are split into a bit toplevel page table field an bit secondlevel page table field and an offset how large are the pages and how many are there in the address space a computer has bit virtual addresses and kb pages the program and data together fit in the lowest page the stack fits in the highest page how many entries are needed in the page table if traditional onelevel paging is used how many page table entries are needed for twolevel paging with bits in each part below is an execution trace of a program fragment for a computer with byte pages the program is located at address and its stack pointer is at the stack grows toward give the page reference string generated by this program each instruction occupies bytes word including immediate constants both instruction and data references count in the reference string load word into register push register onto the stack call a procedure at stacking the return address subtract the immediate constant from the stack pointer compare the actual parameter to the immediate constant jump if equal to a computer whose processes have pages in their address spaces keeps its page tables in memory the overhead required for reading a word from the page table is nsec to reduce this overhead the computer has a tlb which holds virtual page physical page frame pairs and can do a lookup in nsec what hit rate is needed to reduce the mean overhead to nsec how can the associative memory device needed for a tlb be implemented in hardware and what are the implications of such a design for expandability a machine has bit virtual addresses and bit physical addresses pages are kb how many entries are needed for a singlelevel linear page table a computer with an kb page a kb main memory and a gb virtual address space uses an inverted page table to implement its virtual memory how big should the hash table be to ensure a mean hash chain length of less than assume that the hashtable size is a power of two a student in a compiler design course proposes to the professor a project of writing a compiler that will produce a list of page references that can be used to implement the optimal page replacement algorithm is this possible why or why not is there anything that could be done to improve paging efficiency at run time suppose that the virtual page reference stream contains repetitions of long sequences of page references followed occasionally by a random page reference for example the sequence consists of repetitions of the sequence followed by a random reference to pages and chap problems a why will the standard replacement algorithms lru fifo clock not be effective in handling this workload for a page allocation that is less than the sequence length b if this program were allocated page frames describe a page replacement approach that would perform much better than the lru fifo or clock algorithms if fifo page replacement is used with four page frames and eight pages how many page faults will occur with the reference string if the four frames are initially empty now repeat this problem for lru consider the page sequence of fig b suppose that the r bits for the pages b through a are respectively which page will second chance remove a small computer on a smart card has four page frames at the first clock tick the r bits are page is the rest are at subsequent clock ticks the values are and if the aging algorithm is used with an bit counter give the values of the four counters after the last tick give a simple example of a page reference sequence where the first page selected for replacement will be different for the clock and lru page replacement algorithms assume that a process is allocated three frames and the reference string contains page numbers from the set in the wsclock algorithm of fig c the hand points to a page with r if will this page be removed what about if suppose that the wsclock page replacement algorithm uses a of two ticks and the system state is the following page time stamp v r m where the three flag bits v r and m stand for valid referenced and modified respectively a if a clock interrupt occurs at tick show the contents of the new table entries explain you can omit entries that are unchanged b suppose that instead of a clock interrupt a page fault occurs at tick due to a read request to page show the contents of the new table entries explain you can omit entries that are unchanged a student has claimed that in the abstract the basic page replacement algorithms fifo lru optimal are identical except for the attribute used for selecting the page to be replaced a what is that attribute for the fifo algorithm lru algorithm optimal algorithm b give the generic algorithm for these page replacement algorithms memory management chap how long does it take to load a kb program from a disk whose average seek time is msec whose rotation time is msec and whose tracks hold mb a for a kb page size b for a kb page size the pages are spread randomly around the disk and the number of cylinders is so large that the chance of two pages being on the same cylinder is negligible a computer has four page frames the time of loading time of last access and the r and m bits for each page are as shown below the times are in clock ticks page loaded last ref r m a which page will nru replace b which page will fifo replace c which page will lru replace d which page will second chance replace suppose that two processes a and b share a page that is not in memory if process a faults on the shared page the page table entry for process a must be updated once the page is read into memory a under what conditions should the page table update for process b be delayed even though the handling of process as page fault will bring the shared page into memory explain b what is the potential cost of delaying the page table update consider the following twodimensional array int x suppose that a system has four page frames and each frame is words an integer occupies one word programs that manipulate the x array fit into exactly one page and always occupy page the data are swapped in and out of the other three frames the x array is stored in rowmajor order ie x follows x in memory which of the two code fragments shown below will generate the lowest number of page faults explain and compute the total number of page faults fragment a for int j j j for int i i i xij fragment b for int i i i for int j j j xij chap problems you have been hired by a cloud computing company that deploys thousands of servers at each of its data centers they have recently heard that it would be worthwhile to handle a page fault at server a by reading the page from the ram memory of some other server rather than its local disk drive a how could that be done b under what conditions would the approach be worthwhile be feasible one of the first timesharing machines the dec pdp had a core memory of k bit words it held one process at a time in its memory when the scheduler decided to run another process the process in memory was written to a paging drum with k bit words around the circumference of the drum the drum could start writing or reading at any word rather than only at word why do you suppose this drum was chosen a computer provides each process with bytes of address space divided into pages of bytes each a particular program has a text size of bytes a data size of bytes and a stack size of bytes will this program fit in the machines address space suppose that instead of bytes the page size were bytes would it then fit each page must contain either text data or stack not a mixture of two or three of them it has been observed that the number of instructions executed between page faults is directly proportional to the number of page frames allocated to a program if the available memory is doubled the mean interval between page faults is also doubled suppose that a normal instruction takes microsec but if a page fault occurs it takes sec ie msec to handle the fault if a program takes sec to run during which time it gets page faults how long would it take to run if twice as much memory were available a group of operating system designers for the frugal computer company are thinking about ways to reduce the amount of backing store needed in their new operating system the head guru has just suggested not bothering to save the program text in the swap area at all but just page it in directly from the binary file whenever it is needed under what conditions if any does this idea work for the program text under what conditions if any does it work for the data a machinelanguage instruction to load a bit word into a register contains the bit address of the word to be loaded what is the maximum number of page faults this instruction can cause explain the difference between internal fragmentation and external fragmentation which one occurs in paging systems which one occurs in systems using pure segmentation when segmentation and paging are both being used as in multics first the segment descriptor must be looked up then the page descriptor does the tlb also work this way with two levels of lookup we consider a program which has the two segments shown below consisting of instructions in segment and readwrite data in segment segment has readexecute protection and segment has just readwrite protection the memory system is a demand memory management chap paged virtual memory system with virtual addresses that have a bit page number and a bit offset the page tables and protection are as follows all numbers in the table are in decimal segment segment readexecute readwrite virtual page page frame virtual page page frame on disk on disk on disk on disk on disk for each of the following cases either give the real actual memory address which results from dynamic address translation or identify the type of fault which occurs either page or protection fault a fetch from segment page offset b store into segment page offset c fetch from segment page offset d jump to location in segment page offset can you think of any situations where supporting virtual memory would be a bad idea and what would be gained by not having to support virtual memory explain virtual memory provides a mechanism for isolating one process from another what memory management difficulties would be involved in allowing two operating systems to run concurrently how might these difficulties be addressed plot a histogram and calculate the mean and median of the sizes of executable binary files on a computer to which you have access on a windows system look at all exe and dll files on a unix system look at all executable files in bin usrbin and localbin that are not scripts or use the file utility to find all executables determine the optimal page size for this computer just considering the code not data consider internal fragmentation and page table size making some reasonable assumption about the size of a page table entry assume that all programs are equally likely to be run and thus should be weighted equally write a program that simulates a paging system using the aging algorithm the number of page frames is a parameter the sequence of page references should be read from a file for a given input file plot the number of page faults per memory references as a function of the number of page frames available write a program that simulates a toy paging system that uses the wsclock algorithm the system is a toy in that we will assume there are no write references not very chap problems realistic and process termination and creation are ignored eternal life the inputs will be the reclamation age threshhold the clock interrupt interval expressed as number of memory references a file containing the sequence of page references a describe the basic data structures and algorithms in your implementation b show that your simulation behaves as expected for a simple but nontrivial input example c plot the number of page faults and working set size per memory references d explain what is needed to extend the program to handle a page reference stream that also includes writes write a program that demonstrates the effect of tlb misses on the effective memory access time by measuring the peraccess time it takes to stride through a large array a explain the main concepts behind the program and describe what you expect the output to show for some practical virtual memory architecture b run the program on some computer and explain how well the data fit your expectations c repeat part b but for an older computer with a different architecture and explain any major differences in the output write a program that will demonstrate the difference between using a local page replacement policy and a global one for the simple case of two processes you will need a routine that can generate a page reference string based on a statistical model this model has n states numbered from to n representing each of the possible page references and a probability pi associated with each state i representing the chance that the next reference is to the same page otherwise the next page reference will be one of the other pages with equal probability a demonstrate that the page reference stringgeneration routine behaves properly for some small n b compute the page fault rate for a small example in which there is one process and a fixed number of page frames explain why the behavior is correct c repeat part b with two processes with independent page reference sequences and twice as many page frames as in part b d repeat part c but using a global policy instead of a local one also contrast the perprocess page fault rate with that of the local policy approach write a program that can be used to compare the effectiveness of adding a tag field to tlb entries when control is toggled between two programs the tag field is used to effectively label each entry with the process id note that a nontagged tlb can be simulated by requiring that all tlb entries have the same tag at any one time the inputs will be the number of tlb entries available the clock interrupt interval expressed as number of memory references a file containing a sequence of process page references entries the cost to update one tlb entry memory management chap a describe the basic data structures and algorithms in your implementation b show that your simulation behaves as expected for a simple but nontrivial input example c plot the number of tlb updates per references file systems all computer applications need to store and retrieve information while a process is running it can store a limited amount of information within its own address space however the storage capacity is restricted to the size of the virtual address space for some applications this size is adequate but for others such as airline reservations banking or corporate record keeping it is far too small a second problem with keeping information within a process address space is that when the process terminates the information is lost for many applications eg for databases the information must be retained for weeks months or even forever having it vanish when the process using it terminates is unacceptable furthermore it must not go away when a computer crash kills the process a third problem is that it is frequently necessary for multiple processes to access parts of the information at the same time if we have an online telephone directory stored inside the address space of a single process only that process can access it the way to solve this problem is to make the information itself independent of any one process thus we have three essential requirements for longterm information storage it must be possible to store a very large amount of information the information must survive the termination of the process using it multiple processes must be able to access the information at once magnetic disks have been used for years for this longterm storage in recent years solidstate drives have become increasingly popular as they do not have any files or bitmaps are used to keep track of free storage and how many sectors there are in a logical disk block are of no interest although they are of great importance to the designers of the file system for this reason we have structured the chapter as several sections the first two are concerned with the user interface to files and directories respectively then comes a detailed discussion of how the file system is implemented and managed finally we give some examples of real file systems directories to keep track of files file systems normally have directories or folders which are themselves files in this section we will discuss directories their organization their properties and the operations that can be performed on them singlelevel directory systems the simplest form of directory system is having one directory containing all the files sometimes it is called the root directory but since it is the only one the name does not matter much on early personal computers this system was common in part because there was only one user interestingly enough the worlds first supercomputer the cdc also had only a single directory for all files even though it was used by many users at once this decision was no doubt made to keep the software design simple an example of a system with one directory is given in fig here the directory contains four files the advantages of this scheme are its simplicity and the ability to locate files quickly there is only one place to look after all it is sometimes still used on simple embedded devices such as digital cameras and some portable music players hierarchical directory systems the single level is adequate for very simple dedicated applications and was even used on the first personal computers but for modern users with thousands of files it would be impossible to find anything if all files were in a single directory sec filesystem implementation now it is time to turn from the users view of the file system to the implementors view users are concerned with how files are named what operations are allowed on them what the directory tree looks like and similar interface issues implementors are interested in how files and directories are stored how disk space is managed and how to make everything work efficiently and reliably in the following sections we will examine a number of these areas to see what the issues and tradeoffs are filesystem layout file systems are stored on disks most disks can be divided up into one or more partitions with independent file systems on each partition sector of the disk is called the mbr master boot record and is used to boot the computer the end of the mbr contains the partition table this table gives the starting and ending addresses of each partition one of the partitions in the table is marked as active when the computer is booted the bios reads in and executes the mbr the first thing the mbr program does is locate the active partition read in its first block which is called the boot block and execute it the program in the boot block loads the operating system contained in that partition for uniformity every file systems chap partition starts with a boot block even if it does not contain a bootable operating system besides it might contain one in the future other than starting with a boot block the layout of a disk partition varies a lot from file system to file system often the file system will contain some of the items shown in fig the first one is the superblock it contains all the key parameters about the file system and is read into memory when the computer is booted or the file system is first touched typical information in the superblock includes a magic number to identify the filesystem type the number of blocks in the file system and other key administrative information entire disk partition table disk partition mbr boot block superblock free space mgmt inodes root dir files and directories figure a possible filesystem layout next might come information about free blocks in the file system for example in the form of a bitmap or a list of pointers this might be followed by the inodes an array of data structures one per file telling all about the file after that might come the root directory which contains the top of the filesystem tree finally the remainder of the disk contains all the other directories and files implementing files probably the most important issue in implementing file storage is keeping track of which disk blocks go with which file various methods are used in different operating systems in this section we will examine a few of them contiguous allocation the simplest allocation scheme is to store each file as a contiguous run of disk blocks thus on a disk with kb blocks a kb file would be allocated consecutive blocks with kb blocks it would be allocated consecutive blocks we see an example of contiguous storage allocation in fig a here the first disk blocks are shown starting with block on the left initially the disk sec introduction a modern computer consists of one or more processors some main memory disks printers a keyboard a mouse a display network interfaces and various other inputoutput devices all in all a complex systemoo if every application programmer had to understand how all these things work in detail no code would ever get written furthermore managing all these components and using them optimally is an exceedingly challenging job for this reason computers are equipped with a layer of software called the operating system whose job is to provide user programs with a better simpler cleaner model of the computer and to handle managing all the resources just mentioned operating systems are the subject of this book most readers will have had some experience with an operating system such as windows linux freebsd or os x but appearances can be deceiving the program that users interact with usually called the shell when it is text based and the gui graphical user interface which is pronounced gooey when it uses icons is actually not part of the operating system although it uses the operating system to get its work done a simple overview of the main components under discussion here is given in fig here we see the hardware at the bottom the hardware consists of chips boards disks a keyboard a monitor and similar physical objects on top of the hardware is the software most computers have two modes of operation kernel mode and user mode the operating system the most fundamental piece of software runs in kernel mode also called supervisor mode in this mode it has filesystem management and optimization making the file system work is one thing making it work efficiently and robustly in real life is something quite different in the following sections we will look at some of the issues involved in managing disks file systems chap diskspace management files are normally stored on disk so management of disk space is a major concern to filesystem designers two general strategies are possible for storing an n byte file n consecutive bytes of disk space are allocated or the file is split up into a number of not necessarily contiguous blocks the same tradeoff is present in memorymanagement systems between pure segmentation and paging as we have seen storing a file as a contiguous sequence of bytes has the obvious problem that if a file grows it may have to be moved on the disk the same problem holds for segments in memory except that moving a segment in memory is a relatively fast operation compared to moving a file from one disk position to another for this reason nearly all file systems chop files up into fixedsize blocks that need not be adjacent block size once it has been decided to store files in fixedsize blocks the question arises how big the block should be given the way disks are organized the sector the track and the cylinder are obvious candidates for the unit of allocation although these are all device dependent which is a minus in a paging system the page size is also a major contender having a large block size means that every file even a byte file ties up an entire cylinder it also means that small files waste a large amount of disk space on the other hand a small block size means that most files will span multiple blocks and thus need multiple seeks and rotational delays to read them reducing performance thus if the allocation unit is too large we waste space if it is too small we waste time making a good choice requires having some information about the filesize distribution tanenbaum et al studied the filesize distribution in the computer science department of a large research university the vu in and then again in as well as on a commercial web server hosting a political website wwwelectoralvotecom the results are shown in fig where for each poweroftwo file size the percentage of all files smaller or equal to it is listed for each of the three data sets for example in of all files at the vu were kb or smaller and of all files were kb or smaller the median file size was bytes some people may find this small size surprising what conclusions can we draw from these data for one thing with a block size of kb only about of all files fit in a single block whereas with a kb block the percentage of files that fit in one block goes up to the range other data in the paper show that with a kb block of the disk blocks are used by the largest files this means that wasting some space at the end of each small file hardly matters because the disk is filled up by a small number of sec what is an operating system system such as the file system run in user space in such systems it is difficult to draw a clear boundary everything running in kernel mode is clearly part of the operating system but some programs running outside it are arguably also part of it or at least closely associated with it operating systems differ from user ie application programs in ways other than where they reside in particular they are huge complex and longlived the source code of the heart of an operating system like linux or windows is on the order of five million lines of code or more to conceive of what this means think of printing out five million lines in book form with lines per page and pages per volume larger than this book it would take volumes to list an operating system of this size essentially an entire bookcase can you imagine getting a job maintaining an operating system and on the first day having your boss bring you to a bookcase with the code and say go learn that and this is only for the part that runs in the kernel when essential shared libraries are included windows is well over million lines of code or to bookcases and this excludes basic application software things like windows explorer windows media player and so on it should be clear now why operating systems live a long time they are very hard to write and having written one the owner is loath to throw it out and start again instead such systems evolve over long periods of time windows me was basically one operating system and windows ntxpvistawindows is a different one they look similar to the users because microsoft made very sure that the user interface of windows xpvistawindows was quite similar to that of the system it was replacing mostly windows nevertheless there were very good reasons why microsoft got rid of windows we will come to these when we study windows in detail in chap besides windows the other main example we will use throughout this book is unix and its variants and clones it too has evolved over the years with versions like system v solaris and freebsd being derived from the original system whereas linux is a fresh code base although very closely modeled on unix and highly compatible with it we will use examples from unix throughout this book and look at linux in detail in chap in this chapter we will briefly touch on a number of key aspects of operating systems including what they are their history what kinds are around some of the basic concepts and their structure we will come back to many of these important topics in later chapters in more detail example file systems in the following sections we will discuss several example file systems ranging from quite simple to more sophisticated since modern unix file systems and windows s native file system are covered in the chapter on unix chap and the chapter on windows chap we will not cover those systems here we will however examine their predecessors below the msdos file system the msdos file system is the one the first ibm pcs came with it was the main file system up through windows and windows me it is still supported on windows windows xp and windows vista although it is no longer standard on new pcs now except for floppy disks however it and an extension of it fat have become widely used for many embedded systems most digital cameras use it many mp players use it exclusively the popular apple ipod uses it as the default file system although knowledgeable hackers can reformat the ipod and install a different file system thus the number of electronic devices using the msdos file system is vastly larger now than at any time in the past and certainly much larger than the number using the more modern ntfs file system for that reason alone it is worth looking at in some detail to read a file an msdos program must first make an open system call to get a handle for it the open system call specifies a path which may be either absolute or relative to the current working directory the path is looked up component by component until the final directory is located and read into memory it is then searched for the file to be opened although msdos directories are variable sized they use a fixedsize byte directory entry the format of an msdos directory entry is shown in fig it contains the file name attributes creation date and time starting block and exact sec research on file systems file systems have always attracted more research than other parts of the operating system and that is still the case entire conferences such as fast msst and nas are devoted largely to file and storage systems while standard file systems are fairly well understood there is still quite a bit of research going on about backups smaldone et al and wallace et al caching koller et al oh and zhang et al a erasing data securely wei et al file compression harnik et al flash file systems no park and shen and narayanan performance leventhal and schindler et al raid moon and reddy reliability and recovery from errors chidambaram et al ma et al mckusick and van moolenbroek et al userlevel file systems rajgarhia and gehani verifying consistency fryer et al and versioning file systems mashtizadeh et al just measuring what is actually going in a file system is also a research topic harter et al security is a perennial topic botelho et al li et al c and lorch et al in contrast a hot new topic is cloud file systems mazurek et al summary when seen from the outside a file system is a collection of files and directories plus operations on them files can be read and written directories can be created and destroyed and files can be moved from directory to directory most modern file systems support a hierarchical directory system in which directories may have subdirectories and these may have subsubdirectories ad infinitum when seen from the inside a file system looks quite different the file system designers have to be concerned with how storage is allocated and how the system keeps track of which block goes with which file possibilities include contiguous files linked lists fileallocation tables and inodes different systems have different directory structures attributes can go in the directories or somewhere else eg an inode disk space can be managed using free lists or bitmaps filesystem reliability is enhanced by making incremental dumps and by having a program that can repair sick file systems filesystem performance is important and can be enhanced in several ways including caching read ahead and carefully placing the blocks of a file close to each other logstructured file systems also improve performance by doing writes in large units examples of file systems include iso dos and unix these differ in many ways including how they keep track of which blocks go with which file directory structure and management of free disk space problems give five different path names for the file etcpasswd hint think about the directory entries and in windows when a user double clicks on a file listed by windows explorer a program is run and given that file as a parameter list two different ways the operating system could know which program to run chap problems in early unix systems executable files aout files began with a very specific magic number not one chosen at random these files began with a header followed by the text and data segments why do you think a very specific number was chosen for executable files whereas other file types had a moreorless random magic number as the first word is the open system call in unix absolutely essential what would the consequences be of not having it systems that support sequential files always have an operation to rewind files do systems that support randomaccess files need this too some operating systems provide a system call rename to give a file a new name is there any difference at all between using this call to rename a file and just copying the file to a new file with the new name followed by deleting the old one in some systems it is possible to map part of a file into memory what restrictions must such systems impose how is this partial mapping implemented a simple operating system supports only a single directory but allows it to have arbitrarily many files with arbitrarily long file names can something approximating a hierarchical file system be simulated how in unix and windows random access is done by having a special system call that moves the current position pointer associated with a file to a given byte in the file propose an alternative way to do random access without having this system call consider the directory tree of fig if usrjim is the working directory what is the absolute path name for the file whose relative path name is astx contiguous allocation of files leads to disk fragmentation as mentioned in the text because some space in the last disk block will be wasted in files whose length is not an integral number of blocks is this internal fragmentation or external fragmentation make an analogy with something discussed in the previous chapter describe the effects of a corrupted data block for a given file for a contiguous b linked and c indexed or table based one way to use contiguous allocation of the disk and not suffer from holes is to compact the disk every time a file is removed since all files are contiguous copying a file requires a seek and rotational delay to read the file followed by the transfer at full speed writing the file back requires the same work assuming a seek time of msec a rotational delay of msec a transfer rate of mbsec and an average file size of kb how long does it take to read a file into main memory and then write it back to the disk at a new location using these numbers how long would it take to compact half of a gb disk in light of the answer to the previous question does compacting the disk ever make any sense some digital consumer devices need to store data for example as files name a modern device that requires file storage and for which contiguous allocation would be a fine idea file systems chap consider the inode shown in fig if it contains direct addresses and these were bytes each and all disk blocks were kb what would the largest possible file be for a given class the student records are stored in a file the records are randomly accessed and updated assume that each students record is of fixed size which of the three allocation schemes contiguous linked and tableindexed will be most appropriate consider a file whose size varies between kb and mb during its lifetime which of the three allocation schemes contiguous linked and tableindexed will be most appropriate it has been suggested that efficiency could be improved and disk space saved by storing the data of a short file within the inode for the inode of fig how many bytes of data could be stored inside the inode two computer science students carolyn and elinor are having a discussion about inodes carolyn maintains that memories have gotten so large and so cheap that when a file is opened it is simpler and faster just to fetch a new copy of the inode into the inode table rather than search the entire table to see if it is already there elinor disagrees who is right name one advantage of hard links over symbolic links and one advantage of symbolic links over hard links explain how hard links and soft links differ with respective to inode allocations consider a tb disk that uses kb blocks and the freelist method how many block addresses can be stored in one block free disk space can be kept track of using a free list or a bitmap disk addresses require d bits for a disk with b blocks f of which are free state the condition under which the free list uses less space than the bitmap for d having the value bits express your answer as a percentage of the disk space that must be free the beginning of a freespace bitmap looks like this after the disk partition is first formatted the first block is used by the root directory the system always searches for free blocks starting at the lowestnumbered block so after writing file a which uses six blocks the bitmap looks like this show the bitmap after each of the following additional actions a file b is written using five blocks b file a is deleted c file c is written using eight blocks d file b is deleted what would happen if the bitmap or free list containing the information about free disk blocks was completely lost due to a crash is there any way to recover from this disaster or is it byebye disk discuss your answers for unix and the fat file system separately chap problems oliver owls night job at the university computing center is to change the tapes used for overnight data backups while waiting for each tape to complete he works on writing his thesis that proves shakespeares plays were written by extraterrestrial visitors his text processor runs on the system being backed up since that is the only one they have is there a problem with this arrangement we discussed making incremental dumps in some detail in the text in windows it is easy to tell when to dump a file because every file has an archive bit this bit is missing in unix how do unix backup programs know which files to dump suppose that file in fig was not modified since the last dump in what way would the four bitmaps of fig be different it has been suggested that the first part of each unix file be kept in the same disk block as its inode what good would this do consider fig is it possible that for some particular block number the counters in both lists have the value how should this problem be corrected the performance of a file system depends upon the cache hit rate fraction of blocks found in the cache if it takes msec to satisfy a request from the cache but msec to satisfy a request if a disk read is needed give a formula for the mean time required to satisfy a request if the hit rate is h plot this function for values of h varying from to for an external usb hard drive attached to a computer which is more suitable a writethrough cache or a block cache consider an application where students records are stored in a file the application takes a student id as input and subsequently reads updates and writes the corresponding student record this is repeated till the application quits would the block readahead technique be useful here consider a disk that has data blocks starting from block through let there be files on the disk f and f the directory structure lists that the first data blocks of f and f are respectively and given the fat table entries as below what are the data blocks allotted to f and f in the above notation x y indicates that the value stored in table entry x points to data block y consider the idea behind fig but now for a disk with a mean seek time of msec a rotational rate of rpm and bytes per track what are the data rates for block sizes of kb kb and kb respectively a certain file system uses kb disk blocks the median file size is kb if all files were exactly kb what fraction of the disk space would be wasted do you think the wastage for a real file system will be higher than this number or lower than it explain your answer file systems chap given a diskblock size of kb and blockpointer address value of bytes what is the largest file size in bytes that can be accessed using direct addresses and one indirect block files in msdos have to compete for space in the fat table in memory if one file uses k entries that is k entries that are not available to any other file what constraint does this place on the total length of all files combined a unix file system has kb blocks and byte disk addresses what is the maximum file size if inodes contain direct entries and one single double and triple indirect entry each how many disk operations are needed to fetch the inode for afile with the path name usrastcoursesoshandoutt assume that the inode for the root directory is in memory but nothing else along the path is in memory also assume that all directories fit in one disk block in many unix systems the inodes are kept at the start of the disk an alternative design is to allocate an inode when a file is created and put the inode at the start of the first block of the file discuss the pros and cons of this alternative write a program that reverses the bytes of a file so that the last byte is now first and the first byte is now last it must work with an arbitrarily long file but try to make it reasonably efficient write a program that starts at a given directory and descends the file tree from that point recording the sizes of all the files it finds when it is all done it should print a histogram of the file sizes using a bin width specified as a parameter eg with file sizes of to go in one bin to go in the next bin etc write a program that scans all directories in a unix file system and finds and locates all inodes with a hard link count of two or more for each such file it lists together all file names that point to the file write a new version of the unix ls program this version takes as an argument one or more directory names and for each directory lists all the files in that directory one line per file each field should be formatted in a reasonable way given its type list only the first disk address if any implement a program to measure the impact of applicationlevel buffer sizes on read time this involves writing to and reading from a large file say gb vary the application buffer size say from bytes to kb use timing measurement routines such as gettimeofday and getitimer on unix to measure the time taken for different buffer sizes analyze the results and report your findings does buffer size make a difference to the overall write time and perwrite time implement a simulated file system that will be fully contained in a single regular file stored on the disk this disk file will contain directories inodes freeblock information file data blocks etc choose appropriate algorithms for maintaining freeblock information and for allocating data blocks contiguous indexed linked your program will accept system commands from the user to createdelete directories createdeleteopen files readwrite fromto a selected file and to list directory contents principles of io hardware different people look at io hardware in different ways electrical engineers look at it in terms of chips wires power supplies motors and all the other physical components that comprise the hardware programmers look at the interface inputoutput chap presented to the software the commands the hardware accepts the functions it carries out and the errors that can be reported back in this book we are concerned with programming io devices not designing building or maimtaining them so our interest is in how the hardware is programmed not how it works inside nevertheless the programming of many io devices is often intimately connected with their internal operation in the next three sections we will provide a little general background on io hardware as it relates to programming it may be regarded as a review and expansion of the introductory material in sec io devices io devices can be roughly divided into two categories block devices and character devices a block device is one that stores information in fixedsize blocks each one with its own address common block sizes range from to bytes all transfers are in units of one or more entire consecutive blocks the essential property of a block device is that it is possible to read or write each block independently of all the other ones hard disks bluray discs and usb sticks are common block devices if you look very closely the boundary between devices that are block addressable and those that are not is not well defined everyone agrees that a disk is a block addressable device because no matter where the arm currently is it is always possible to seek to another cylinder and then wait for the required block to rotate under the head now consider an oldfashioned tape drive still used sometimes for making disk backups because tapes are cheap tapes contain a sequence of blocks if the tape drive is given a command to read block n it can always rewind the tape and go forward until it comes to block n this operation is analogous to a disk doing a seek except that it takes much longer also it may or may not be possible to rewrite one block in the middle of a tape even if it were possible to use tapes as random access block devices that is stretching the point somewhat they are normally not used that way the other type of io device is the character device a character device delivers or accepts a stream of characters without regard to any block structure it is not addressable and does not have any seek operation printers network interfaces mice for pointing rats for psychology lab experiments and most other devices that are not disklike can be seen as character devices this classification scheme is not perfect some devices do not fit in clocks for example are not block addressable nor do they generate or accept character streams all they do is cause interrupts at welldefined intervals memorymapped screens do not fit the model well either nor do touch screens for that matter still the model of block and character devices is general enough that it can be used as a basis for making some of the operating system software dealing with io device independent the file system for example deals just with abstract block devices and leaves the devicedependent part to lowerlevel software sec history of operating systems operating systems have been evolving through the years in the following sections we will briefly look at a few of the highlights since operating systems have historically been closely tied to the architecture of the computers on which they sec principles of io software let us now turn away from the io hardware and look at the io software first we will look at its goals and then at the different ways io can be done from the point of view of the operating system goals of the io software a key concept in the design of io software is known as device independence what it means is that we should be able to write programs that can access any io device without having to specify the device in advance for example a program that reads a file as input should be able to read a file on a hard disk a dvd or on a usb stick without having to be modified for each different device similarly one should be able to type a command such as sort input output and have it work with input coming from any kind of disk or the keyboard and the output going to any kind of disk or the screen it is up to the operating system to take care of the problems caused by the fact that these devices really are different and require very different command sequences to read or write closely related to device independence is the goal of uniform naming the name of a file or a device should simply be a string or an integer and not depend on the device in any way in unix all disks can be integrated in the filesystem hierarchy in arbitrary ways so the user need not be aware of which name corresponds to which device for example a usb stick can be mounted on top of the directory usrastbackup so that copying a file to usrastbackupmonday copies the file to the usb stick in this way all files and devices are addressed the same way by a path name another important issue for io software is error handling in general errors should be handled as close to the hardware as possible if the controller discovers a read error it should try to correct the error itself if it can if it can not then the device driver should handle it perhaps by just trying to read the block again many errors are transient such as read errors caused by specks of dust on the read head and will frequently go away if the operation is repeated only if the lower layers inputoutput chap are not able to deal with the problem should the upper layers be told about it in many cases error recovery can be done transparently at a low level without the upper levels even knowing about the error still another important issue is that of synchronous blocking vs asynchronous interruptdriven transfers most physical io is asynchronous the cpu starts the transfer and goes off to do something else until the interrupt arrives user programs are much easier to write if the io operations are blocking after a read system call the program is automatically suspended until the data are available in the buffer it is up to the operating system to make operations that are actually interruptdriven look blocking to the user programs however some very highperformance applications need to control all the details of the io so some operating systems make asynchronous io available to them another issue for the io software is buffering often data that come off a device can not be stored directly in their final destination for example when a packet comes in off the network the operating system does not know where to put it until it has stored the packet somewhere and examined it also some devices have severe realtime constraints for example digital audio devices so the data must be put into an output buffer in advance to decouple the rate at which the buffer is filled from the rate at which it is emptied in order to avoid buffer underruns buffering involves considerable copying and often has a major impact on io performance the final concept that we will mention here is sharable vs dedicated devices some io devices such as disks can be used by many users at the same time no problems are caused by multiple users having open files on the same disk at the same time other devices such as printers have to be dedicated to a single user until that user is finished then another user can have the printer having two or more users writing characters intermixed at random to the same page will definitely not work introducing dedicated unshared devices also introduces a variety of problems such as deadlocks again the operating system must be able to hanfle both shared and dedicated devices in a way that avoids problems programmed io there are three fundamentally different ways that io can be performed in this section we will look at the first one programmed io in the next two sections we will examine the others interruptdriven io and io using dma the simplest form of io is to have the cpu do all the work this method is called programmed io it is simplest to illustrate how programmed io works by means of an example consider a user process that wants to print the eightcharacter string abcdefgh on the printer via a serial interface displays on small embedded systems sometimes work this way the software first assembles the string in a buffer in user space as shown in fig a sec io software layers io software is typically organized in four layers as shown in fig each layer has a welldefined function to perform and a welldefined interface to the adjacent layers the functionality and interfaces differ from system to system so the discussion that follows which examines all the layers starting at the bottom is not specific to one machine userlevel io software deviceindependent operating system software device drivers interrupt handlers hardware figure layers of the io software system interrupt handlers while programmed io is occasionally useful for most io interrupts are an unpleasant fact of life and can not be avoided they should be hidden away deep in the bowels of the operating system so that as little of the operating system as possible knows about them the best way to hide them is to have the driver starting an io operation block until the io has completed and the interrupt occurs the driver can block itself for example by doing a down on a semaphore a wait on a condition variable a receive on a message or something similar when the interrupt happens the interrupt procedure does whatever it has to in order to handle the interrupt then it can unblock the driver that was waiting for it in some cases it will just complete up on a semaphore in others it will do a signal on a condition variable in a monitor in still others it will send a message to the blocked driver in all cases the net effect of the interrupt will be that a driver that was previously blocked will now be able to run this model works best if drivers are structured as kernel processes with their own states stacks and program counters of course reality is not quite so simple processing an interrupt is not just a matter of taking the interrupt doing an up on some semaphore and then executing an iret instruction to return from the interrupt to the previous process there is a great deal more work involved for the operating system we will now give an outline of this work as a series of steps that must be performed in software after the hardware interrupt has completed it should be noted that the details are highly sec disks now we will begin studying some real io devices we will begin with disks which are conceptually simple yet very important after that we will examine clocks keyboards and displays disk hardware disks come in a variety of types the most common ones are the magnetic hard disks they are characterized by the fact that reads and writes are equally fast which makes them suitable as secondary memory paging file systems etc arrays of these disks are sometimes used to provide highly reliable storage for distribution of programs data and movies optical disks dvds and bluray are also important finally solidstate disks are increasingly popular as they are fast and do not contain moving parts in the following sections we will discuss magnetic disks as an example of the hardware and then describe the software for disk devices in general magnetic disks magnetic disks are organized into cylinders each one containing as many tracks as there are heads stacked vertically the tracks are divided into sectors with the number of sectors around the circumference typically being to on floppy disks and up to several hundred on hard disks the number of heads varies from to about older disks have little electronics and just deliver a simple serial bit stream on these disks the controller does most of the work on other disks in particular ide integrated drive electronics and sata serial ata disks the disk drive itself contains a microcontroller that does considerable work and allows the real controller to issue a set of higherlevel commands the controller often does track caching badblock remapping and much more a device feature that has important implications for the disk driver is the possibility of a controller doing seeks on two or more drives at the same time these are known as overlapped seeks while the controller and software are waiting for a seek to complete on one drive the controller can initiate a seek on another drive many controllers can also read or write on one drive while seeking on one or more other drives but a floppy disk controller can not read or write on two drives at the inputoutput chap same time reading or writing requires the controller to move bits on a microsecond time scale so one transfer uses up most of its computing power the situation is different for hard disks with integrated controllers and in a system with more than one of these hard drives they can operate simultaneously at least to the extent of transferring between the disk and the controllers buffer memory only one transfer between the controller and the main memory is possible at once however the ability to perform two or more operations at the same time can reduce the average access time considerably figure compares parameters of the standard storage medium for the original ibm pc with parameters of a disk made three decades later to show how much disks changed in that time it is interesting to note that not all parameters have improved as much average seek time is almost times better than it was transfer rate is times better while capacity is up by a factor of this pattern has to do with relatively gradual improvements in the moving parts but much higher bit densities on the recording surfaces parameter ibm kb floppy disk wd hlfs hard disk number of cylinders tracks per cylinder sectors per track avg sectors per disk bytes per sector disk capacity kb gb seek time adjacent cylinders msec msec seek time average case msec msec rotation time msec msec time to transfer sector msec sec figure disk parameters for the original ibm pc kb floppy disk and a western digital wd hlfs velociraptor hard disk one thing to be aware of in looking at the specifications of modern hard disks is that the geometry specified and used by the driver software is almost always different from the physical format on old disks the number of sectors per track was the same for all cylinders modern disks are divided into zones with more sectors on the outer zones than the inner ones fig a illustrates a tiny disk with two zones the outer zone has sectors per track the inner one has sectors per track a real disk such as the wd hlfs typically has or more zones with the number of sectors increasing by about per zone as one goes out from the innermost to the outermost zone to hide the details of how many sectors each track has most modern disks have a virtual geometry that is presented to the operating system the software is instructed to act as though there are x cylinders y heads and z sectors per track sec clocks clocks also called timers are essential to the operation of any multiprogrammed system for a variety of reasons they maintain the time of day and prevent one process from monopolizing the cpu among other things the clock software can take the form of a device driver even though a clock is neither a block device like a disk nor a character device like a mouse our examination of clocks will follow the same pattern as in the previous section first a look at clock hardware and then a look at the clock software clock hardware two types of clocks are commonly used in computers and both are quite different from the clocks and watches used by people the simpler clocks are tied to the or volt power line and cause an interrupt on every voltage cycle at or hz these clocks used to dominate but are rare nowadays the other kind of clock is built out of three components a crystal oscillator a counter and a holding register as shown in fig when a piece of quartz crystal is properly cut and mounted under tension it can be made to generate a periodic signal of very great accuracy typically in the range of several hundred megahertz to a few gigahertz depending on the crystal chosen using electronics sec user interfaces keyboard mouse monitor every generalpurpose computer has a keyboard and monitor and sometimes a mouse to allow people to interact with it although the keyboard and monitor are technically separate devices they work closely together on mainframes there are frequently many remote users each with a device containing a keyboard and an attached display as a unit these devices have historically been called terminals people frequently still use that term even when discussing personal computer keyboards and monitors mostly for lack of a better term input software user input comes primarily from the keyboard and mouse or somtimes touch screens so let us look at those on a personal computer the keyboard contains an embedded microprocessor which usually communicates through a specialized serial port with a controller chip on the parentboard although increasingly keyboards are connected to a usb port an interrupt is generated whenever a key is struck and a second one is generated whenever a key is released at each of these keyboard interrupts the keyboard driver extracts the information about what happens from the io port associated with the keyboard everything else happens in software and is pretty much independent of the hardware most of the rest of this section can be best understood when thinking of typing commands to a shell window commandline interface this is how programmers commonly work we will discuss graphical interfaces below some devices in particular touch screens are used for input and output we have made an arbitrary choice to discuss them in the section on output devices we will discuss graphical interfaces later in this chapter keyboard software the number in the io register is the key number called the scan code not the ascii code normal keyboards have fewer than keys so only bits are needed to represent the key number the eighth bit is set to on a key press and to on sec thin clients over the years the main computing paradigm has oscillated between centralized and decentralized computing the first computers such as the eniac were in fact personal computers albeit large ones because only one person could use one at once then came timesharing systems in which many remote users at simple terminals shared a big central computer next came the pc era in which the users had their own personal computers again while the decentralized pc model has advantages it also has some severe disadvantages that are only beginning to be taken seriously probably the biggest problem is that each pc has a large hard disk and complex software that must be maintained for example when a new release of the operating system comes out a great deal of work has to be done to perform the upgrade on each machine separately at most corporations the labor costs of doing this kind of software maintenance dwarf the actual hardware and software costs for home users the labor is technically free but few people are capable of doing it correctly and fewer still enjoy doing it with a centralized system only one or a few machines have to be updated and those machines have a staff of experts to do the work a related issue is that users should make regular backups of their gigabyte file systems but few of them do when disaster strikes a great deal of moaning and wringing of hands tends to follow with a centralized system backups can be made every night by automated tape robots another advantage is that resource sharing is easier with centralized systems a system with remote users each with mb of ram will have most of that ram idle most of the time with a centralized system with gb of ram it never happens that some user temporarily needs a lot of ram but can not get it because it is on someone elses pc the same argument holds for disk space and other resources finally we are starting to see a shift from pccentric computing to webcentric computing one area where this shift is very far along is email people used to get their email delivered to their home machine and read it there nowadays many people log into gmail hotmail or yahoo and read their mail there the next step is for people to log into other websites to do word processing build spreadsheets power management the first generalpurpose electronic computer the eniac had vacuum tubes and consumed watts of power as a result it ran up a nontrivial electricity bill after the invention of the transistor power usage dropped dramatically and the computer industry lost interest in power requirements however nowadays power management is back in the spotlight for several reasons and the operating system is playing a role here let us start with desktop pcs a desktop pc often has a watt power supply which is typically efficient that is loses of the incoming energy to heat if million of these machines are turned on at once worldwide together they use megawatts of electricity this is the total output of averagesized nuclear power plants if power requirements could be cut in half we could get rid of nuclear power plants from an environmental point of view getting rid of nuclear power plants or an equivalent number of fossilfuel plants is a big win and well worth pursuing the other place where power is a big issue is on batterypowered computers including notebooks handhelds and webpads among others the heart of the problem is that the batteries can not hold enough charge to last very long a few hours at most furthermore despite massive research efforts by battery companies computer companies and consumer electronics companies progress is glacial to inputoutput chap an industry used to a doubling of performance every months moores law having no progress at all seems like a violation of the laws of physics but that is the current situation as a consequence making computers use less energy so existing batteries last longer is high on everyones agenda the operating system plays a major role here as we will see below at the lowest level hardware vendors are trying to make their electronics more energy efficient techniques used include reducing transistor size employing dynamic voltage scaling using lowswing and adiabatic buses and similar techniques these are outside the scope of this book but interested readers can find a good survey in a paper by venkatachalam and franz there are two general approaches to reducing energy consumption the first one is for the operating system to turn off parts of the computer mostly io devices when they are not in use because a device that is off uses little or no energy the second one is for the application program to use less energy possibly degrading the quality of the user experience in order to stretch out battery time we will look at each of these approaches in turn but first we will say a little bit about hardware design with respect to power usage hardware issues batteries come in two general types disposable and rechargeable disposable batteries most commonly aaa aa and d cells can be used to run handheld devices but do not have enough energy to power notebook computers with large bright screens a rechargeable battery in contrast can store enough energy to power a notebook for a few hours nickel cadmium batteries used to dominate here but they gave way to nickel metal hydride batteries which last longer and do not pollute the environment quite as badly when they are eventually discarded lithium ion batteries are even better and may be recharged without first being fully drained but their capacities are also severely limited the general approach most computer vendors take to battery conservation is to design the cpu memory and io devices to have multiple states on sleeping hibernating and off to use the device it must be on when the device will not be needed for a short time it can be put to sleep which reduces energy consumption when it is not expected to be needed for a longer interval it can be made to hibernate which reduces energy consumption even more the tradeoff here is that getting a device out of hibernation often takes more time and energy than getting it out of sleep state finally when a device is off it does nothing and consumes no power not all devices have all these states but when they do it is up to the operating system to manage the state transitions at the right moments some computers have two or even three power buttons one of these may put the whole computer in sleep state from which it can be awakened quickly by typing a character or moving the mouse another may put the computer into hibernation from which wakeup takes far longer in both cases these buttons typically do sec research on inputoutput there is a fair amount of research on inputoutput some of it is focused on specific devices rather than io in general other work focuses on the entire io infrastructure for instance the streamline architecture aims to provide applicationtailored io that minimizes overhead due to copying context switching signaling and poor use of the cache and tlb debruijn et al it builds on the notion of beltway buffers advanced circular buffers that are more efficient than sec summary inputoutput is an often neglected but important topic a substantial fraction of any operating system is concerned with io io can be accomplished in one of three ways first there is programmed io in which the main cpu inputs or outputs each byte or word and sits in a tight loop waiting until it can get or send the next one second there is interruptdriven io in which the cpu starts an io transfer for a character or word and goes off to do something else until an interrupt arrives signaling completion of the io third there is dma in which a separate chip manages the complete transfer of a block of data given an interrupt only when the entire block has been transferred io can be structured in four levels the interruptservice procedures the device drivers the deviceindependent io software and the io libraries and spoolers that run in user space the device drivers handle the details of running the devices and providing uniform interfaces to the rest of the operating system the deviceindependent io software does things like buffering and error reporting disks come in a variety of types including magnetic disks raids flash drives and optical disks on rotating disks disk arm scheduling algorithms can often be used to improve disk performance but the presence of virtual geometries complicates matters by pairing two disks a stable storage medium with certain useful properties can be constructed clocks are used for keeping track of the real time limiting how long processes can run handling watchdog timers and doing accounting sec deadlocks computer systems are full of resources that can be used only by one process at a time common examples include printers tape drives for backing up company data and slots in the systems internal tables having two processes simultaneously writing to the printer leads to gibberish having two processes using the same filesystem table slot invariably will lead to a corrupted file system consequently all operating systems have the ability to temporarily grant a process exclusive access to certain resources for many applications a process needs exclusive access to not one resource but several suppose for example two processes each want to record a scanned document on a bluray disc process a requests permission to use the scanner and is granted it process b is programmed differently and requests the bluray recorder first and is also granted it now a asks for the bluray recorder but the request is suspended until b releases it unfortunately instead of releasing the bluray recorder b asks for the scanner at this point both processes are blocked and will remain so forever this situation is called a deadlock deadlocks can also occur across machines for example many offices have a local area network with many computers connected to it often devices such as scanners bluraydvd recorders printers and tape drives are connected to the network as shared resources available to any user on any machine if these devices can be reserved remotely ie from the users home machine deadlocks of the same kind can occur as described above more complicated situations can cause deadlocks involving three four or more devices and users resources a major class of deadlocks involves resources to which some process has been granted exclusive access these resources include devices data records files and so forth to make the discussion of deadlocks as general as possible we will refer to the objects granted as resources a resource can be a hardware device eg a bluray drive or a piece of information eg a record in a database a computer will normally have many different resources that a process can acquire for some resources several identical instances may be available such as three bluray drives when several copies of a resource are available any one of them can be used to satisfy any request for the resource in short a resource is anything that must be acquired used and released over the course of time preemptable and nonpreemptable resources resources come in two types preemptable and nonpreemptable a preemptable resource is one that can be taken away from the process owning it with no ill effects memory is an example of a preemptable resource consider for example a system with gb of user memory one printer and two gb processes that each want to print something process a requests and gets the printer then starts to compute the values to print before it has finished the computation it exceeds its time quantum and is swapped out to disk process b now runs and tries unsuccessfully as it turns out to acquire the printer potenially we now have a deadlock situation because a has the printer and b has the memory and neither one can proceed without the resource held by the other fortunately it is possible to preempt take away the memory from b by sec introduction to deadlocks typedef int semaphore semaphore resource semaphore resource semaphore resource semaphore resource void process avoid void process avoid downresource downresource downresource downresource use both resources use both resources upresource upresource upresource upresource void process bvoid void process bvoid downresource downresource downresource downresource use both resources use both resources upresource upresource upresource upresource a b figure a deadlockfree code b code with a potential deadlock deadlock detection and recovery a second technique is detection and recovery when this technique is used the system does not attempt to prevent deadlocks from occurring instead it lets them occur tries to detect when this happens and then takes some action to actually this bit of folklore is nonsense ostriches can run at kmhour and their kick is powerful enough to kill any lion with visions of a big chicken dinner and lions know this deadlocks chap recover after the fact in this section we will look at some of the ways deadlocks can be detected and some of the ways recovery from them can be handled deadlock detection with one resource of each type let us begin with the simplest case there is only one resource of each type such a system might have one scanner one bluray recorder one plotter and one tape drive but no more than one of each class of resource in other words we are excluding systems with two printers for the moment we will treat them later using a different method for such a system we can construct a resource graph of the sort illustrated in fig if this graph contains one or more cycles a deadlock exists any process that is part of a cycle is deadlocked if no cycles exist the system is not deadlocked as an example of a system more complex than those we have looked at so far consider a system with seven processes a though g and six resources r through w the state of which resources are currently owned and which ones are currently being requested is as follows process a holds r and wants s process b holds nothing but wants t process c holds nothing but wants s process d holds u and wants s and t process e holds t and wants v process f holds w and wants s process g holds v and wants u the question is is this system deadlocked and if so which processes are involved to answer this question we can construct the resource graph of fig a this graph contains one cycle which can be seen by visual inspection the cycle is shown in fig b from this cycle we can see that processes d e and g are all deadlocked processes a c and f are not deadlocked because s can be allocated to any one of them which then finishes and returns it then the other two can take it in turn and also complete note that to make this example more interesting we have allowed processes namely d to ask for two resources at once although it is relatively simple to pick out the deadlocked processes by visual inspection from a simple graph for use in actual systems we need a formal algorithm for detecting deadlocks many algorithms for detecting cycles in directed graphs are known below we will give a simple one that inspects a graph and terminates either when it has found a cycle or when it has shown that none exists it sec deadlock avoidance in the discussion of deadlock detection we tacitly assumed that when a process asks for resources it asks for them all at once the r matrix of fig in most systems however resources are requested one at a time the system must be able to decide whether granting a resource is safe or not and make the allocation only when it is safe thus the question arises is there an algorithm that can always avoid deadlock by making the right choice all the time the answer is a qualified yes we can avoid deadlocks but only if certain information is available in advance in this section we examine ways to avoid deadlock by careful resource allocation resource trajectories the main algorithms for deadlock avoidance are based on the concept of safe states before describing them we will make a slight digression to look at the concept of safety in a graphic and easytounderstand way although the graphical approach does not translate directly into a usable algorithm it gives a good intuitive feel for the nature of the problem sec deadlock prevention having seen that deadlock avoidance is essentially impossible because it requires information about future requests which is not known how do real systems avoid deadlock the answer is to go back to the four conditions stated by coffman et al to see if they can provide a clue if we can ensure that at least one of these conditions is never satisfied then deadlocks will be structurally impossible havender attacking the mutualexclusion condition first let us attack the mutual exclusion condition if no resource were ever assigned exclusively to a single process we would never have deadlocks for data the simplest method is to make data read only so that processes can use the data concurrently however it is equally clear that allowing two processes to write on the printer at the same time will lead to chaos by spooling printer output several processes can generate output at the same time in this model the only process that actually requests the physical printer is the printer daemon since the daemon never requests any other resources we can eliminate deadlock for the printer if the daemon is programmed to begin printing even before all the output is spooled the printer might lie idle if an output process decides to wait several hours after the first burst of output for this reason daemons are normally programmed to print only after the complete output file is available however this decision itself could lead to deadlock what would happen if two processes each filled up one half of the available spooling space with output and neither was finished producing its full output in this case we would have two processes that had each finished part but not all of their output and could not continue neither process will ever finish so we would have a deadlock on the disk nevertheless there is a germ of an idea here that is frequently applicable avoid assigning a resource unless absolutely necessary and try to make sure that as few processes as possible may actually claim the resource attacking the holdandwait condition the second of the conditions stated by coffman et al looks slightly more promising if we can prevent processes that hold resources from waiting for more resources we can eliminate deadlocks one way to achieve this goal is to require sec other issues in this section we will discuss a few miscellaneous issues related to deadlocks these include twophase locking nonresource deadlocks and starvation sec summary deadlock is a potential problem in any operating system it occurs when all the members of a set of processes are blocked waiting for an event that only other members of the same set can cause this situation causes all the processes to wait sec virtualization and the cloud in some situations an organization has a multicomputer but does not actually want it a common example is where a company has an email server a web server an ftp server some ecommerce servers and others these all run on different computers in the same equipment rack all connected by a highspeed network in other words a multicomputer one reason all these servers run on separate machines may be that one machine can not handle the load but another is reliability management simply does not trust the operating system to run hours a day or days a year with no failures by putting each service on a separate computer if one of the servers crashes at least the other ones are not affected this is good for security also even if some malevolent intruder manages to compromise the web server he will not immediately have access to sensitive emails also a property sometimes referred to as sandboxing while isolation and fault tolerance are achieved this way this solution is expensive and hard to manage because so many machines are involved mind you these are just two out of many reasons for keeping separate machines for instance organizations often depend on more than one operating system for their daily operations a web server on linux a mail server on windows an ecommerce server for customers running on os x and a few other services running on various flavors of unix again this solution works but cheap it is definitely not what to do a possible and popular solution is to use virtual machine technology which sounds very hip and modern but the idea is old dating back to the history amount of critical state information about every process is kept in operating system tables including information relating to open files alarms signal handlers and more when migrating a virtual machine all that have to be moved are the memory and disk images since all the operating system tables move too another use for virtual machines is to run legacy applications on operating systems or operating system versions no longer supported or which do not work on current hardware these can run at the same time and on the same hardware as current applications in fact the ability to run at the same time applications that use different operating systems is a big argument in favor of virtual machines yet another important use of virtual machines is for software development a programmer who wants to make sure his software works on windows windows several versions of linux freebsd openbsd netbsd and os x among other systems no longer has to get a dozen computers and install different operating systems on all of them instead he merely creates a dozen virtual machines on a single computer and installs a different operating system on each one of course he could have partitioned the hard disk and installed a different operating system in each partition but that approach is more difficult first of all standard pcs support only four primary disk partitions no matter how big the disk is second although a multiboot program could be installed in the boot block it would be necessary to reboot the computer to work on a new operating system with virtual machines all of them can run at once since they are really just glorified processes perhaps the most important and buzzwordcompliant use case for virtualization nowadays is found in the cloud the key idea of a cloud is straightforward outsource your computation or storage needs to a wellmanaged data center run by a company specializing in this and staffed by experts in the area because the data center typically belongs to someone else you will probably have to pay for the use of the resources but at least you will not have to worry about the physical machines power cooling and maintenance because of the isolation offered by virtualizaton cloudproviders can allow multiple clients even competitors to share a single physical machine each client gets a piece of the pie at the risk of stretching the cloud metaphor we mention that early critics maintained that the pie was only in the sky and that real organizations would not want to put their sensitive data and computations on someone elses resources by now however virtualized machines in the cloud are used by countless organization for countless applications and while it may not be for all organizations and all data there is no doubt that cloud computing has been a success requirements for virtualization it is important that virtual machines act just like the real mccoy in particular it must be possible to boot them like real machines and install arbitrary operating systems on them just as can be done on the real hardware it is the task of the sec type and type hypervisors goldberg distinguished between two approaches to virtualization one kind of hypervisor dubbed a type hypervisor is illustrated in fig a technically it is like an operating system since it is the only program running in the most privileged mode its job is to support multiple copies of the actual hardware called virtual machines similar to the processes a normal operating system runs in contrast a type hypervisor shown in fig b is a different kind of animal it is a program that relies on say windows or linux to allocate and schedule resources very much like a regular process of course the type hypervisor still pretends to be a full computer with a cpu and various devices both types of hypervisor must execute the machines instruction set in a safe manner for instance an operating system running on top of the hypervisor may change and even mess up its own page tables but not those of others the operating system running on top of the hypervisor in both cases is called the guest operating system for a type hypervisor the operating system running on the hardware is called the host operating system the first type hypervisor techniques for efficient virtualization virtualizability and performance are important issues so let us examine them more closely assume for the moment that we have a type hypervisor supporting one virtual machine as shown in fig like all type hypervisors it sec computer hardware review an operating system is intimately tied to the hardware of the computer it runs on it extends the computers instruction set and manages its resources to work it must know a great deal about the hardware at least about how the hardware appears to the programmer for this reason let us briefly review computer hardware as found in modern personal computers after that we can start getting into the details of what operating systems do and how they work conceptually a simple personal computer can be abstracted to a model resembling that of fig the cpu memory and io devices are all connected by a system bus and communicate with one another over it modern personal computers have a more complicated structure involving multiple buses which we will look at later for the time being this model will be sufficient in the following sections we will briefly review these components and examine some of the hardware issues that are of concern to operating system designers needless to say this will be a very compact summary many books have been written on the subject of computer hardware and computer organization two wellknown ones are by tanenbaum and austin and patterson and hennessy monitor hard keyboard usb printer disk drive video keyboard usb hard cpu memory controller controller controller disk mmu controller bus figure some of the components of a simple personal computer sec are hypervisors microkernels done right both type and type hypervisors work with unmodified guest operating systems but have to jump through hoops to get good performance we have seen that paravirtualization takes a different approach by modifying the source code of the guest operating system instead rather than performing sensitive instructions the paravirtualized guest executes hypercalls in effect the guest operating system is acting like a user program making system calls to the operating system the hypervisor when this route is taken the hypervisor must define an interface consisting of a set of procedure calls that guest operating systems can use this set of calls forms what is effectively an api application programming interface even though it is an interface for use by guest operating systems not application programs going one step further by removing all the sensitive instructions from the operating system and just having it make hypercalls to get system services like io we have turned the hypervisor into a microkernel like that of fig the idea explored in paravirtualization is that emulating peculiar hardware instructions is an unpleasant and timeconsuming task it requires a call into the hypervisor and then emulating the exact semantics of a complicated instruction it is far better just to have the guest operating system call the hypervisor or microkernel to do io and so on indeed some researchers have argued that we should perhaps consider hypervisors as microkernels done right hand et al the first thing to mention is that this is a highly controversial topic and some researchers have vocally opposed the notion arguing that the difference between the two is not fundamental to begin with heiser et al others suggest that compared to microkernels hypervisors may not even be that well suited for building secure systems and advocate that they be extended with kernel functionality like message passing and memory sharing hohmuth et al finally some researchers argue that perhaps hypervisors are not even operating systems research done right roscoe et al since nobody said anything about operating system textbooks done right virtualization and the cloud chap or wrong yet we think we do right by exploring the similarity between hypervisors and microkernels a bit more the main reason the first hypervisors emulated the complete machine was the lack of availability of source code for the guest operating system eg for windows or the vast number of variants eg for linux perhaps in the future the hypervisormicrokernel api will be standardized and subsequent operating systems will be designed to call it instead of using sensitive instructions doing so would make virtual machine technology easier to support and use the difference between true virtualization and paravirtualization is illustrated in fig here we have two virtual machines being supported on vt hardware on the left is an unmodified version of windows as the guest operating system when a sensitive instruction is executed the hardware causes a trap to the hypervisor which then emulates it and returns on the right is a version of linux modified so that it no longer contains any sensitive instructions instead when it needs to do io or change critical internal registers such as the one pointing to the page tables it makes a hypervisor call to get the work done just like an application program making a system call in standard linux true virtualization paravirtualization trap due trap due unmodified windows to sensitive modified linux to hypervisor instruction call type hypervisor microkernel hardware figure true virtualization and paravirtualization in fig we have shown the hypervisor as being divided into two parts separated by a dashed line in reality only one program is running on the hardware one part of it is responsible for interpreting trapped sensitive instructions in this case from windows the other part of it just carries out hypercalls in the figure the latter part is labeled microkernel if the hypervisor is intended to run only paravirtualized guest operating systems there is no need for the emulation of sensitive instructions and we have a true microkernel which just provides very basic services such as process dispatching and managing the mmu the boundary between a type hypervisor and a microkernel is vague already and will get even less clear as hypervisors begin acquiring more and more functionality and hypercalls as seems likely again this subject is controversial but it is increasingly clear that the program running in kernel mode on the bare hardware should be small and reliable and consist of thousands not millions of lines of code sec memory virtualization so far we have addressed the issue of how to virtualize the cpu but a computer system has more than just a cpu it also has memory and io devices they have to be virtualized too let us see how that is done modern operating systems nearly all support virtual memory which is basically a mapping of pages in the virtual address space onto pages of physical memory this mapping is defined by multilevel page tables typically the mapping is set in motion by having the operating system set a control register in the cpu that points to the toplevel page table virtualization greatly complicates memory management in fact it took hardware manufacturers two tries to get it right suppose for example a virtual machine is running and the guest operating system in it decides to map its virtual pages and onto physical pages and respectively it builds page tables containing this mapping and loads a hardware register to point to the toplevel page table this instruction is sensitive on a vt cpu it will trap with dynamic translation it will cause a call to a hypervisor procedure on a paravirtualized operating system it will generate a hypercall for simplicity let us assume it traps into a type hypervisor but the problem is the same in all three cases what does the hypervisor do now one solution is to actually allocate physical pages and to this virtual machine and set up the actual page tables to map the virtual machines virtual pages and to use them so far so good now suppose a second virtual machine starts and maps its virtual pages and onto physical pages and and loads the control register to point to its page tables the hypervisor catches the trap but what should it do it can not use this mapping because physical pages and are already in use it can find some free pages say and and use them but it first has to create new page tables mapping the virtual pages and of virtual machine onto and if another virtual machine starts and tries to use physical pages and it has to create a mapping for them in general for each virtual machine the hypervisor needs to create a shadow page table that maps the virtual pages used by the virtual machine onto the actual pages the hypervisor gave it worse yet every time the guest operating system changes its page tables the hypervisor must change the shadow page tables as well for example if the guest os remaps virtual page onto what it sees as physical page instead of the hypervisor has to know about this change the trouble is that the guest operating system can change its page tables by just writing to memory no sensitive operations are required so the hypervisor does not even know about the change and certainly can not update the shadow page tables used by the actual hardware a possible but clumsy solution is for the hypervisor to keep track of which page in the guests virtual memory contains the toplevel page table it can get this information the first time the guest attempts to load the hardware register that points to it because this instruction is sensitive and traps the hypervisor can create sec io virtualization having looked at cpu and memory virtualization we next examine io virtualization the guest operating system will typically start out probing the hardware to find out what kinds of io devices are attached these probes will trap to the hypervisor what should the hypervisor do one approach is for it to report back that the disks printers and so on are the ones that the hardware actually has the guest will then load device drivers for these devices and try to use them when the device drivers try to do actual io they will read and write the devices hardware device registers these instructions are sensitive and will trap to the hypervisor which could then copy the needed values to and from the hardware registers as needed but here too we have a problem each guest os could think it owns an entire disk partition and there may be many more virtual machines hundreds than there are actual disk partitions the usual solution is for the hypervisor to create a file or region on the actual disk for each virtual machines physical disk since the guest os is trying to control a disk that the real hardware has and which the hypervisor understands it can convert the block number being accessed into an offset into the file or disk region being used for storage and do the io it is also possible for the disk that the guest is using to be different from the real one for example if the actual disk is some brandnew highperformance disk or raid with a new interface the hypervisor could advertise to the guest os that it has a plain old ide disk and let the guest os install an ide disk driver when this driver issues ide disk commands the hypervisor converts them into commands to drive the new disk this strategy can be used to upgrade the hardware without changing the software in fact this ability of virtual machines to remap sec virtual appliances virtual machines offer an interesting solution to a problem that has long plagued users especially users of open source software how to install new application programs the problem is that many applications are dependent on numerous other applications and libraries which are themselves dependent on a host of other software packages and so on furthermore there may be dependencies on particular versions of the compilers scripting languages and the operating system with virtual machines now available a software developer can carefully construct a virtual machine load it with the required operating system compilers libraries and application code and freeze the entire unit ready to run this virtual machine image can then be put on a cdrom or a website for customers to install or download this approach means that only the software developer has to understand all the dependencies the customers get a complete package that actually works completely independent of which operating system they are running and which other software packages and libraries they have installed these shrinkwrapped virtual machines are often called virtual appliances as an example amazons ec cloud has many prepackaged virtual appliances available for its clients which it offers as convenient software services software as a service licensing issues some software is licensed on a percpu basis especially software for companies in other words when they buy a program they have the right to run it on just one cpu whats a cpu anyway does this contract give them the right to run clouds virtualization technology played a crucial role in the dizzying rise of cloud computing there are many clouds some clouds are public and available to anyone willing to pay for the use of resources others are private to an organization likewise different clouds offer different things some give their users access to physical hardware but most virtualize their environments some offer the bare machines virtual or not and nothing more but others offer software that is ready to use and can be combined in interesting ways or platforms that make it easy for their users to develop new services cloud providers typically offer different categories of resources such as big machines versus little machines etc for all the talk about clouds few people seem really sure about what they are exactly the national institute of standards and technology always a good source to fall back on lists five essential characteristics ondemand selfservice users should be able to provision resources automatically without requiring human interaction broad network access all these resources should be available over the network via standard mechanisms so that heterogeneous devices can make use of them resource pooling the computing resource owned by the provider should be pooled to serve multiple users and with the ability to assign and reassign resources dynamically the users generally do not even know the exact location of their resources or even which country they are located in rapid elasticity it should be possible to acquire and release resources elastically perhaps even automatically to scale immediately with the users demands measured service the cloud provider meters the resources used in a way that matches the type of service agreed upon virtualization and the cloud chap clouds as a service in this section we will look at clouds with a focus on virtualization and operating systems specifically we consider clouds that offer direct access to a virtual machine which the user can use in any way he sees fit thus the same cloud may run different operating systems possibly on the same hardware in cloud terms this is known as iaas infrastructure as a service as opposed to paas platform as a service which delivers an environment that includes things such as a specific os database web server and so on saas software as a service which offers access to specific software such as microsoft office or google apps and many other types of asaservice one example of an iaas cloud is amazon ec which happens to be based on the xen hypervisor and counts multiple hundreds of thousands of physical machines provided you have the cash you can have as much computing power as you need clouds can transform the way companies do computing overall consolidating the computing resources in a small number of places conveniently located near a power source and cheap cooling benefits from economy of scale outsourcing your processing means that you need not worry so much about managing your it infrastructure backups maintenance depreciation scalability reliability performance and perhaps security all of that is done in one place and assuming the cloud provider is competent done well you would think that it managers are happier today than ten years ago however as these worries disappeared new ones emerged can you really trust your cloud provider to keep your sensitive data safe will a competitor running on the same infrastructure be able to infer information you wanted to keep private what laws apply to your data for instance if the cloud provider is from the united states is your data subject to the patriot act even if your company is in europe once you store all your data in cloud x will you be able to get them out again or will you be tied to that cloud and its provider forever something known as vendor lockin virtual machine migration virtualization technology not only allows iaas clouds to run multiple different operating systems on the same hardware at the same time it also permits clever management we have already discussed the ability to overcommit resources especially in combination with deduplication now we will look at another management issue what if a machine needs servicing or even replacement while it is running lots of important machines probably clients will not be happy if their systems go down because the cloud provider wants to replace a disk drive hypervisors decouple the virtual machine from the physical hardware in other words it does not really matter to the virtual machine if it runs on this machine or that machine thus the administrator could simply shut down all the virtual machines and restart them again on a shiny new machine doing so however results sec case study vmware since vmware inc has been the leading commercial provider of virtualization solutions with products for desktops servers the cloud and now even on cell phones it provides not only hypervisors but also the software that manages virtual machines on a large scale we will start this case study with a brief history of how the company got started we will then describe vmware workstation a type hypervisor and the companys first product the challenges in its design and the key elements of the solution we then describe the evolution of vmware workstation over the years we conclude with a description of esx server vmwares type hypervisor the early history of vmware although the idea of using virtual machines was popular in the s and s in both the computing industry and academic research interest in virtualization was totally lost after the s and the rise of the personal computer industry only ibms mainframe division still cared about virtualization indeed the computer architectures designed at the time and in particular intels x architecture did not provide architectural support for virtualization ie they failed the popekgoldberg criteria this is extremely unfortunate since the cpu a complete redesign of the was done a decade after the popekgoldberg paper and the designers should have known better in at stanford three of the future founders of vmware had built a prototype hypervisor called disco bugnion et al with the goal of running commodity operating systems in particular unix on a very large scale multiprocessor then being developed at stanford the flash machine during that project the authors realized that using virtual machines could solve simply and elegantly a number of hard system software problems rather than trying to solve these problems within existing operating systems one could innovate in a layer below existing operating systems the key observation of disco was that while the high complexity of modern operating systems made innovation difficult the relative simplicity of a virtual machine monitor and its position in the software stack provided a powerful foothold to address limitations of operating systems although disco was aimed at very large servers and designed for the mips architecture the authors realized that the same approach could equally apply and be commercially relevant for the x marketplace and so vmware inc was founded in with the goal of bringing virtualization to the x architecture and the personal computer industry vmwares first product vmware workstation was the first virtualization solution available for bit xbased platforms the product was first released in and came in two variants vmware workstation for linux a type hypervisor that ran on top of linux host operating systems and vmware workstation for windows which sec research on virtualization and the cloud virtualization technology and cloud computing are both extremely active research areas the research produced in these fields is way too much to enumerate each has multiple research conferences for instance the virtual execution environments vee conference focuses on virtualization in the broadest sense you will find papers on migration deduplication scaling out and so on likewise the acm symposium on cloud computing socc is one of the bestknown venues on cloud computing papers in socc include work on fault resilience scheduling of data center workloads management and debugging in clouds and so on old topics never really die as in penneman et al which looks at the problems of virtualizing the arm in the light of the popek and goldberg criteria security is perpetually a hot topic beham et al mao and pearce et al as is reducing energy usage botero and hesselbach and yuan et al with so many data centers now using virtualization technology the networks connecting these machines are also a major subject of research theodorou et al virtualization in wireless networks is also an upandcoming subject wang et al a one interesting area which has seen a lot of interesting research is nested virtualization benyehuda et al and zhang et al the idea is that a virtual machine itself can be further virtualized into multiple higherlevel virtual machines which in turn may be virtualized and so on one of these projects is appropriately called turtles because once you start its turtles all the way down sec multiple processor systems since its inception the computer industry has been driven by an endless quest for more and more computing power the eniac could perform operations per second easily times faster than any calculator before it yet people were not satisfied with it we now have machines millions of times faster than the eniac and still there is a demand for yet more horsepower astronomers are trying to make sense of the universe biologists are trying to understand the implications of the human genome and aeronautical engineers are interested in building safer and more efficient aircraft and all want more cpu cycles however much computing power there is it is never enough in the past the solution was always to make the clock run faster unfortunately we have begun to hit some fundamental limits on clock speed according to einsteins special theory of relativity no electrical signal can propagate faster than the speed of light which is about cmnsec in vacuum and about cmnsec in copper wire or optical fiber this means that in a computer with a ghz clock the signals can not travel more than cm in total for a ghz computer the total path length is at most mm a thz ghz computer will have to be smaller than microns just to let the signal get from one end to the other and back once within a single clock cycle making computers this small may be possible but then we hit another fundamental problem heat dissipation the faster the computer runs the more heat it generates and the smaller the computer the harder it is to get rid of this heat already on highend x systems the cpu cooler is bigger than the cpu itself all multiprocessors a sharedmemory multiprocessor or just multiprocessor henceforth is a computer system in which two or more cpus share full access to a common ram a program running on any of the cpus sees a normal usually paged virtual address space the only unusual property this system has is that the cpu can write some value into a memory word and then read the word back and get a different value because another cpu has changed it when organized correctly this property forms the basis of interprocessor communication one cpu writes some data into memory and another one reads the data out for the most part multiprocessor operating systems are normal operating systems they handle system calls do memory management provide a file system and manage io devices nevertheless there are some areas in which they have unique features these include process synchronization resource management and scheduling below we will first take a brief look at multiprocessor hardware and then move on to these operating systems issues multiprocessor hardware although all multiprocessors have the property that every cpu can address all of memory some multiprocessors have the additional property that every memory word can be read as fast as every other memory word these machines are called uma uniform memory access multiprocessors in contrast numa nonuniform memory access multiprocessors do not have this property why this difference exists will become clear later we will first examine uma multiprocessors and then move on to numa multiprocessors uma multiprocessors with busbased architectures the simplest multiprocessors are based on a single bus as illustrated in fig a two or more cpus and one or more memory modules all use the same bus for communication when a cpu wants to read a memory word it first checks to see if the bus is busy if the bus is idle the cpu puts the address of the word it wants on the bus asserts a few control signals and waits until the memory puts the desired word on the bus if the bus is busy when a cpu wants to read or write memory the cpu just waits until the bus becomes idle herein lies the problem with this design with two or three cpus contention for the bus will be manageable with or it will be unbearable the system will be totally limited by the bandwidth of the bus and most of the cpus will be idle most of the time sec multicomputers cpu a a a a a a b b b c c c d d d d d e time e e e e e e slot a a a a a a b b b c c c d d d d d e e e e e e e figure gang scheduling distributed systems having now completed our study of multicores multiprocessors and multicomputers we are now ready to turn to the last type of multiple processor system the distributed system these systems are similar to multicomputers in that sec research on multiple processor systems few topics in operating systems research are as popular as multicores multiprocessors and distributed systems besides the direct problems of mapping operating system functionality on a system consisting of multiple processing cores there are many open research problems related to synchronization and consistency and the way to make such systems faster and more reliable some research efforts have aimed at designing new operating systems from scratch specifically for multicore hardware for instance the corey operating system addresses the performance problems caused by data structure sharing across multiple cores boydwickizer et al by carefully arranging kernel data structures in such a way that no sharing is needed many of the performance bottlenecks disappear similarly barrelfish baumann et al is a new operating system motivated by the rapid growth in the number of cores on the one hand and the growth in hardware diversity on the other it models the operating system after distributed systems with message passing instead of shared memory as the communication model other operating systems aim at scalability and performance fos wentzlaff et al is an operating system that was designed to scale from the small multicore cpus to the very large clouds meanwhile newtos hruby et al and hruby et al is a new multiserver operating system that aims for both dependability with a modular design and many isolated components based originally on minix and performance which has traditionally been the weak point of such modular multiserver systems multicore is not just for new designs in boydwickizer et al the researchers study and remove the bottlenecks they encounter when scaling linux to a core machine they show that such systems if designed carefully can be made to scale quite well clements et al investigate the fundamental principle that govern whether or not an api can be implemented in a scalable fashion they show that whenever interface operations commute a scalable implementation of that interface exists with this knowledge operating system designers can build more scalable operating systems summary computer systems can be made faster and more reliable by using multiple cpus four organizations for multicpu systems are multiprocessors multicomputers virtual machines and distributed systems each of these has its own properties and issues a multiprocessor consists of two or more cpus that share a common ram often these cpus themselves consists of multiple cores the cores and cpus can be interconnected by a bus a crossbar switch or a multistage switching network various operating system configurations are possible including giving each cpu its own operating system having one master operating system with the rest being slaves or having a symmetric multiprocessor in which there is one copy of the operating system that any cpu can run in the latter case locks are needed to provide synchronization when a lock is not available a cpu can spin or do a context switch various scheduling algorithms are possible including time sharing space sharing and gang scheduling multicomputers also have two or more cpus but these cpus each have their own private memory they do not share any common ram so all communication uses message passing in some cases the network interface board has its own cpu in which case the communication between the main cpu and the interfaceboard cpu has to be carefully organized to avoid race conditions userlevel communication on multicomputers often uses remote procedure calls but distributed shared memory can also be used load balancing of processes is an issue here and the various algorithms used for it include senderinitiated algorithms receiverinitiated algorithms and bidding algorithms distributed systems are loosely coupled systems each of whose nodes is a complete computer with a complete set of peripherals and its own operating system often these systems are spread over a large geographical area middleware is sec security many companies possess valuable information they want to guard closely among many things this information can be technical eg a new chip design or software commercial eg studies of the competition or marketing plans financial eg plans for a stock offering or legal eg documents about a potential merger or takeover most of this information is stored on computers home computers increasingly have valuable data on them too many people keep their financial information including tax returns and credit card numbers on their computer love letters have gone digital and hard disks these days are full of important photos videos and movies as more and more of this information is stored in computer systems the need to protect it is becoming increasingly important guarding the information against unauthorized usage is therefore a major concern of all operating systems unfortunately it is also becoming increasingly difficult due to the widespread acceptance of system bloat and the accompanying bugs as a normal phenomenon in this chapter we will examine computer security as it applies to operating systems the issues relating to operating system security have changed radically in the past few decades up until the early s few people had a computer at home and most computing was done at companies universities and other organizations on multiuser computers ranging from large mainframes to minicomputers nearly all of these machines were isolated not connected to any networks as a consequence security was almost entirely focused on how to keep the users out of each the security environment phrased differently while camille may think she is the only user on the computer she really is not alone at all attackers may launch exploits manually or automatically by means of a virus or a worm the difference between a virus and worm is not always very clear most people agree that a virus needs at least some user interaction to propagate for instance the user should click on an attachment to get infected worms on the other hand are self propelled they will propagate regardless of what the user does it is also possible that a user willingly installs the attackers code herself for instance the attacker may repackage popular but expensive software like a game or a word processor and offer it for free on the internet for many users free is irresistible however installing the free game automatically also installs additional functionality the kind that hands over the pc and everything in it to a cybercriminal far away such software is known as a trojan horse a subject we will discuss shortly to cover all the bases this chapter has two main parts it starts by looking at the security landscape in detail we will look at threats and attackers sec the nature of security and attacks sec different approaches to provide access control sec and security models sec in addition we will look at cryptography as a core approach to help provide security sec and different ways to perform authentication sec so far so good then reality kicks in the next four major sections are practical security problems that occur in daily life we will talk about the tricks that attackers use to take control over a computer system as well as counter measures to prevent this from happening we will also discuss insider attacks and various kinds of digital pests we conclude the chapter with a short discussion of ongoing research on computer security and finally a short summary also worth noting is that while this is a book on operating systems operating systems security and network security are so intertwined that it is really impossible to separate them for example viruses come in over the network but affect the operating system on the whole we have tended to err on the side of caution and included some material that is germane to the subject but not strictly an operating systems issue operating systems security there are many ways to compromise the security of a computer system often they are not sophisticated at all for instance many people set their pin codes to or their password to password easy to remember but not very secure there are also people who do the opposite they pick very complicated passwords so that they can not remember them and have to write them down on a postit note which they attach to their screen or keyboard this way anyone with physical access to the machine including the cleaning staff secretary and all visitors also has access to everything on the machine there are many other examples and they include highranking officials losing usb sticks with sensitive information old hard drives with trade secrets that are not properly wiped before being dropped in the recycling bin and so on nevertheless some of the most important security incidents are due to sophisticated cyber attacks in this book we are specifically interested in attacks that are related to the operating system in other words we will not look at web attacks or attacks on sql databases instead we focus on attacks where the operating system is either the target of the attack or plays an important role in enforcing or more commonly failing to enforce the security policies security chap in general we distinguish between attacks that passively try to steal information and attacks that actively try to make a computer program misbehave an example of a passive attack is an adversary that sniffs the network traffic and tries to break the encryption if any to get to the data in an active attack the intruder may take control of a users web browser to make it execute malicious code for instance to steal credit card details in the same vein we distinguish between cryptography which is all about shuffling a message or file in such a way that it becomes hard to recover the original data unless you have the key and software hardening which adds protection mechanisms to programs to make it hard for attackers to make them misbehave the operating system uses cryptography in many places to transmit data securely over the network to store files securely on disk to scramble the passwords in a password file etc program hardening is also used all over the place to prevent attackers from injecting new code into running software to make sure that each process has exactly those privileges it needs to do what it is supposed to do and no more etc can we build secure systems nowadays it is hard to open a newspaper without reading yet another story about attackers breaking into computer systems stealing information or controlling millions of computers a naive person might logically ask two questions concerning this state of affairs is it possible to build a secure computer system if so why is it not done the answer to the first one is in theory yes in principle software can be free of bugs and we can even verify that it is secure as long as that software is not too large or complicated unfortunately computer systems today are horrendously complicated and this has a lot to do with the second question the second question why secure systems are not being built comes down to two fundamental reasons first current systems are not secure but users are unwilling to throw them out if microsoft were to announce that in addition to windows it had a new product secureos that was resistant to viruses but did not run windows applications it is far from certain that every person and company would drop windows like a hot potato and buy the new system immediately in fact microsoft has a secure os fandrich et al but is not marketing it the second issue is more subtle the only known way to build a secure system is to keep it simple features are the enemy of security the good folks in the marketing dept at most tech companies believe rightly or wrongly that what users want is more features bigger features and better features they make sure that the system architects designing their products get the word however all these mean more complexity more code more bugs and more security errors sec controlling access to resources security is easier to achieve if there is a clear model of what is to be protected and who is allowed to do what quite a bit of work has been done in this area so we can only scratch the surface in this brief treatment we will focus on a few general models and the mechanisms for enforcing them sec formal models of secure systems protection matrices such as that of fig are not static they frequently change as new objects are created old objects are destroyed and owners decide to increase or restrict the set of users for their objects a considerable amount of attention has been paid to modeling protection systems in which the protection matrix is constantly changing we will now touch briefly upon some of this work decades ago harrison et al identified six primitive operations on the protection matrix that can be used as a base to model any protection system these primitive operations are create object delete object create domain delete domain insert right and remove right the two latter primitives insert and remove rights from specific matrix elements such as granting domain permission to read file these six primitives can be combined into protection commands it is these protection commands that user programs can execute to change the matrix they may not execute the primitives directly for example the system might have a command to create a new file which would test to see if the file already existed and if not create a new object and give the owner all rights to it there might also be a command to allow the owner to grant permission to read the file to everyone in the system in effect inserting the read right in the new files entry in every domain at any instant the matrix determines what a process in any domain can do not what it is authorized to do the matrix is what is enforced by the system authorization has to do with management policy as an example of this distinction let us consider the simple system of fig in which domains correspond to users in fig a we see the intended protection policy henry can read and write mailbox robert can read and write secret and all three users can read and execute compiler now imagine that robert is very clever and has found a way to issue commands to have the matrix changed to fig b he has now gained access to mailbox something he is not authorized to have if he tries to read it the operating system will carry out his request because it does not know that the state of fig b is unauthorized security chap objects objects compiler mailbox secret compiler mailbox secret eric read eric read execute execute henry read read henry read read execute write execute write robert read read robert read read read execute write execute write a b figure a an authorized state b an unauthorized state it should now be clear that the set of all possible matrices can be partitioned into two disjoint sets the set of all authorized states and the set of all unauthorized states a question around which much theoretical research has revolved is this given an initial authorized state and a set of commands can it be proven that the system can never reach an unauthorized state in effect we are asking if the available mechanism the protection commands is adequate to enforce some protection policy given this policy some initial state of the matrix and the set of commands for modifying the matrix what we would like is a way to prove that the system is secure such a proof turns out quite difficult to acquire many generalpurpose systems are not theoretically secure harrison et al proved that in the case of an arbitrary configuration for an arbitrary protection system security is theoretically undecidable however for a specific system it may be possible to prove whether the system can ever move from an authorized state to an unauthorized state for more information see landwehr multilevel security most operating systems allow individual users to determine who may read and write their files and other objects this policy is called discretionary access control in many environments this model works fine but there are other environments where much tighter security is required such as the military corporate patent departments and hospitals in the latter environments the organization has stated rules about who can see what and these may not be modified by individual soldiers lawyers or doctors at least not without getting special permission from the boss and probably from the boss lawyers as well these environments need mandatory access controls to ensure that the stated security policies are enforced by the system in addition to the standard discretionary access controls what these mandatory access controls do is regulate the flow of information to make sure that it does not leak out in a way it is not supposed to sec basics of cryptography cryptography plays an important role in security many people are familiar with newspaper cryptograms which are little puzzles in which each letter has been systematically replaced by a different one these have as much to do with modern cryptography as hot dogs have to do with haute cuisine in this section we will give a birdseye view of cryptography in the computer era as mentioned earlier operating systems use cryptography in many places for instance some file systems can encrypt all the data on disk protocols like ipsec may encrypt andor sign all security chap network packets and most operating systems scramble passwords to prevent attackers from recovering them moreover in sec we will discuss the role of encryption in another important aspect of security authentication we will look at the basic primitives used by these systems however a serious discussion of cryptography is beyond the scope of this book many excellent books on computer security discuss the topic at length the interested reader is referred to these eg kaufman et al and gollman below we will give a very quick discussion of cryptography for readers not familiar with it at all the purpose of cryptography is to take a message or file called the plaintext and encrypt it into ciphertext in such a way that only authorized people know how to convert it back to plaintext for all others the ciphertext is just an incomprehensible pile of bits strange as it may sound to beginners in the area the encryption and decryption algorithms functions should always be public trying to keep them secret almost never works and gives the people trying to keep the secrets a false sense of security in the trade this tactic is called security by obscurity and is employed only by security amateurs oddly enough the category of amateurs also includes many huge multinational corporations that really should know better instead the secrecy depends on parameters to the algorithms called keys if p is the plaintext file ke is the encryption key c is the ciphertext and e is the encryption algorithm ie function then c ep ke this is the definition of encryption it says that the ciphertext is obtained by using the known encryption algorithm e with the plaintext p and the secret encryption key ke as parameters the idea that the algorithms should all be public and the secrecy should reside exclusively in the keys is called kerckhoffs principle formulated by the th century dutch cryptographer auguste kerckoffs all serious cryptographers subscribe to this idea similarly p dc kd where d is the decryption algorithm and kd is the decryption key this says that to get the plaintext p back from the ciphertext c and the decryption key kd one runs the algorithm d with c and kd as parameters the relation between the various pieces is shown in fig secretkey cryptography to make this clearer consider an encryption algorithm in which each letter is replaced by a different letter for example all as are replaced by qs all bs are replaced by w s all cs are replaced by es and so on like this plaintext a b c d e f gh i j k lmno p q r s t uvwxy z ciphertext qwe r t yu i o pa s d f gh j k l z x c v b nm this general system is called a monoalphabetic substitution with the key being the letter string corresponding to the full alphabet the encryption key in this example is qwertyuiopasdfghjklzxcvbnm for the key given above the sec authentication every secured computer system must require all users to be authenticated at login time after all if the operating system can not be sure who the user is it can not know which files and other resources he can access while authentication may sound like a trivial topic it is a bit more complicated than you might expect read on user authentication is one of those things we meant by ontogeny recapitulates phylogeny in sec early mainframes such as the eniac did not have an operating system let alone a login procedure later mainframe batch and timesharing systems generally did have a login procedure for authenticating jobs and users early minicomputers eg pdp and pdp did not have a login procedure but with the spread of unix on the pdp minicomputer logging in was again needed early personal computers eg apple ii and the original ibm pc did not sec the operating system zoo operating systems have been around now for over half a century during this time quite a variety of them have been developed not all of them widely known in this section we will briefly touch upon nine of them we will come back to some of these different kinds of systems later in the book mainframe operating systems at the high end are the operating systems for mainframes those roomsized computers still found in major corporate data centers these computers differ from personal computers in terms of their io capacity a mainframe with disks and millions of gigabytes of data is not unusual a personal computer with these specifications would be the envy of its friends mainframes are also making something of a comeback as highend web servers servers for largescale electronic commerce sites and servers for businesstobusiness transactions the operating systems for mainframes are heavily oriented toward processing many jobs at once most of which need prodigious amounts of io they typically offer three kinds of services batch transaction processing and timesharing a batch system is one that processes routine jobs without any interactive user present claims processing in an insurance company or sales reporting for a chain of stores is typically done in batch mode transactionprocessing systems handle large numbers of small requests for example check processing at a bank or airline reservations each unit of work is small but the system must handle hundreds or thousands per second timesharing systems allow multiple remote users to run jobs on the computer at once such as querying a big database these functions are closely related mainframe operating systems often perform all of them an example mainframe operating system is os a descendant of os however mainframe operating systems are gradually being replaced by unix variants such as linux server operating systems one level down are the server operating systems they run on servers which are either very large personal computers workstations or even mainframes they serve multiple users at once over a network and allow the users to share hardware and software resources servers can provide print service file service or web introduction chap service internet providers run many server machines to support their customers and websites use servers to store the web pages and handle the incoming requests typical server operating systems are solaris freebsd linux and windows server x multiprocessor operating systems an increasingly common way to get majorleague computing power is to connect multiple cpus into a single system depending on precisely how they are connected and what is shared these systems are called parallel computers multicomputers or multiprocessors they need special operating systems but often these are variations on the server operating systems with special features for communication connectivity and consistency with the recent advent of multicore chips for personal computers even conventional desktop and notebook operating systems are starting to deal with at least smallscale multiprocessors and the number of cores is likely to grow over time luckily quite a bit is known about multiprocessor operating systems from years of previous research so using this knowledge in multicore systems should not be hard the hard part will be having applications make use of all this computing power many popular operating systems including windows and linux run on multiprocessors personal computer operating systems the next category is the personal computer operating system modern ones all support multiprogramming often with dozens of programs started up at boot time their job is to provide good support to a single user they are widely used for word processing spreadsheets games and internet access common examples are linux freebsd windows windows and apples os x personal computer operating systems are so widely known that probably little introduction is needed in fact many people are not even aware that other kinds exist handheld computer operating systems continuing on down to smaller and smaller systems we come to tablets smartphones and other handheld computers a handheld computer originally known as a pda personal digital assistant is a small computer that can be held in your hand during operation smartphones and tablets are the bestknown examples as we have already seen this market is currently dominated by googles android and apples ios but they have many competitors most of these devices boast multicore cpus gps cameras and other sensors copious amounts of memory and sophisticated operating systems moreover all of them have more thirdparty applications apps than you can shake a usb stick at sec exploiting software insider attacks a whole different category of attacks are what might be termed inside jobs these are executed by programmers and other employees of the company running the computer to be protected or making critical software these attacks differ from external attacks because the insiders have specialized knowledge and access that outsiders do not have below we will give a few examples all of them have occurred repeatedly in the past each one has a different flavor in terms of who is doing the attacking who is being attacked and what the attacker is trying to achieve logic bombs in these times of massive outsourcing programmers often worry about their jobs sometimes they even take steps to make their potential involuntary departure less painful for those who are inclined toward blackmail one strategy is to write a logic bomb this device is a piece of code written by one of a companys currently employed programmers and secretly inserted into the production system as long as the programmer feeds it its daily password it is happy and does nothing however if the programmer is suddenly fired and physically removed security chap from the premises without warning the next day or next week the logic bomb does not get fed its daily password so it goes off many variants on this theme are also possible in one famous case the logic bomb checked the payroll if the personnel number of the programmer did not appear in it for two consecutive payroll periods it went off spafford et al going off might involve clearing the disk erasing files at random carefully making hardtodetect changes to key programs or encrypting essential files in the latter case the company has a tough choice about whether to call the police which may or may not result in a conviction many months later but certainly does not restore the missing files or to give in to the blackmail and rehire the exprogrammer as a consultant for an astronomical sum to fix the problem and hope that he does not plant new logic bombs while doing so there have been recorded cases in which a virus planted a logic bomb on the computers it infected generally these were programmed to go off all at once at some date and time in the future however since the programmer has no idea in advance of which computers will be hit logic bombs can not be used for job protection or blackmail often they are set to go off on a date that has some political significance sometimes these are called time bombs back doors another security hole caused by an insider is the back door this problem is created by code inserted into the system by a system programmer to bypass some normal check for example a programmer could add code to the login program to allow anyone to log in using the login name zzzzz no matter what was in the password file the normal code in the login program might look something like fig a the back door would be the change to fig b while true while true printflogin printflogin get stringname get stringname disable echoing disable echoing printfpassword printfpassword get stringpassword get stringpassword enable echoing enable echoing v check validityname password v check validityname password if v break if v strcmpname zzzzz break execute shellname execute shellname a b figure a normal code b code with a back door inserted what the call to strcmp does is check if the login name is zzzzz if so the login succeeds no matter what password is typed if this backdoor code were sec operating system concepts most operating systems provide certain basic concepts and abstractions such as processes address spaces and files that are central to understanding them in the following sections we will look at some of these basic concepts ever so briefly as sec malware in ancient times say before bored but clever teenagers would sometimes fill their idle hours by writing malicious software that they would then release into the world for the heck of it this software which included trojan horses viruses and worms and collectively called malware often quickly spread around the world as reports were published about how many millions of dollars of damage the malware caused and how many people lost their valuable data as a result the authors would be very impressed with their programming skills to them it was just a fun prank they were not making any money off it after all those days are gone malware is now written on demand by wellorganized criminals who prefer not to see their work publicized in the newspapers they are in it entirely for the money a large fraction of all malware is now designed to spread over the internet and infect victim machines in an extremely stealthy manner when a machine is infected software is installed that reports the address of the captured machine back to certain machines a backdoor is also installed on the machine that allows the criminals who sent out the malware to easily command the machine to do what it is instructed to do a machine taken over in this fashion is called a zombie and a collection of them is called a botnet a contraction of robot network a criminal who controls a botnet can rent it out for various nefarious and always commercial purposes a common one is for sending out commercial spam if a major spam attack occurs and the police try to track down the origin all they see is that it is coming from thousands of machines all over the world if they approach some of the owners of these machines they will discover kids small business owners housewives grandmothers and many other people all of whom vigorously deny that they are mass spammers using other peoples machines to do the dirty work makes it hard to track down the criminals behind the operation once installed malware can also be used for other criminal purposes blackmail is a possibility imagine a piece of malware that encrypts all the files on the victims hard disk then displays the following message sec defenses with problems lurking everywhere is there any hope of making systems secure actually there is and in the following sections we will look at some of the ways systems can be designed and implemented to increase their security one of the most important concepts is defense in depth basically the idea here is that you should have multiple layers of security so that if one of them is breached there are still others to overcome think about a house with a high spiky locked iron fence around it motion detectors in the yard two industrialstrength locks on the front door and a computerized burglar alarm system inside while each technique is valuable by itself to rob the house the burglar would have to defeat all of them sec research on security computer security is an extremely hot topic research is taking place in all areas cryptography attacks malware defenses compilers etc a moreorless continuous stream of highprofile security incidents ensures that research interest summary computers frequently contain valuable and confidential data including tax returns credit card numbers business plans trade secrets and much more the owners of these computers are usually quite keen on having them remain private and sec case study unix linux and android in the previous chapters we took a close look at many operating system principles abstractions algorithms and techniques in general now it is time to look at some concrete systems to see how these principles are applied in the real world we will begin with linux a popular variant of unix which runs on a wide variety of computers it is one of the dominant operating systems on highend workstations and servers but it is also used on systems ranging from smartphones android is based on linux to supercomputers our discussion will start with its history and evolution of unix and linux then we will provide an overview of linux to give an idea of how it is used this overview will be of special value to readers familiar only with windows since the latter hides virtually all the details of the system from its users although graphical interfaces may be easy for beginners they provide little flexibility and no insight into how the system works next we come to the heart of this chapter an examination of processes memory management io the file system and security in linux for each topic we will first discuss the fundamental concepts then the system calls and finally the implementation right off the bat we should address the question why linux linux is a variant of unix but there are many other versions and variants of unix including aix freebsd hpux sco unix system v solaris and others fortunately the fundamental principles and system calls are pretty much the same for all of them by design furthermore the general implementation strategies algorithms history of unix and linux unix and linux have a long and interesting history so we will begin our study there what started out as the pet project of one young researcher ken thompson has become a billiondollar industry involving universities multinational corporations governments and international standardization bodies in the following pages we will tell how this story has unfolded unics way back in the s and s all computers were personal computers in the sense that the thennormal way to use a computer was to sign up for an hour of time and take over the entire machine for that period of course these machines were physically immense but only one person the programmer could use them at any given time when batch systems took over in the s the programmer submitted a job on punched cards by bringing it to the machine room when enough jobs had been assembled the operator read them all in as a single batch it usually took an hour or more after submitting a job until the output was returned under these circumstances debugging was a timeconsuming process because a single misplaced comma might result in wasting several hours of the programmers time to get around what everyone viewed as an unsatisfactory unproductive and frustrating arrangement timesharing was invented at dartmouth college and mit the dartmouth system ran only basic and enjoyed a shortterm commercial success before vanishing the mit system ctss was general purpose and was a big success in the scientific community within a short time researchers at mit joined forces with bell labs and general electric then a computer vendor and began designing a secondgeneration system multics multiplexed information and computing service as we discussed in chap although bell labs was one of the founding partners in the multics project it later pulled out which left one of the bell labs researchers ken thompson looking around for something interesting to do he eventually decided to write a strippeddown multics all by himself in assembly language this time on an old sec overview of linux processes in linux in the previous sections we started out by looking at linux as viewed from the keyboard that is what the user sees in an xterm window we gave examples of shell commands and utility programs that are frequently used we ended with a brief overview of the system structure now it is time to dig deeply into the kernel and look more closely at the basic concepts linux supports namely processes memory the file system and inputoutput these notions are important because the system calls the interface to the operating system itself manipulate them for example system calls exist to create processes and threads allocate memory open files and do io unfortunately with so many versions of linux in existence there are some differences between them in this chapter we will emphasize the features common to all of them rather than focus on any one specific version thus in certain sections especially implementation sections the discussion may not apply equally to every version fundamental concepts the main active entities in a linux system are the processes linux processes are very similar to the classical sequential processes that we studied in chap each process runs a single program and initially has a single thread of control in other words it has one program counter which keeps track of the next instruction to be executed linux allows a process to create additional threads once it starts case study unix linux and android chap linux is a multiprogramming system so multiple independent processes may be running at the same time furthermore each user may have several active processes at once so on a large system there may be hundreds or even thousands of processes running in fact on most singleuser workstations even when the user is absent dozens of background processes called daemons are running these are started by a shell script when the system is booted daemon is a variant spelling of demon which is a selfemployed evil spirit a typical daemon is the cron daemon it wakes up once a minute to check if there is any work for it to do if so it does the work then it goes back to sleep until it is time for the next check this daemon is needed because it is possible in linux to schedule activities minutes hours days or even months in the future for example suppose a user has a dentist appointment at oclock next tuesday he can make an entry in the cron daemons database telling the daemon to beep at him at say when the appointed day and time arrives the cron daemon sees that it has work to do and starts up the beeping program as a new process the cron daemon is also used to start up periodic activities such as making daily disk backups at am or reminding forgetful users every year on october to stock up on trickortreat goodies for halloween other daemons handle incoming and outgoing electronic mail manage the line printer queue check if there are enough free pages in memory and so forth daemons are straightforward to implement in linux because each one is a separate process independent of all other processes processes are created in linux in an especially simple manner the fork system call creates an exact copy of the original process the forking process is called the parent process the new process is called the child process the parent and child each have their own private memory images if the parent subsequently changes any of its variables the changes are not visible to the child and vice versa open files are shared between parent and child that is if a certain file was open in the parent before the fork it will continue to be open in both the parent and the child afterward changes made to the file by either one will be visible to the other this behavior is only reasonable because these changes are also visible to any unrelated process that opens the file the fact that the memory images variables registers and everything else are identical in the parent and child leads to a small difficulty how do the processes know which one should run the parent code and which one should run the child code the secret is that the fork system call returns a to the child and a nonzero value the childs pid process identifier to the parent both processes normally check the return value and act accordingly as shown in fig processes are named by their pids when a process is created the parent is given the childs pid as mentioned above if the child wants to know its own pid there is a system call getpid that provides it pids are used in a variety of ways for example when a child terminates the parent is given the pid of the child that sec memory management in linux the linux memory model is straightforward to make programs portable and to make it possible to implement linux on machines with widely differing memory management units ranging from essentially nothing eg the original ibm pc to sophisticated paging hardware this is an area of the design that has barely changed in decades it has worked well so it has not needed much revision we will now examine the model and how it is implemented case study unix linux and android chap fundamental concepts every linux process has an address space that logically consists of three segments text data and stack an example process address space is illustrated in fig a as process a the text segment contains the machine instructions that form the programs executable code it is produced by the compiler and assembler by translating the c c or other program into machine code the text segment is normally readonly selfmodifying programs went out of style in about because they were too difficult to understand and debug thus the text segment neither grows nor shrinks nor changes in any other way process a physical memory process b stack pointer stack pointer unused memory bss k k bss k data data k text os text a b c k figure a process as virtual address space b physical memory c process bs virtual address space the data segment contains storage for all the programs variables strings arrays and other data it has two parts the initialized data and the uninitialized data for historical reasons the latter is known as the bss historically called block started by symbol the initialized part of the data segment contains variables and compiler constants that need an initial value when the program is started all the variables in the bss part are initialized to zero after loading for example in c it is possible to declare a character string and initialize it at the same time when the program starts up it expects that the string has its initial value to implement this construction the compiler assigns the string a location in the address space and ensures that when the program is started up this location contains the proper string from the operating systems point of view initialized data are not all that different from program text both contain bit patterns produced by the compiler that must be loaded into memory when the program starts the existence of uninitialized data is actually just an optimization when a global variable is not explicitly initialized the semantics of the c language say that its sec inputoutput in linux the io system in linux is fairly straightforward and the same as in other unices basically all io devices are made to look like files and are accessed as such with the same read and write system calls that are used to access all ordinary files in some cases device parameters must be set and this is done using a special system call we will study these issues in the following sections fundamental concepts like all computers those running linux have io devices such as disks printers and networks connected to them some way is needed to allow programs to access these devices although various solutions are possible the linux one is to integrate the devices into the file system as what are called special files each io case study unix linux and android chap device is assigned a path name usually in dev for example a disk might be devhd a printer might be devlp and the network might be devnet these special files can be accessed the same way as any other files no special commands or system calls are needed the usual open read and write system calls will do just fine for example the command cp file devlp copies the file to printer causing it to be printed assuming that the user has permission to access devlp programs can open read and write special files exactly the same way as they do regular files in fact cp in the above example is not even aware that it is printing in this way no special mechanism is needed for doing io special files are divided into two categories block and character a block special file is one consisting of a sequence of numbered blocks the key property of the block special file is that each block can be individually addressed and accessed in other words a program can open a block special file and read say block without first having to read blocks to block special files are typically used for disks character special files are normally used for devices that input or output a character stream keyboards printers networks mice plotters and most other io devices that accept or produce data for people use character special files it is not possible or even meaningful to seek to block on a mouse associated with each special file is a device driver that handles the corresponding device each driver has what is called a major device number that serves to identify it if a driver supports multiple devices say two disks of the same type each disk has a minor device number that identifies it together the major and minor device numbers uniquely specify every io device in few cases a single driver handles two closely related devices for example the driver corresponding to devtty controls both the keyboard and the screen often thought of as a single device the terminal although most character special files can not be randomly accessed they often need to be controlled in ways that block special files do not consider for example input typed on the keyboard and displayed on the screen when a user makes a typing error and wants to erase the last character typed he presses some key some people prefer to use backspace and others prefer del similarly to erase the entire line just typed many conventions abound traditionally was used but with the spread of email which uses within email address many systems have adopted ctrlu or some other character likewise to interrupt the running program some special key must be hit here too different people have different preferences ctrlc is a common choice but it is not universal rather than making a choice and forcing everyone to use it linux allows all these special functions and many others to be customized by the user a special system call is generally provided for setting these options this system call also sec the linux file system the most visible part of any operating system including linux is the file system in the following sections we will examine the basic ideas behind the linux file system the system calls and how the file system is implemented some of these ideas derive from multics and many of them have been copied by msdos windows and other systems but others are unique to unixbased systems the linux design is especially interesting because it clearly illustrates the principle of small is beautiful with minimal mechanism and a very limited number of system calls linux nevertheless provides a powerful and elegant file system fundamental concepts the initial linux file system was the minix file system however because it limited file names to characters in order to be compatible with unix version and its maximum file size was mb which was overkill on the mb hard case study unix linux and android chap disks of its era there was interest in better file systems almost from the beginning of the linux development which began about years after minix was released the first improvement was the ext file system which allowed file names of characters and files of gb but it was slower than the minix file system so the search continued for a while eventually the ext file system was invented with long file names long files and better performance and it has become the main file system however linux supports several dozen file systems using the virtual file system vfs layer described in the next section when linux is linked a choice is offered of which file systems should be built into the kernel others can be dynamically loaded as modules during execution if need be a linux file is a sequence of or more bytes containing arbitrary information no distinction is made between ascii files binary files or any other kinds of files the meaning of the bits in a file is entirely up to the files owner the system does not care file names are limited to characters and all the ascii characters except nul are allowed in file names so a file name consisting of three carriage returns is a legal file name but not an especially convenient one by convention many programs expect file names to consist of a base name and an extension separated by a dot which counts as a character thus progc is typically a c program progpy is typically a python program and progo is usually an object file compiler output these conventions are not enforced by the operating system but some compilers and other programs expect them extensions may be of any length and files may have multiple extensions as in progjavagz which is probably a gzip compressed java program files can be grouped together in directories for convenience directories are stored as files and to a large extent can be treated like files directories can contain subdirectories leading to a hierarchical file system the root directory is called and always contains several subdirectories the character is also used to separate directory names so that the name usrastx denotes the file x located in the directory ast which itself is in the usr directory some of the major directories near the top of the tree are shown in fig directory contents bin binary executable programs dev special files for io devices etc miscellaneous system files lib libraries usr user directories figure some important directories found in most linux systems there are two ways to specify file names in linux both to the shell and when opening a file from inside a program the first way is by means of an absolute path which means telling how to get to the file starting at the root directory an sec system calls we have seen that operating systems have two main functions providing abstractions to user programs and managing the computers resources for the most part the interaction between user programs and the operating system deals with the former for example creating writing reading and deleting files the resourcemanagement part is largely transparent to the users and done automatically thus the interface between user programs and the operating system is primarily about dealing with the abstractions to really understand what operating systems do we must examine this interface closely the system calls available in the interface vary from one operating system to another although the underlying concepts tend to be similar we are thus forced to make a choice between vague generalities operating systems have system calls for reading files and some specific system unix has a read system call with three parameters one to specify the file one to tell where the data are to be put and one to tell how many bytes to read we have chosen the latter approach its more work that way but it gives more insight into what operating systems really do although this discussion specifically refers to posix international standard hence also to unix system v bsd linux minix and so on most other modern operating systems have system calls that perform the same functions even if the details differ since the actual sec security in linux linux as a clone of minix and unix has been a multiuser system almost from the beginning this history means that security and control of information was built in very early on in the following sections we will look at some of the security aspects of linux fundamental concepts the user community for a linux system consists of some number of registered users each of whom has a unique uid user id a uid is an integer between and files but also processes and other resources are marked with the sec android from being stored in unencrypted form anywhere in the system if the password is correct the login program looks in etcpasswd to see the name of the users preferred shell possibly bash but possibly some other shell such as csh or ksh the login program then uses setuid and setgid to give itself the users uid and gid remember it started out as setuid root then it opens the keyboard for standard input file descriptor the screen for standard output file descriptor and the screen for standard error file descriptor finally it executes the preferred shell thus terminating itself at this point the preferred shell is running with the correct uid and gid and standard input output and error all set to their default devices all processes that it forks off ie commands typed by the user automatically inherit the shells uid and gid so they also will have the correct owner and group all files they create also get these values when any process attempts to open a file the system first checks the protection bits in the files inode against the callers effective uid and effective gid to see if the access is permitted if so the file is opened and a file descriptor returned if not the file is not opened and is returned no checks are made on subsequent read or write calls as a consequence if the protection mode changes after a file is already open the new mode will not affect processes that already have the file open the linux security model and its implementation are essentially the same as in most other traditional unix systems summary process state importance system core part of operating system system phone always running for telephony stack persistent email current foreground application foreground camera in use by email to load attachment foreground music running background service playing music perceptible media in use by music app for accessing users music perceptible download downloading a file for the user service launcher app launcher not current in use home maps previously used mapping application cached figure typical state of process importance process state importance system core part of operating system system phone always running for telephony stack persistent email current foreground application foreground music running background service playing music perceptible media inuse by music app for accessing users music perceptible download downloading a file for the user service launcher app launcher not current in use home camera previously used by email cached maps previously used mapping application cached figure process state after email stops using camera history of windows through windows microsofts development of the windows operating system for pcbased computers as well as servers can be divided into four eras msdos msdosbased windows ntbased windows and modern windows technically each of these systems is substantially different from the others each was dominant during different decades in the history of the personal computer figure shows the dates of the major microsoft operating system releases for desktop computers below we will briefly sketch each of the eras shown in the table case study windows chap year msdos msdos ntbased modern notes based windows windows windows initial release for ibm pc support for pcxt support for pcat ten million copies in years added memory management ran only on and later nt msdos embedded in win nt me win me was inferior to win xp replaced win vista vista could not supplant xp significantly improved upon vista first modern version microsoft moved to rapid releases figure major releases in the history of microsoft operating systems for desktop pcs s msdos in the early s ibm at the time the biggest and most powerful computer company in the world was developing a personal computer based the intel microprocessor since the mids microsoft had become the leading provider of the basic programming language for bit microcomputers based on the and z when ibm approached microsoft about licensing basic for the new ibm pc microsoft readily agreed and suggested that ibm contact digital research to license its cpm operating system since microsoft was not then in the operating system business ibm did that but the president of digital research gary kildall was too busy to meet with ibm this was probably the worst blunder in all of business history since had he licensed cpm to ibm kildall would probably have become the richest man on the planet rebuffed by kildall ibm came back to bill gates the cofounder of microsoft and asked for help again within a short time microsoft bought a cpm clone from a local company seattle computer products ported it to the ibm pc and licensed it to ibm it was then renamed msdos microsoft disk operating system and shipped with the first ibm pc in sec programming windows it is now time to start our technical study of windows before getting into the details of the internal structure however we will take a look at the native nt api for system calls the win programming subsystem introduced as part of ntbased windows and the modern winrt programming environment introduced with windows figure shows the layers of the windows operating system beneath the applet and gui layers of windows are the programming interfaces that applications build on as in most operating systems these consist largely of code libraries dlls to which programs dynamically link for access to operating system features windows also includes a number of programming interfaces which are implemented as services that run as separate processes applications communicate with usermode services through rpcs remoteprocedurecalls the core of the nt operating system is the ntos kernelmode program ntoskrnlexe which provides the traditional systemcall interfaces upon which the rest of the operating system is built in windows only programmers at microsoft write to the systemcall layer the published usermode interfaces all belong to operating system personalities that are implemented using subsystems that run on top of the ntos layers originally nt supported three personalities os posix and win os was discarded in windows xp support for posix was finally removed in windows today all windows applications are written using apis that are built on top of the win subsystem such as the winfx api in the net programming model the winfx api includes many of the features of win and in fact many sec system structure in the previous sections we examined windows as seen by the programmer writing code for user mode now we are going to look under the hood to see how the system is organized internally what the various components do and how they interact with each other and with user programs this is the part of the system seen by the programmer implementing lowlevel usermode code like subsystems and native services as well as the view of the system provided to devicedriver writers although there are many books on how to use windows there are many fewer on how it works inside one of the best places to look for additional information on this topic is microsoft windows internals th ed parts and russinovich and solomon operating system structure as described earlier the windows operating system consists of many layers as depicted in fig in the following sections we will dig into the lowest levels of the operating system those that run in kernel mode the central layer is the ntos kernel itself which is loaded from ntoskrnlexe when windows boots ntos itself consists of two layers the executive which containing most of the services and a smaller layer which is also called the kernel and implements the underlying thread scheduling and synchronization abstractions a kernel within the kernel as well as implementing trap handlers interrupts and other aspects of how the cpu is managed case study windows chap the division of ntos into kernel and executive is a reflection of nts vaxvms roots the vms operating system which was also designed by cutler had four hardwareenforced layers user supervisor executive and kernel corresponding to the four protection modes provided by the vax processor architecture the intel cpus also support four rings of protection but some of the early target processors for nt did not so the kernel and executive layers represent a softwareenforced abstraction and the functions that vms provides in supervisor mode such as printer spooling are provided by nt as usermode services the kernelmode layers of nt are shown in fig the kernel layer of ntos is shown above the executive layer because it implements the trap and interrupt mechanisms used to transition from user mode to kernel mode user mode system library kernel usermode dispatch routines ntdlldll kernel mode ntos trapexceptioninterrupt dispatch kernel layer cpu scheduling and synchronization threads isrs dpcs apcs drivers procs and threads virtual memory object manager config manager file systems volume manager lpc cache manager io manager security monitor tcpip stack net interfaces executive runtime library graphics devices all other devices ntos executive layer hardware abstraction layer hardware cpu mmu interrupt controllers memory physical devices bios figure windows kernelmode organization the uppermost layer in fig is the system library ntdlldll which actually runs in user mode the system library includes a number of support functions for the compiler runtime and lowlevel libraries similar to what is in libc in unix ntdlldll also contains special code entry points used by the kernel to initialize threads and dispatch exceptions and usermode apcs asynchronous procedure calls because the system library is so integral to the operation of the kernel every usermode process created by ntos has ntdll mapped at the same fixed address when ntos is initializing the system it creates a section object to use when mapping ntdll and it also records addresses of the ntdll entry points used by the kernel below the ntos kernel and executive layers is a layer of software called the hal hardware abstraction layer which abstracts lowlevel hardware details like access to device registers and dma operations and the way the parentboard sec operating system structure now that we have seen what operating systems look like on the outside ie the programmers interface it is time to take a look inside in the following sections we will examine six different structures that have been tried in order to get some idea of the spectrum of possibilities these are by no means exhaustive but they give an idea of some designs that have been tried in practice the six designs we will discuss here are monolithic systems layered systems microkernels clientserver systems virtual machines and exokernels sec processes and threads in windows windows has a number of concepts for managing the cpu and grouping resources together in the following sections we will examine these discussing some of the relevant win api calls and show how they are implemented fundamental concepts in windows processes are containers for programs they hold the virtual address space the handles that refer to kernelmode objects and threads in their role as a container for threads they hold common resources used for thread execution such as the pointer to the quota structure the shared token object and default parameters used to initialize threads including the priority and scheduling class each process has usermode system data called the peb process environment block the peb includes the list of loaded modules ie the exe and dlls the memory containing environment strings the current working directory and data for managing the process heaps as well as lots of specialcase win cruft that has been added over time threads are the kernels abstraction for scheduling the cpu in windows priorities are assigned to each thread based on the priority value in the containing process threads can also be affinitized to run only on certain processors this helps concurrent programs running on multicore chips or multiprocessors to explicitly spread out work each thread has two separate call stacks one for execution in user mode and one for kernel mode there is also a teb thread environment block that keeps usermode data specific to the thread including perthread storage thread local storage and fields for win language and cultural localization and other specialized fields that have been added by various facilities besides the pebs and tebs there is another data structure that kernel mode shares with each process namely user shared data this is a page that is writable by the kernel but readonly in every usermode process it contains a number of values maintained by the kernel such as various forms of time version information amount of physical memory and a large number of shared flags used by various usermode components such as com terminal services and the debuggers the use of this readonly shared page is purely a performance optimization as the values could also be obtained by a system call into kernel mode but system calls are much more expensive than a single memory access so for some systemmaintained fields such as the time this makes a lot of sense the other fields such as the current time zone change infrequently except on airborne computers sec memory management windows has an extremely sophisticated and complex virtual memory system it has a number of win functions for using it implemented by the memory manager the largest component of the ntos executive layer in the following sections we will look at the fundamental concepts the win api calls and finally the implementation case study windows chap fundamental concepts in windows every user process has its own virtual address space for x machines virtual addresses are bits long so each process has gb of virtual address space with the user and kernel each receiving gb for x machines both the user and kernel receive more virtual addresses than they can reasonably use in the foreseeable future for both x and x the virtual address space is demand paged with a fixed page size of kb though in some cases as we will see shortly mb large pages are also used by using a page directory only and bypassing the corresponding page table the virtual address space layouts for three x processes are shown in fig in simplified form the bottom and top kb of each process virtual address space is normally unmapped this choice was made intentionally to help catch programming errors and mitigate the exploitability of certain types of vulnerabilities process a process b process c gb nonpaged pool nonpaged pool nonpaged pool paged pool paged pool paged pool as page tables bs page tables cs page tables stacks data etc stacks data etc stacks data etc hal os hal os hal os gb system data system data system data process as process bs process cs private code private code private code and data and data and data bottom and top kb are invalid figure virtual address space layout for three user processes on the x the white areas are private per process the shaded areas are shared among all processes starting at kb comes the users private code and data this extends up to almost gb the upper gb contains the operating system including the code data and the paged and nonpaged pools the upper gb is the kernels virtual memory and is shared among all user processes except for virtual memory data like the page tables and workingset lists which are perprocess kernel virtual sec caching in windows the windows cache improves the performance of file systems by keeping recently and frequently used regions of files in memory rather than cache physical addressed blocks from the disk the cache manager manages virtually addressed blocks that is regions of files this approach fits well with the structure of the native nt file system ntfs as we will see in sec ntfs stores all of its data as files including the filesystem metadata the cached regions of files are called views because they represent regions of kernel virtual addresses that are mapped onto filesystem files thus the actual management of the physical memory in the cache is provided by the memory manager the role of the cache manager is to manage the use of kernel virtual addresses for views arrange with the memory manager to pin pages in physical memory and provide interfaces for the file systems inputoutput in windows the goals of the windows io manager are to provide a fundamentally extensive and flexible framework for efficiently handling a very wide variety of io devices and services support automatic device discovery and driver installation plug case study windows chap and play and power management for devices and the cpu all using a fundamentally asynchronous structure that allows computation to overlap with io transfers there are many hundreds of thousands of devices that work with windows for a large number of common devices it is not even necessary to install a driver because there is already a driver that shipped with the windows operating system but even so counting all the revisions there are almost a million distinct driver binaries that run on windows in the following sections we will examine some of the issues relating to io fundamental concepts the io manager is on intimate terms with the plugandplay manager the basic idea behind plug and play is that of an enumerable bus many buses including pc card pci pcie agp usb ieee eide scsi and sata have been designed so that the plugandplay manager can send a request to each slot and ask the device there to identify itself having discovered what is out there the plugandplay manager allocates hardware resources such as interrupt levels locates the appropriate drivers and loads them into memory as each driver is loaded a driver object is created for it and then for each device at least one device object is allocated for some buses such as scsi enumeration happens only at boot time but for other buses such as usb it can happen at any time requiring close cooperation between the plugandplay manager the bus drivers which actually do the enumerating and the io manager in windows all the file systems antivirus filters volume managers network protocol stacks and even kernel services that have no associated hardware are implemented using io drivers the system configuration must be set to cause some of these drivers to load because there is no associated device to enumerate on the bus others like the file systems are loaded by special code that detects they are needed such as the filesystem recognizer that looks at a raw volume and deciphers what type of file system format it contains an interesting feature of windows is its support for dynamic disks these disks may span multiple partitions and even multiple disks and may be reconfigured on the fly without even having to reboot in this way logical volumes are no longer constrained to a single partition or even a single disk so that a single file system may span multiple drives in a transparent way the io to volumes can be filtered by a special windows driver to produce volume shadow copies the filter driver creates a snapshot of the volume which can be separately mounted and represents a volume at a previous point in time it does this by keeping track of changes after the snapshot point this is very convenient for recovering files that were accidentally deleted or traveling back in time to see the state of a file at periodic snapshots made in the past but shadow copies are also valuable for making accurate backups of server systems the operating system works with server applications to have them reach sec the windows nt file system windows supports several file systems the most important of which are fat fat and ntfs nt file system fat is the old msdos file system it uses bit disk addresses which limits it to disk partitions no larger than gb mostly it is used to access floppy disks for those customers that still sec windows power management the power manager rides herd on power usage throughout the system historically management of power consumption consisted of shutting off the monitor display and stopping the disk drives from spinning but the issue is rapidly becoming more complicated due to requirements for extending how long notebooks can run on batteries and energyconservation concerns related to desktop computers being left on all the time and the high cost of supplying power to the huge server farms that exist today newer powermanagement facilities include reducing the power consumption of components when the system is not in use by switching individual devices to standby states or even powering them off completely using soft power switches multiprocessors shut down individual cpus when they are not needed and even the clock rates of the running cpus can be adjusted downward to reduce power consumption when a processor is idle its power consumption is also reduced since it needs to do nothing except wait for an interrupt to occur windows supports a special shut down mode called hibernation which copies all of physical memory to disk and then reduces power consumption to a small trickle notebooks can run weeks in a hibernated state with little battery drain because all the memory state is written to disk you can even replace the battery on a notebook while it is hibernated when the system resumes after hibernation it restores the saved memory state and reinitializes the io devices this brings the computer back into the same state it was before hibernation without having to login again and start up all the applications and services that were running windows optimizes this process by ignoring unmodified pages backed by disk already and compressing other memory pages to reduce the amount of io bandwidth required the hibernation algorithm automatically tunes itself to balance between io and processor throughput if there is more processor available it uses expensive but more effective compression to reduce the io bandwidth needed when io bandwidth is sufficient hibernation will skip the compression altogether with sec security in windows nt was originally designed to meet the us department of defenses c security requirements dod std the orange book which secure dod systems must meet this standard requires operating systems to have certain properties in order to be classified as secure enough for certain kinds of military work although windows was not specifically designed for c compliance it inherits many security properties from the original security design of nt including the following secure login with antispoofing measures discretionary access controls privileged access controls addressspace protection per process new pages must be zeroed before being mapped in security auditing let us review these items briefly secure login means that the system administrator can require all users to have a password in order to log in spoofing is when a malicious user writes a program that displays the login prompt or screen and then walks away from the computer in the hope that an innocent user will sit down and enter a name and password the name and password are then written to disk and the user is told that login has sec operatingsystem structure operatingsystem debugging we have mentioned debugging frequently in this chapter here we take a closer look broadly debugging is the activity of finding and fixing errors in a system both in hardware and in software performance problems are considered bugs so debugging can also include performance tuning which seeks to improve performance by removing processing bottlenecks in this section we explore debugging process and kernel errors and performance problems hardware debugging is outside the scope of this text failure analysis if a process fails most operating systems write the error information to a log file to alert system operators or users that the problem occurred the operating system can also take a core dump a capture of the memory of the process and store it in a file for later analysis memory was referred to as the core in the early days of computing running programs and core dumps can be probed by a debugger which allows a programmer to explore the code and memory of a process debugging userlevel process code is a challenge operatingsystem kernel debugging is even more complex because of the size and complexity of the kernel its control of the hardware and the lack of userlevel debugging tools a failure in the kernel is called a crash when a crash occurs error information is saved to a log file and the memory state is saved to a crash dump operatingsystem debugging and process debugging frequently use different tools and techniques due to the very different nature of these two tasks consider that a kernel failure in the filesystem code would make it risky for the kernel to try to save its state to a file on the file system before rebooting a common technique is to save the kernels memory state to a section of disk set aside for this purpose that contains no file system if the kernel detects an unrecoverable error it writes the entire contents of memory or at least the kernelowned parts of the system memory to the disk area when the system reboots a process runs to gather the data from that area and write it to a crash operatingsystem generation dtrace s schedd dtrace script schedd matched probes c gnomesettingsd gnomevfsdaemon dsdm wnckapplet gnomepanel clockapplet mappingdaemon xscreensaver metacity xorg gnometerminal mixer applet java figure output of the d code systems do not have conflicting license agreements for example dtrace has been added to mac os x and freebsd and will likely spread further due to its unique capabilities other operating systems especially the linux derivatives are adding kerneltracing functionality as well still other operating systems are beginning to include performance and tracing tools fostered by research at various institutions including the paradyn project system boot after an operating system is generated it must be made available for use by the hardware but how does the hardware know where the kernel is or how to load that kernel the procedure of starting a computer by loading the kernel is known as booting the system on most computer systems a small piece of code known as the bootstrap program or bootstrap loader locates the kernel loads it into main memory and starts its execution some computer systems such as pcs use a twostep process in which a simple bootstrap loader fetches a more complex boot program from disk which in turn loads the kernel when a cpu receives a reset event for instance when it is powered up or rebooted the instruction register is loaded with a predefined memory summary location and execution starts there at that location is the initial bootstrap program this program is in the form of readonly memory rom because the ram is in an unknown state at system startup rom is convenient because it needs no initialization and can not easily be infected by a computer virus the bootstrap program can perform a variety of tasks usually one task is to run diagnostics to determine the state of the machine if the diagnostics pass the program can continue with the booting steps it can also initialize all aspects of the system from cpu registers to device controllers and the contents of main memory sooner or later it starts the operating system some systems such as cellular phones tablets and game consoles store the entire operating system in rom storing the operating system in rom is suitable for small operating systems simple supporting hardware and rugged operation a problem with this approach is that changing the bootstrap code requires changing the rom hardware chips some systems resolve this problem by using erasable programmable readonly memory eprom which is readonly except when explicitly given a command to become writable all forms of rom are also known as firmware since their characteristics fall somewhere between those of hardware and those of software a problem with firmware in general is that executing code there is slower than executing code in ram some systems store the operating system in firmware and copy it to ram for fast execution a final issue with firmware is that it is relatively expensive so usually only small amounts are available for large operating systems including most generalpurpose operating systems like windows mac os x and unix or for systems that change frequently the bootstrap loader is stored in firmware and the operating system is on disk in this case the bootstrap runs diagnostics and has a bit of code that can read a single block at a fixed location say block zero from disk into memory and execute the code from that boot block the program stored in the boot block may be sophisticated enough to load the entire operating system into memory and begin its execution more typically it is simple code as it fits in a single disk block and knows only the address on disk and length of the remainder of the bootstrap program grub is an example of an opensource bootstrap program for linux systems all of the diskbound bootstrap and the operating system itself can be easily changed by writing new versions to disk a disk that has a boot partition more on that in section is called a boot disk or system disk now that the full bootstrap program has been loaded it can traverse the file system to find the operating system kernel load it into memory and start its execution it is only at this point that the system is said to be running process concept a question that arises in discussing operating systems involves what to call all the cpu activities a batch system executes jobs whereas a timeshared chapter processes system has user programs or tasks even on a singleuser system a user may be able to run several programs at one time a word processor a web browser and an email package and even if a user can execute only one program at a time such as on an embedded device that does not support multitasking the operating system may need to support its own internal programmed activities such as memory management in many respects all these activities are similar so we call all of them processes the terms job and process are used almost interchangeably in this text although we personally prefer the term process much of operatingsystem theory and terminology was developed during a time when the major activity of operating systems was job processing it would be misleading to avoid the use of commonly accepted terms that include the word job such as job scheduling simply because process has superseded job the process informally as mentioned earlier a process is a program in execution a process is more than the program code which is sometimes known as the text section it also includes the current activity as represented by the value of the program counter and the contents of the processors registers a process generally also includes the process stack which contains temporary data such as function parameters return addresses and local variables and a data section which contains global variables a process may also include a heap which is memory that is dynamically allocated during process run time the structure of a process in memory is shown in figure we emphasize that a program by itself is not a process a program is a passive entity such as a file containing a list of instructions stored on disk often called an executable file in contrast a process is an active entity with a program counter specifying the next instruction to execute and a set of associated resources a program becomes a process when an executable file is loaded into memory two common techniques for loading executable files max stack heap data text figure process in memory process scheduling the objective of multiprogramming is to have some process running at all times to maximize cpu utilization the objective of time sharing is to switch the cpu among processes so frequently that users can interact with each program operations on processes multitasking in mobile systems because of the constraints imposed on mobile devices early versions of ios did not provide userapplication multitasking only one application runs in the foreground and all other user applications are suspended operatingsystem tasks were multitasked because they were written by apple and well behaved however beginning with ios apple now provides a limited form of multitasking for user applications thus allowing a single foreground application to run concurrently with multiple background applications on a mobile device the foreground application is the application currently open and appearing on the display the background application remains in memory but does not occupy the display screen the ios programming api provides support for multitasking thus allowing a process to run in the background without being suspended however it is limited and only available for a limited number of application types including applications running a single finitelength task such as completing a download of content from a network receiving notifications of an event occurring such as a new email message with longrunning background tasks such as an audio player apple probably limits multitasking due to battery life and memory use concerns the cpu certainly has the features to support multitasking but apple chooses to not take advantage of some of them in order to better manage resource use android does not place such constraints on the types of applications that can run in the background if an application requires processing while in the background the application must use a service a separate application component that runs on behalf of the background process consider a streaming audio application if the application moves to the background the service continues to send audio files to the audio device driver on behalf of the background application in fact the service will continue to run even if the background application is suspended services do not have a user interface and have a small memory footprint thus providing an efficient technique for multitasking in a mobile environment interprocess communication processes executing concurrently in the operating system may be either independent processes or cooperating processes a process is independent if it can not affect or be affected by the other processes executing in the system any process that does not share data with any other process is independent a process is cooperating if it can affect or be affected by the other processes executing in the system clearly any process that shares data with other processes is a cooperating process there are several reasons for providing an environment that allows process cooperation information sharing since several users may be interested in the same piece of information for instance a shared file we must provide an environment to allow concurrent access to such information computation speedup if we want a particular task to run faster we must break it into subtasks each of which will be executing in parallel with the others notice that such a speedup can be achieved only if the computer has multiple processing cores modularity we may want to construct the system in a modular fashion dividing the system functions into separate processes or threads as we discussed in chapter convenience even an individual user may work on many tasks at the same time for instance a user may be editing listening to music and compiling in parallel cooperating processes require an interprocess communication ipc mechanism that will allow them to exchange data and information there are two fundamental models of interprocess communication shared memory and message passing in the sharedmemory model a region of memory that is shared by cooperating processes is established processes can then exchange information by reading and writing data to the shared region in the messagepassing model communication takes place by means of messages exchanged between the cooperating processes the two communications models are contrasted in figure both of the models just mentioned are common in operating systems and many systems implement both message passing is useful for exchanging smaller amounts of data because no conflicts need be avoided message passing is also easier to implement in a distributed system than shared memory although there are systems that provide distributed shared memory we do not consider them in this text shared memory can be faster than message passing since messagepassing systems are typically implemented using system calls examples of ipc systems in this section we explore three different ipc systems we first cover the posix api for shared memory and then discuss message passing in the mach operating system we conclude with windows which interestingly uses shared memory as a mechanism for providing certain types of message passing an example posix shared memory several ipc mechanisms are available for posix systems including shared memory and message passing here we explore the posix api for shared memory posix shared memory is organized using memorymapped files which associate the region of shared memory with a file a process must first create message next consumed while true receivenext consumed consume the item in next consumed figure the consumer process using message passing communication in clientserver systems chapter processes client server connection request connection handle port handle client communication port server handle communication port shared section object bytes figure advanced local procedure calls in windows communication in client server systems in section we described how processes can communicate using shared memory and message passing these techniques can be used for communication in clientserver systems section as well in this section we explore three other strategies for communication in clientserver systems sockets remote procedure calls rpcs and pipes sockets a socket is defined as an endpoint for communication a pair of processes communicating over a network employs a pair of sockets one for each process a socket is identified by an ip address concatenated with a port number in general sockets use a clientserver architecture the server waits for incoming client requests by listening to a specified port once a request is received the server accepts a connection from the client socket to complete the connection servers implementing specific services such as telnet ftp and http listen to wellknown ports a telnet server listens to port an ftp server listens to port and a web or http server listens to port all ports below are considered well known we can use them to implement standard services when a client process initiates a request for a connection it is assigned a port by its host computer this port has some arbitrary number greater than for example if a client on host x with ip address wishes to establish a connection with a web server which is listening on port at address host x may be assigned port the connection will consist of a pair of sockets on host x and on the web server this situation is illustrated in figure the packets traveling between the hosts are delivered to the appropriate process based on the destination port number all connections must be unique therefore if another process also on host x wished to establish another connection with the same web server it would be assigned a port number greater than and not equal to this ensures that all connections consist of a unique pair of sockets communication in client server systems host x socket web server socket figure communication using sockets although most program examples in this text use c we will illustrate sockets using java as it provides a much easier interface to sockets and has a rich library for networking utilities those interested in socket programming in c or c should consult the bibliographical notes at the end of the chapter java provides three different types of sockets connectionoriented tcp sockets are implemented with the socket class connectionless udp sockets use the datagramsocket class finally the multicastsocket class is a subclass of the datagramsocket class a multicast socket allows data to be sent to multiple recipients our example describes a date server that uses connectionoriented tcp sockets the operation allows clients to request the current date and time from the server the server listens to port although the port could have any arbitrary number greater than when a connection is received the server returns the date and time to the client the date server is shown in figure the server creates a serversocket that specifies that it will listen to port the server then begins listening to the port with the accept method the server blocks on the accept method waiting for a client to request a connection when a connection request is received accept returns a socket that the server can use to communicate with the client the details of how the server communicates with the socket are as follows the server first establishes a printwriter object that it will use to communicate with the client a printwriter object allows the server to write to the socket using the routine print and println methods for output the server process sends the date to the client calling the method println once it has written the date to the socket the server closes the socket to the client and resumes listening for more requests a client communicates with the server by creating a socket and connecting to the port on which the server is listening we implement such a client in the java program shown in figure the client creates a socket and requests a connection with the server at ip address on port once the connection is made the client can read from the socket using normal stream io statements after it has received the date from the server the client closes chapter processes import javanet import javaio public class dateserver public static void mainstring args try serversocket sock new serversocket now listen for connections while true socket client sockaccept printwriter pout new printwriterclientgetoutputstream true write the date to the socket poutprintlnnew javautildatetostring close the socket and resume listening for connections clientclose catch ioexception ioe systemerrprintlnioe figure date server the socket and exits the ip address is a special ip address known as the loopback when a computer refers to ip address it is referring to itself this mechanism allows a client and server on the same host to communicate using the tcpip protocol the ip address could be replaced with the ip address of another host running the date server in addition to an ip address an actual host name such as wwwwestminstercollegeedu can be used as well communication using sockets although common and efficient is considered a lowlevel form of communication between distributed processes one reason is that sockets allow only an unstructured stream of bytes to be exchanged between the communicating threads it is the responsibility of the client or server application to impose a structure on the data in the next two subsections we look at two higherlevel methods of communication remote procedure calls rpcs and pipes remote procedure calls one of the most common forms of remote service is the rpc paradigm which we discussed briefly in section the rpc was designed as a way to communication in client server systems import javanet import javaio public class dateclient public static void mainstring args try make connection to server socket socket sock new socket inputstream in sockgetinputstream bufferedreader bin new bufferedreadernew inputstreamreaderin read the date from the socket string line while line binreadline null systemoutprintlnline close the socket connection sockclose catch ioexception ioe systemerrprintlnioe figure date client abstract the procedurecall mechanism for use between systems with network connections it is similar in many respects to the ipc mechanism described in section and it is usually built on top of such a system here however because we are dealing with an environment in which the processes are executing on separate systems we must use a messagebased communication scheme to provide remote service in contrast to ipc messages the messages exchanged in rpc communication are well structured and are thus no longer just packets of data each message is addressed to an rpc daemon listening to a port on the remote system and each contains an identifier specifying the function to execute and the parameters to pass to that function the function is then executed as requested and any output is sent back to the requester in a separate message a port is simply a number included at the start of a message packet whereas a system normally has one network address it can have many ports within that address to differentiate the many network services it supports if a remote process needs a service it addresses a message to the proper port for instance if a system wished to allow other systems to be able to list its current users it would have a daemon supporting such an rpc attached to a port say port any remote system could obtain the needed information that chapter processes is the list of current users by sending an rpc message to port on the server the data would be received in a reply message the semantics of rpcs allows a client to invoke a procedure on a remote host as it would invoke a procedure locally the rpc system hides the details that allow communication to take place by providing a stub on the client side typically a separate stub exists for each separate remote procedure when the client invokes a remote procedure the rpc system calls the appropriate stub passing it the parameters provided to the remote procedure this stub locates the port on the server and marshals the parameters parameter marshalling involves packaging the parameters into a form that can be transmitted over a network the stub then transmits a message to the server using message passing a similar stub on the server side receives this message and invokes the procedure on the server if necessary return values are passed back to the client using the same technique on windows systems stub code is compiled from a specification written in the microsoft interface definition language midl which is used for defining the interfaces between client and server programs one issue that must be dealt with concerns differences in data representation on the client and server machines consider the representation of bit integers some systems known as bigendian store the most significant byte first while other systems known as littleendian store the least significant byte first neither order is better per se rather the choice is arbitrary within a computer architecture to resolve differences like this many rpc systems define a machineindependent representation of data one such representation is known as external data representation xdr on the client side parameter marshalling involves converting the machinedependent data into xdr before they are sent to the server on the server side the xdr data are unmarshalled and converted to the machinedependent representation for the server another important issue involves the semantics of a call whereas local procedure calls fail only under extreme circumstances rpcs can fail or be duplicated and executed more than once as a result of common network errors one way to address this problem is for the operating system to ensure that messages are acted on exactly once rather than at most once most local procedure calls have the exactly once functionality but it is more difficult to implement first consider at most once this semantic can be implemented by attaching a timestamp to each message the server must keep a history of all the timestamps of messages it has already processed or a history large enough to ensure that repeated messages are detected incoming messages that have a timestamp already in the history are ignored the client can then send a message one or more times and be assured that it only executes once for exactly once we need to remove the risk that the server will never receive the request to accomplish this the server must implement the at most once protocol described above but must also acknowledge to the client that the rpc call was received and executed these ack messages are common throughout networking the client must resend each rpc call periodically until it receives the ack for that call yet another important issue concerns the communication between a server and a client with standard procedure calls some form of binding takes place during link load or execution time chapter so that a procedure calls name communication in client server systems client messages server user calls kernel to send rpc message to procedure x kernel sends from client matchmaker message to to server receives matchmaker to port matchmaker message looks find port number re address up answer for rpc x from server kernel places to client matchmaker port p in user port kernel replies to client rpc message re rpc x with port p port p from client daemon kernel sends to server listening to rpc port port p port p receives contents message from rpc daemon kernel receives port p processes reply passes to client request and it to user port kernel processes send output output figure execution of a remote procedure call rpc is replaced by the memory address of the procedure call the rpc scheme requires a similar binding of the client and the server port but how does a client know the port numbers on the server neither system has full information about the other because they do not share memory two approaches are common first the binding information may be predetermined in the form of fixed port addresses at compile time an rpc call has a fixed port number associated with it once a program is compiled the server can not change the port number of the requested service second binding can be done dynamically by a rendezvous mechanism typically an operating system provides a rendezvous also called a matchmaker daemon on a fixed rpc port a client then sends a message containing the name of the rpc to the rendezvous daemon requesting the port address of the rpc it needs to execute the port number is returned and the rpc calls can be sent to that port until the process terminates or the server crashes this method requires the extra overhead of the initial request but is more flexible than the first approach figure shows a sample interaction the rpc scheme is useful in implementing a distributed file system chapter such a system can be implemented as a set of rpc daemons chapter processes and clients the messages are addressed to the distributed file system port on a server on which a file operation is to take place the message contains the disk operation to be performed the disk operation might be read write rename delete or status corresponding to the usual filerelated system calls the return message contains any data resulting from that call which is executed by the dfs daemon on behalf of the client for instance a message might contain a request to transfer a whole file to a client or be limited to a simple block request in the latter case several requests may be needed if a whole file is to be transferred pipes a pipe acts as a conduit allowing two processes to communicate pipes were one of the first ipc mechanisms in early unix systems they typically provide one of the simpler ways for processes to communicate with one another although they also have some limitations in implementing a pipe four issues must be considered does the pipe allow bidirectional communication or is communication unidirectional if twoway communication is allowed is it half duplex data can travel only one way at a time or full duplex data can travel in both directions at the same time must a relationship such as parentchild exist between the communicating processes can the pipes communicate over a network or must the communicating processes reside on the same machine in the following sections we explore two common types of pipes used on both unix and windows systems ordinary pipes and named pipes ordinary pipes ordinary pipes allow two processes to communicate in standard producer consumer fashion the producer writes to one end of the pipe the writeend and the consumer reads from the other end the readend as a result ordinary pipes are unidirectional allowing only oneway communication if twoway communication is required two pipes must be used with each pipe sending data in a different direction we next illustrate constructing ordinary pipes on both unix and windows systems in both program examples one process writes the message greetings to the pipe while the other process reads this message from the pipe on unix systems ordinary pipes are constructed using the function pipeint fd this function creates a pipe that is accessed through the int fd file descriptors fd is the readend of the pipe and fd is the writeend communication in client server systems parent child fd fd fd fd pipe figure file descriptors for an ordinary pipe unix treats a pipe as a special type of file thus pipes can be accessed using ordinary read and write system calls an ordinary pipe can not be accessed from outside the process that created it typically a parent process creates a pipe and uses it to communicate with a child process that it creates via fork recall from section that a child process inherits open files from its parent since a pipe is a special type of file the child inherits the pipe from its parent process figure illustrates the relationship of the file descriptor fd to the parent and child processes in the unix program shown in figure the parent process creates a pipe and then sends a fork call creating the child process what occurs after the fork call depends on how the data are to flow through the pipe in this instance the parent writes to the pipe and the child reads from it it is important to notice that both the parent process and the child process initially close their unused ends of the pipe although the program shown in figure does not require this action it is an important step to ensure that a process reading from the pipe can detect endoffile read returns when the writer has closed its end of the pipe ordinary pipes on windows systems are termed anonymous pipes and they behave similarly to their unix counterparts they are unidirectional and include systypesh include stdioh include stringh include unistdh define buffer size define read end define write end int mainvoid char write msgbuffer size greetings char read msgbuffer size int fd pid t pid program continues in figure figure ordinary pipe in unix chapter processes create the pipe if pipefd fprintfstderrpipe failed return fork a child process pid fork if pid error occurred fprintfstderr fork failed return if pid parent process close the unused end of the pipe closefdread end write to the pipe writefdwrite end write msg strlenwrite msg close the write end of the pipe closefdwrite end else child process close the unused end of the pipe closefdwrite end read from the pipe readfdread end read msg buffer size printfread sread msg close the write end of the pipe closefdread end return figure figure continued employ parentchild relationships between the communicating processes in addition reading and writing to the pipe can be accomplished with the ordinary readfile and writefile functions the windows api for creating pipes is the createpipe function which is passed four parameters the parameters provide separate handles for reading and writing to the pipe as well as an instance of the startupinfo structure which is used to specify that the child process is to inherit the handles of the pipe furthermore the size of the pipe in bytes may be specified figure illustrates a parent process creating an anonymous pipe for communicating with its child unlike unix systems in which a child process communication in client server systems include stdioh include stdlibh include windowsh define buffer size int mainvoid handle readhandle writehandle startupinfo si process information pi char messagebuffer size greetings dword written program continues in figure figure windows anonymous pipe parent process automatically inherits a pipe created by its parent windows requires the programmer to specify which attributes the child process will inherit this is accomplished by first initializing the security attributes structure to allow handles to be inherited and then redirecting the child processs handles for standard input or standard output to the read or write handle of the pipe since the child will be reading from the pipe the parent must redirect the childs standard input to the read handle of the pipe furthermore as the pipes are half duplex it is necessary to prohibit the child from inheriting the writeend of the pipe the program to create the child process is similar to the program in figure except that the fifth parameter is set to true indicating that the child process is to inherit designated handles from its parent before writing to the pipe the parent first closes its unused read end of the pipe the child process that reads from the pipe is shown in figure before reading from the pipe this program obtains the read handle to the pipe by invoking getstdhandle note that ordinary pipes require a parentchild relationship between the communicating processes on both unix and windows systems this means that these pipes can be used only for communication between processes on the same machine named pipes ordinary pipes provide a simple mechanism for allowing a pair of processes to communicate however ordinary pipes exist only while the processes are communicating with one another on both unix and windows systems once the processes have finished communicating and have terminated the ordinary pipe ceases to exist named pipes provide a much more powerful communication tool communication can be bidirectional and no parentchild relationship is required once a named pipe is established several processes can use it for communication in fact in a typical scenario a named pipe has several writers additionally named pipes continue to exist after communicating processes have chapter processes set up security attributes allowing pipes to be inherited security attributes sa sizeofsecurity attributesnulltrue allocate memory zeromemorypi sizeofpi create the pipe if createpipereadhandle writehandle sa fprintfstderr create pipe failed return establish the start info structure for the child process getstartupinfosi sihstdoutput getstdhandlestd output handle redirect standard input to the read end of the pipe sihstdinput readhandle sidwflags startf usestdhandles dont allow the child to inherit the write end of pipe sethandleinformationwritehandle handle flag inherit create the child process createprocessnull childexe null null true inherit handles null null si pi close the unused end of the pipe closehandlereadhandle the parent writes to the pipe if writefilewritehandle messagebuffer sizewrittennull fprintfstderr error writing to pipe close the write end of the pipe closehandlewritehandle wait for the child to exit waitforsingleobjectpihprocess infinite closehandlepihprocess closehandlepihthread return figure figure continued finished both unix and windows systems support named pipes although the details of implementation differ greatly next we explore named pipes in each of these systems summary include stdioh include windowsh define buffer size int mainvoid handle readhandle char bufferbuffer size dword read get the read handle of the pipe readhandle getstdhandlestd input handle the child reads from the pipe if readfilereadhandle buffer buffer size read null printfchild read sbuffer else fprintfstderr error reading from pipe return figure windows anonymous pipes child process named pipes are referred to as fifos in unix systems once created they appear as typical files in the file system a fifo is created with the mkfifo system call and manipulated with the ordinary open read write and close system calls it will continue to exist until it is explicitly deleted from the file system although fifos allow bidirectional communication only halfduplex transmission is permitted if data must travel in both directions two fifos are typically used additionally the communicating processes must reside on the same machine if intermachine communication is required sockets section must be used named pipes on windows systems provide a richer communication mechanism than their unix counterparts fullduplex communication is allowed and the communicating processes may reside on either the same or different machines additionally only byteoriented data may be transmitted across a unix fifo whereas windows systems allow either byteor messageoriented data named pipes are created with the createnamedpipe function and a client can connect to a named pipe using connectnamedpipe communication over the named pipe can be accomplished using the readfile and writefile functions overview a thread is a basic unit of cpu utilization it comprises a thread id a program counter a register set and a stack it shares with other threads belonging to the same process its code section data section and other operatingsystem resources such as open files and signals a traditional or heavyweight process has a single thread of control if a process has multiple threads of control it can perform more than one task at a time figure illustrates the difference between a traditional singlethreaded process and a multithreaded process motivation most software applications that run on modern computers are multithreaded an application typically is implemented as a separate process with several chapter threads code data files code data files registers stack registers registers registers stack stack stack thread thread singlethreaded process multithreaded process figure singlethreaded and multithreaded processes threads of control a web browser might have one thread display images or text while another thread retrieves data from the network for example a word processor may have a thread for displaying graphics another thread for responding to keystrokes from the user and a third thread for performing spelling and grammar checking in the background applications can also be designed to leverage processing capabilities on multicore systems such applications can perform several cpuintensive tasks in parallel across the multiple computing cores in certain situations a single application may be required to perform several similar tasks for example a web server accepts client requests for web pages images sound and so forth a busy web server may have several perhaps thousands of clients concurrently accessing it if the web server ran as a traditional singlethreaded process it would be able to service only one client at a time and a client might have to wait a very long time for its request to be serviced one solution is to have the server run as a single process that accepts requests when the server receives a request it creates a separate process to service that request in fact this processcreation method was in common use before threads became popular process creation is time consuming and resource intensive however if the new process will perform the same tasks as the existing process why incur all that overhead it is generally more efficient to use one process that contains multiple threads if the webserver process is multithreaded the server will create a separate thread that listens for client requests when a request is made rather than creating another process the server creates a new thread to service the request and resume listening for additional requests this is illustrated in figure threads also play a vital role in remote procedure call rpc systems recall from chapter that rpcs allow interprocess communication by providing a communication mechanism similar to ordinary function or procedure calls typically rpc servers are multithreaded when a server receives a message it multicore programming earlier in the history of computer design in response to the need for more computing performance singlecpu systems evolved into multicpu systems a more recent similar trend in system design is to place multiple computing cores on a single chip each core appears as a separate processor to the operating system section whether the cores appear across cpu chips or within cpu chips we call these systems multicore or multiprocessor systems multithreaded programming provides a mechanism for more efficient use of these multiple computing cores and improved concurrency consider an application with four threads on a system with a single computing core concurrency merely means that the execution of the threads will be interleaved over time figure because the processing core is capable of executing only one thread at a time on a system with multiple cores however concurrency means that the threads can run in parallel because the system can assign a separate thread to each core figure notice the distinction between parallelism and concurrency in this discussion a system is parallel if it can perform more than one task simultaneously in contrast a concurrent system supports more than one task by allowing all the tasks to make progress thus it is possible to have concurrency without parallelism before the advent of smp and multicore architectures most computer systems had only a single processor cpu schedulers were designed to provide the illusion of parallelism by rapidly switching between processes in core t t t t t core t t t t t time figure parallel execution on a multicore system multithreading models fundamentally then data parallelism involves the distribution of data across multiple cores and task parallelism on the distribution of tasks across multiple cores in practice however few applications strictly follow either data or task parallelism in most instances applications use a hybrid of these two strategies thread libraries user thread k k k k kernel thread figure twolevel model create the manytomany model suffers from neither of these shortcomings developers can create as many user threads as necessary and the corresponding kernel threads can run in parallel on a multiprocessor also when a thread performs a blocking system call the kernel can schedule another thread for execution one variation on the manytomany model still multiplexes many userlevel threads to a smaller or equal number of kernel threads but also allows a userlevel thread to be bound to a kernel thread this variation is sometimes referred to as the twolevel model figure the solaris operating system supported the twolevel model in versions older than solaris however beginning with solaris this system uses the onetoone model implicit threading figure shows the java version of a multithreaded program that determines the summation of a nonnegative integer the summation class implements the runnable interface thread creation is performed by creating an object instance of the thread class and passing the constructor a runnable object creating a thread object does not specifically create the new thread rather the start method creates the new thread calling the start method for the new object does two things it allocates memory and initializes a new thread in the jvm it calls the run method making the thread eligible to be run by the jvm note again that we never call the run method directly rather we call the start method and it calls the run method on our behalf when the summation program runs the jvm creates two threads the first is the parent thread which starts execution in the main method the second thread is created when the start method on the thread object is invoked this child thread begins execution in the run method of the summation class after outputting the value of the summation this thread terminates when it exits from its run method data sharing between threads occurs easily in windows and pthreads since shared data are simply declared globally as a pure objectoriented language java has no such notion of global data if two or more threads are to share data in a java program the sharing occurs by passing references to the shared object to the appropriate threads in the java program shown in figure the main thread and the summation thread share the object instance of the sum class this shared object is referenced through the appropriate getsum and setsum methods you might wonder why we dont use an integer object rather than designing a new sum class the reason is that the integer class is immutable that is once its value is set it can not change recall that the parent threads in the pthreads and windows libraries use pthread join and waitforsingleobject respectively to wait for the summation threads to finish before proceeding the join method in java provides similar functionality notice that join can throw an interruptedexception which we choose to ignore if the parent must wait for several threads to finish the join method can be enclosed in a for loop similar to that shown for pthreads in figure threading issues dispatch queue t queue dispatch get global queue dispatch queue priority default dispatch asyncqueue printfi am a block internally gcds thread pool is composed of posix threads gcd actively manages the pool allowing the number of threads to grow and shrink according to application demand and system capacity other approaches thread pools openmp and grand central dispatch are just a few of many emerging technologies for managing multithreaded applications other commercial approaches include parallel and concurrent libraries such as intels threading building blocks tbb and several products from microsoft the java language and api have seen significant movement toward supporting concurrent programming as well a notable example is the javautilconcurrent package which supports implicit thread creation and management operatingsystem examples at this point we have examined a number of concepts and issues related to threads we conclude the chapter by exploring how threads are implemented in windows and linux systems windows threads windows implements the windows api which is the primary api for the family of microsoft operating systems windows nt and xp as well as windows indeed much of what is mentioned in this section applies to this entire family of operating systems a windows application runs as a separate process and each process may contain one or more threads the windows api for creating threads is covered in summary a thread is a flow of control within a process a multithreaded process contains several different flows of control within the same address space the benefits of multithreading include increased responsiveness to the user resource sharing within the process economy and scalability factors such as more efficient use of multiple processing cores userlevel threads are threads that are visible to the programmer and are unknown to the kernel the operatingsystem kernel supports and manages kernellevel threads in general userlevel threads are faster to create and manage than are kernel threads because no intervention from the kernel is required three different types of models relate user and kernel threads the manytoone model maps many user threads to a single kernel thread the onetoone model maps each user thread to a corresponding kernel thread the manytomany model multiplexes many user threads to a smaller or equal number of kernel threads most modern operating systems provide kernel support for threads these include windows mac os x linux and solaris thread libraries provide the application programmer with an api for creating and managing threads three primary thread libraries are in common use posix pthreads windows threads and java threads in addition to explicitly creating threads using the api provided by a library we can use implicit threading in which the creation and management of threading is transferred to compilers and runtime libraries strategies for implicit threading include thread pools openmp and grand central dispatch multithreaded programs introduce many challenges for programmers including the semantics of the fork and exec system calls other issues include signal handling thread cancellation threadlocal storage and scheduler activations practice exercises provide two programming examples in which multithreading provides better performance than a singlethreaded solution what are two differences between userlevel threads and kernellevel threads under what circumstances is one type better than the other describe the actions taken by a kernel to contextswitch between kernellevel threads what resources are used when a thread is created how do they differ from those used when a process is created chapter threads assume that an operating system maps userlevel threads to the kernel using the manytomany model and that the mapping is done through lwps furthermore the system allows developers to create realtime threads for use in realtime systems is it necessary to bind a realtime thread to an lwp explain exercises provide two programming examples in which multithreading does not provide better performance than a singlethreaded solution under what circumstances does a multithreaded solution using multiple kernel threads provide better performance than a singlethreaded solution on a singleprocessor system which of the following components of program state are shared across threads in a multithreaded process a register values b heap memory c global variables d stack memory can a multithreaded solution using multiple userlevel threads achieve better performance on a multiprocessor system than on a singleprocessor system explain in chapter we discussed googles chrome browser and its practice of opening each new website in a separate process would the same benefits have been achieved if instead chrome had been designed to open each new website in a separate thread explain is it possible to have concurrency but not parallelism explain using amdahls law calculate the speedup gain of an application that has a percent parallel component for a two processing cores and b four processing cores determine if the following problems exhibit task or data parallelism the multithreaded statistical program described in exercise the multithreaded sudoku validator described in project in this chapter the multithreaded sorting program described in project in this chapter the multithreaded web server described in section a system with two dualcore processors has four processors available for scheduling a cpuintensive application is running on this system all input is performed at program startup when a single file must be opened similarly all output is performed just before the program exercises terminates when the program results must be written to a single file between startup and termination the program is entirely cpubound your task is to improve the performance of this application by multithreading it the application runs on a system that uses the onetoone threading model each user thread maps to a kernel thread how many threads will you create to perform the input and output explain how many threads will you create for the cpuintensive portion of the application explain consider the following code segment pid t pid pid fork if pid child process fork thread create fork a how many unique processes are created b how many unique threads are created as described in section linux does not distinguish between processes and threads instead linux treats both in the same way allowing a task to be more akin to a process or a thread depending on the set of flags passed to the clone system call however other operating systems such as windows treat processes and threads differently typically such systems use a notation in which the data structure for a process contains pointers to the separate threads belonging to the process contrast these two approaches for modeling processes and threads within the kernel the program shown in figure uses the pthreads api what would be the output from the program at line c and line p consider a multicore system and a multithreaded program written using the manytomany threading model let the number of userlevel threads in the program be greater than the number of processing cores in the system discuss the performance implications of the following scenarios a the number of kernel threads allocated to the program is less than the number of processing cores b the number of kernel threads allocated to the program is equal to the number of processing cores c the number of kernel threads allocated to the program is greater than the number of processing cores but less than the number of userlevel threads chapter threads include pthreadh include stdioh include typesh int value void runnervoid param the thread int mainint argc char argv pid t pid pthread t tid pthread attr t attr pid fork if pid child process pthread attr initattr pthread createtidattrrunnernull pthread jointidnull printfchild value dvalue line c else if pid parent process waitnull printfparent value dvalue line p void runnervoid param value pthread exit figure c program for exercise pthreads provides an api for managing thread cancellation the pthread setcancelstate function is used to set the cancellation state its prototype appears as follows pthread setcancelstateint state int oldstate the two possible values for the state are pthread cancel enable and pthread cancel disable using the code segment shown in figure provide examples of two operations that would be suitable to perform between the calls to disable and enable thread cancellation programming problems int oldstate pthread setcancelstatepthread cancel disable oldstate what operations would be performed here pthread setcancelstatepthread cancel enable oldstate figure c program for exercise programming problems modify programming problem exercise from chapter which asks you to design a pid manager this modification will consist of writing a multithreaded program that tests your solution to exercise you will create a number of threads for example and each thread will request a pid sleep for a random period of time and then release the pid sleeping for a random period of time approximates the typical pid usage in which a pid is assigned to a new process the process executes and then terminates and the pid is released on the processs termination on unix and linux systems sleeping is accomplished through the sleep function which is passed an integer value representing the number of seconds to sleep this problem will be modified in chapter write a multithreaded program that calculates various statistical values for a list of numbers this program will be passed a series of numbers on the command line and will then create three separate worker threads one thread will determine the average of the numbers the second will determine the maximum value and the third will determine the minimum value for example suppose your program is passed the integers the program will report the average value is the minimum value is the maximum value is the variables representing the average minimum and maximum values will be stored globally the worker threads will set these values and the parent thread will output the values once the workers have exited we could obviously expand this program by creating additional threads that determine other statistical values such as median and standard deviation an interesting way of calculating is to use a technique known as monte carlo which involves randomization this technique works as follows suppose you have a circle inscribed within a square as shown in figure chapter threads figure monte carlo technique for calculating pi assume that the radius of this circle is first generate a series of random points as simple x y coordinates these points must fall within the cartesian coordinates that bound the square of the total number of random points that are generated some will occur within the circle next estimate by performing the following calculation number of points in circle total number of points write a multithreaded version of this algorithm that creates a separate thread to generate a number of random points the thread will count the number of points that occur within the circle and store that result in a global variable when this thread has exited the parent thread will calculate and output the estimated value of it is worth experimenting with the number of random points generated as a general rule the greater the number of points the closer the approximation to in the sourcecode download for this text we provide a sample program that provides a technique for generating random numbers as well as determining if the random x y point occurs within the circle readers interested in the details of the monte carlo method for estimating should consult the bibliography at the end of this chapter in chapter we modify this exercise using relevant material from that chapter repeat exercise but instead of using a separate thread to generate random points use openmp to parallelize the generation of points be careful not to place the calculcation of in the parallel region since you want to calculcate only once write a multithreaded program that outputs prime numbers this program should work as follows the user will run the program and will enter a number on the command line the program will then create a separate thread that outputs all the prime numbers less than or equal to the number entered by the user modify the socketbased date server figure in chapter so that the server services each client request in a separate thread programming projects the fibonacci sequence is the series of numbers formally it can be expressed as f ib f ib f ibn f i bn f i bn write a multithreaded program that generates the fibonacci sequence this program should work as follows on the command line the user will enter the number of fibonacci numbers that the program is to generate the program will then create a separate thread that will generate the fibonacci numbers placing the sequence in data that can be shared by the threads an array is probably the most convenient data structure when the thread finishes execution the parent thread will output the sequence generated by the child thread because the parent thread can not begin outputting the fibonacci sequence until the child thread finishes the parent thread will have to wait for the child thread to finish use the techniques described in section to meet this requirement exercise in chapter involves designing an echo server using the java threading api this server is singlethreaded meaning that the server can not respond to concurrent echo clients until the current client exits modify the solution to exercise so that the echo server services each client in a separate request programming projects project sudoku solution validator a sudoku puzzle uses a grid in which each column and row as well as each of the nine subgrids must contain all of the digits figure presents an example of a valid sudoku puzzle this project consists of designing a multithreaded application that determines whether the solution to a sudoku puzzle is valid there are several different ways of multithreading this application one suggested strategy is to create threads that check the following criteria a thread to check that each column contains the digits through a thread to check that each row contains the digits through nine threads to check that each of the subgrids contains the digits through this would result in a total of eleven separate threads for validating a sudoku puzzle however you are welcome to create even more threads for this project for example rather than creating one thread that checks all nine chapter threads figure solution to a sudoku puzzle columns you could create nine separate threads and have each of them check one column passing parameters to each thread the parent thread will create the worker threads passing each worker the location that it must check in the sudoku grid this step will require passing several parameters to each thread the easiest approach is to create a data structure using a struct for example a structure to pass the row and column where a thread must begin validating would appear as follows structure for passing data to threads typedef struct int row int column parameters both pthreads and windows programs will create worker threads using a strategy similar to that shown below parameters data parameters mallocsizeofparameters datarow datacolumn now create the thread passing it data as a parameter the data pointer will be passed to either the pthread create pthreads function or the createthread windows function which in turn will pass it as a parameter to the function that is to run as a separate thread returning results to the parent thread each worker thread is assigned the task of determining the validity of a particular region of the sudoku puzzle once a worker has performed this bibliographical notes original list sorting sorting thread thread merge thread sorted list figure multithreaded sorting check it must pass its results back to the parent one good way to handle this is to create an array of integer values that is visible to each thread the ith index in this array corresponds to the ith worker thread if a worker sets its corresponding value to it is indicating that its region of the sudoku puzzle is valid a value of would indicate otherwise when all worker threads have completed the parent thread checks each entry in the result array to determine if the sudoku puzzle is valid project multithreaded sorting application write a multithreaded sorting program that works as follows a list of integers is divided into two smaller lists of equal size two separate threads which we will term sorting threads sort each sublist using a sorting algorithm of your choice the two sublists are then merged by a third thread a merging thread which merges the two sublists into a single sorted list because global data are shared cross all threads perhaps the easiest way to set up the data is to create a global array each sorting thread will work on one half of this array a second global array of the same size as the unsorted integer array will also be established the merging thread will then merge the two sublists into this second array graphically this program is structured according to figure this programming project will require passing parameters to each of the sorting threads in particular it will be necessary to identify the starting index from which each thread is to begin sorting refer to the instructions in project for details on passing parameters to a thread the parent thread will output the sorted array once all sorting threads have exited bibliographical notes threads have had a long evolution starting as cheap concurrency in programming languages and moving to lightweight processes with early examples that included the thoth system cheriton et al and the pilot chapter threads system redell et al binding described moving threads into the unix kernel mach accetta et al tevanian et al and v cheriton made extensive use of threads and eventually almost all major operating systems implemented them in some form or another vahalia covers threading in several versions of unix mcdougall and mauro describes developments in threading the solaris kernel russinovich and solomon discuss threading in the windows operating system family mauerer and love explain how linux handles threading and singh covers threads in mac os x information on pthreads programming is given in lewis and berg and butenhof oaks and wong and lewis and berg discuss multithreading in java goetz et al present a detailed discussion of concurrent programming in java hart describes multithreading using windows details on using openmp can be found at httpopenmporg an analysis of an optimal threadpool size can be found in ling et al scheduler activations were first presented in anderson et al and williams discusses scheduler activations in the netbsd system breshears and pacheco cover parallel programming in detail hill and marty examine amdahls law with respect to multicore systems the monte carlo technique for estimating is further discussed in httpmathfullertonedumathewsnmontecarlopimodhtml bibliography accetta et al m accetta r baron w bolosky d b golub r rashid a tevanian and m young mach a new kernel foundation for unix development proceedings of the summer usenix conference pages anderson et al t e anderson b n bershad e d lazowska and h m levy scheduler activations effective kernel support for the userlevel management of parallelism proceedings of the acm symposium on operating systems principles pages binding c binding cheap concurrency in c sigplan notices volume number pages breshears c breshears the art of concurrency oreilly associates butenhof d butenhof programming with posix threads addisonwesley cheriton d cheriton the v distributed system communications of the acm volume number pages cheriton et al d r cheriton m a malcolm l s melen and g r sager thoth a portable realtime operating system communications of the acm volume number pages bibliography goetz et al b goetz t peirls j bloch j bowbeer d holmes and d lea java concurrency in practice addisonwesley hart j m hart windows system programming third edition addisonwesley hill and marty m hill and m marty amdahls law in the multicore era ieee computer volume number pages lewis and berg b lewis and d berg multithreaded programming with pthreads sun microsystems press lewis and berg b lewis and d berg multithreaded programming with java technology sun microsystems press ling et al y ling t mullen and x lin analysis of optimal thread pool size operating system review volume number pages love r love linux kernel development third edition developers library mauerer w mauerer professional linux kernel architecture john wiley and sons mcdougall and mauro r mcdougall and j mauro solaris internals second edition prentice hall oaks and wong s oaks and h wong java threads second edition oreilly associates pacheco p s pacheco an introduction to parallel programming morgan kaufmann redell et al d d redell y k dalal t r horsley h c lauer w c lynch p r mcjones h g murray and s p purcell pilot an operating system for a personal computer communications of the acm volume number pages russinovich and solomon m e russinovich and d a solomon windows internals including windows server and windows vista fifth edition microsoft press singh a singh mac os x internals a systems approach addisonwesley tevanian et al a tevanian jr r f rashid d b golub d l black e cooper and m w young mach threads and the unix kernel the battle for control proceedings of the summer usenix conference vahalia u vahalia unix internals the new frontiers prentice hall williams n williams an implementation of scheduler activations on the netbsd operating system usenix annual technical conference freenix track background weve already seen that processes can execute concurrently or in parallel section introduced the role of process scheduling and described how the cpu scheduler switches rapidly between processes to provide concurrent execution this means that one process may only partially complete execution before another process is scheduled in fact a process may be interrupted at any point in its instruction stream and the processing core may be assigned to execute instructions of another process additionally section introduced parallel execution in which two instruction streams representing different processes execute simultaneously on separate processing cores in this chapter chapter process synchronization we explain how concurrent or parallel execution can contribute to issues involving the integrity of data shared by several processes lets consider an example of how this can happen in chapter we developed a model of a system consisting of cooperating sequential processes or threads all running asynchronously and possibly sharing data we illustrated this model with the producerconsumer problem which is representative of operating systems specifically in section we described how a bounded buffer could be used to enable processes to share memory we now return to our consideration of the bounded buffer as we pointed out our original solution allowed at most buffer size items in the buffer at the same time suppose we want to modify the algorithm to remedy this deficiency one possibility is to add an integer variable counter initialized to counter is incremented every time we add a new item to the buffer and is decremented every time we remove one item from the buffer the code for the producer process can be modified as follows while true produce an item in next produced while counter buffer size do nothing bufferin next produced in in buffer size counter the code for the consumer process can be modified as follows while true while counter do nothing next consumed bufferout out out buffer size counter consume the item in next consumed although the producer and consumer routines shown above are correct separately they may not function correctly when executed concurrently as an illustration suppose that the value of the variable counter is currently and that the producer and consumer processes concurrently execute the statements counter and counter following the execution of these two statements the value of the variable counter may be or the only correct result though is counter which is generated correctly if the producer and consumer execute separately the criticalsection problem we begin our consideration of process synchronization by discussing the socalled criticalsection problem consider a system consisting of n processes p p pn each process has a segment of code called a critical section in which the process may be changing common variables updating a table writing a file and so on the important feature of the system is that when one process is executing in its critical section no other process is allowed to execute in its critical section that is no two processes are executing in their critical sections at the same time the criticalsection problem is to design a protocol that the processes can use to cooperate each process must request permission to enter its critical section the section of code implementing this request is the entry section the critical section may be followed by an exit section the remaining code is the remainder section the general structure of a typical process pi is shown in figure the entry section and exit section are enclosed in boxes to highlight these important segments of code a solution to the criticalsection problem must satisfy the following three requirements mutual exclusion if process pi is executing in its critical section then no other processes can be executing in their critical sections progress if no process is executing in its critical section and some processes wish to enter their critical sections then only those processes that are not executing in their remainder sections can participate in deciding which will enter its critical section next and this selection can not be postponed indefinitely bounded waiting there exists a bound or limit on the number of times that other processes are allowed to enter their critical sections after a petersons solution process has made a request to enter its critical section and before that request is granted we assume that each process is executing at a nonzero speed however we can make no assumption concerning the relative speed of the n processes at a given point in time many kernelmode processes may be active in the operating system as a result the code implementing an operating system kernel code is subject to several possible race conditions consider as an example a kernel data structure that maintains a list of all open files in the system this list must be modified when a new file is opened or closed adding the file to the list or removing it from the list if two processes were to open files simultaneously the separate updates to this list could result in a race condition other kernel data structures that are prone to possible race conditions include structures for maintaining memory allocation for maintaining process lists and for interrupt handling it is up to kernel developers to ensure that the operating system is free from such race conditions two general approaches are used to handle critical sections in operating systems preemptive kernels and nonpreemptive kernels a preemptive kernel allows a process to be preempted while it is running in kernel mode a nonpreemptive kernel does not allow a process running in kernel mode to be preempted a kernelmode process will run until it exits kernel mode blocks or voluntarily yields control of the cpu obviously a nonpreemptive kernel is essentially free from race conditions on kernel data structures as only one process is active in the kernel at a time we can not say the same about preemptive kernels so they must be carefully designed to ensure that shared kernel data are free from race conditions preemptive kernels are especially difficult to design for smp architectures since in these environments it is possible for two kernelmode processes to run simultaneously on different processors why then would anyone favor a preemptive kernel over a nonpreemptive one a preemptive kernel may be more responsive since there is less risk that a kernelmode process will run for an arbitrarily long period before relinquishing the processor to waiting processes of course this risk can also be minimized by designing kernel code that does not behave in this way furthermore a preemptive kernel is more suitable for realtime programming as it will allow a realtime process to preempt a process currently running in the kernel later in this chapter we explore how various operating systems manage preemption within the kernel synchronization hardware value of turn can be either or but can not be both hence one of the processes say pj must have successfully executed the while statement whereas pi had to execute at least one additional statement turn j however at that time flagj true and turn j and this condition will persist as long as pj is in its critical section as a result mutual exclusion is preserved to prove properties and we note that a process pi can be prevented from entering the critical section only if it is stuck in the while loop with the condition flagj true and turn j this loop is the only one possible if pj is not ready to enter the critical section then flagj false and pi can enter its critical section if pj has set flagj to true and is also executing in its while statement then either turn i or turn j if turn i then pi will enter the critical section if turn j then pj will enter the critical section however once pj exits its critical section it will reset flagj to false allowing pi to enter its critical section if pj resets flagj to true it must also set turn to i thus since pi does not change the value of the variable turn while executing the while statement pi will enter the critical section progress after at most one entry by pj bounded waiting mutex locks the hardwarebased solutions to the criticalsection problem presented in section are complicated as well as generally inaccessible to application programmers instead operatingsystems designers build software tools to solve the criticalsection problem the simplest of these tools is the mutex lock in fact the term mutex is short for mutual exclusion we use the mutex lock to protect critical regions and thus prevent race conditions that is a process must acquire the lock before entering a critical section it releases the lock when it exits the critical section the acquirefunction acquires the lock and the release function releases the lock as illustrated in figure a mutex lock has a boolean variable available whose value indicates if the lock is available or not if the lock is available a call to acquire succeeds and the lock is then considered unavailable a process that attempts to acquire an unavailable lock is blocked until the lock is released the definition of acquire is as follows acquire while available busy wait available false semaphores do acquire lock critical section release lock remainder section while true figure solution to the criticalsection problem using mutex locks the definition of release is as follows release available true calls to either acquire or release must be performed atomically thus mutex locks are often implemented using one of the hardware mechanisms described in section and we leave the description of this technique as an exercise the main disadvantage of the implementation given here is that it requires busy waiting while a process is in its critical section any other process that tries to enter its critical section must loop continuously in the call to acquire in fact this type of mutex lock is also called a spinlock because the process spins while waiting for the lock to become available we see the same issue with the code examples illustrating the test and set instruction and the compare and swap instruction this continual looping is clearly a problem in a real multiprogramming system where a single cpu is shared among many processes busy waiting wastes cpu cycles that some other process might be able to use productively spinlocks do have an advantage however in that no context switch is required when a process must wait on a lock and a context switch may take considerable time thus when locks are expected to be held for short times spinlocks are useful they are often employed on multiprocessor systems where one thread can spin on one processor while another thread performs its critical section on another processor later in this chapter section we examine how mutex locks can be used to solve classical synchronization problems we also discuss how these locks are used in several operating systems as well as in pthreads classic problems of synchronization do produce an item in next produced waitempty waitmutex add next produced to the buffer signalmutex signalfull while true figure the structure of the producer process monitors do waitchopsticki waitchopsticki eat for awhile signalchopsticki signalchopsticki think for awhile while true figure the structure of philosopher i where all the elements of chopstick are initialized to the structure of philosopher i is shown in figure although this solution guarantees that no two neighbors are eating simultaneously it nevertheless must be rejected because it could create a deadlock suppose that all five philosophers become hungry at the same time and each grabs her left chopstick all the elements of chopstick will now be equal to when each philosopher tries to grab her right chopstick she will be delayed forever several possible remedies to the deadlock problem are replaced by allow at most four philosophers to be sitting simultaneously at the table allow a philosopher to pick up her chopsticks only if both chopsticks are available to do this she must pick them up in a critical section use an asymmetric solution that is an oddnumbered philosopher picks up first her left chopstick and then her right chopstick whereas an evennumbered philosopher picks up her right chopstick and then her left chopstick in section we present a solution to the diningphilosophers problem that ensures freedom from deadlocks note however that any satisfactory solution to the diningphilosophers problem must guard against the possibility that one of the philosophers will starve to death a deadlockfree solution does not necessarily eliminate the possibility of starvation synchronization examples we next describe the synchronization mechanisms provided by the windows linux and solaris operating systems as well as the pthreads api we have chosen these three operating systems because they provide good examples of different approaches to synchronizing the kernel and we have included the alternative approaches with the emergence of multicore systems has come increased pressure to develop multithreaded applications that take advantage of multiple processing summary given a collection of cooperating sequential processes that share data mutual exclusion must be provided to ensure that a critical section of code is used by only one process or thread at a time typically computer hardware provides several operations that ensure mutual exclusion however such hardwarebased solutions are too complicated for most developers to use mutex locks and semaphores overcome this obstacle both tools can be used to solve various synchronization problems and can be implemented efficiently especially if hardware support for atomic operations is available various synchronization problems such as the boundedbuffer problem the readerswriters problem and the diningphilosophers problem are important mainly because they are examples of a large class of concurrencycontrol problems these problems are used to test nearly every newly proposed synchronization scheme the operating system must provide the means to guard against timing errors and several language constructs have been proposed to deal with these problems monitors provide a synchronization mechanism for sharing abstract data types a condition variable provides a method by which a monitor function can block its execution until it is signaled to continue operating systems also provide support for synchronization for example windows linux and solaris provide mechanisms such as semaphores mutex locks spinlocks and condition variables to control access to shared data the pthreads api provides support for mutex locks and semaphores as well as condition variables several alternative approaches focus on synchronization for multicore systems one approach uses transactional memory which may address synchronization issues using either software or hardware techniques another approach uses the compiler extensions offered by openmp finally functional programming languages address synchronization issues by disallowing mutability practice exercises in section we mentioned that disabling interrupts frequently can affect the systems clock explain why this can occur and how such effects can be minimized explain why windows linux and solaris implement multiple locking mechanisms describe the circumstances under which they use spinlocks mutex locks semaphores adaptive mutex locks and condition variables in each case explain why the mechanism is needed exercises what is the meaning of the term busy waiting what other kinds of waiting are there in an operating system can busy waiting be avoided altogether explain your answer explain why spinlocks are not appropriate for singleprocessor systems yet are often used in multiprocessor systems show that if the wait and signal semaphore operations are not executed atomically then mutual exclusion may be violated illustrate how a binary semaphore can be used to implement mutual exclusion among n processes exercises race conditions are possible in many computer systems consider a banking system that maintains an account balance with two functions depositamount and withdrawamount these two functions are passed the amount that is to be deposited or withdrawn from the bank account balance assume that a husband and wife share a bank account concurrently the husband calls the withdraw function and the wife calls deposit describe how a race condition is possible and what might be done to prevent the race condition from occurring the first known correct software solution to the criticalsection problem for two processes was developed by dekker the two processes p and p share the following variables boolean flag initially false int turn the structure of process pi i or is shown in figure the other process is pj j or prove that the algorithm satisfies all three requirements for the criticalsection problem the first known correct software solution to the criticalsection problem for n processes with a lower bound on waiting of n turns was presented by eisenberg and mcguire the processes share the following variables enum pstate idle want in in cs pstate flagn int turn all the elements of flag are initially idle the initial value of turn is immaterial between and n the structure of process pi is shown in figure prove that the algorithm satisfies all three requirements for the criticalsection problem explain why implementing synchronization primitives by disabling interrupts is not appropriate in a singleprocessor system if the synchronization primitives are to be used in userlevel programs chapter process synchronization do flagi true while flagj if turn j flagi false while turn j do nothing flagi true critical section turn j flagi false remainder section while true figure the structure of process pi in dekkers algorithm explain why interrupts are not appropriate for implementing synchronization primitives in multiprocessor systems the linux kernel has a policy that a process can not hold a spinlock while attempting to acquire a semaphore explain why this policy is in place describe two kernel data structures in which race conditions are possible be sure to include a description of how a race condition can occur describe how the compare and swap instruction can be used to provide mutual exclusion that satisfies the boundedwaiting requirement consider how to implement a mutex lock using an atomic hardware instruction assume that the following structure defining the mutex lock is available typedef struct int available lock available indicates that the lock is available and a value of indicates that the lock is unavailable using this struct illustrate how the following functions can be implemented using the test and set and compare and swap instructions void acquirelock mutex void releaselock mutex be sure to include any initialization that may be necessary exercises do while true flagi want in j turn while j i if flagj idle j turn else j j n flagi in cs j while j n j i flagj in cs j if j n turn i flagturn idle break critical section j turn n while flagj idle j j n turn j flagi idle remainder section while true figure the structure of process pi in eisenberg and mcguires algorithm the implementation of mutex locks provided in section suffers from busy waiting describe what changes would be necessary so that a process waiting to acquire a mutex lock would be blocked and placed into a waiting queue until the lock became available assume that a system has multiple processing cores for each of the following scenarios describe which is a better locking mechanism a spinlock or a mutex lock where waiting processes sleep while waiting for the lock to become available the lock is to be held for a short duration the lock is to be held for a long duration a thread may be put to sleep while holding the lock chapter process synchronization define max processes int number of processes the implementation of fork calls this function int allocate process int new pid if number of processes max processes return else allocate necessary process resources number of processes return new pid the implementation of exit calls this function void release process release process resources number of processes figure allocating and releasing processes assume that a context switch takes t time suggest an upper bound in terms of t for holding a spinlock if the spinlock is held for any longer a mutex lock where waiting threads are put to sleep is a better alternative a multithreaded web server wishes to keep track of the number of requests it services known as hits consider the two following strategies to prevent a race condition on the variable hits the first strategy is to use a basic mutex lock when updating hits int hits mutex lock hit lock hit lockacquire hits hit lockrelease a second strategy is to use an atomic integer atomic t hits atomic inchits explain which of these two strategies is more efficient consider the code example for allocating and releasing processes shown in figure exercises a identify the race conditions b assume you have a mutex lock named mutex with the operations acquire and release indicate where the locking needs to be placed to prevent the race conditions c could we replace the integer variable int number of processes with the atomic integer atomic t number of processes to prevent the race conditions servers can be designed to limit the number of open connections for example a server may wish to have only n socket connections at any point in time as soon as n connections are made the server will not accept another incoming connection until an existing connection is released explain how semaphores can be used by a server to limit the number of concurrent connections windows vista provides a lightweight synchronization tool called slim readerwriter locks whereas most implementations of readerwriter locks favor either readers or writers or perhaps order waiting threads using a fifo policy slim readerwriter locks favor neither readers nor writers nor are waiting threads ordered in a fifo queue explain the benefits of providing such a synchronization tool show how to implement the wait and signal semaphore operations in multiprocessor environments using the test and set instruction the solution should exhibit minimal busy waiting exercise requires the parent thread to wait for the child thread to finish its execution before printing out the computed values if we let the parent thread access the fibonacci numbers as soon as they have been computed by the child thread rather than waiting for the child thread to terminate what changes would be necessary to the solution for this exercise implement your modified solution demonstrate that monitors and semaphores are equivalent insofar as they can be used to implement solutions to the same types of synchronization problems design an algorithm for a boundedbuffer monitor in which the buffers portions are embedded within the monitor itself the strict mutual exclusion within a monitor makes the boundedbuffer monitor of exercise mainly suitable for small portions a explain why this is true b design a new scheme that is suitable for larger portions discuss the tradeoff between fairness and throughput of operations in the readers writers problem propose a method for solving the readerswriters problem without causing starvation chapter process synchronization how does the signal operation associated with monitors differ from the corresponding operation defined for semaphores suppose the signal statement can appear only as the last statement in a monitor function suggest how the implementation described in section can be simplified in this situation consider a system consisting of processes p p pn each of which has a unique priority number write a monitor that allocates three identical printers to these processes using the priority numbers for deciding the order of allocation a file is to be shared among different processes each of which has a unique number the file can be accessed simultaneously by several processes subject to the following constraint the sum of all unique numbers associated with all the processes currently accessing the file must be less than n write a monitor to coordinate access to the file when a signal is performed on a condition inside a monitor the signaling process can either continue its execution or transfer control to the process that is signaled how would the solution to the preceding exercise differ with these two different ways in which signaling can be performed suppose we replace the wait and signal operations of monitors with a single construct awaitb where b is a general boolean expression that causes the process executing it to wait until b becomes true a write a monitor using this scheme to implement the readers writers problem b explain why in general this construct can not be implemented efficiently c what restrictions need to be put on the await statement so that it can be implemented efficiently hint restrict the generality of b see kessels design an algorithm for a monitor that implements an alarm clock that enables a calling program to delay itself for a specified number of time units ticks you may assume the existence of a real hardware clock that invokes a function tick in your monitor at regular intervals programming problems programming exercise required you to design a pid manager that allocated a unique process identifier to each process exercise required you to modify your solution to exercise by writing a program that created a number of threads that requested and released process identifiers now modify your solution to exercise by ensuring that the data structure used to represent the availability of process identifiers is safe from race conditions use pthreads mutex locks described in section programming problems assume that a finite number of resources of a single resource type must be managed processes may ask for a number of these resources and will return them once finished as an example many commercial software packages provide a given number of licenses indicating the number of applications that may run concurrently when the application is started the license count is decremented when the application is terminated the license count is incremented if all licenses are in use requests to start the application are denied such requests will only be granted when an existing license holder terminates the application and a license is returned the following program segment is used to manage a finite number of instances of an available resource the maximum number of resources and the number of available resources are declared as follows define max resources int available resources max resources when a process wishes to obtain a number of resources it invokes the decrease count function decrease available resources by count resources return if sufficient resources available otherwise return int decrease countint count if available resources count return else available resources count return when a process wants to return a number of resources it calls the increase count function increase available resources by count int increase countint count available resources count return the preceding program segment produces a race condition do the following a identify the data involved in the race condition b identify the location or locations in the code where the race condition occurs chapter process synchronization c using a semaphore or mutex lock fix the race condition it is permissible to modify the decrease count function so that the calling process is blocked until sufficient resources are available the decrease count function in the previous exercise currently returns if sufficient resources are available and otherwise this leads to awkward programming for a process that wishes to obtain a number of resources while decrease countcount rewrite the resourcemanager code segment using a monitor and condition variables so that the decrease count function suspends the process until sufficient resources are available this will allow a process to invoke decrease count by simply calling decrease countcount the process will return from this function call only when sufficient resources are available exercise asked you to design a multithreaded program that estimated using the monte carlo technique in that exercise you were asked to create a single thread that generated random points storing the result in a global variable once that thread exited the parent thread performed the calcuation that estimated the value of modify that program so that you create several threads each of which generates random points and determines if the points fall within the circle each thread will have to update the global count of all points that fall within the circle protect against race conditions on updates to the shared global variable by using mutex locks exercise asked you to design a program using openmp that estimated using the monte carlo technique examine your solution to that program looking for any possible race conditions if you identify a race condition protect against it using the strategy outlined in section a barrier is a tool for synchronizing the activity of a number of threads when a thread reaches a barrier point it can not proceed until all other threads have reached this point as well when the last thread reaches the barrier point all threads are released and can resume concurrent execution assume that the barrier is initialized to n the number of threads that must wait at the barrier point initn each thread then performs some work until it reaches the barrier point programming projects do some work for awhile barrier point do some work for awhile using synchronization tools described in this chapter construct a barrier that implements the following api int initint n initializes the barrier to the specified size int barrier pointvoid identifies the barrier point all threads are released from the barrier when the last thread reaches this point the return value of each function is used to identify error conditions each function will return under normal operation and will return if an error occurs a testing harness is provided in the source code download to test your implementation of the barrier programming projects project the sleeping teaching assistant a university computer science department has a teaching assistant ta who helps undergraduate students with their programming assignments during regular office hours the tas office is rather small and has room for only one desk with a chair and computer there are three chairs in the hallway outside the office where students can sit and wait if the ta is currently helping another student when there are no students who need help during office hours the ta sits at the desk and takes a nap if a student arrives during office hours and finds the ta sleeping the student must awaken the ta to ask for help if a student arrives and finds the ta currently helping another student the student sits on one of the chairs in the hallway and waits if no chairs are available the student will come back at a later time using posix threads mutex locks and semaphores implement a solution that coordinates the activities of the ta and the students details for this assignment are provided below the students and the ta using pthreads section begin by creating n students each will run as a separate thread the ta will run as a separate thread as well student threads will alternate between programming for a period of time and seeking help from the ta if the ta is available they will obtain help otherwise they will either sit in a chair in the hallway or if no chairs are available will resume programming and will seek help at a later time if a student arrives and notices that the ta is sleeping the student must notify the ta using a semaphore when the ta finishes helping a student the ta must check to see if there are students waiting for help in the hallway if so the ta must help each of these students in turn if no students are present the ta may return to napping chapter process synchronization perhaps the best option for simulating students programming as well as the ta providing help to a student is to have the appropriate threads sleep for a random period of time posix synchronization coverage of posix mutex locks and semaphores is provided in section consult that section for details project the dining philosophers problem in section we provide an outline of a solution to the diningphilosophers problem using monitors this problem will require implementing a solution using pthreads mutex locks and condition variables the philosophers begin by creating five philosophers each identified by a number each philosopher will run as a separate thread thread creation using pthreads is covered in section philosophers alternate between thinking and eating to simulate both activities have the thread sleep for a random period between one and three seconds when a philosopher wishes to eat she invokes the function pickup forksint philosopher number where philosopher number identifies the number of the philosopher wishing to eat when a philosopher finishes eating she invokes return forksint philosopher number pthreads condition variables condition variables in pthreads behave similarly to those described in section however in that section condition variables are used within the context of a monitor which provides a locking mechanism to ensure data integrity since pthreads is typically used in c programs and since c does not have a monitor we accomplish locking by associating a condition variable with a mutex lock pthreads mutex locks are covered in section we cover pthreads condition variables here condition variables in pthreads use the pthread cond t data type and are initialized using the pthread cond init function the following code creates and initializes a condition variable as well as its associated mutex lock pthread mutex t mutex pthread cond t cond var pthread mutex initmutexnull pthread cond initcond varnull programming projects the pthread cond wait function is used for waiting on a condition variable the following code illustrates how a thread can wait for the condition a b to become true using a pthread condition variable pthread mutex lockmutex while a b pthread cond waitmutex cond var pthread mutex unlockmutex the mutex lock associated with the condition variable must be locked before the pthread cond wait function is called since it is used to protect the data in the conditional clause from a possible race condition once this lock is acquired the thread can check the condition if the condition is not true the thread then invokes pthread cond wait passing the mutex lock and the condition variable as parameters calling pthread cond wait releases the mutex lock thereby allowing another thread to access the shared data and possibly update its value so that the condition clause evaluates to true to protect against program errors it is important to place the conditional clause within a loop so that the condition is rechecked after being signaled a thread that modifies the shared data can invoke the pthread cond signal function thereby signaling one thread waiting on the condition variable this is illustrated below pthread mutex lockmutex a b pthread cond signalcond var pthread mutex unlockmutex it is important to note that the call to pthread cond signal does not release the mutex lock it is the subsequent call to pthread mutex unlock that releases the mutex once the mutex lock is released the signaled thread becomes the owner of the mutex lock and returns control from the call to pthread cond wait project producer consumer problem in section we presented a semaphorebased solution to the producer consumer problem using a bounded buffer in this project you will design a programming solution to the boundedbuffer problem using the producer and consumer processes shown in figures and the solution presented in section uses three semaphores empty and full which count the number of empty and full slots in the buffer and mutex which is a binary or mutualexclusion semaphore that protects the actual insertion or removal of items in the buffer for this project you will use standard counting semaphores for empty and full and a mutex lock rather than a binary semaphore to represent mutex the producer and consumer running as separate threads will move items to and from a buffer that is synchronized with the empty full and mutex structures you can solve this problem using either pthreads or the windows api chapter process synchronization include bufferh the buffer buffer item bufferbuffer size int insert itembuffer item item insert item into buffer return if successful otherwise return indicating an error condition int remove itembuffer item item remove an object from buffer placing it in item return if successful otherwise return indicating an error condition figure outline of buffer operations the buffer internally the buffer will consist of a fixedsize array of type buffer item which will be defined using a typedef the array of buffer item objects will be manipulated as a circular queue the definition of buffer item along with the size of the buffer can be stored in a header file such as the following bufferh typedef int buffer item define buffer size the buffer will be manipulated with two functions insert item and remove item which are called by the producer and consumer threads respectively a skeleton outlining these functions appears in figure the insert item and remove item functions will synchronize the producer and consumer using the algorithms outlined in figures and the buffer will also require an initialization function that initializes the mutualexclusion object mutex along with the empty and full semaphores the main function will initialize the buffer and create the separate producer and consumer threads once it has created the producer and consumer threads the main function will sleep for a period of time and upon awakening will terminate the application the main function will be passed three parameters on the command line how long to sleep before terminating the number of producer threads the number of consumer threads programming projects include bufferh int mainint argc char argv get command line arguments argvargvargv initialize buffer create producer threads create consumer threads sleep exit figure outline of skeleton program a skeleton for this function appears in figure the producer and consumer threads the producer thread will alternate between sleeping for a random period of time and inserting a random integer into the buffer random numbers will be produced using the rand function which produces random integers between and rand max the consumer will also sleep for a random period of time and upon awakening will attempt to remove an item from the buffer an outline of the producer and consumer threads appears in figure as noted earlier you can solve this problem using either pthreads or the windows api in the following sections we supply more information on each of these choices pthreads thread creation and synchronization creating threads using the pthreads api is discussed in section coverage of mutex locks and semaphores using pthreads is provided in section refer to those sections for specific instructions on pthreads thread creation and synchronization windows section discusses thread creation using the windows api refer to that section for specific instructions on creating threads windows mutex locks mutex locks are a type of dispatcher object as described in section the following illustrates how to create a mutex lock using the createmutex function include windowsh handle mutex mutex createmutexnull false null chapter process synchronization include stdlibh required for rand include bufferh void producervoid param buffer item item while true sleep for a random period of time sleep generate a random number item rand if insert itemitem fprintfreport error condition else printfproducer produced dnitem void consumervoid param buffer item item while true sleep for a random period of time sleep if remove itemitem fprintfreport error condition else printfconsumer consumed dnitem figure an outline of the producer and consumer threads the first parameter refers to a security attribute for the mutex lock by setting this attribute to null we disallow any children of the process creating this mutex lock to inherit the handle of the lock the second parameter indicates whether the creator of the mutex lock is the locks initial owner passing a value of false indicates that the thread creating the mutex is not the initial owner we shall soon see how mutex locks are acquired the third parameter allows us to name the mutex however because we provide a value of null we do not name the mutex if successful createmutex returns a handle to the mutex lock otherwise it returns null in section we identified dispatcher objects as being either signaled or nonsignaled a signaled dispatcher object such as a mutex lock is available for ownership once it is acquired it moves to the nonsignaled state when it is released it returns to signaled mutex locks are acquired by invoking the waitforsingleobject function the function is passed the handle to the lock along with a flag indicating how long to wait the following code demonstrates how the mutex lock created above can be acquired waitforsingleobjectmutex infinite programming projects the parameter value infinite indicates that we will wait an infinite amount of time for the lock to become available other values could be used that would allow the calling thread to time out if the lock did not become available within a specified time if the lock is in a signaled state waitforsingleobject returns immediately and the lock becomes nonsignaled a lock is released moves to the signaled state by invoking releasemutex for example as follows releasemutexmutex windows semaphores semaphores in the windows api are dispatcher objects and thus use the same signaling mechanism as mutex locks semaphores are created as follows include windowsh handle sem sem createsemaphorenull null the first and last parameters identify a security attribute and a name for the semaphore similar to what we described for mutex locks the second and third parameters indicate the initial value and maximum value of the semaphore in this instance the initial value of the semaphore is and its maximum value is if successful createsemaphore returns a handle to the mutex lock otherwise it returns null semaphores are acquired with the same waitforsingleobject function as mutex locks we acquire the semaphore sem created in this example by using the following statement waitforsingleobjectsemaphore infinite if the value of the semaphore is the semaphore is in the signaled state and thus is acquired by the calling thread otherwise the calling thread blocks indefinitely as we are specifying infinite until the semaphore returns to the signaled state the equivalent of the signal operation for windows semaphores is the releasesemaphore function this function is passed three parameters the handle of the semaphore how much to increase the value of the semaphore a pointer to the previous value of the semaphore we can use the following statement to increase sem by releasesemaphoresem null both releasesemaphore and releasemutex return a nonzero value if successful and otherwise chapter process synchronization bibliographical notes the mutualexclusion problem was first discussed in a classic paper by dijkstra dekkers algorithm exercise the first correct software solution to the twoprocess mutualexclusion problem was developed by the dutch mathematician t dekker this algorithm also was discussed by dijkstra a simpler solution to the twoprocess mutualexclusion problem has since been presented by peterson figure the semaphore concept was suggested by dijkstra the classic processcoordination problems that we have described are paradigms for a large class of concurrencycontrol problems the boundedbuffer problem and the diningphilosophers problem were suggested in dijkstra and dijkstra the readers writers problem was suggested by courtois et al the criticalregion concept was suggested by hoare and by brinchhansen the monitor concept was developed by brinchhansen hoare gave a complete description of the monitor some details of the locking mechanisms used in solaris were presented in mauro and mcdougall as noted earlier the locking mechanisms used by the kernel are implemented for userlevel threads as well so the same types of locks are available inside and outside the kernel details of windows synchronization can be found in solomon and russinovich love describes synchronization in the linux kernel information on pthreads programming can be found in lewis and berg and butenhof hart describes thread synchronization using windows goetz et al present a detailed discussion of concurrent programming in java as well as the javautilconcurrent package breshears and pacheco provide detailed coverage of synchronization issues in relation to parallel programming lu et al provide a study of concurrency bugs in realworld applications adltabatabai et al discuss transactional memory details on using openmp can be found at httpopenmporg functional programming using erlang and scala is covered in armstrong and odersky et al respectively bibliography adltabatabai et al ar adltabatabai c kozyrakis and b saha unlocking concurrency queue volume number pages armstrong j armstrong programming erlang software for a concurrent world the pragmatic bookshelf breshears c breshears the art of concurrency oreilly associates brinchhansen p brinchhansen structured multiprogramming communications of the acm volume number pages bibliography brinchhansen p brinchhansen operating system principles prentice hall butenhof d butenhof programming with posix threads addisonwesley courtois et al p j courtois f heymans and d l parnas concurrent control with readers and writers communications of the acm volume number pages dijkstra e w dijkstra cooperating sequential processes technical report technological university eindhoven the netherlands dijkstra e w dijkstra hierarchical ordering of sequential processes acta informatica volume number pages goetz et al b goetz t peirls j bloch j bowbeer d holmes and d lea java concurrency in practice addisonwesley hart j m hart windows system programming third edition addisonwesley hoare c a r hoare towards a theory of parallel programming in hoare and perrott pages hoare c a r hoare monitors an operating system structuring concept communications of the acm volume number pages kessels j l w kessels an alternative to event queues for synchronization in monitors communications of the acm volume number pages lewis and berg b lewis and d berg multithreaded programming with pthreads sun microsystems press love r love linux kernel development third edition developers library lu et al s lu s park e seo and y zhou learning from mistakes a comprehensive study on real world concurrency bug characteristics sigplan notices volume number pages mauro and mcdougall j mauro and r mcdougall solaris internals core kernel architecture prentice hall odersky et al m odersky v cremet i dragos g dubochet b emir s mcdirmid s micheloud n mihaylov m schinz e stenman l spoon and m zenger pacheco p s pacheco an introduction to parallel programming morgan kaufmann peterson g l peterson myths about the mutual exclusion problem information processing letters volume number solomon and russinovich d a solomon and m e russinovich inside microsoft windows third edition microsoft press introduction an operating system is a program that manages a computers hardware it also provides a basis for application programs and acts as an intermediary between the computer user and the computer hardware an amazing aspect of operating systems is how they vary in accomplishing these tasks mainframe operating systems are designed primarily to optimize utilization of hardware personal computer pc operating systems support complex games business applications and everything in between operating systems for mobile computers provide an environment in which a user can easily interface with the computer to execute programs thus some operating systems are designed to be convenient others to be efficient and others to be some combination of the two before we can explore the details of computer system operation we need to know something about system structure we thus discuss the basic functions of system startup io and storage early in this chapter we also describe the basic computer architecture that makes it possible to write a functional operating system because an operating system is large and complex it must be created piece by piece each of these pieces should be a welldelineated portion of the system with carefully defined inputs outputs and functions in this chapter we provide a general overview of the major components of a contemporary computer system as well as the functions provided by the operating system additionally we cover several other topics to help set the stage for the remainder of this text data structures used in operating systems computing environments and opensource operating systems chapter objectives to describe the basic organization of computer systems to provide a grand tour of the major components of operating systems to give an overview of the many types of computing environments to explore several opensource operating systems what operating systems do we begin our discussion by looking at the operating systems role in the overall computer system a computer system can be divided roughly into four components the hardware the operating system the application programs and the users figure the hardware the central processing unit cpu the memory and the inputoutput io devices provides the basic computing resources for the system the application programs such as word processors spreadsheets compilers and web browsers define the ways in which these resources are used to solve users computing problems the operating system controls the hardware and coordinates its use among the various application programs for the various users we can also view a computer system as consisting of hardware software and data the operating system provides the means for proper use of these resources in the operation of the computer system an operating system is similar to a government like a government it performs no useful function by itself it simply provides an environment within which other programs can do useful work to understand more fully the operating systems role we next explore operating systems from two viewpoints that of the user and that of the system user view the user s view of the computer varies according to the interface being used most computer users sit in front of a pc consisting of a monitor keyboard mouse and system unit such a system is designed for one user basic concepts in a singleprocessor system only one process can run at a time others must wait until the cpu is free and can be rescheduled the objective of multiprogramming is to have some process running at all times to maximize cpu utilization the idea is relatively simple a process is executed until it must wait typically for the completion of some io request in a simple computer system the cpu then just sits idle all this waiting time is wasted no useful work is accomplished with multiprogramming we try to use this time productively several processes are kept in memory at one time when chapter cpu scheduling load store add store cpu burst read from file wait for io io burst store increment index cpu burst write to file wait for io io burst load store add store cpu burst read from file wait for io io burst figure alternating sequence of cpu and io bursts one process has to wait the operating system takes the cpu away from that process and gives the cpu to another process this pattern continues every time one process has to wait another process can take over use of the cpu scheduling of this kind is a fundamental operatingsystem function almost all computer resources are scheduled before use the cpu is of course one of the primary computer resources thus its scheduling is central to operatingsystem design cpu io burst cycle the success of cpu scheduling depends on an observed property of processes process execution consists of a cycle of cpu execution and io wait processes alternate between these two states process execution begins with a cpu burst that is followed by an io burst which is followed by another cpu burst then another io burst and so on eventually the final cpu burst ends with a system request to terminate execution figure the durations of cpu bursts have been measured extensively although they vary greatly from process to process and from computer to computer they tend to have a frequency curve similar to that shown in figure the curve is generally characterized as exponential or hyperexponential with a large number of short cpu bursts and a small number of long cpu bursts scheduling criteria dispatcher another component involved in the cpuscheduling function is the dispatcher the dispatcher is the module that gives control of the cpu to the process selected by the shortterm scheduler this function involves the following switching context switching to user mode jumping to the proper location in the user program to restart that program the dispatcher should be as fast as possible since it is invoked during every process switch the time it takes for the dispatcher to stop one process and start another running is known as the dispatch latency scheduling algorithms cpu scheduling deals with the problem of deciding which of the processes in the ready queue is to be allocated the cpu there are many different cpuscheduling algorithms in this section we describe several of them firstcome firstserved scheduling by far the simplest cpuscheduling algorithm is the firstcome firstserved fcfs scheduling algorithm with this scheme the process that requests the cpu first is allocated the cpu first the implementation of the fcfs policy is easily managed with a fifo queue when a process enters the ready queue its pcb is linked onto the tail of the queue when the cpu is free it is allocated to the process at the head of the queue the running process is then removed from the queue the code for fcfs scheduling is simple to write and understand on the negative side the average waiting time under the fcfs policy is often quite long consider the following set of processes that arrive at time with the length of the cpu burst given in milliseconds process burst time p p p thread scheduling since defining the best scheduler requires some means by which to select values for all the parameters multipleprocessor scheduling our discussion thus far has focused on the problems of scheduling the cpu in a system with a single processor if multiple cpus are available load sharing becomes possible but scheduling problems become correspondingly more complex many possibilities have been tried and as we saw with singleprocessor cpu scheduling there is no one best solution here we discuss several concerns in multiprocessor scheduling we concentrate on systems in which the processors are identical homogeneous in terms of their functionality we can then use any available processor to run any process in the queue note however that even with homogeneous multiprocessors there are sometimes limitations on scheduling consider a system with an io device attached to a private bus of one processor processes that wish to use that device must be scheduled to run on that processor approaches to realtime cpu scheduling be flushed before the other thread can begin execution on the processor core once this new thread begins execution it begins filling the pipeline with its instructions finegrained or interleaved multithreading switches between threads at a much finer level of granularity typically at the boundary of an instruction cycle however the architectural design of finegrained systems includes logic for thread switching as a result the cost of switching between threads is small notice that a multithreaded multicore processor actually requires two different levels of scheduling on one level are the scheduling decisions that must be made by the operating system as it chooses which software thread to run on each hardware thread logical processor for this level of scheduling the operating system may choose any scheduling algorithm such as those described in section a second level of scheduling specifies how each core decides which hardware thread to run there are several strategies to adopt in this situation the ultrasparc t mentioned earlier uses a simple roundrobin algorithm to schedule the eight hardware threads to each core another example the intel itanium is a dualcore processor with two hardwaremanaged threads per core assigned to each hardware thread is a dynamic urgency value ranging from to with representing the lowest urgency and the highest the itanium identifies five different events that may trigger a thread switch when one of these events occurs the threadswitching logic compares the urgency of the two threads and selects the thread with the highest urgency value to execute on the processor core computersystem organization is increasing mobile operating systems often include not only a core kernel but also middleware a set of software frameworks that provide additional services to application developers for example each of the two most prominent mobile operating systems apples ios and googles android features a core kernel along with middleware that supports databases multimedia and graphics to name a only few operatingsystem examples we turn next to a description of the scheduling policies of the linux windows and solaris operating systems it is important to note that we use the term process scheduling in a general sense here in fact we are describing the scheduling of kernel threads with solaris and windows systems and of tasks with the linux scheduler example linux scheduling process scheduling in linux has had an interesting history prior to version the linux kernel ran a variation of the traditional unix scheduling algorithm algorithm evaluation how do we select a cpuscheduling algorithm for a particular system as we saw in section there are many scheduling algorithms each with its own parameters as a result selecting an algorithm can be difficult the first problem is defining the criteria to be used in selecting an algorithm as we saw in section criteria are often defined in terms of cpu utilization response time or throughput to select an algorithm we must first define the relative importance of these elements our criteria may include several measures such as these maximizing cpu utilization under the constraint that the maximum response time is second maximizing throughput such that turnaround time is on average linearly proportional to total execution time once the selection criteria have been defined we want to evaluate the algorithms under consideration we next describe the various evaluation methods we can use deterministic modeling one major class of evaluation methods is analytic evaluation analytic evaluation uses the given algorithm and the system workload to produce a formula or number to evaluate the performance of the algorithm for that workload deterministic modeling is one type of analytic evaluation this method takes a particular predetermined workload and defines the performance of each algorithm for that workload for example assume that we have the workload shown below all five processes arrive at time in the order given with the length of the cpu burst given in milliseconds process burst time p p p p p consider the fcfs sjf and rr quantum milliseconds scheduling algorithms for this set of processes which algorithm would give the minimum average waiting time for the fcfs algorithm we would execute the processes as p p p p p summary cpu scheduling is the task of selecting a waiting process from the ready queue and allocating the cpu to it the cpu is allocated to the selected process by the dispatcher firstcome firstserved fcfs scheduling is the simplest scheduling algorithm but it can cause short processes to wait for very long processes shortestjobfirst sjf scheduling is provably optimal providing the shortest average waiting time implementing sjf scheduling is difficult however because predicting the length of the next cpu burst is difficult the sjf algorithm is a special case of the general priority scheduling algorithm which simply allocates the cpu to the highestpriority process both priority and sjf scheduling may suffer from starvation aging is a technique to prevent starvation roundrobin rr scheduling is more appropriate for a timeshared interactive system rr scheduling allocates the cpu to the first process in the ready queue for q time units where q is the time quantum after q time units if the process has not relinquished the cpu it is preempted and the process is put at the tail of the ready queue the major problem is the selection of the time quantum if the quantum is too large rr scheduling degenerates to fcfs scheduling if the quantum is too small scheduling overhead in the form of contextswitch time becomes excessive practice exercises the fcfs algorithm is nonpreemptive the rr algorithm is preemptive the sjf and priority algorithms may be either preemptive or nonpreemptive multilevel queue algorithms allow different algorithms to be used for different classes of processes the most common model includes a foreground interactive queue that uses rr scheduling and a background batch queue that uses fcfs scheduling multilevel feedback queues allow processes to move from one queue to another many contemporary computer systems support multiple processors and allow each processor to schedule itself independently typically each processor maintains its own private queue of processes or threads all of which are available to run additional issues related to multiprocessor scheduling include processor affinity load balancing and multicore processing a realtime computer system requires that results arrive within a deadline period results arriving after the deadline has passed are useless hard realtime systems must guarantee that realtime tasks are serviced within their deadline periods soft realtime systems are less restrictive assigning realtime tasks higher scheduling priority than other tasks realtime scheduling algorithms include ratemonotonic and earliestdeadlinefirst scheduling ratemonotonic scheduling assigns tasks that require the cpu more often a higher priority than tasks that require the cpu less often earliestdeadlinefirst scheduling assigns priority according to upcoming deadlines the earlier the deadline the higher the priority proportional share scheduling divides up processor time into shares and assigning each process a number of shares thus guaranteeing each process a proportional share of cpu time the posix pthread api provides various features for scheduling realtime threads as well operating systems supporting threads at the kernel level must schedule threads not processes for execution this is the case with solaris and windows both of these systems schedule threads using preemptive prioritybased scheduling algorithms including support for realtime threads the linux process scheduler uses a prioritybased algorithm with realtime support as well the scheduling algorithms for these three operating systems typically favor interactive over cpubound processes the wide variety of scheduling algorithms demands that we have methods to select among algorithms analytic methods use mathematical analysis to determine the performance of an algorithm simulation methods determine performance by imitating the scheduling algorithm on a representative sample of processes and computing the resulting performance however simulation can at best provide an approximation of actual system performance the only reliable technique for evaluating a scheduling algorithm is to implement the algorithm on an actual system and monitor its performance in a realworld environment practice exercises a cpuscheduling algorithm determines an order for the execution of its scheduled processes given n processes to be scheduled on one processor how many different schedules are possible give a formula in terms of n chapter cpu scheduling explain the difference between preemptive and nonpreemptive scheduling suppose that the following processes arrive for execution at the times indicated each process will run for the amount of time listed in answering the questions use nonpreemptive scheduling and base all decisions on the information you have at the time the decision must be made process arrival time burst time p p p a what is the average turnaround time for these processes with the fcfs scheduling algorithm b what is the average turnaround time for these processes with the sjf scheduling algorithm c the sjf algorithm is supposed to improve performance but notice that we chose to run process p at time because we did not know that two shorter processes would arrive soon compute what the average turnaround time will be if the cpu is left idle for the first unit and then sjf scheduling is used remember that processes p and p are waiting during this idle time so their waiting time may increase this algorithm could be called futureknowledge scheduling what advantage is there in having different timequantum sizes at different levels of a multilevel queueing system many cpuscheduling algorithms are parameterized for example the rr algorithm requires a parameter to indicate the time slice multilevel feedback queues require parameters to define the number of queues the scheduling algorithm for each queue the criteria used to move processes between queues and so on these algorithms are thus really sets of algorithms for example the set of rr algorithms for all time slices and so on one set of algorithms may include another for example the fcfs algorithm is the rr algorithm with an infinite time quantum what if any relation holds between the following pairs of algorithm sets a priority and sjf b multilevel feedback queues and fcfs c priority and fcfs d rr and sjf suppose that a scheduling algorithm at the level of shortterm cpu scheduling favors those processes that have used the least processor exercises time in the recent past why will this algorithm favor iobound programs and yet not permanently starve cpubound programs distinguish between pcs and scs scheduling assume that an operating system maps userlevel threads to the kernel using the manytomany model and that the mapping is done through the use of lwps furthermore the system allows program developers to create realtime threads is it necessary to bind a realtime thread to an lwp the traditional unix scheduler enforces an inverse relationship between priority numbers and priorities the higher the number the lower the priority the scheduler recalculates process priorities once per second using the following function priority recent cpu usage base where base and recent cpu usage refers to a value indicating how often a process has used the cpu since priorities were last recalculated assume that recent cpu usage is for process p for process p and for process p what will be the new priorities for these three processes when priorities are recalculated based on this information does the traditional unix scheduler raise or lower the relative priority of a cpubound process exercises why is it important for the scheduler to distinguish iobound programs from cpubound programs discuss how the following pairs of scheduling criteria conflict in certain settings a cpu utilization and response time b average turnaround time and maximum waiting time c io device utilization and cpu utilization one technique for implementing lottery scheduling works by assigning processes lottery tickets which are used for allocating cpu time whenever a scheduling decision has to be made a lottery ticket is chosen at random and the process holding that ticket gets the cpu the btv operating system implements lottery scheduling by holding a lottery times each second with each lottery winner getting milliseconds of cpu time milliseconds second describe how the btv scheduler can ensure that higherpriority threads receive more attention from the cpu than lowerpriority threads in chapter we discussed possible race conditions on various kernel data structures most scheduling algorithms maintain a run queue which lists processes eligible to run on a processor on multicore systems there are two general options each processing core has its own run chapter cpu scheduling queue or a single run queue is shared by all processing cores what are the advantages and disadvantages of each of these approaches consider the exponential average formula used to predict the length of the next cpu burst what are the implications of assigning the following values to the parameters used by the algorithm a and milliseconds b and milliseconds a variation of the roundrobin scheduler is the regressive roundrobin scheduler this scheduler assigns each process a time quantum and a priority the initial value of a time quantum is milliseconds however every time a process has been allocated the cpu and uses its entire time quantum does not block for io milliseconds is added to its time quantum and its priority level is boosted the time quantum for a process can be increased to a maximum of milliseconds when a process blocks before using its entire time quantum its time quantum is reduced by milliseconds but its priority remains the same what type of process cpubound or iobound does the regressive roundrobin scheduler favor explain consider the following set of processes with the length of the cpu burst given in milliseconds process burst time priority p p p p p the processes are assumed to have arrived in the order p p p p p all at time a draw four gantt charts that illustrate the execution of these processes using the following scheduling algorithms fcfs sjf nonpreemptive priority a larger priority number implies a higher priority and rr quantum b what is the turnaround time of each process for each of the scheduling algorithms in part a c what is the waiting time of each process for each of these scheduling algorithms d which of the algorithms results in the minimum average waiting time over all processes the following processes are being scheduled using a preemptive roundrobin scheduling algorithm each process is assigned a numerical priority with a higher number indicating a higher relative priority in addition to the processes listed below the system also has an idle exercises task which consumes no cpu resources and is identified as pidle this task has priority and is scheduled whenever the system has no other available processes to run the length of a time quantum is units if a process is preempted by a higherpriority process the preempted process is placed at the end of the queue thread priority burst arrival p p p p p p a show the scheduling order of the processes using a gantt chart b what is the turnaround time for each process c what is the waiting time for each process d what is the cpu utilization rate the nice command is used to set the nice value of a process on linux as well as on other unix systems explain why some systems may allow any user to assign a process a nice value yet allow only the root user to assign nice values which of the following scheduling algorithms could result in starvation a firstcome firstserved b shortest job first c round robin d priority consider a variant of the rr scheduling algorithm in which the entries in the ready queue are pointers to the pcbs a what would be the effect of putting two pointers to the same process in the ready queue b what would be two major advantages and two disadvantages of this scheme c how would you modify the basic rr algorithm to achieve the same effect without the duplicate pointers consider a system running ten iobound tasks and one cpubound task assume that the iobound tasks issue an io operation once for every millisecond of cpu computing and that each io operation takes milliseconds to complete also assume that the contextswitching overhead is millisecond and that all processes are longrunning tasks describe the cpu utilization for a roundrobin scheduler when chapter cpu scheduling a the time quantum is millisecond b the time quantum is milliseconds consider a system implementing multilevel queue scheduling what strategy can a computer user employ to maximize the amount of cpu time allocated to the users process consider a preemptive priority scheduling algorithm based on dynamically changing priorities larger priority numbers imply higher priority when a process is waiting for the cpu in the ready queue but not running its priority changes at a rate when it is running its priority changes at a rate all processes are given a priority of when they enter the ready queue the parameters and can be set to give many different scheduling algorithms a what is the algorithm that results from b what is the algorithm that results from explain the differences in how much the following scheduling algorithms discriminate in favor of short processes a fcfs b rr c multilevel feedback queues using the windows scheduling algorithm determine the numeric priority of each of the following threads a a thread in the realtime priority class with a relative priority of normal b a thread in the above normal priority class with a relative priority of highest c a thread in the below normal priority class with a relative priority of above normal assuming that no threads belong to the realtime priority class and that none may be assigned a time critical priority what combination of priority class and priority corresponds to the highest possible relative priority in windows scheduling consider the scheduling algorithm in the solaris operating system for timesharing threads a what is the time quantum in milliseconds for a thread with priority with priority b assume that a thread with priority has used its entire time quantum without blocking what new priority will the scheduler assign this thread c assume that a thread with priority blocks for io before its time quantum has expired what new priority will the scheduler assign this thread bibliographical notes assume that two tasks a and b are running on a linux system the nice values of aand b are and respectively using the cfs scheduler as a guide describe how the respective values of vruntime vary between the two processes given each of the following scenarios both a and b are cpubound a is iobound and b is cpubound a is cpubound and b is iobound discuss ways in which the priority inversion problem could be addressed in a realtime system also discuss whether the solutions could be implemented within the context of a proportional share scheduler under what circumstances is ratemonotonic scheduling inferior to earliestdeadlinefirst scheduling in meeting the deadlines associated with processes consider two processes p and p where p t p and t a can these two processes be scheduled using ratemonotonic scheduling illustrate your answer using a gantt chart such as the ones in figure figure b illustrate the scheduling of these two processes using earliestdeadlinefirst edf scheduling explain why interrupt and dispatch latency times must be bounded in a hard realtime system bibliographical notes feedback queues were originally implemented on the ctss system described in corbato et al this feedback queue scheduling system was analyzed by schrage the preemptive priority scheduling algorithm of exercise was suggested by kleinrock the scheduling algorithms for hard realtime systems such as rate monotonic scheduling and earliestdeadlinefirst scheduling are presented in liu and layland anderson et al lewis and berg and philbin et al discuss thread scheduling multicore scheduling is examined in mcnairy and bhatia and kongetira et al fisher hall et al and lowney et al describe scheduling techniques that take into account information regarding process execution times from previous runs fairshare schedulers are covered by henry woodside and kay and lauder scheduling policies used in the unix v operating system are described by bach those for unix freebsd are presented by mckusick and nevilleneil and those for the mach operating system are discussed by black love and mauerer cover scheduling in chapter cpu scheduling linux faggioli et al discuss adding an edf scheduler to the linux kernel details of the ule scheduler can be found in roberson solaris scheduling is described by mauro and mcdougall russinovich and solomon discusses scheduling in windows internals butenhof and lewis and berg describe scheduling in pthreads systems siddha et al discuss scheduling challenges on multicore systems bibliography anderson et al t e anderson e d lazowska and h m levy the performance implications of thread management alternatives for sharedmemory multiprocessors ieee transactions on computers volume number pages bach m j bach the design of the unix operating system prentice hall black d l black scheduling support for concurrency and parallelism in the mach operating system ieee computer volume number pages butenhof d butenhof programming with posix threads addisonwesley corbato et al f j corbato m merwindaggett and r c daley an experimental timesharing system proceedings of the afips fall joint computer conference pages faggioli et al d faggioli f checconi m trimarchi and c scordino an edf scheduling class for the linux kernel proceedings of the th realtime linux workshop fisher j a fisher trace scheduling a technique for global microcode compaction ieee transactions on computers volume number pages hall et al l hall d shmoys and j wein scheduling to minimize average completion time offline and online algorithms soda acmsiam symposium on discrete algorithms henry g henry the fair share scheduler att bell laboratories technical journal kay and lauder j kay and p lauder a fair share scheduler communications of the acm volume number pages kleinrock l kleinrock queueing systems volume ii computer applications wileyinterscience kongetira et al p kongetira k aingaran and k olukotun niagara a way multithreaded sparc processor ieee micro magazine volume number pages bibliography lewis and berg b lewis and d berg multithreaded programming with pthreads sun microsystems press liu and layland c l liu and j w layland scheduling algorithms for multiprogramming in a hard realtime environment communications of the acm volume number pages love r love linux kernel development third edition developers library lowney et al p g lowney s m freudenberger t j karzes w d lichtenstein r p nix j s odonnell and j c ruttenberg the multiflow trace scheduling compiler journal of supercomputing volume number pages mauerer w mauerer professional linux kernel architecture john wiley and sons mauro and mcdougall j mauro and r mcdougall solaris internals core kernel architecture prentice hall mckusick and nevilleneil m k mckusick and g v nevilleneil the design and implementation of the freebsd unix operating system addison wesley mcnairy and bhatia c mcnairy and r bhatia montecito a dual core dualthreaded itanium processor ieee micro magazine volume number pages philbin et al j philbin j edler o j anshus c c douglas and k li thread scheduling for cache locality architectural support for programming languages and operating systems pages roberson j roberson ule a modern scheduler for freebsd proceedings of the usenix bsdcon conference pages russinovich and solomon m e russinovich and d a solomon windows internals including windows server and windows vista fifth edition microsoft press schrage l e schrage the queue mgi with feedback to lower priority queues management science volume pages siddha et al s siddha v pallipadi and a mallick process scheduling challenges in the era of multicore processors intel technology journal volume number woodside c woodside controllability of computer performance tradeoffs obtained using controlledshare queue schedulers ieee transactions on software engineering volume se number pages system model a system consists of a finite number of resources to be distributed among a number of competing processes the resources may be partitioned into several chapter deadlocks types or classes each consisting of some number of identical instances cpu cycles files and io devices such as printers and dvd drives are examples of resource types if a system has two cpus then the resource type cpu has two instances similarly the resource type printer may have five instances if a process requests an instance of a resource type the allocation of any instance of the type should satisfy the request if it does not then the instances are not identical and the resource type classes have not been defined properly for example a system may have two printers these two printers may be defined to be in the same resource class if no one cares which printer prints which output however if one printer is on the ninth floor and the other is in the basement then people on the ninth floor may not see both printers as equivalent and separate resource classes may need to be defined for each printer chapter discussed various synchronization tools such as mutex locks and semaphores these tools are also considered system resources and they are a common source of deadlock however a lock is typically associated with protecting a specific data structure that is one lock may be used to protect access to a queue another to protect access to a linked list and so forth for that reason each lock is typically assigned its own resource class and definition is not a problem a process must request a resource before using it and must release the resource after using it a process may request as many resources as it requires to carry out its designated task obviously the number of resources requested may not exceed the total number of resources available in the system in other words a process can not request three printers if the system has only two under the normal mode of operation a process may utilize a resource in only the following sequence request the process requests the resource if the request can not be granted immediately for example if the resource is being used by another process then the requesting process must wait until it can acquire the resource use the process can operate on the resource for example if the resource is a printer the process can print on the printer release the process releases the resource the request and release of resources may be system calls as explained in chapter examples are the request and release device open and close file and allocate and free memory system calls similarly as we saw in chapter the request and release of semaphores can be accomplished through the wait and signal operations on semaphores or through acquire and release of a mutex lock for each use of a kernelmanaged resource by a process or thread the operating system checks to make sure that the process has requested and has been allocated the resource a system table records whether each resource is free or allocated for each resource that is allocated the table also records the process to which it is allocated if a process requests a resource that is currently allocated to another process it can be added to a queue of processes waiting for this resource a set of processes is in a deadlocked state when every process in the set is waiting for an event that can be caused only by another process in the set the deadlock characterization events with which we are mainly concerned here are resource acquisition and release the resources may be either physical resources for example printers tape drives memory space and cpu cycles or logical resources for example semaphores mutex locks and files however other types of events may result in deadlocks for example the ipc facilities discussed in chapter to illustrate a deadlocked state consider a system with three cd rw drives suppose each of three processes holds one of these cd rw drives if each process now requests another drive the three processes will be in a deadlocked state each is waiting for the event cd rw is released which can be caused only by one of the other waiting processes this example illustrates a deadlock involving the same resource type deadlocks may also involve different resource types for example consider a system with one printer and one dvd drive suppose that process pi is holding the dvd and process pj is holding the printer if pi requests the printer and pj requests the dvd drive a deadlock occurs developers of multithreaded applications must remain aware of the possibility of deadlocks the locking tools presented in chapter are designed to avoid race conditions however in using these tools developers must pay careful attention to how locks are acquired and released otherwise deadlock can occur as illustrated in the diningphilosophers problem in section methods for handling deadlocks generally speaking we can deal with the deadlock problem in one of three ways we can use a protocol to prevent or avoid deadlocks ensuring that the system will never enter a deadlocked state we can allow the system to enter a deadlocked state detect it and recover we can ignore the problem altogether and pretend that deadlocks never occur in the system the third solution is the one used by most operating systems including linux and windows it is then up to the application developer to write programs that handle deadlocks next we elaborate briefly on each of the three methods for handling deadlocks then in sections through we present detailed algorithms before proceeding we should mention that some researchers have argued that none of the basic approaches alone is appropriate for the entire spectrum of resourceallocation problems in operating systems the basic approaches can be combined however allowing us to select an optimal approach for each class of resources in a system to ensure that deadlocks never occur the system can use either a deadlockprevention or a deadlockavoidance scheme deadlock prevention provides a set of methods to ensure that at least one of the necessary conditions section can not hold these methods prevent deadlocks by constraining how requests for resources can be made we discuss these methods in section deadlock avoidance requires that the operating system be given additional information in advance concerning which resources a process will request and use during its lifetime with this additional knowledge the operating system can decide for each request whether or not the process should wait to decide whether the current request can be satisfied or must be delayed the system must consider the resources currently available the resources currently allocated to each process and the future requests and releases of each process we discuss these schemes in section if a system does not employ either a deadlockprevention or a deadlockavoidance algorithm then a deadlock situation may arise in this environment the system can provide an algorithm that examines the state of the system to determine whether a deadlock has occurred and an algorithm to recover from the deadlock if a deadlock has indeed occurred we discuss these issues in section and section deadlock prevention in the absence of algorithms to detect and recover from deadlocks we may arrive at a situation in which the system is in a deadlocked state yet has no way of recognizing what has happened in this case the undetected deadlock will cause the systems performance to deteriorate because resources are being held by processes that can not run and because more and more processes as they make requests for resources will enter a deadlocked state eventually the system will stop functioning and will need to be restarted manually although this method may not seem to be a viable approach to the deadlock problem it is nevertheless used in most operating systems as mentioned earlier expense is one important consideration ignoring the possibility of deadlocks is cheaper than the other approaches since in many systems deadlocks occur infrequently say once per year the extra expense of the other methods may not seem worthwhile in addition methods used to recover from other conditions may be put to use to recover from deadlock in some circumstances a system is in a frozen state but not in a deadlocked state we see this situation for example with a realtime process running at the highest priority or any process running on a nonpreemptive scheduler and never returning control to the operating system the system must have manual recovery methods for such conditions and may simply use those techniques for deadlock recovery deadlock avoidance void transactionaccount from account to double amount mutex lock lock lock get lockfrom lock get lockto acquirelock acquirelock withdrawfrom amount depositto amount releaselock releaselock figure deadlock example with lock ordering deadlock is possible if two threads simultaneously invoke the transaction function transposing different accounts that is one thread might invoke transactionchecking account savings account and another might invoke transactionsavings account checking account we leave it as an exercise for students to fix this situation deadlock detection the content of the matrix need is defined to be max allocation and is as follows need abc p p p p p we claim that the system is currently in a safe state indeed the sequence p p p p p satisfies the safety criteria suppose now that process p requests one additional instance of resource type a and two instances of resource type c so req uest to decide whether this request can be immediately granted we first check that req uest available that is that which is true we then pretend that this request has been fulfilled and we arrive at the following new state allocation need available abc abc abc p p p p p we must determine whether this new system state is safe to do so we execute our safety algorithm and find that the sequence p p p p p satisfies the safety requirement hence we can immediately grant the request of process p you should be able to see however that when the system is in this state a request for by p can not be granted since the resources are not available furthermore a request for by p can not be granted even though the resources are available since the resulting state is unsafe we leave it as a programming exercise for students to implement the bankers algorithm computersystem architecture in section we introduced the general structure of a typical computer system a computer system can be organized in a number of different ways which we recovery from deadlock processes but also the specific process that caused the deadlock in reality each of the deadlocked processes is a link in the cycle in the resource graph so all of them jointly caused the deadlock if there are many different resource types one request may create many cycles in the resource graph each cycle completed by the most recent request and caused by the one identifiable process of course invoking the deadlockdetection algorithm for every resource request will incur considerable overhead in computation time a less expensive alternative is simply to invoke the algorithm at defined intervals for example once per hour or whenever cpu utilization drops below percent a deadlock eventually cripples system throughput and causes cpu utilization to drop if the detection algorithm is invoked at arbitrary points in time the resource graph may contain many cycles in this case we generally can not tell which of the many deadlocked processes caused the deadlock summary a deadlocked state occurs when two or more processes are waiting indefinitely for an event that can be caused only by one of the waiting processes there are three principal methods for dealing with deadlocks use some protocol to prevent or avoid deadlocks ensuring that the system will never enter a deadlocked state allow the system to enter a deadlocked state detect it and then recover ignore the problem altogether and pretend that deadlocks never occur in the system the third solution is the one used by most operating systems including linux and windows a deadlock can occur only if four necessary conditions hold simultaneously in the system mutual exclusion hold and wait no preemption and circular wait to prevent deadlocks we can ensure that at least one of the necessary conditions never holds a method for avoiding deadlocks rather than preventing them requires that the operating system have a priori information about how each process will utilize system resources the bankers algorithm for example requires a priori information about the maximum number of each resource class that each process may request using this information we can define a deadlockavoidance algorithm if a system does not employ a protocol to ensure that deadlocks will never occur then a detectionandrecovery scheme may be employed a deadlockdetection algorithm must be invoked to determine whether a deadlock has occurred if a deadlock is detected the system must recover either by terminating some of the deadlocked processes or by preempting resources from some of the deadlocked processes where preemption is used to deal with deadlocks three issues must be addressed selecting a victim rollback and starvation in a system that selects victims for rollback primarily on the basis of cost factors starvation may occur and the selected process can never complete its designated task researchers have argued that none of the basic approaches alone is appropriate for the entire spectrum of resourceallocation problems in operating systems the basic approaches can be combined however allowing us to select an optimal approach for each class of resources in a system practice exercises list three examples of deadlocks that are not related to a computersystem environment suppose that a system is in an unsafe state show that it is possible for the processes to complete their execution without entering a deadlocked state chapter deadlocks consider the following snapshot of a system allocation max available abcd abcd abcd p p p p p answer the following questions using the bankers algorithm a what is the content of the matrix need b is the system in a safe state c if a request from process p arrives for can the request be granted immediately a possible method for preventing deadlocks is to have a single higherorder resource that must be requested before any other resource for example if multiple threads attempt to access the synchronization objects a e deadlock is possible such synchronization objects may include mutexes semaphores condition variables and the like we can prevent the deadlock by adding a sixth object f whenever a thread wants to acquire the synchronization lock for any object a e it must first acquire the lock for object f this solution is known as containment the locks for objects a e are contained within the lock for object f compare this scheme with the circularwait scheme of section prove that the safety algorithm presented in section requires an order of m n operations consider a computer system that runs jobs per month and has no deadlockprevention or deadlockavoidance scheme deadlocks occur about twice per month and the operator must terminate and rerun about ten jobs per deadlock each job is worth about two dollars in cpu time and the jobs terminated tend to be about half done when they are aborted a systems programmer has estimated that a deadlockavoidance algorithm like the bankers algorithm could be installed in the system with an increase of about percent in the average execution time per job since the machine currently has percent idle time all jobs per month could still be run although turnaround time would increase by about percent on average a what are the arguments for installing the deadlockavoidance algorithm b what are the arguments against installing the deadlockavoidance algorithm exercises can a system detect that some of its processes are starving if you answer yes explain how it can if you answer no explain how the system can deal with the starvation problem consider the following resourceallocation policy requests for and releases of resources are allowed at any time if a request for resources can not be satisfied because the resources are not available then we check any processes that are blocked waiting for resources if a blocked process has the desired resources then these resources are taken away from it and are given to the requesting process the vector of resources for which the blocked process is waiting is increased to include the resources that were taken away for example a system has three resource types and the vector available is initialized to if process p asks for it gets them if p asks for it gets them then if p asks for it is blocked resource not available if p now asks for it gets the available one as well as one that was allocated to p since p is blocked ps allocation vector goes down to and its need vector goes up to a can deadlock occur if you answer yes give an example if you answer no specify which necessary condition can not occur b can indefinite blocking occur explain your answer suppose that you have coded the deadlockavoidance safety algorithm and now have been asked to implement the deadlockdetection algorithm can you do so by simply using the safety algorithm code and redefining maxi waitingi allocationi where waitingi is a vector specifying the resources for which process i is waiting and allocationi is as defined in section explain your answer is it possible to have a deadlock involving only one singlethreaded process explain your answer exercises consider the traffic deadlock depicted in figure a show that the four necessary conditions for deadlock hold in this example b state a simple rule for avoiding deadlocks in this system assume a multithreaded application uses only readerwriter locks for synchronization applying the four necessary conditions for deadlock is deadlock still possible if multiple readerwriter locks are used the program example shown in figure doesnt always lead to deadlock describe what role the cpu scheduler plays and how it can contribute to deadlock in this program chapter deadlocks figure traffic deadlock for exercise in section we describe a situation in which we prevent deadlock by ensuring that all locks are acquired in a certain order however we also point out that deadlock is possible in this situation if two threads simultaneously invoke the transaction function fix the transaction function to prevent deadlocks compare the circularwait scheme with the various deadlockavoidance schemes like the bankers algorithm with respect to the following issues a runtime overheads b system throughput in a real computer system neither the resources available nor the demands of processes for resources are consistent over long periods months resources break or are replaced new processes come and go and new resources are bought and added to the system if deadlock is controlled by the bankers algorithm which of the following changes can be made safely without introducing the possibility of deadlock and under what circumstances a increase available new resources added b decrease available resource permanently removed from system c increase max for one process the process needs or wants more resources than allowed d decrease max for one process the process decides it does not need that many resources exercises e increase the number of processes f decrease the number of processes consider a system consisting of four resources of the same type that are shared by three processes each of which needs at most two resources show that the system is deadlock free consider a system consisting of m resources of the same type being shared by n processes a process can request or release only one resource at a time show that the system is deadlock free if the following two conditions hold a the maximum need of each process is between one resource and m resources b the sum of all maximum needs is less than m n consider the version of the diningphilosophers problem in which the chopsticks are placed at the center of the table and any two of them can be used by a philosopher assume that requests for chopsticks are made one at a time describe a simple rule for determining whether a particular request can be satisfied without causing deadlock given the current allocation of chopsticks to philosophers consider again the setting in the preceding question assume now that each philosopher requires three chopsticks to eat resource requests are still issued one at a time describe some simple rules for determining whether a particular request can be satisfied without causing deadlock given the current allocation of chopsticks to philosophers we can obtain the bankers algorithm for a single resource type from the general bankers algorithm simply by reducing the dimensionality of the various arrays by show through an example that we can not implement the multipleresourcetype bankers scheme by applying the singleresourcetype scheme to each resource type individually consider the following snapshot of a system allocation max abcd abcd p p p p p using the bankers algorithm determine whether or not each of the following states is unsafe if the state is safe illustrate the order in which the processes may complete otherwise illustrate why the state is unsafe a available b available chapter deadlocks consider the following snapshot of a system allocation max available abcd abcd abcd p p p p p answer the following questions using the bankers algorithm a illustrate that the system is in a safe state by demonstrating an order in which the processes may complete b if a request from process p arrives for can the request be granted immediately c if a request from process p arrives for can the request be granted immediately what is the optimistic assumption made in the deadlockdetection algorithm how can this assumption be violated a singlelane bridge connects the two vermont villages of north tunbridge and south tunbridge farmers in the two villages use this bridge to deliver their produce to the neighboring town the bridge can become deadlocked if a northbound and a southbound farmer get on the bridge at the same time vermont farmers are stubborn and are unable to back up using semaphores andor mutex locks design an algorithm in pseudocode that prevents deadlock initially do not be concerned about starvation the situation in which northbound farmers prevent southbound farmers from using the bridge or vice versa modify your solution to exercise so that it is starvationfree programming problems implement your solution to exercise using posix synchronization in particular represent northbound and southbound farmers as separate threads once a farmer is on the bridge the associated thread will sleep for a random period of time representing traveling across the bridge design your program so that you can create several threads representing the northbound and southbound farmers programming projects programming projects bankers algorithm for this project you will write a multithreaded program that implements the bankers algorithm discussed in section several customers request and release resources from the bank the banker will grant a request only if it leaves the system in a safe state a request that leaves the system in an unsafe state will be denied this programming assignment combines three separate topics multithreading preventing race conditions and deadlock avoidance the banker the banker will consider requests from n customers for m resources types as outlined in section the banker will keep track of the resources using the following data structures these may be any values define number of customers define number of resources the available amount of each resource int availablenumber of resources the maximum demand of each customer int maximumnumber of customersnumber of resources the amount currently allocated to each customer int allocationnumber of customersnumber of resources the remaining need of each customer int neednumber of customersnumber of resources the customers create n customer threads that request and release resources from the bank the customers will continually loop requesting and then releasing random numbers of resources the customers requests for resources will be bounded by their respective values in the need array the banker will grant a request if it satisfies the safety algorithm outlined in section if a request does not leave the system in a safe state the banker will deny it function prototypes for requesting and releasing resources are as follows int request resourcesint customer num int request int release resourcesint customer num int release these two functions should return if successful the request has been granted and if unsuccessful multiple threads customers will concurrently chapter deadlocks access shared data through these two functions therefore access must be controlled through mutex locks to prevent race conditions both the pthreads and windows apis provide mutex locks the use of pthreads mutex locks is covered in section mutex locks for windows systems are described in the project entitled producerconsumer problem at the end of chapter implementation you should invoke your program by passing the number of resources of each type on the command line for example if there were three resource types with ten instances of the first type five of the second type and seven of the third type you would invoke your program follows aout the available array would be initialized to these values you may initialize the maximum array which holds the maximum demand of each customer using any method you find convenient bibliographical notes most research involving deadlock was conducted many years ago dijkstra was one of the first and most influential contributors in the deadlock area holt was the first person to formalize the notion of deadlocks in terms of an allocationgraph model similar to the one presented in this chapter starvation was also covered by holt hyman provided the deadlock example from the kansas legislature a study of deadlock handling is provided in levine the various prevention algorithms were suggested by havender who devised the resourceordering scheme for the ibm os system the bankers algorithm for avoiding deadlocks was developed for a single resource type by dijkstra and was extended to multiple resource types by habermann the deadlockdetection algorithm for multiple instances of a resource type which is described in section was presented by coffman et al bach describes how many of the algorithms in the traditional unix kernel handle deadlock solutions to deadlock problems in networks are discussed in works such as culler et al and rodeheffer and schroeder the witness lockorder verifier is presented in baldwin bibliography bach m j bach the design of the unix operating system prentice hall baldwin j baldwin locking in the multithreaded freebsd kernel usenix bsd bibliography coffman et al e g coffman m j elphick and a shoshani system deadlocks computing surveys volume number pages culler et al d e culler j p singh and a gupta parallel computer architecture a hardwaresoftware approach morgan kaufmann publishers inc dijkstra e w dijkstra cooperating sequential processes technical report technological university eindhoven the netherlands habermann a n habermann prevention of system deadlocks communications of the acm volume number pages havender j w havender avoiding deadlock in multitasking systems ibm systems journal volume number pages holt r c holt some deadlock properties of computer systems computing surveys volume number pages hyman d hyman the columbus chicken statute and more bonehead legislation s greene press levine g levine defining deadlock operating systems review volume number rodeheffer and schroeder t l rodeheffer and m d schroeder automatic reconfiguration in autonet proceedings of the acm symposium on operating systems principles pages part three memory management the main purpose of a computer system is to execute programs these programs together with the data they access must be at least partially in main memory during execution to improve both the utilization of the cpu and the speed of its response to users a generalpurpose computer must keep several processes in memory many memorymanagement schemes exist reflecting various approaches and the effectiveness of each algorithm depends on the situation selection of a memorymanagement scheme for a system depends on many factors especially on the hardware design of the system most algorithms require hardware support background as we saw in chapter memory is central to the operation of a modern computer system memory consists of a large array of bytes each with its own address the cpu fetches instructions from memory according to the value of the program counter these instructions may cause additional loading from and storing to specific memory addresses a typical instructionexecution cycle for example first fetches an instruction from memory the instruction is then decoded and may cause operands to be fetched from memory after the instruction has been executed on the operands results may be stored back in memory the memory unit sees only chapter main memory a stream of memory addresses it does not know how they are generated by the instruction counter indexing indirection literal addresses and so on or what they are for instructions or data accordingly we can ignore how a program generates a memory address we are interested only in the sequence of memory addresses generated by the running program we begin our discussion by covering several issues that are pertinent to managing memory basic hardware the binding of symbolic memory addresses to actual physical addresses and the distinction between logical and physical addresses we conclude the section with a discussion of dynamic linking and shared libraries basic hardware main memory and the registers built into the processor itself are the only generalpurpose storage that the cpu can access directly there are machine instructions that take memory addresses as arguments but none that take disk addresses therefore any instructions in execution and any data being used by the instructions must be in one of these directaccess storage devices if the data are not in memory they must be moved there before the cpu can operate on them registers that are built into the cpu are generally accessible within one cycle of the cpu clock most cpus can decode instructions and perform simple operations on register contents at the rate of one or more operations per clock tick the same can not be said of main memory which is accessed via a transaction on the memory bus completing a memory access may take many cycles of the cpu clock in such cases the processor normally needs to stall since it does not have the data required to complete the instruction that it is executing this situation is intolerable because of the frequency of memory accesses the remedy is to add fast memory between the cpu and main memory typically on the cpu chip for fast access such a cache was described in section to manage a cache built into the cpu the hardware automatically speeds up memory access without any operatingsystem control not only are we concerned with the relative speed of accessing physical memory but we also must ensure correct operation for proper system operation we must protect the operating system from access by user processes on multiuser systems we must additionally protect user processes from one another this protection must be provided by the hardware because the operating system doesnt usually intervene between the cpu and its memory accesses because of the resulting performance penalty hardware implements this production in several different ways as we show throughout the chapter here we outline one possible implementation we first need to make sure that each process has a separate memory space separate perprocess memory space protects the processes from each other and is fundamental to having multiple processes loaded in memory for concurrent execution to separate memory spaces we need the ability to determine the range of legal addresses that the process may access and to ensure that the process can access only these legal addresses we can provide this protection by using two registers usually a base and a limit as illustrated in figure the base register holds the smallest legal physical memory address the limit register specifies the size of the range for example if the base register holds swapping a process must be in memory to be executed a process however can be swapped temporarily out of memory to a backing store and then brought back into memory for continued execution figure contiguous memory allocation the main memory must accommodate both the operating system and the various user processes we therefore need to allocate main memory in the most efficient way possible this section explains one early method contiguous memory allocation the memory is usually divided into two partitions one for the resident operating system and one for the user processes we can place the operating system in either low memory or high memory the major factor affecting this decision is the location of the interrupt vector since the interrupt vector is often in low memory programmers usually place the operating system in low memory as well thus in this text we discuss only the situation in which segmentation as weve already seen the users view of memory is not the same as the actual physical memory this is equally true of the programmers view of memory indeed dealing with memory in terms of its physical properties is inconvenient to both the operating system and the programmer what if the hardware could provide a memory mechanism that mapped the programmers view to the actual physical memory the system would have more freedom to manage memory while the programmer would have a more natural programming environment paging segmentation permits the physical address space of a process to be noncontiguous structure of the page table in this section we explore some of the most common techniques for structuring the page table including hierarchical paging hashed page tables and inverted page tables hierarchical paging most modern computer systems support a large logical address space to in such an environment the page table itself becomes excessively large for example consider a system with a bit logical address space if the page size in such a system is kb then a page table may consist of up to million entries assuming that each entry consists of bytes each process may need up to mb of physical address space for the page table alone clearly we would not want to allocate the page table contiguously in main memory one simple solution to this problem is to divide the page table into smaller pieces we can accomplish this division in several ways one way is to use a twolevel paging algorithm in which the page table itself is also paged figure for example consider again the system with a bit logical address space and a page size of kb a logical address is divided into a page number consisting of bits and a page offset consisting of bits because we page the page table the page number is further divided outer page table page of page table page table memory figure a twolevel pagetable scheme example intel and bit architectures physical page can not have two or more shared virtual addresses a simple technique for addressing this issue is to allow the page table to contain only one mapping of a virtual address to the shared physical address this means that references to virtual addresses that are not mapped result in page faults oracle sparc solaris consider as a final example a modern bit cpu and operating system that are tightly integrated to provide lowoverhead virtual memory solaris running on the sparc cpu is a fully bit operating system and as such has to solve the problem of virtual memory without using up all of its physical memory by keeping multiple levels of page tables its approach is a bit complex but solves the problem efficiently using hashed page tables there are two hash tables one for the kernel and one for all user processes each maps memory addresses from virtual to physical memory each hashtable entry represents a contiguous area of mapped virtual memory which is more efficient than having a separate hashtable entry for each page each entry has a base address and a span indicating the number of pages the entry represents virtualtophysical translation would take too long if each address required searching through a hash table so the cpu implements a tlb that holds translation table entries ttes for fast hardware lookups a cache of these ttes reside in a translation storage buffer tsb which includes an entry per recently accessed page when a virtual address reference occurs the hardware searches the tlb for a translation if none is found the hardware walks through the inmemory tsb looking for the tte that corresponds to the virtual address that caused the lookup this tlb walk functionality is found on many modern cpus if a match is found in the tsb the cpu copies the tsb entry into the tlb and the memory translation completes if no match is found in the tsb the kernel is interrupted to search the hash table the kernel then creates a tte from the appropriate hash table and stores it in the tsb for automatic loading into the tlb by the cpu memorymanagement unit finally the interrupt handler returns control to the mmu which completes the address translation and retrieves the requested byte or word from main memory example arm architecture although intel chips have dominated the personal computer market for over years chips for mobile devices such as smartphones and tablet computers often instead run on bit arm processors interestingly whereas intel both designs and manufactures chips arm only designs them it then licenses its designs to chip manufacturers apple has licensed the arm design for its iphone and ipad mobile devices and several androidbased smartphones use arm processors as well the bit arm architecture supports the following page sizes kb and kb pages mb and mb pages termed sections the paging system in use depends on whether a page or a section is being referenced onelevel paging is used for mb and mb sections twolevel paging is used for kb and kb pages address translation with the arm mmu is shown in figure the arm architecture also supports two levels of tlbs at the outer level are two micro tlbs a separate tlb for data and another for instructions the micro tlb supports asids as well at the inner level is a single main tlb address translation begins at the micro tlb level in the case of a miss the main tlb is then checked if both tlbs yield misses a page table walk must be performed in hardware bits outer page inner page offset kb or kb page mb or mb section figure logical address translation in arm summary background the memorymanagement algorithms outlined in chapter are necessary because of one basic requirement the instructions being executed must be chapter virtual memory in physical memory the first approach to meeting this requirement is to place the entire logical address space in physical memory dynamic loading can help to ease this restriction but it generally requires special precautions and extra work by the programmer the requirement that instructions must be in physical memory to be executed seems both necessary and reasonable but it is also unfortunate since it limits the size of a program to the size of physical memory in fact an examination of real programs shows us that in many cases the entire program is not needed for instance consider the following programs often have code to handle unusual error conditions since these errors seldom if ever occur in practice this code is almost never executed arrays lists and tables are often allocated more memory than they actually need an array may be declared by elements even though it is seldom larger than by elements an assembler symbol table may have room for symbols although the average program has less than symbols certain options and features of a program may be used rarely for instance the routines on us government computers that balance the budget have not been used in many years even in those cases where the entire program is needed it may not all be needed at the same time the ability to execute a program that is only partially in memory would confer many benefits a program would no longer be constrained by the amount of physical memory that is available users would be able to write programs for an extremely large virtual address space simplifying the programming task because each user program could take less physical memory more programs could be run at the same time with a corresponding increase in cpu utilization and throughput but with no increase in response time or turnaround time less io would be needed to load or swap user programs into memory so each user program would run faster thus running a program that is not entirely in memory would benefit both the system and the user virtual memory involves the separation of logical memory as perceived by users from physical memory this separation allows an extremely large virtual memory to be provided for programmers when only a smaller physical memory is available figure virtual memory makes the task of programming much easier because the programmer no longer needs to worry about the amount of physical memory available she can concentrate instead on the problem to be programmed the virtual address space of a process refers to the logical or virtual view of how a process is stored in memory typically this view is that a process begins at a certain logical address say address and exists in contiguous memory as shown in figure recall from chapter though that in fact demand paging operatingsystem structure operating system job job job job max figure memory layout for a multiprogramming system copyonwrite in section we illustrated how a process can start quickly by demandpaging in the page containing the first instruction however process creation using the fork system call may initially bypass the need for demand paging by using a technique similar to page sharing covered in section this technique provides rapid process creation and minimizes the number of new pages that must be allocated to the newly created process recall that the fork system call creates a child process that is a duplicate of its parent traditionally fork worked by creating a copy of the parents address space for the child duplicating the pages belonging to the parent however considering that many child processes invoke the exec system call immediately after creation the copying of the parents address space may be unnecessary instead we can use a technique known as copyonwrite which works by allowing the parent and child processes initially to share the same pages these shared pages are marked as copyonwrite pages meaning that if either process writes to a shared page a copy of the shared page is created copyonwrite is illustrated in figures and which show the contents of the physical memory before and after process modifies page c for example assume that the child process attempts to modify a page containing portions of the stack with the pages set to be copyonwrite the operating system will create a copy of this page mapping it to the address space of the child process the child process will then modify its copied page and not the page belonging to the parent process obviously when the copyonwrite technique is used only the pages that are modified by either process are copied all unmodified pages can be shared by the parent and child processes note too that only pages that can be modified need be marked as copyonwrite pages that can not be modified pages containing executable code can be shared by the parent and child copyonwrite is a common technique used by several operating systems including windows xp linux and solaris when it is determined that a page is going to be duplicated using copyonwrite it is important to note the location from which the free page will be allocated many operating systems provide a pool of free pages for such requests these free pages are typically allocated when the stack or heap for a process must expand or when there are copyonwrite pages to be managed physical process memory process page a page b page c figure before process modifies page c page replacement physical process memory process page a page b page c copy of page c figure after process modifies page c operating systems typically allocate these pages using a technique known as zerofillondemand zerofillondemand pages have been zeroedout before being allocated thus erasing the previous contents several versions of unix including solaris and linux provide a variation of the fork system call vfork for virtual memory fork that operates differently from fork with copyonwrite with vfork the parent process is suspended and the child process uses the address space of the parent because vfork does not use copyonwrite if the child process changes any pages of the parents address space the altered pages will be visible to the parent once it resumes therefore vfork must be used with caution to ensure that the child process does not modify the address space of the parent vfork is intended to be used when the child process calls exec immediately after creation because no copying of pages takes place vfork is an extremely efficient method of process creation and is sometimes used to implement unix commandline shell interfaces allocation of frames another modification is to keep a pool of free frames but to remember which page was in each frame since the frame contents are not modified when a frame is written to the disk the old page can be reused directly from the freeframe pool if it is needed before that frame is reused no io is needed in this case when a page fault occurs we first check whether the desired page is in the freeframe pool if it is not we must select a free frame and read into it this technique is used in the vaxvms system along with a fifo replacement algorithm when the fifo replacement algorithm mistakenly replaces a page that is still in active use that page is quickly retrieved from the freeframe pool and no io is necessary the freeframe buffer provides protection against the relatively poor but simple fifo replacement algorithm this method is necessary because the early versions of vax did not implement the reference bit correctly some versions of the unix system use this method in conjunction with the secondchance algorithm it can be a useful augmentation to any pagereplacement algorithm to reduce the penalty incurred if the wrong victim page is selected applications and page replacement in certain cases applications accessing data through the operating systems virtual memory perform worse than if the operating system provided no buffering at all a typical example is a database which provides its own memory management and io buffering applications like this understand their memory use and disk use better than does an operating system that is implementing algorithms for generalpurpose use if the operating system is buffering io and the application is doing so as well however then twice the memory is being used for a set of io in another example data warehouses frequently perform massive sequential disk reads followed by computations and writes the lru algorithm would be removing old pages and preserving new ones while the application would more likely be reading older pages than newer ones as it starts its sequential reads again here mfu would actually be more efficient than lru because of such problems some operating systems give special programs the ability to use a disk partition as a large sequential array of logical blocks without any filesystem data structures this array is sometimes called the raw disk and io to this array is termed raw io raw io bypasses all the filesystem services such as file io demand paging file locking prefetching space allocation file names and directories note that although certain applications are more efficient when implementing their own specialpurpose storage services on a raw partition most applications perform better when they use the regular filesystem services thrashing computer systems that is not the case often in systems with multiple cpus section a given cpu can access some sections of main memory faster than it can access others these performance differences are caused by how cpus and memory are interconnected in the system frequently such a system is made up of several system boards each containing multiple cpus and some memory the system boards are interconnected in various ways ranging from system buses to highspeed network connections like infiniband as you might expect the cpus on a particular board can access the memory on that board with less delay than they can access memory on other boards in the system systems in which memory access times vary significantly are known collectively as nonuniform memory access numa systems and without exception they are slower than systems in which memory and cpus are located on the same motherboard managing which page frames are stored at which locations can significantly affect performance in numa systems if we treat memory as uniform in such a system cpus may wait significantly longer for memory access than if we modify memory allocation algorithms to take numa into account similar changes must be made to the scheduling system the goal of these changes is to have memory frames allocated as close as possible to the cpu on which the process is running the definition of close is with minimum latency which typically means on the same system board as the cpu the algorithmic changes consist of having the scheduler track the last cpu on which each process ran if the scheduler tries to schedule each process onto its previous cpu and the memorymanagement system tries to allocate frames for the process close to the cpu on which it is being scheduled then improved cache hits and decreased memory access times will result the picture is more complicated once threads are added for example a process with many running threads may end up with those threads scheduled on many different system boards how is the memory to be allocated in this case solaris solves the problem by creating lgroups for latency groups in the kernel each lgroup gathers together close cpus and memory in fact there is a hierarchy of lgroups based on the amount of latency between the groups solaris tries to schedule all threads of a process and allocate all memory of a process within an lgroup if that is not possible it picks nearby lgroups for the rest of the resources needed this practice minimizes overall memory latency and maximizes cpu cache hit rates operatingsystem operations memory chapter the main advantage of the virtualmemory scheme is that it enables users to run programs that are larger than actual physical memory further it abstracts main memory into a large uniform array of storage separating logical memory as viewed by the user from physical memory this arrangement frees programmers from concern over memorystorage limitations a timesharing system must also provide a file system chapters and the file system resides on a collection of disks hence disk management must be provided chapter in addition a timesharing system provides a mechanism for protecting resources from inappropriate use chapter to ensure orderly execution the system must provide mechanisms for job synchronization and communication chapter and it may ensure that jobs do not get stuck in a deadlock forever waiting for one another chapter memorymapped files consider a sequential read of a file on disk using the standard system calls open read and write each file access requires a system call and disk access alternatively we can use the virtual memory techniques discussed so far to treat file io as routine memory accesses this approach known as memory mapping a file allows a part of the virtual address space to be logically associated with the file as we shall see this can lead to significant performance increases basic mechanism memory mapping a file is accomplished by mapping a disk block to a page or pages in memory initial access to the file proceeds through ordinary demand paging resulting in a page fault however a pagesized portion of the file is read from the file system into a physical page some systems may opt to read allocating kernel memory when a process running in user mode requests additional memory pages are allocated from the list of free page frames maintained by the kernel this list is typically populated using a pagereplacement algorithm such as those discussed in section and most likely contains free pages scattered throughout physical memory as explained earlier remember too that if a user process requests a single byte of memory internal fragmentation will result as the process will be granted an entire page frame kernel memory is often allocated from a freememory pool different from the list used to satisfy ordinary usermode processes there are two primary reasons for this the kernel requests memory for data structures of varying sizes some of which are less than a page in size as a result the kernel must use memory conservatively and attempt to minimize waste due to fragmentation this is especially important because many operating systems do not subject kernel code or data to the paging system pages allocated to usermode processes do not necessarily have to be in contiguous physical memory however certain hardware devices interact directly with physical memory without the benefit of a virtual memory interface and consequently may require memory residing in physically contiguous pages in the following sections we examine two strategies for managing free memory that is assigned to kernel processes the buddy system and slab allocation buddy system the buddy system allocates memory from a fixedsize segment consisting of physically contiguous pages memory is allocated from this segment using a powerof allocator which satisfies requests in units sized as a power of kb kb kb and so forth a request in units not appropriately sized is rounded up to the next highest power of for example a request for kb is satisfied with a kb segment lets consider a simple example assume the size of a memory segment is initially kb and the kernel requests kb of memory the segment is initially divided into two buddies which we will call al and ar each kb in size one of these buddies is further divided into two kb buddies bl and br however the nexthighest power of from kb is kb so either bl or br is again divided into two kb buddies cl and cr one of these other considerations chunks the size of the objects being represented thus when the kernel requests memory for an object the slab allocator returns the exact amount of memory required to represent the object memory requests can be satisfied quickly the slab allocation scheme is thus particularly effective for managing memory when objects are frequently allocated and deallocated as is often the case with requests from the kernel the act of allocating and releasing memory can be a timeconsuming process however objects are created in advance and thus can be quickly allocated from the cache furthermore when the kernel has finished with an object and releases it it is marked as free and returned to its cache thus making it immediately available for subsequent requests from the kernel the slab allocator first appeared in the solaris kernel because of its generalpurpose nature this allocator is now also used for certain usermode memory requests in solaris linux originally used the buddy system however beginning with version the linux kernel adopted the slab allocator recent distributions of linux now include two other kernel memory allocators the slob and slub allocators linux refers to its slab implementation as slab the slob allocator is designed for systems with a limited amount of memory such as embedded systems slob which stands for simple list of blocks works by maintaining three lists of objects small for objects less than bytes medium for objects less than bytes and large for objects less than bytes memory requests are allocated from an object on an appropriately sized list using a firstfit policy beginning with version the slub allocator replaced slab as the default allocator for the linux kernel slub addresses performance issues with slab allocation by reducing much of the overhead required by the slab allocator one change is to move the metadata that is stored with each slab under slab allocation to the page structure the linux kernel uses for each page additionally slub removes the percpu queues that the slab allocator maintains for objects in each cache for systems with a large number of processors the amount of memory allocated to these queues was not insignificant thus slub provides better performance as the number of processors on a system increases operatingsystem examples lock bits are used in various situations frequently some or all of the operatingsystem kernel is locked into memory many operating systems can not tolerate a page fault caused by the kernel or by a specific kernel module including the one performing memory management user processes may also need to lock pages into memory a database process may want to manage a chunk of memory for example moving blocks between disk and memory itself because it has the best knowledge of how it is going to use its data such pinning of pages in memory is fairly common and most operating systems have a system call allowing an application to request that a region of its logical address space be pinned note that this feature could be abused and could cause stress on the memorymanagement algorithms therefore an application frequently requires special privileges to make such a request another use for a lock bit involves normal page replacement consider the following sequence of events a lowpriority process faults selecting a replacement frame the paging system reads the necessary page into memory ready to continue the lowpriority process enters the ready queue and waits for the cpu since it is a lowpriority process it may not be selected by the cpu scheduler for a time while the lowpriority process waits a highpriority process faults looking for a replacement the paging system sees a page that is in memory but has not been referenced or modified it is the page that the lowpriority process just brought in this page looks like a perfect replacement it is clean and will not need to be written out and it apparently has not been used for a long time whether the highpriority process should be able to replace the lowpriority process is a policy decision after all we are simply delaying the lowpriority process for the benefit of the highpriority process however we are wasting the effort spent to bring in the page for the lowpriority process if we decide to prevent replacement of a newly broughtin page until it can be used at least once then we can use the lock bit to implement this mechanism when a page is selected for replacement its lock bit is turned on it remains on until the faulting process is again dispatched using a lock bit can be dangerous the lock bit may get turned on but never turned off should this situation occur because of a bug in the operating system for example the locked frame becomes unusable on a singleuser system the overuse of locking would hurt only the user doing the locking multiuser systems must be less trusting of users for instance solaris allows locking hints but it is free to disregard these hints if the freeframe pool becomes too small or if an individual process requests that too many pages be locked in memory summary it is desirable to be able to execute a process whose logical address space is larger than the available physical address space virtual memory is a technique that enables us to map a large logical address space onto a smaller physical memory virtual memory allows us to run extremely large processes and to raise the degree of multiprogramming increasing cpu utilization further it frees application programmers from worrying about memory availability in addition with virtual memory several processes can share system libraries and memory with virtual memory we can also use an efficient type of process creation known as copyonwrite wherein parent and child processes share actual pages of memory virtual memory is commonly implemented by demand paging pure demand paging never brings in a page until that page is referenced the first reference causes a page fault to the operating system the operatingsystem kernel consults an internal table to determine where the page is located on the backing store it then finds a free frame and reads the page in from the backing store the page table is updated to reflect this change and the instruction that caused the page fault is restarted this approach allows a process to run even though its entire memory image is not in main memory at once as long as the pagefault rate is reasonably low performance is acceptable we can use demand paging to reduce the number of frames allocated to a process this arrangement can increase the degree of multiprogramming allowing more processes to be available for execution at one time and in theory at least the cpu utilization of the system it also allows processes to be run even though their memory requirements exceed the total available physical memory such processes run in virtual memory if total memory requirements exceed the capacity of physical memory then it may be necessary to replace pages from memory to free frames for new pages various pagereplacement algorithms are used fifo page replacement is easy to program but suffers from beladys anomaly optimal page replacement requires future knowledge lru replacement is an approximation of optimal page replacement but even it may be difficult to implement most pagereplacement algorithms such as the secondchance algorithm are approximations of lru replacement in addition to a pagereplacement algorithm a frameallocation policy is needed allocation can be fixed suggesting local page replacement or dynamic suggesting global replacement the workingset model assumes that processes execute in localities the working set is the set of pages in the current locality accordingly each process should be allocated enough frames for its current working set if a process does not have enough memory for its working set it will thrash providing enough frames to each process to avoid thrashing may require process swapping and scheduling most operating systems provide features for memory mapping files thus allowing file io to be treated as routine memory access the win api implements shared memory through memory mapping of files practice exercises kernel processes typically require memory to be allocated using pages that are physically contiguous the buddy system allocates memory to kernel processes in units sized according to a power of which often results in fragmentation slab allocators assign kernel data structures to caches associated with slabs which are made up of one or more physically contiguous pages with slab allocation no memory is wasted due to fragmentation and memory requests can be satisfied quickly in addition to requiring us to solve the major problems of page replacement and frame allocation the proper design of a paging system requires that we consider prepaging page size tlb reach inverted page tables program structure io interlock and page locking and other issues practice exercises under what circumstances do page faults occur describe the actions taken by the operating system when a page fault occurs assume that you have a pagereference string for a process with m frames initially all empty the pagereference string has length p and n distinct page numbers occur in it answer these questions for any pagereplacement algorithms a what is a lower bound on the number of page faults b what is an upper bound on the number of page faults consider the page table shown in figure for a system with bit virtual and physical addresses and with byte pages the list of free page frames is d e f that is d is at the head of the list e is second and f is last page page frame c a b figure page table for exercise chapter virtual memory convert the following virtual addresses to their equivalent physical addresses in hexadecimal all numbers are given in hexadecimal a dash for a page frame indicates that the page is not in memory ef ff consider the following pagereplacement algorithms rank these algorithms on a fivepoint scale from bad to perfect according to their pagefault rate separate those algorithms that suffer from beladys anomaly from those that do not a lru replacement b fifo replacement c optimal replacement d secondchance replacement discuss the hardware support required to support demand paging an operating system supports a paged virtual memory the central processor has a cycle time of microsecond it costs an additional microsecond to access a page other than the current one pages have words and the paging device is a drum that rotates at revolutions per minute and transfers million words per second the following statistical measurements were obtained from the system one percent of all instructions executed accessed a page other than the current page of the instructions that accessed another page percent accessed a page already in memory when a new page was required the replaced page was modified percent of the time calculate the effective instruction time on this system assuming that the system is running one process only and that the processor is idle during drum transfers consider the twodimensional array a int a new int where a is at location in a paged memory system with pages of size a small process that manipulates the matrix resides in page locations to thus every instruction fetch will be from page for three page frames how many page faults are generated by the following arrayinitialization loops use lru replacement and assume practice exercises that page frame contains the process and the other two are initially empty a for int j j j for int i i i aij b for int i i i for int j j j aij consider the following page reference string how many page faults would occur for the following replacement algorithms assuming one two three four five six and seven frames remember that all frames are initially empty so your first unique pages will cost one fault each lru replacement fifo replacement optimal replacement suppose that you want to use a paging algorithm that requires a reference bit such as secondchance replacement or workingset model but the hardware does not provide one sketch how you could simulate a reference bit even if one were not provided by the hardware or explain why it is not possible to do so if it is possible calculate what the cost would be you have devised a new pagereplacement algorithm that you think may be optimal in some contorted test cases beladys anomaly occurs is the new algorithm optimal explain your answer segmentation is similar to paging but uses variablesized pages define two segmentreplacement algorithms one based on the fifo pagereplacement scheme and the other on the lru pagereplacement scheme remember that since segments are not the same size the segment that is chosen for replacement may be too small to leave enough consecutive locations for the needed segment consider strategies for systems where segments can not be relocated and strategies for systems where they can consider a demandpaged computer system where the degree of multiprogramming is currently fixed at four the system was recently measured to determine utilization of the cpu and the paging disk three alternative results are shown below for each case what is happening can the degree of multiprogramming be increased to increase the cpu utilization is the paging helping a cpu utilization percent disk utilization percent b cpu utilization percent disk utilization percent c cpu utilization percent disk utilization percent chapter virtual memory we have an operating system for a machine that uses base and limit registers but we have modified the machine to provide a page table can the page tables be set up to simulate base and limit registers how can they be or why can they not be exercises assume that a program has just referenced an address in virtual memory describe a scenario in which each of the following can occur if no such scenario can occur explain why tlb miss with no page fault tlb miss and page fault tlb hit and no page fault tlb hit and page fault a simplified view of thread states is ready running and blocked where a thread is either ready and waiting to be scheduled is running on the processor or is blocked for example waiting for io this is illustrated in figure assuming a thread is in the running state answer the following questions and explain your answer a will the thread change state if it incurs a page fault if so to what state will it change b will the thread change state if it generates a tlb miss that is resolved in the page table if so to what state will it change c will the thread change state if an address reference is resolved in the page table if so to what state will it change consider a system that uses pure demand paging a when a process first starts execution how would you characterize the pagefault rate b once the working set for a process is loaded into memory how would you characterize the pagefault rate ready blocked running figure thread state diagram for exercise exercises c assume that a process changes its locality and the size of the new working set is too large to be stored in available free memory identify some options system designers could choose from to handle this situation what is the copyonwrite feature and under what circumstances is its use beneficial what hardware support is required to implement this feature a certain computer provides its users with a virtual memory space of bytes the computer has bytes of physical memory the virtual memory is implemented by paging and the page size is bytes a user process generates the virtual address explain how the system establishes the corresponding physical location distinguish between software and hardware operations assume that we have a demandpaged memory the page table is held in registers it takes milliseconds to service a page fault if an empty frame is available or if the replaced page is not modified and milliseconds if the replaced page is modified memoryaccess time is nanoseconds assume that the page to be replaced is modified percent of the time what is the maximum acceptable pagefault rate for an effective access time of no more than nanoseconds when a page fault occurs the process requesting the page must block while waiting for the page to be brought from disk into physical memory assume that there exists a process with five userlevel threads and that the mapping of user threads to kernel threads is one to one if one user thread incurs a page fault while accessing its stack would the other user threads belonging to the same process also be affected by the page fault that is would they also have to wait for the faulting page to be brought into memory explain consider the following page reference string assuming demand paging with three frames how many page faults would occur for the following replacement algorithms lru replacement fifo replacement optimal replacement the page table shown in figure is for a system with bit virtual and physical addresses and with byte pages the reference bit is set to when the page has been referenced periodically a thread zeroes out all values of the reference bit a dash for a page frame indicates the page is not in memory the pagereplacement algorithm is localized lru and all numbers are provided in decimal a convert the following virtual addresses in hexadecimal to the equivalent physical addresses you may provide answers in either chapter virtual memory page page frame reference bit figure page table for exercise hexadecimal or decimal also set the reference bit for the appropriate entry in the page table xec xad xad x xaca b using the above addresses as a guide provide an example of a logical address in hexadecimal that results in a page fault c from what set of page frames will the lru pagereplacement algorithm choose in resolving a page fault assume that you are monitoring the rate at which the pointer in the clock algorithm moves the pointer indicates the candidate page for replacement what can you say about the system if you notice the following behavior a pointer is moving fast b pointer is moving slow discuss situations in which the least frequently used lfu pagereplacement algorithm generates fewer page faults than the least recently used lru pagereplacement algorithm also discuss under what circumstances the opposite holds discuss situations in which the most frequently used mfu pagereplacement algorithm generates fewer page faults than the least recently used lru pagereplacement algorithm also discuss under what circumstances the opposite holds exercises the vaxvms system uses a fifo replacement algorithm for resident pages and a freeframe pool of recently used pages assume that the freeframe pool is managed using the lru replacement policy answer the following questions a if a page fault occurs and the page does not exist in the freeframe pool how is free space generated for the newly requested page b if a page fault occurs and the page exists in the freeframe pool how is the resident page set and the freeframe pool managed to make space for the requested page c what does the system degenerate to if the number of resident pages is set to one d what does the system degenerate to if the number of pages in the freeframe pool is zero consider a demandpaging system with the following timemeasured utilizations cpu utilization paging disk other io devices for each of the following indicate whether it will or is likely to improve cpu utilization explain your answers a install a faster cpu b install a bigger paging disk c increase the degree of multiprogramming d decrease the degree of multiprogramming e install more main memory f install a faster hard disk or multiple controllers with multiple hard disks g add prepaging to the pagefetch algorithms h increase the page size suppose that a machine provides instructions that can access memory locations using the onelevel indirect addressing scheme what sequence of page faults is incurred when all of the pages of a program are currently nonresident and the first instruction of the program is an indirect memoryload operation what happens when the operating system is using a perprocess frame allocation technique and only two pages are allocated to this process suppose that your replacement policy in a paged system is to examine each page regularly and to discard that page if it has not been used since the last examination what would you gain and what would you lose by using this policy rather than lru or secondchance replacement chapter virtual memory a pagereplacement algorithm should minimize the number of page faults we can achieve this minimization by distributing heavily used pages evenly over all of memory rather than having them compete for a small number of page frames we can associate with each page frame a counter of the number of pages associated with that frame then to replace a page we can search for the page frame with the smallest counter a define a pagereplacement algorithm using this basic idea specifically address these problems i what is the initial value of the counters ii when are counters increased iii when are counters decreased iv how is the page to be replaced selected b how many page faults occur for your algorithm for the following reference string with four page frames c what is the minimum number of page faults for an optimal pagereplacement strategy for the reference string in part b with four page frames consider a demandpaging system with a paging disk that has an average access and transfer time of milliseconds addresses are translated through a page table in main memory with an access time of microsecond per memory access thus each memory reference through the page table takes two accesses to improve this time we have added an associative memory that reduces access time to one memory reference if the pagetable entry is in the associative memory assume that percent of the accesses are in the associative memory and that of those remaining percent or percent of the total cause page faults what is the effective memory access time what is the cause of thrashing how does the system detect thrashing once it detects thrashing what can the system do to eliminate this problem is it possible for a process to have two working sets one representing data and another representing code explain consider the parameter used to define the workingset window in the workingset model when is set to a small value what is the effect on the pagefault frequency and the number of active nonsuspended processes currently executing in the system what is the effect when is set to a very high value in a kb segment memory is allocated using the buddy system using figure as a guide draw a tree illustrating how the following memory requests are allocated request kb programming problems request bytes request bytes request bytes request kb next modify the tree for the following releases of memory perform coalescing whenever possible release bytes release bytes release bytes a system provides support for userlevel and kernellevel threads the mapping in this system is one to one there is a corresponding kernel thread for each user thread does a multithreaded process consist of a a working set for the entire process or b a working set for each thread explain the slaballocation algorithm uses a separate cache for each different object type assuming there is one cache per object type explain why this scheme doesnt scale well with multiple cpus what could be done to address this scalability issue consider a system that allocates pages of different sizes to its processes what are the advantages of such a paging scheme what modifications to the virtual memory system provide this functionality programming problems write a program that implements the fifo lru and optimal pagereplacement algorithms presented in this chapter first generate a random pagereference string where page numbers range from to apply the random pagereference string to each algorithm and record the number of page faults incurred by each algorithm implement the replacement algorithms so that the number of page frames can vary from to assume that demand paging is used repeat exercise this time using windows shared memory in particular using the producer consumer strategy design two programs that communicate with shared memory using the windows api as outlined in section the producer will generate the numbers specified in the collatz conjecture and write them to a shared memory object the consumer will then read and output the sequence of numbers from shared memory in this instance the producer will be passed an integer parameter on the command line specifying how many numbers to produce for example providing on the command line means the producer process will generate the first five numbers chapter virtual memory programming projects designing a virtual memory manager this project consists of writing a program that translates logical to physical addresses for a virtual address space of size bytes your program will read from a file containing logical addresses and using a tlb as well as a page table will translate each logical address to its corresponding physical address and output the value of the byte stored at the translated physical address the goal behind this project is to simulate the steps involved in translating logical to physical addresses specifics your program will read a file containing several bit integer numbers that represent logical addresses however you need only be concerned with bit addresses so you must mask the rightmost bits of each logical address these bits are divided into an bit page number and bit page offset hence the addresses are structured as shown in figure other specifics include the following entries in the page table page size of bytes entries in the tlb frame size of bytes frames physical memory of bytes frames byte frame size additionally your program need only be concerned with reading logical addresses and translating them to their corresponding physical addresses you do not need to support writing to the logical address space address translation your program will translate logical to physical addresses using a tlb and page table as outlined in section first the page number is extracted from the logical address and the tlb is consulted in the case of a tlbhit the frame number is obtained from the tlb in the case of a tlbmiss the page table must be consulted in the latter case either the frame number is obtained page offset number figure address structure programming projects page offset number page frame number number frame tlb hit frame frame frame offset number tlb page frame page physical page memory tlb miss page page table figure a representation of the addresstranslation process from the page table or a page fault occurs a visual representation of the addresstranslation process appears in figure handling page faults your program will implement demand paging as described in section the backing store is represented by the file backing storebin a binary file of size bytes when a page fault occurs you will read in a byte page from the file backing store and store it in an available page frame in physical memory for example if a logical address with page number resulted in a page fault your program would read in page from backing store remember that pages begin at and are bytes in size and store it in a page frame in physical memory once this frame is stored and the page table and tlb are updated subsequent accesses to page will be resolved by either the tlb or the page table you will need to treat backing storebin as a randomaccess file so that you can randomly seek to certain positions of the file for reading we suggest using the standard c library functions for performing io including fopen fread fseek and fclose the size of physical memory is the same as the size of the virtual address space bytes so you do not need to be concerned about page replacements during a page fault later we describe a modification to this project using a smaller amount of physical memory at that point a pagereplacement strategy will be required chapter virtual memory test file we provide the file addressestxt which contains integer values representing logical addresses ranging from the size of the virtual address space your program will open this file read each logical address and translate it to its corresponding physical address and output the value of the signed byte at the physical address how to begin first write a simple program that extracts the page number and offset based on figure from the following integer numbers perhaps the easiest way to do this is by using the operators for bitmasking and bitshifting once you can correctly establish the page number and offset from an integer number you are ready to begin initially we suggest that you bypass the tlb and use only a page table you can integrate the tlb once your page table is working properly remember address translation can work without a tlb the tlb just makes it faster when you are ready to implement the tlb recall that it has only entries so you will need to use a replacement strategy when you update a full tlb you may use either a fifo or an lru policy for updating your tlb how to run your program your program should run as follows aout addressestxt your program will read in the file addressestxt which contains logical addresses ranging from to your program is to translate each logical address to a physical address and determine the contents of the signed byte stored at the correct physical address recall that in the c language the char data type occupies a byte of storage so we suggest using char values your program is to output the following values the logical address being translated the integer value being read from addressestxt the corresponding physical address what your program translates the logical address to the signed byte value stored at the translated physical address we also provide the file correcttxt which contains the correct output values for the file addressestxt you should use this file to determine if your program is correctly translating logical to physical addresses statistics after completion your program is to report the following statistics bibliographical notes pagefault rate the percentage of address references that resulted in page faults tlb hit rate the percentage of address references that were resolved in the tlb since the logical addresses in addressestxt were generated randomly and do not reflect any memory access locality do not expect to have a high tlb hit rate modifications this project assumes that physical memory is the same size as the virtual address space in practice physical memory is typically much smaller than a virtual address space a suggested modification is to use a smaller physical address space we recommend using page frames rather than this change will require modifying your program so that it keeps track of free page frames as well as implementing a pagereplacement policy using either fifo or lru section bibliographical notes demand paging was first used in the atlas system implemented on the manchester university muse computer around kilburn et al another early demandpaging system was multics implemented on the ge system organick virtual memory was added to unix in babaoglu and joy belady et al were the first researchers to observe that the fifo replacement strategy may produce the anomaly that bears beladys name mattson et al demonstrated that stack algorithms are not subject to beladys anomaly the optimal replacement algorithm was presented by belady and was proved to be optimal by mattson et al beladys optimal algorithm is for a fixed allocation prieve and fabry presented an optimal algorithm for situations in which the allocation can vary the enhanced clock algorithm was discussed by carr and hennessy the workingset model was developed by denning discussions concerning the workingset model were presented by denning the scheme for monitoring the pagefault rate was developed by wulf who successfully applied this technique to the burroughs b computer system buddy system memory allocators were described in knowlton peterson and norman and purdom jr and stigler bonwick discussed the slab allocator and bonwick and adams extended the discussion to multiple processors other memoryfitting algorithms can be found in stephenson bays and brent a survey of memoryallocation strategies can be found in wilson et al solomon and russinovich and russinovich and solomon described how windows implements virtual memory mcdougall and mauro chapter virtual memory discussed virtual memory in solaris virtual memory techniques in linux and freebsd were described by love and mckusick and nevilleneil respectively ganapathy and schimmel and navarro et al discussed operating system support for multiple page sizes bibliography babaoglu and joy o babaoglu and w joy converting a swapbased system to do paging in an architecture lacking pagereference bits proceedings of the acm symposium on operating systems principles pages bays c bays a comparison of nextfit firstfit and bestfit communications of the acm volume number pages belady l a belady a study of replacement algorithms for a virtualstorage computer ibm systems journal volume number pages belady et al l a belady r a nelson and g s shedler an anomaly in spacetime characteristics of certain programs running in a paging machine communications of the acm volume number pages bonwick j bonwick the slab allocator an objectcaching kernel memory allocator usenix summer pages bonwick and adams j bonwick and j adams magazines and vmem extending the slab allocator to many cpus and arbitrary resources proceedings of the usenix annual technical conference brent r brent efficient implementation of the firstfit strategy for dynamic storage allocation acm transactions on programming languages and systems volume number pages carr and hennessy w r carr and j l hennessy wsclock a simple and effective algorithm for virtual memory management proceedings of the acm symposium on operating systems principles pages denning p j denning the working set model for program behavior communications of the acm volume number pages denning p j denning working sets past and present ieee transactions on software engineering volume se number pages ganapathy and schimmel n ganapathy and c schimmel general purpose operating system support for multiple page sizes proceedings of the usenix technical conference kilburn et al t kilburn d j howarth r b payne and f h sumner the manchester university atlas operating system part i internal organization computer journal volume number pages bibliography knowlton k c knowlton a fast storage allocator communications of the acm volume number pages love r love linux kernel development third edition developers library mattson et al r l mattson j gecsei d r slutz and i l traiger evaluation techniques for storage hierarchies ibm systems journal volume number pages mcdougall and mauro r mcdougall and j mauro solaris internals second edition prentice hall mckusick and nevilleneil m k mckusick and g v nevilleneil the design and implementation of the freebsd unix operating system addison wesley navarro et al j navarro s lyer p druschel and a cox practical transparent operating system support for superpages proceedings of the usenix symposium on operating systems design and implementation organick e i organick the multics system an examination of its structure mit press peterson and norman j l peterson and t a norman buddy systems communications of the acm volume number pages prieve and fabry b g prieve and r s fabry vmin an optimal variable space pagereplacement algorithm communications of the acm volume number pages purdom jr and stigler p w purdom jr and s m stigler statistical properties of the buddy system j acm volume number pages russinovich and solomon m e russinovich and d a solomon microsoft windows internals fourth edition microsoft press solomon and russinovich d a solomon and m e russinovich inside microsoft windows third edition microsoft press stephenson c j stephenson fast fits a new method for dynamic storage allocation proceedings of the ninth symposium on operating systems principles pages wilson et al p r wilson m s johnstone m neely and d boles dynamic storage allocation a survey and critical review proceedings of the international workshop on memory management pages wulf w a wulf performance monitors for multiprogramming systems proceedings of the acm symposium on operating systems principles pages part four storage management since main memory is usually too small to accommodate all the data and programs permanently the computer system must provide secondary storage to back up main memory modern computer systems use disks as the primary online storage medium for information both programs and data the file system provides the mechanism for online storage of and access to both data and programs residing on the disks a file is a collection of related information defined by its creator the files are mapped by the operating system onto physical devices files are normally organized into directories for ease of use the devices that attach to a computer vary in many aspects some devices transfer a character or a block of characters at a time some can be accessed only sequentially others randomly some transfer data synchronously others asynchronously some are dedicated some shared they can be readonly or read write they vary greatly in speed in many ways they are also the slowest major component of the computer because of all this device variation the operating system needs to provide a wide range of functionality to applications to allow them to control all aspects of the devices one key goal of an operating systems io subsystem is to provide the simplest interface possible to the rest of the system because devices are a performance bottleneck another key is to optimize io for maximum concurrency process management a program does nothing unless its instructions are executed by a cpu a program in execution as mentioned is a process a timeshared user program such as a compiler is a process a wordprocessing program being run by an individual user on a pc is a process a system task such as sending output to a printer can also be a process or at least part of one for now you can consider a process to be a job or a timeshared program but later you will learn that the concept is more general as we shall see in chapter it is possible to provide system calls that allow processes to create subprocesses to execute concurrently a process needs certain resources including cpu time memory files and io devices to accomplish its task these resources are either given to the process when it is created or allocated to it while it is running in addition to the various physical and logical resources that a process obtains when it is created various initialization data input may be passed along for example consider a process whose function is to display the status of a file on the screen of a terminal the process will be given the name of the file as an input and will execute the appropriate instructions and system calls to obtain and display the desired information on the terminal when the process terminates the operating system will reclaim any reusable resources we emphasize that a program by itself is not a process a program is a passive entity like the contents of a file stored on disk whereas a process memory management is an active entity a singlethreaded process has one program counter specifying the next instruction to execute threads are covered in chapter the execution of such a process must be sequential the cpu executes one instruction of the process after another until the process completes further at any time one instruction at most is executed on behalf of the process thus although two processes may be associated with the same program they are nevertheless considered two separate execution sequences a multithreaded process has multiple program counters each pointing to the next instruction to execute for a given thread a process is the unit of work in a system a system consists of a collection of processes some of which are operatingsystem processes those that execute system code and the rest of which are user processes those that execute user code all these processes can potentially execute concurrently by multiplexing on a single cpu for example the operating system is responsible for the following activities in connection with process management scheduling processes and threads on the cpus creating and deleting both user and system processes suspending and resuming processes providing mechanisms for process synchronization providing mechanisms for process communication we discuss processmanagement techniques in chapters through overview of massstorage structure in this section we present a general overview of the physical structure of secondary and tertiary storage devices magnetic disks magnetic disks provide the bulk of secondary storage for modern computer systems conceptually disks are relatively simple figure each disk platter has a flat circular shape like a cd common platter diameters range from to inches the two surfaces of a platter are covered with a magnetic material we store information by recording it magnetically on the platters chapter massstorage structure track t spindle arm assembly sector s cylinder c readwrite head platter arm rotation figure movinghead disk mechanism a readwrite head flies just above each surface of every platter the heads are attached to a disk arm that moves all the heads as a unit the surface of a platter is logically divided into circular tracks which are subdivided into sectors the set of tracks that are at one arm position makes up a cylinder there may be thousands of concentric cylinders in a disk drive and each track may contain hundreds of sectors the storage capacity of common disk drives is measured in gigabytes when the disk is in use a drive motor spins it at high speed most drives rotate to times per second specified in terms of rotations per minute rpm common drives spin at and rpm disk speed has two parts the transfer rate is the rate at which data flow between the drive and the computer the positioning time or randomaccess time consists of two parts the time necessary to move the disk arm to the desired cylinder called the seek time and the time necessary for the desired sector to rotate to the disk head called the rotational latency typical disks can transfer several megabytes of data per second and they have seek times and rotational latencies of several milliseconds because the disk head flies on an extremely thin cushion of air measured in microns there is a danger that the head will make contact with the disk surface although the disk platters are coated with a thin protective layer the head will sometimes damage the magnetic surface this accident is called a head crash a head crash normally can not be repaired the entire disk must be replaced a disk can be removable allowing different disks to be mounted as needed removable magnetic disks generally consist of one platter held in a plastic case to prevent damage while not in the disk drive other forms of removable disks include cds dvds and bluray discs as well as removable flashmemory devices known as flash drives which are a type of solidstate drive disk structure modern magnetic disk drives are addressed as large onedimensional arrays of logical blocks where the logical block is the smallest unit of transfer the size of a logical block is usually bytes although some disks can be lowlevel formatted to have a different logical block size such as bytes this option is described in section the onedimensional array of logical blocks is mapped onto the sectors of the disk sequentially sector is the first sector of the first track on the outermost cylinder the mapping proceeds in order through that track then through the rest of the tracks in that cylinder and then through the rest of the cylinders from outermost to innermost by using this mapping we can at least in theory convert a logical block number into an oldstyle disk address that consists of a cylinder number a track number within that cylinder and a sector number within that track in practice it is difficult to perform this translation for two reasons first most disks have some defective sectors but the mapping hides this by substituting spare sectors from elsewhere on the disk second the number of sectors per track is not a constant on some drives lets look more closely at the second reason on media that use constant linear velocity clv the density of bits per track is uniform the farther a track is from the center of the disk the greater its length so the more sectors it can hold as we move from outer zones to inner zones the number of sectors per track decreases tracks in the outermost zone typically hold percent more sectors than do tracks in the innermost zone the drive increases its rotation speed as the head moves from the outer to the inner tracks to keep the same rate of data moving under the head this method is used in cdrom disk attachment and dvdrom drives alternatively the disk rotation speed can stay constant in this case the density of bits decreases from inner tracks to outer tracks to keep the data rate constant this method is used in hard disks and is known as constant angular velocity cav the number of sectors per track has been increasing as disk technology improves and the outer zone of a disk usually has several hundred sectors per track similarly the number of cylinders per disk has been increasing large disks have tens of thousands of cylinders disk scheduling one of the responsibilities of the operating system is to use the hardware efficiently for the disk drives meeting this responsibility entails having fast storage management to make the computer system convenient for users the operating system provides a uniform logical view of information storage the operating system abstracts from the physical properties of its storage devices to define a logical storage unit the file the operating system maps files onto physical media and accesses these files via the storage devices filesystem management file management is one of the most visible components of an operating system computers can store information on several different types of physical media magnetic disk optical disk and magnetic tape are the most common each of these media has its own characteristics and physical organization each medium is controlled by a device such as a disk drive or tape drive that also has its own unique characteristics these properties include access speed capacity datatransfer rate and access method sequential or random a file is a collection of related information defined by its creator commonly files represent programs both source and object forms and data data files may be numeric alphabetic alphanumeric or binary files may be freeform for example text files or they may be formatted rigidly for example fixed fields clearly the concept of a file is an extremely general one the operating system implements the abstract concept of a file by managing massstorage media such as tapes and disks and the devices that control them in addition files are normally organized into directories to make them easier to use finally when multiple users have access to files it may be desirable to control which user may access a file and how that user may access it for example read write append the operating system is responsible for the following activities in connection with file management creating and deleting files disk management the operating system is responsible for several other aspects of disk management too here we discuss disk initialization booting from disk and badblock recovery swapspace management swapping was first presented in section where we discussed moving entire processes between disk and main memory swapping in that setting occurs when the amount of physical memory reaches a critically low point and processes are moved from memory to swap space to free available memory in practice very few modern operating systems implement swapping in this fashion rather systems now combine swapping with virtual memory techniques chapter and swap pages not necessarily entire processes in fact some systems now use the terms swapping and paging interchangeably reflecting the merging of these two concepts swapspace management is another lowlevel task of the operating system virtual memory uses disk space as an extension of main memory since disk access is much slower than memory access using swap space significantly decreases system performance the main goal for the design and implementation of swap space is to provide the best throughput for the virtual memory system in this section we discuss how swap space is used where swap space is located on disk and how swap space is managed swapspace use swap space is used in various ways by different operating systems depending on the memorymanagement algorithms in use for instance systems that implement swapping may use swap space to hold an entire process image including the code and data segments paging systems may simply store pages that have been pushed out of main memory the amount of swap space needed on a system can therefore vary from a few megabytes of disk space to gigabytes depending on the amount of physical memory the amount of virtual memory it is backing and the way in which the virtual memory is used note that it may be safer to overestimate than to underestimate the amount of swap space required because if a system runs out of swap space it may be forced to abort processes or may crash entirely overestimation wastes disk space that could otherwise be used for files but it does no other harm some systems recommend the amount to be set aside for swap space solaris for example suggests setting swap space equal to the amount by which virtual memory exceeds pageable physical memory in the past linux has suggested raid structure disk drives have continued to get smaller and cheaper so it is now economically feasible to attach many disks to a computer system having a large number of disks in a system presents opportunities for improving the rate at which data can be read or written if the disks are operated in parallel furthermore this setup offers the potential for improving the reliability of data storage because redundant information can be stored on multiple disks thus failure of one disk does not lead to loss of data a variety of diskorganization techniques collectively called redundant arrays of independent disks raid are commonly used to address the performance and reliability issues in the past raids composed of small cheap disks were viewed as a costeffective alternative to large expensive disks today raids are used for stablestorage implementation in chapter we introduced the writeahead log which requires the availability of stable storage by definition information residing in stable storage is never lost to implement such storage we need to replicate the required information summary disk drives are the major secondary storage io devices on most computers most secondary storage devices are either magnetic disks or magnetic tapes although solidstate disks are growing in importance modern disk drives are structured as large onedimensional arrays of logical disk blocks generally these logical blocks are bytes in size disks may be attached to a computer system in one of two ways through the local io ports on the host computer or through a network connection requests for disk io are generated by the file system and by the virtual memory system each request specifies the address on the disk to be referenced in the form of a logical block number diskscheduling algorithms can improve the effective bandwidth the average response time and the variance in response time algorithms such as sstf scan cscan look and clook are designed to make such improvements through strategies for diskqueue ordering performance of diskscheduling algorithms can vary greatly on magnetic disks in contrast because solidstate disks have no moving parts performance varies little among algorithms and quite often a simple fcfs strategy is used performance can be harmed by external fragmentation some systems have utilities that scan the file system to identify fragmented files they then move blocks around to decrease the fragmentation defragmenting a badly fragmented file system can significantly improve performance but the system may have reduced performance while the defragmentation is in progress sophisticated file systems such as the unix fast file system incorporate many strategies to control fragmentation during space allocation so that disk reorganization is not needed the operating system manages the disk blocks first a disk must be lowlevelformatted to create the sectors on the raw hardware new disks usually come preformatted then the disk is partitioned file systems are created and practice exercises boot blocks are allocated to store the systems bootstrap program finally when a block is corrupted the system must have a way to lock out that block or to replace it logically with a spare because an efficient swap space is a key to good performance systems usually bypass the file system and use rawdisk access for paging io some systems dedicate a rawdisk partition to swap space and others use a file within the file system instead still other systems allow the user or system administrator to make the decision by providing both options because of the amount of storage required on large systems disks are frequently made redundant via raid algorithms these algorithms allow more than one disk to be used for a given operation and allow continued operation and even automatic recovery in the face of a disk failure raid algorithms are organized into different levels each level provides some combination of reliability and high transfer rates practice exercises is disk scheduling other than fcfs scheduling useful in a singleuser environment explain your answer explain why sstf scheduling tends to favor middle cylinders over the innermost and outermost cylinders why is rotational latency usually not considered in disk scheduling how would you modify sstf scan and cscan to include latency optimization why is it important to balance filesystem io among the disks and controllers on a system in a multitasking environment what are the tradeoffs involved in rereading code pages from the file system versus using swap space to store them is there any way to implement truly stable storage explain your answer it is sometimes said that tape is a sequentialaccess medium whereas a magnetic disk is a randomaccess medium in fact the suitability of a storage device for random access depends on the transfer size the term streaming transfer rate denotes the rate for a data transfer that is underway excluding the effect of access latency in contrast the effective transfer rate is the ratio of total bytes per total seconds including overhead time such as access latency suppose we have a computer with the following characteristics the level cache has an access latency of nanoseconds and a streaming transfer rate of megabytes per second the main memory has an access latency of nanoseconds and a streaming transfer rate of megabytes per second the magnetic disk has an access latency of milliseconds and a streaming transfer rate of megabytes per second and a tape drive has an access latency of seconds and a streaming transfer rate of megabytes per second chapter massstorage structure a random access causes the effective transfer rate of a device to decrease because no data are transferred during the access time for the disk described what is the effective transfer rate if an average access is followed by a streaming transfer of bytes kilobytes megabyte and megabytes b the utilization of a device is the ratio of effective transfer rate to streaming transfer rate calculate the utilization of the disk drive for each of the four transfer sizes given in part a c suppose that a utilization of percent or higher is considered acceptable using the performance figures given compute the smallest transfer size for disk that gives acceptable utilization d complete the following sentence a disk is a randomaccess device for transfers larger than bytes and is a sequentialaccess device for smaller transfers e compute the minimum transfer sizes that give acceptable utilization for cache memory and tape f when is a tape a randomaccess device and when is it a sequentialaccess device could a raid level organization achieve better performance for read requests than a raid level organization with nonredundant striping of data if so how exercises none of the diskscheduling disciplines except fcfs is truly fair starvation may occur a explain why this assertion is true b describe a way to modify algorithms such as scan to ensure fairness c explain why fairness is an important goal in a timesharing system d give three or more examples of circumstances in which it is important that the operating system be unfair in serving io requests explain why ssds often use an fcfs diskscheduling algorithm suppose that a disk drive has cylinders numbered to the drive is currently serving a request at cylinder and the previous request was at cylinder the queue of pending requests in fifo order is exercises starting from the current head position what is the total distance in cylinders that the disk arm moves to satisfy all the pending requests for each of the following diskscheduling algorithms a fcfs b sstf c scan d look e cscan f clook elementary physics states that when an object is subjected to a constant acceleration a the relationship between distance d and time t is given by d a t suppose that during a seek the disk in exercise accelerates the disk arm at a constant rate for the first half of the seek then decelerates the disk arm at the same rate for the second half of the seek assume that the disk can perform a seek to an adjacent cylinder in millisecond and a fullstroke seek over all cylinders in milliseconds a the distance of a seek is the number of cylinders over which the head moves explain why the seek time is proportional to the square root of the seek distance b write an equation for the seek time as a function of the seek distance this equation should be of the form t x y l where t is the time in milliseconds and l is the seek distance in cylinders c calculate the total seek time for each of the schedules in exercise determine which schedule is the fastest has the smallest total seek time d the percentage speedup is the time saved divided by the original time what is the percentage speedup of the fastest schedule over fcfs suppose that the disk in exercise rotates at rpm a what is the average rotational latency of this disk drive b what seek distance can be covered in the time that you found for part a describe some advantages and disadvantages of using ssds as a caching tier and as a diskdrive replacement compared with using only magnetic disks compare the performance of cscan and scan scheduling assuming a uniform distribution of requests consider the average response time the time between the arrival of a request and the completion of that requests service the variation in response time and the effective chapter massstorage structure bandwidth how does performance depend on the relative sizes of seek time and rotational latency requests are not usually uniformly distributed for example we can expect a cylinder containing the filesystem metadata to be accessed more frequently than a cylinder containing only files suppose you know that percent of the requests are for a small fixed number of cylinders a would any of the scheduling algorithms discussed in this chapter be particularly good for this case explain your answer b propose a diskscheduling algorithm that gives even better performance by taking advantage of this hot spot on the disk consider a raid level organization comprising five disks with the parity for sets of four blocks on four disks stored on the fifth disk how many blocks are accessed in order to perform the following a a write of one block of data b a write of seven continuous blocks of data compare the throughput achieved by a raid level organization with that achieved by a raid level organization for the following a read operations on single blocks b read operations on multiple contiguous blocks compare the performance of write operations achieved by a raid level organization with that achieved by a raid level organization assume that you have a mixed configuration comprising disks organized as raid level and raid level disks assume that the system has flexibility in deciding which disk organization to use for storing a particular file which files should be stored in the raid level disks and which in the raid level disks in order to optimize performance the reliability of a harddisk drive is typically described in terms of a quantity called mean time between failures mtbf although this quantity is called a time the mtbf actually is measured in drivehours per failure a if a system contains disk drives each of which has a hour mtbf which of the following best describes how often a drive failure will occur in that disk farm once per thousand years once per century once per decade once per year once per month once per week once per day once per hour once per minute or once per second b mortality statistics indicate that on the average a us resident has about chance in of dying between the ages of and deduce the mtbf hours for yearolds convert this figure from hours to years what does this mtbf tell you about the expected lifetime of a yearold bibliographical notes c the manufacturer guarantees a millionhour mtbf for a certain model of disk drive what can you conclude about the number of years for which one of these drives is under warranty discuss the relative advantages and disadvantages of sector sparing and sector slipping discuss the reasons why the operating system might require accurate information on how blocks are stored on a disk how could the operating system improve filesystem performance with this knowledge programming problems write a program that implements the following diskscheduling algorithms a fcfs b sstf c scan d cscan e look f clook your program will service a disk with cylinders numbered to the program will generate a random series of cylinder requests and service them according to each of the algorithms listed above the program will be passed the initial position of the disk head as a parameter on the command line and report the total amount of head movement required by each algorithm bibliographical notes services provides an overview of data storage in a variety of modern computing environments teorey and pinkerton present an early comparative analysis of diskscheduling algorithms using simulations that model a disk for which seek time is linear in the number of cylinders crossed scheduling optimizations that exploit disk idle times are discussed in lumb et al kim et al discusses diskscheduling algorithms for ssds discussions of redundant arrays of independent disks raids are presented by patterson et al russinovich and solomon mcdougall and mauro and love discuss file system details in windows solaris and linux respectively the io size and randomness of the workload influence disk performance considerably ousterhout et al and ruemmler and wilkes report numerous interesting workload characteristics for example most files are small most newly created files are deleted soon thereafter most files that chapter massstorage structure are opened for reading are read sequentially in their entirety and most seeks are short the concept of a storage hierarchy has been studied for more than forty years for instance a paper by mattson et al describes a mathematical approach to predicting the performance of a storage hierarchy bibliography kim et al j kim y oh e kim j c d lee and s noh disk schedulers for solid state drivers pages love r love linux kernel development third edition developers library lumb et al c lumb j schindler g r ganger d f nagle and e riedel towards higher disk head utilization extracting free bandwidth from busy disk drives symposium on operating systems design and implementation mattson et al r l mattson j gecsei d r slutz and i l traiger evaluation techniques for storage hierarchies ibm systems journal volume number pages mcdougall and mauro r mcdougall and j mauro solaris internals second edition prentice hall ousterhout et al j k ousterhout h d costa d harrison j a kunze m kupfer and j g thompson a tracedriven analysis of the unix bsd file system proceedings of the acm symposium on operating systems principles pages patterson et al d a patterson g gibson and r h katz a case for redundant arrays of inexpensive disks raid proceedings of the acm sigmod international conference on the management of data pages ruemmler and wilkes c ruemmler and j wilkes unix disk access patterns proceedings of the winter usenix conference pages russinovich and solomon m e russinovich and d a solomon windows internals including windows server and windows vista fifth edition microsoft press services e e services information storage and management storing managing and protecting digital information in classic virtualized and cloud environments wiley teorey and pinkerton t j teorey and t b pinkerton a comparative analysis of disk scheduling policies communications of the acm volume number pages file concept computers can store information on various storage media such as magnetic disks magnetic tapes and optical disks so that the computer system will be convenient to use the operating system provides a uniform logical view of stored information the operating system abstracts from the physical properties of its storage devices to define a logical storage unit the file files are mapped by the operating system onto physical devices these storage devices are usually nonvolatile so the contents are persistent between system reboots chapter filesystem interface a file is a named collection of related information that is recorded on secondary storage from a users perspective a file is the smallest allotment of logical secondary storage that is data can not be written to secondary storage unless they are within a file commonly files represent programs both source and object forms and data data files may be numeric alphabetic alphanumeric or binary files may be free form such as text files or may be formatted rigidly in general a file is a sequence of bits bytes lines or records the meaning of which is defined by the files creator and user the concept of a file is thus extremely general the information in a file is defined by its creator many different types of information may be stored in a file source or executable programs numeric or text data photos music video and so on a file has a certain defined structure which depends on its type a text file is a sequence of characters organized into lines and possibly pages a source file is a sequence of functions each of which is further organized as declarations followed by executable statements an executable file is a series of code sections that the loader can bring into memory and execute file attributes a file is named for the convenience of its human users and is referred to by its name a name is usually a string of characters such as examplec some systems differentiate between uppercase and lowercase characters in names whereas other systems do not when a file is named it becomes independent of the process the user and even the system that created it for instance one user might create the file examplec and another user might edit that file by specifying its name the files owner might write the file to a usb disk send it as an email attachment or copy it across a network and it could still be called examplec on the destination system a files attributes vary from one operating system to another but typically consist of these name the symbolic file name is the only information kept in humanreadable form identifier this unique tag usually a number identifies the file within the file system it is the nonhumanreadable name for the file type this information is needed for systems that support different types of files location this information is a pointer to a device and to the location of the file on that device size the current size of the file in bytes words or blocks and possibly the maximum allowed size are included in this attribute protection accesscontrol information determines who can do reading writing executing and so on time date and user identification this information may be kept for creation last modification and last use these data can be useful for protection security and usage monitoring access methods beginning current position end rewind read or write figure sequentialaccess file functions operate in terms of blocks the conversion from logical records to physical blocks is a relatively simple software problem because disk space is always allocated in blocks some portion of the last block of each file is generally wasted if each block were bytes for example then a file of bytes would be allocated four blocks bytes the last bytes would be wasted the waste incurred to keep everything in units of blocks instead of bytes is internal fragmentation all file systems suffer from internal fragmentation the larger the block size the greater the internal fragmentation directory and disk structure sequential access implementation for direct access reset cp readnext read cp cp cp writenext write cp cp cp figure simulation of sequential access on a directaccess file find a record in the file we first search the index and then use the pointer to access the file directly and to find the desired record for example a retailprice file might list the universal product codes upcs for items with the associated prices each record consists of a digit upc and a digit price for a byte record if our disk has bytes per block we can store records per block a file of records would occupy about blocks million bytes by keeping the file sorted by upc we can define an index consisting of the first upc in each block this index would have entries of digits each or bytes and thus could be kept in memory to find the price of a particular item we can make a binary search of the index from this search we learn exactly which block contains the desired record and access that block this structure allows us to search a large file doing little io with large files the index file itself may become too large to be kept in memory one solution is to create an index for the index file the primary index file contains pointers to secondary index files which point to the actual data items for example ibms indexed sequentialaccess method isam uses a small master index that points to disk blocks of a secondary index the secondary index blocks point to the actual file blocks the file is kept sorted on a defined key to find a particular item we first make a binary search of the master index which provides the block number of the secondary index this block is read in and again a binary search is used to find the block containing the desired record finally this block is searched sequentially in this way any record can be located from its key by at most two directaccess reads figure shows a similar situation as implemented by vms index and relative files protection and security if a computer system has multiple users and allows the concurrent execution of multiple processes then access to data must be regulated for that purpose mechanisms ensure that files memory segments cpu and other resources can be operated on by only those processes that have gained proper authorization from the operating system for example memoryaddressing hardware ensures that a process can execute only within its own address space the timer ensures that no process can gain control of the cpu without eventually relinquishing control devicecontrol registers are not accessible to users so the integrity of the various peripheral devices is protected protection then is any mechanism for controlling the access of processes or users to the resources defined by a computer system this mechanism must provide means to specify the controls to be imposed and to enforce the controls protection can improve reliability by detecting latent errors at the interfaces between component subsystems early detection of interface errors can often prevent contamination of a healthy subsystem by another subsystem that is malfunctioning furthermore an unprotected resource can not defend against use or misuse by an unauthorized or incompetent user a protectionoriented system provides a means to distinguish between authorized and unauthorized usage as we discuss in chapter a system can have adequate protection but still be prone to failure and allow inappropriate access consider a user whose authentication information her means of identifying herself to the system is stolen her data could be copied or deleted even though file and memory protection are working it is the job of security to defend a system from external and internal attacks such attacks spread across a huge range and include viruses and worms denialofservice attacks which use all of a systems resources and so keep legitimate users out of the system identity theft and theft of service unauthorized use of a system prevention of some of these attacks is considered an operatingsystem function on some systems while other systems leave it to policy or additional software due to the alarming rise in security incidents kernel data structures operatingsystem security features represent a fastgrowing area of research and implementation we discuss security in chapter protection and security require the system to be able to distinguish among all its users most operating systems maintain a list of user names and associated user identifiers user ids in windows parlance this is a security id sid these numerical ids are unique one per user when a user logs in to the system the authentication stage determines the appropriate user id for the user that user id is associated with all of the users processes and threads when an id needs to be readable by a user it is translated back to the user name via the user name list in some circumstances we wish to distinguish among sets of users rather than individual users for example the owner of a file on a unix system may be allowed to issue all operations on that file whereas a selected set of users may be allowed only to read the file to accomplish this we need to define a group name and the set of users belonging to that group group functionality can be implemented as a systemwide list of group names and group identifiers a user can be in one or more groups depending on operatingsystem design decisions the users group ids are also included in every associated process and thread in the course of normal system use the user id and group id for a user are sufficient however a user sometimes needs to escalate privileges to gain extra permissions for an activity the user may need access to a device that is restricted for example operating systems provide various methods to allow privilege escalation on unix for instance the setuid attribute on a program causes that program to run with the user id of the owner of the file rather than the current users id the process runs with this effective uid until it turns off the extra privileges or terminates filesystem mounting just as a file must be opened before it is used a file system must be mounted before it can be available to processes on the system more specifically the directory structure may be built out of multiple volumes which must be mounted to make them available within the filesystem name space the mount procedure is straightforward the operating system is given the name of the device and the mount point the location within the file structure where the file system is to be attached some operating systems require that a file system type be provided while others inspect the structures of the device and determine the type of file system typically a mount point is an empty directory for instance on a unix system a file system containing a users home directories might be mounted as home then to access the directory structure within that file system we could precede the directory names with home as in homejane mounting that file system under users would result in the path name usersjane which we could use to reach the same directory next the operating system verifies that the device contains a valid file system it does so by asking the device driver to read the device directory and verifying that the directory has the expected format finally the operating system notes in its directory structure that a file system is mounted at the specified mount point this scheme enables the operating system to traverse its directory structure switching among file systems and even file systems of varying types as appropriate file sharing in the previous sections we explored the motivation for file sharing and some of the difficulties involved in allowing users to share files such file sharing is very desirable for users who want to collaborate and to reduce the effort required to achieve a computing goal therefore useroriented operating systems must accommodate the need to share files in spite of the inherent difficulties in this section we examine more aspects of file sharing we begin by discussing general issues that arise when multiple users share files once multiple users are allowed to share files the challenge is to extend sharing to multiple file systems including remote file systems we discuss that challenge as well finally we consider what to do about conflicting actions occurring on shared files for instance if multiple users are writing to a file should all the writes be allowed to occur or should the operating system protect the users actions from one another multiple users when an operating system accommodates multiple users the issues of file sharing file naming and file protection become preeminent given a directory structure that allows files to be shared by users the system must mediate the file sharing the system can either allow a user to access the files of other users by default or require that a user specifically grant access to the files these are the issues of access control and protection which are covered in section to implement sharing and protection the system must maintain more file and directory attributes than are needed on a singleuser system although many approaches have been taken to meet this requirement most systems have evolved to use the concepts of file or directory owner or user and group the owner is the user who can change attributes and grant access and who has protection one mode of sharing allows users to share the pointer of current location into the file thus the advancing of the pointer by one user affects all sharing users here a file has a single image that interleaves all accesses regardless of their origin in the unix semantics a file is associated with a single physical image that is accessed as an exclusive resource contention for this single image causes delays in user processes session semantics the andrew file system openafs uses the following consistency semantics writes to an open file by a user are not visible immediately to other users that have the same file open once a file is closed the changes made to it are visible only in sessions starting later already open instances of the file do not reflect these changes according to these semantics a file may be associated temporarily with several possibly different images at the same time consequently multiple users are allowed to perform both read and write accesses concurrently on their images of the file without delay almost no constraints are enforced on scheduling accesses immutablesharedfiles semantics a unique approach is that of immutable shared files once a file is declared as shared by its creator it can not be modified an immutable file has two key properties its name may not be reused and its contents may not be altered thus the name of an immutable file signifies that the contents of the file are fixed the implementation of these semantics in a distributed system chapter is simple because the sharing is disciplined readonly summary a file is an abstract data type defined and implemented by the operating system it is a sequence of logical records a logical record may be a byte a line of fixed or variable length or a more complex data item the operating system may specifically support various record types or may leave that support to the application program the major task for the operating system is to map the logical file concept onto physical storage devices such as magnetic disk or tape since the physical record size of the device may not be the same as the logical record size it may be necessary to order logical records into physical records again this task may be supported by the operating system or left for the application program each device in a file system keeps a volume table of contents or a device directory listing the location of the files on the device in addition it is useful to create directories to allow files to be organized a singlelevel directory in a multiuser system causes naming problems since each file must have a unique name a twolevel directory solves this problem by creating a separate directory for each users files the directory lists the files by name and includes the files location on the disk length type owner time of creation time of last use and so on the natural generalization of a twolevel directory is a treestructured directory a treestructured directory allows a user to create subdirectories to organize files acyclicgraph directory structures enable users to share subdirectories and files but complicate searching and deletion a general graph structure allows complete flexibility in the sharing of files and directories but sometimes requires garbage collection to recover unused disk space disks are segmented into one or more volumes each containing a file system or left raw file systems may be mounted into the systems naming practice exercises structures to make them available the naming scheme varies by operating system once mounted the files within the volume are available for use file systems may be unmounted to disable access or for maintenance file sharing depends on the semantics provided by the system files may have multiple readers multiple writers or limits on sharing distributed file systems allow client hosts to mount volumes or directories from servers as long as they can access each other across a network remote file systems present challenges in reliability performance and security distributed information systems maintain user host and access information so that clients and servers can share state information to manage use and access since files are the main informationstorage mechanism in most computer systems file protection is needed access to files can be controlled separately for each type of access read write execute append delete list directory and so on file protection can be provided by access lists passwords or other techniques practice exercises some systems automatically delete all user files when a user logs off or a job terminates unless the user explicitly requests that they be kept other systems keep all files unless the user explicitly deletes them discuss the relative merits of each approach why do some systems keep track of the type of a file while others leave it to the user and others simply do not implement multiple file types which system is better similarly some systems support many types of structures for a files data while others simply support a stream of bytes what are the advantages and disadvantages of each approach could you simulate a multilevel directory structure with a singlelevel directory structure in which arbitrarily long names can be used if your answer is yes explain how you can do so and contrast this scheme with the multilevel directory scheme if your answer is no explain what prevents your simulations success how would your answer change if file names were limited to seven characters explain the purpose of the open and close operations in some systems a subdirectory can be read and written by an authorized user just as ordinary files can be a describe the protection problems that could arise b suggest a scheme for dealing with each of these protection problems consider a system that supports users suppose that you want to allow of these users to be able to access one file a how would you specify this protection scheme in unix chapter filesystem interface b can you suggest another protection scheme that can be used more effectively for this purpose than the scheme provided by unix researchers have suggested that instead of having an access list associated with each file specifying which users can access the file and how we should have a user control list associated with each user specifying which files a user can access and how discuss the relative merits of these two schemes exercises consider a file system in which a file can be deleted and its disk space reclaimed while links to that file still exist what problems may occur if a new file is created in the same storage area or with the same absolute path name how can these problems be avoided the openfile table is used to maintain information about files that are currently open should the operating system maintain a separate table for each user or maintain just one table that contains references to files that are currently being accessed by all users if the same file is being accessed by two different programs or users should there be separate entries in the openfile table explain what are the advantages and disadvantages of providing mandatory locks instead of advisory locks whose use is left to users discretion provide examples of applications that typically access files according to the following methods sequential random some systems automatically open a file when it is referenced for the first time and close the file when the job terminates discuss the advantages and disadvantages of this scheme compared with the more traditional one where the user has to open and close the file explicitly if the operating system knew that a certain application was going to access file data in a sequential manner how could it exploit this information to improve performance give an example of an application that could benefit from operatingsystem support for random access to indexed files discuss the advantages and disadvantages of supporting links to files that cross mount points that is the file link refers to a file that is stored in a different volume some systems provide file sharing by maintaining a single copy of a file other systems maintain several copies one for each of the users sharing the file discuss the relative merits of each approach bibliography discuss the advantages and disadvantages of associating with remote file systems stored on file servers a set of failure semantics different from that associated with local file systems what are the implications of supporting unix consistency semantics for shared access to files stored on remote file systems bibliographical notes database systems and their file structures are described in full in silberschatz et al a multilevel directory structure was first implemented on the multics system organick most operating systems now implement multilevel directory structures these include linux love mac os x singh solaris mcdougall and mauro and all versions of windows russinovich and solomon the network file system nfs designed by sun microsystems allows directory structures to be spread across networked computer systems nfs version is described in rfc httpwwwietforgrfcrfctxt a general discussion of solaris file systems is found in the sun system administration guide devices and file systems httpdocssuncomappdocsdoc dns was first proposed by su and has gone through several revisions since ldap also known as x is a derivative subset of the x distributed directory protocol it was defined by yeong et al and has been implemented on many operating systems bibliography love r love linux kernel development third edition developers library mcdougall and mauro r mcdougall and j mauro solaris internals second edition prentice hall organick e i organick the multics system an examination of its structure mit press russinovich and solomon m e russinovich and d a solomon microsoft windows internals fourth edition microsoft press silberschatz et al a silberschatz h f korth and s sudarshan database system concepts sixth edition mcgrawhill singh a singh mac os x internals a systems approach addisonwesley su z su a distributed system for internet name service network working group request for comments yeong et al w yeong t howes and s kille lightweight directory access protocol network working group request for comments filesystem structure disks provide most of the secondary storage on which file systems are maintained two characteristics make them convenient for this purpose a disk can be rewritten in place it is possible to read a block from the disk modify the block and write it back into the same place a disk can access directly any block of information it contains thus it is simple to access any file either sequentially or randomly and switching from one file to another requires only moving the read write heads and waiting for the disk to rotate we discuss disk structure in great detail in chapter to improve io efficiency io transfers between memory and disk are performed in units of blocks each block has one or more sectors depending chapter on the disk drive sector size varies from bytes to bytes the usual size is bytes file systems provide efficient and convenient access to the disk by allowing data to be stored located and retrieved easily a file system poses two quite different design problems the first problem is defining how the file system should look to the user this task involves defining a file and its attributes the operations allowed on a file and the directory structure for organizing files the second problem is creating algorithms and data structures to map the logical file system onto the physical secondarystorage devices the file system itself is generally composed of many different levels the structure shown in figure is an example of a layered design each level in the design uses the features of lower levels to create new features for use by higher levels the io control level consists of device drivers and interrupt handlers to transfer information between the main memory and the disk system a device driver can be thought of as a translator its input consists of highlevel commands such as retrieve block its output consists of lowlevel hardwarespecific instructions that are used by the hardware controller which interfaces the io device to the rest of the system the device driver usually writes specific bit patterns to special locations in the io controllers memory to tell the controller which device location to act on and what actions to take the details of device drivers and the io infrastructure are covered in chapter the basic file system needs only to issue generic commands to the appropriate device driver to read and write physical blocks on the disk each physical block is identified by its numeric disk address for example drive cylinder track sector this layer also manages the memory buffers and caches that hold various filesystem directory and data blocks a block in the buffer is allocated before the transfer of a disk block can occur when the buffer is full the buffer manager must find more buffer memory or free application programs logical file system fileorganization module basic file system io control devices figure layered file system filesystem implementation directory implementation the selection of directoryallocation and directorymanagement algorithms significantly affects the efficiency performance and reliability of the file system in this section we discuss the tradeoffs involved in choosing one of these algorithms linear list the simplest method of implementing a directory is to use a linear list of file names with pointers to the data blocks this method is simple to program but timeconsuming to execute to create a new file we must first search the directory to be sure that no existing file has the same name then we add a new entry at the end of the directory to delete a file we search the directory for the named file and then release the space allocated to it to reuse the directory entry we can do one of several things we can mark the entry as unused by assigning it a special name such as an allblank name or by including a used unused bit in each entry or we can attach it to a list of free directory entries a third alternative is to copy the last entry in the directory into the freed location and to decrease the length of the directory a linked list can also be used to decrease the time required to delete a file the real disadvantage of a linear list of directory entries is that finding a file requires a linear search directory information is used frequently and users will notice if access to it is slow in fact many operating systems implement a software cache to store the most recently used directory information a cache hit avoids the need to constantly reread the information from disk a sorted list allows a binary search and decreases the average search time however the requirement that the list be kept sorted may complicate creating and deleting files since we may have to move substantial amounts of directory information to maintain a sorted directory a more sophisticated tree data structure such as a balanced tree might help here an advantage of the sorted list is that a sorted directory listing can be produced without a separate sort step hash table another data structure used for a file directory is a hash table here a linear list stores the directory entries but a hash data structure is also used the hash table takes a value computed from the file name and returns a pointer to the file allocation methods name in the linear list therefore it can greatly decrease the directory search time insertion and deletion are also fairly straightforward although some provision must be made for collisions situations in which two file names hash to the same location the major difficulties with a hash table are its generally fixed size and the dependence of the hash function on that size for example assume that we make a linearprobing hash table that holds entries the hash function converts file names into integers from to for instance by using the remainder of a division by if we later try to create a th file we must enlarge the directory hash table say to entries as a result we need a new hash function that must map file names to the range to and we must reorganize the existing directory entries to reflect their new hashfunction values alternatively we can use a chainedoverflow hash table each hash entry can be a linked list instead of an individual value and we can resolve collisions by adding the new entry to the linked list lookups may be somewhat slowed because searching for a name might require stepping through a linked list of colliding table entries still this method is likely to be much faster than a linear search through the entire directory freespace management indexed allocation is more complex if the index block is already in memory then the access can be made directly however keeping the index block in memory requires considerable space if this memory space is not available then we may have to read first the index block and then the desired data block for a twolevel index two indexblock reads might be necessary for an extremely large file accessing a block near the end of the file would require reading in all the index blocks before the needed data block finally could be read thus the performance of indexed allocation depends on the index structure on the size of the file and on the position of the block desired some systems combine contiguous allocation with indexed allocation by using contiguous allocation for small files up to three or four blocks and automatically switching to an indexed allocation if the file grows large since most files are small and contiguous allocation is efficient for small files average performance can be quite good many other optimizations are in use given the disparity between cpu speed and disk speed it is not unreasonable to add thousands of extra instructions to the operating system to save just a few diskhead movements furthermore this disparity is increasing over time to the point where hundreds of thousands of instructions could reasonably be used to optimize head movements efficiency and performance now that we have discussed various blockallocation and directorymanagement options we can further consider their effect on performance and efficient disk use disks tend to represent a major bottleneck in system performance since they are the slowest main computer component in this section we discuss a variety of techniques used to improve the efficiency and performance of secondary storage efficiency the efficient use of disk space depends heavily on the diskallocation and directory algorithms in use for instance unix inodes are preallocated on a volume even an empty disk has a percentage of its space lost to inodes however by preallocating the inodes and spreading them across the volume we improve the file systems performance this improved performance results from the unix allocation and freespace algorithms which try to keep a files data blocks near that files inode block to reduce seek time as another example lets reconsider the clustering scheme discussed in section which improves fileseek and filetransfer performance at the cost of internal fragmentation to reduce this fragmentation bsd unix varies the cluster size as a file grows large clusters are used where they can be filled and small clusters are used for small files and the last cluster of a file this system is described in appendix a the types of data normally kept in a files directory or inode entry also require consideration commonly a last write date is recorded to supply information to the user and to determine whether the file needs to be backed up some systems also keep a last access date so that a user can determine when the file was last read the result of keeping this information is that whenever the file is read a field in the directory structure must be written to that means the block must be read into memory a section changed and the block written back out to disk because operations on disks occur only in block or cluster chunks so any time a file is opened for reading its directory entry must be read and written as well this requirement can be inefficient for frequently accessed files so we must weigh its benefit against its performance cost when designing a file system generally every data item associated with a file needs to be considered for its effect on efficiency and performance consider for instance how efficiency is affected by the size of the pointers used to access data most systems use either bit or bit pointers throughout the operating system using bit pointers limits the size of a file to or gb using bit pointers allows very large file sizes but bit pointers require computing environments linux kernel data structures the data structures used in the linux kernel are available in the kernel source code the include file linuxlisth provides details of the linkedlist data structure used throughout the kernel a queue in linux is known as a kfifo and its implementation can be found in the kfifoc file in the kernel directory of the source code linux also provides a balanced binary search tree implementation using redblack trees details can be found in the include file linuxrbtreeh recovery files and directories are kept both in main memory and on disk and care must be taken to ensure that a system failure does not result in loss of data or in data inconsistency we deal with these issues in this section we also consider how a system can recover from such a failure a system crash can cause inconsistencies among ondisk filesystem data structures such as directory structures freeblock pointers and free fcb pointers many file systems apply changes to these structures in place a typical operation such as creating a file can involve many structural changes within the file system on the disk directory structures are modified fcbs are allocated data blocks are allocated and the free counts for all of these blocks are decreased these changes can be interrupted by a crash and inconsistencies among the structures can result for example the free fcb count might indicate that an fcb had been allocated but the directory structure might not point to the fcb compounding this problem is the caching that operating systems do to optimize io performance some changes may go directly to disk while others may be cached if the cached changes do not reach disk before a crash occurs more corruption is possible in addition to crashes bugs in filesystem implementation disk controllers and even user applications can corrupt a file system file systems have varying methods to deal with corruption depending on the filesystem data structures and algorithms we deal with these issues next consistency checking whatever the cause of corruption a file system must first detect the problems and then correct them for detection a scan of all the metadata on each file system can confirm or deny the consistency of the system unfortunately this scan can take minutes or hours and should occur every time the system boots alternatively a file system can record its state within the filesystem metadata at the start of any metadata change a status bit is set to indicate that the metadata is in flux if all updates to the metadata complete successfully the file system can clear that bit if however the status bit remains set a consistency checker is run the consistency checker a systems program such as fsck in unix compares the data in the directory structure with the data blocks on disk and tries to fix any inconsistencies it finds the allocation and freespacemanagement algorithms dictate what types of problems the checker can find and how successful it will be in fixing them for instance if linked allocation is used and there is a link from any block to its next block then the entire file can be nfs day copy to another medium all files changed since day this is an incremental backup day copy to another medium all files changed since day day n copy to another medium all files changed since day n then go back to day the new cycle can have its backup written over the previous set or onto a new set of backup media using this method we can restore an entire disk by starting restores with the full backup and continuing through each of the incremental backups of course the larger the value of n the greater the number of media that must be read for a complete restore an added advantage of this backup cycle is that we can restore any file accidentally deleted during the cycle by retrieving the deleted file from the backup of the previous day the length of the cycle is a compromise between the amount of backup medium needed and the number of days covered by a restore to decrease the number of tapes that must be read to do a restore an option is to perform a full backup and then each day back up all files that have changed since the full backup in this way a restore can be done via the most recent incremental backup and the full backup with no other incremental backups needed the tradeoff is that more files will be modified each day so each successive incremental backup involves more files and more backup media a user may notice that a particular file is missing or corrupted long after the damage was done for this reason we usually plan to take a full backup from time to time that will be saved forever it is a good idea to store these permanent backups far away from the regular backups to protect against hazard such as a fire that destroys the computer and all the backups too and if the backup cycle reuses media we must take care not to reuse the media too many times if the media wear out it might not be possible to restore any data from the backups example the wafl file system remote operations with the exception of opening and closing files there is an almost onetoone correspondence between the regular unix system calls for file operations and the nfs protocol rpcs thus a remote file operation can be translated directly to the corresponding rpc conceptually nfs adheres to the remoteservice paradigm but in practice buffering and caching techniques are employed for the sake of performance no direct correspondence exists between a remote operation and an rpc instead file blocks and file attributes are fetched by the rpcs and are cached locally future remote operations use the cached data subject to consistency constraints there are two caches the fileattribute inodeinformation cache and the fileblocks cache when a file is opened the kernel checks with the remote server to determine whether to fetch or revalidate the cached attributes the cached file blocks are used only if the corresponding cached attributes are up to date the attribute cache is updated whenever new attributes arrive from the server cached attributes are by default discarded after seconds both readahead and delayedwrite techniques are used between the server and the client clients do not free delayedwrite blocks until the server confirms that the data have been written to disk delayedwrite is retained even when a file is opened concurrently in conflicting modes hence unix semantics section are not preserved tuning the system for performance makes it difficult to characterize the consistency semantics of nfs new files created on a machine may not be visible elsewhere for seconds furthermore writes to a file at one site may or may not be visible at other sites that have this file open for reading new opens of a file observe only the changes that have already been flushed to the server thus nfs provides neither strict emulation of unix semantics nor the session semantics of andrew section in spite of these drawbacks the utility and good performance of the mechanism make it the most widely used multivendordistributed system in operation summary the file system resides permanently on secondary storage which is designed to hold a large amount of data permanently the most common secondarystorage medium is the disk physical disks may be segmented into partitions to control media use and to allow multiple possibly varying file systems on a single spindle these file systems are mounted onto a logical file system architecture to make them available for use file systems are often implemented in a layered or modular structure the lower levels deal with the physical properties of storage devices upper levels deal with symbolic file names and logical properties of files intermediate levels map the logical file concepts into physical device properties any filesystem type can have different structures and algorithms a vfs layer allows the upper layers to deal with each filesystem type uniformly even remote file systems can be integrated into the systems directory structure and acted on by standard system calls via the vfs interface the various files can be allocated space on the disk in three ways through contiguous linked or indexed allocation contiguous allocation can suffer from external fragmentation direct access is very inefficient with linked allocation indexed allocation may require substantial overhead for its index block these algorithms can be optimized in many ways contiguous space can be enlarged through extents to increase flexibility and to decrease external fragmentation indexed allocation can be done in clusters of multiple blocks to increase throughput and to reduce the number of index entries needed indexing in large clusters is similar to contiguous allocation with extents freespace allocation methods also influence the efficiency of diskspace use the performance of the file system and the reliability of secondary storage the methods used include bit vectors and linked lists optimizations include grouping counting and the fat which places the linked list in one contiguous area directorymanagement routines must consider efficiency performance and reliability a hash table is a commonly used method as it is fast and efficient unfortunately damage to the table or a system crash can result in inconsistency between the directory information and the disks contents a consistency checker can be used to repair the damage operatingsystem backup tools allow disk data to be copied to tape enabling the user to recover from data or even disk loss due to hardware failure operating system bug or user error practice exercises network file systems such as nfs use clientserver methodology to allow users to access files and directories from remote machines as if they were on local file systems system calls on the client are translated into network protocols and retranslated into filesystem operations on the server networking and multipleclient access create challenges in the areas of data consistency and performance due to the fundamental role that file systems play in system operation their performance and reliability are crucial techniques such as log structures and caching help improve performance while log structures and raid improve reliability the wafl file system is an example of optimization of performance to match a specific io load practice exercises consider a file currently consisting of blocks assume that the filecontrol block and the index block in the case of indexed allocation is already in memory calculate how many disk io operations are required for contiguous linked and indexed singlelevel allocation strategies if for one block the following conditions hold in the contiguousallocation case assume that there is no room to grow at the beginning but there is room to grow at the end also assume that the block information to be added is stored in memory a the block is added at the beginning b the block is added in the middle c the block is added at the end d the block is removed from the beginning e the block is removed from the middle f the block is removed from the end what problems could occur if a system allowed a file system to be mounted simultaneously at more than one location why must the bit map for file allocation be kept on mass storage rather than in main memory consider a system that supports the strategies of contiguous linked and indexed allocation what criteria should be used in deciding which strategy is best utilized for a particular file one problem with contiguous allocation is that the user must preallocate enough space for each file if the file grows to be larger than the space allocated for it special actions must be taken one solution to this problem is to define a file structure consisting of an initial contiguous area of a specified size if this area is filled the operating system automatically defines an overflow area that is linked to the initial contiguous area if the overflow area is filled another overflow area is allocated compare this implementation of a file with the standard contiguous and linked implementations chapter filesystem implementation how do caches help improve performance why do systems not use more or larger caches if they are so useful why is it advantageous to the user for an operating system to dynamically allocate its internal tables what are the penalties to the operating system for doing so explain how the vfs layer allows an operating system to support multiple types of file systems easily exercises consider a file system that uses a modifed contiguousallocation scheme with support for extents a file is a collection of extents with each extent corresponding to a contiguous set of blocks a key issue in such systems is the degree of variability in the size of the extents what are the advantages and disadvantages of the following schemes a all extents are of the same size and the size is predetermined b extents can be of any size and are allocated dynamically c extents can be of a few fixed sizes and these sizes are predetermined contrast the performance of the three techniques for allocating disk blocks contiguous linked and indexed for both sequential and random file access what are the advantages of the variant of linked allocation that uses a fat to chain together the blocks of a file consider a system where free space is kept in a freespace list a suppose that the pointer to the freespace list is lost can the system reconstruct the freespace list explain your answer b consider a file system similar to the one used by unix with indexed allocation how many disk io operations might be required to read the contents of a small local file at abc assume that none of the disk blocks is currently being cached c suggest a scheme to ensure that the pointer is never lost as a result of memory failure some file systems allow disk storage to be allocated at different levels of granularity for instance a file system could allocate kb of disk space as a single kb block or as eight byte blocks how could we take advantage of this flexibility to improve performance what modifications would have to be made to the freespace management scheme in order to support this feature discuss how performance optimizations for file systems might result in difficulties in maintaining the consistency of the systems in the event of computer crashes programming problems consider a file system on a disk that has both logical and physical block sizes of bytes assume that the information about each file is already in memory for each of the three allocation strategies contiguous linked and indexed answer these questions a how is the logicaltophysical address mapping accomplished in this system for the indexed allocation assume that a file is always less than blocks long b if we are currently at logical block the last block accessed was block and want to access logical block how many physical blocks must be read from the disk consider a file system that uses inodes to represent files disk blocks are kb in size and a pointer to a disk block requires bytes this file system has direct disk blocks as well as single double and triple indirect disk blocks what is the maximum size of a file that can be stored in this file system fragmentation on a storage device can be eliminated by recompaction of the information typical disk devices do not have relocation or base registers such as those used when memory is to be compacted so how can we relocate files give three reasons why recompacting and relocation of files are often avoided assume that in a particular augmentation of a remotefileaccess protocol each client maintains a name cache that caches translations from file names to corresponding file handles what issues should we take into account in implementing the name cache explain why logging metadata updates ensures recovery of a file system after a filesystem crash consider the following backup scheme day copy to a backup medium all files from the disk day copy to another medium all files changed since day day copy to another medium all files changed since day this differs from the schedule given in section by having all subsequent backups copy all files modified since the first full backup what are the benefits of this system over the one in section what are the drawbacks are restore operations made easier or more difficult explain your answer programming problems the following exercise examines the relationship between files and inodes on a unix or linux system on these systems files are represented with inodes that is an inode is a file and vice versa you can complete this exercise on the linux virtual machine that is provided with this text you can also complete the exercise on any linux unix or chapter filesystem implementation mac os x system but it will require creating two simple text files named filetxt and filetxt whose contents are unique sentences in the source code available with this text open filetxt and examine its contents next obtain the inode number of this file with the command ls li filetxt this will produce output similar to the following rwr r os os sep filetxt where the inode number is boldfaced the inode number of filetxt is likely to be different on your system the unix ln command creates a link between a source and target file this command works as follows ln s source file target file unix provides two types of links hard links and soft links a hard link creates a separate target file that has the same inode as the source file enter the following command to create a hard link between filetxt and filetxt ln filetxt filetxt what are the inode values of filetxt and filetxt are they the same or different do the two files have the same or different contents next edit filetxt and change its contents after you have done so examine the contents of filetxt are the contents of filetxt and filetxt the same or different next enter the following command which removes filetxt rm filetxt does filetxt still exist as well now examine the man pages for both the rm and unlink commands afterwards remove filetxt by entering the command strace rm filetxt the strace command traces the execution of system calls as the command rm filetxt is run what system call is used for removing filetxt a soft link or symbolic link creates a new file that points to the name of the file it is linking to in the source code available with this text create a soft link to filetxt by entering the following command ln s filetxt filetxt after you have done so obtain the inode numbers of filetxt and filetxt using the command ls li filetxt bibliography are the inodes the same or is each unique next edit the contents of filetxt have the contents of filetxt been altered as well last delete filetxt after you have done so explain what happens when you attempt to edit filetxt bibliographical notes the msdos fat system is explained in norton and wilton the internals of the bsd unix system are covered in full in mckusick and nevilleneil details concerning file systems for linux can be found in love the google file system is described in ghemawat et al fuse can be found at httpfusesourceforgenet logstructured file organizations for enhancing both performance and consistency are discussed in rosenblum and ousterhout seltzer et al and seltzer et al algorithms such as balanced trees and much more are covered by knuth and cormen et al silvers discusses implementing the page cache in the netbsd operating system the zfs source code for space maps can be found at httpsrcopensolarisorgsourcexrefonnvonnvgateusrsrcutscommon fszfsspace mapc the network file system nfs is discussed in callaghan nfs version is a standard described at httpwwwietforgrfcrfctxt ousterhout discusses the role of distributed state in networked file systems logstructured designs for networked file systems are proposed in hartman and ousterhout and thekkath et al nfs and the unix file system ufs are described in vahalia and mauro and mcdougall the ntfs file system is explained in solomon the ext file system used in linux is described in mauerer and the wafl file system is covered in hitz et al zfs documentation can be found at httpwwwopensolarisorgoscommunityzfsdocs bibliography callaghan b callaghan nfs illustrated addisonwesley cormen et al t h cormen c e leiserson r l rivest and c stein introduction to algorithms third edition mit press ghemawat et al s ghemawat h gobioff and st leung the google file system proceedings of the acm symposium on operating systems principles hartman and ousterhout j h hartman and j k ousterhout the zebra striped network file system acm transactions on computer systems volume number pages hitz et al d hitz j lau and m malcolm file system design for an nfs file server appliance technical report netapp chapter filesystem implementation knuth d e knuth the art of computer programming volume sorting and searching second edition addisonwesley love r love linux kernel development third edition developers library mauerer w mauerer professional linux kernel architecture john wiley and sons mauro and mcdougall j mauro and r mcdougall solaris internals core kernel architecture prentice hall mckusick and nevilleneil m k mckusick and g v nevilleneil the design and implementation of the freebsd unix operating system addison wesley norton and wilton p norton and r wilton the new peter norton programmers guide to the ibm pc ps microsoft press ousterhout j ousterhout the role of distributed state in cmu computer science a th anniversary commemorative r f rashid ed addisonwesley rosenblum and ousterhout m rosenblum and j k ousterhout the design and implementation of a logstructured file system proceedings of the acm symposium on operating systems principles pages seltzer et al m i seltzer k bostic m k mckusick and c staelin an implementation of a logstructured file system for unix usenix winter pages seltzer et al m i seltzer k a smith h balakrishnan j chang s mcmains and v n padmanabhan file system logging versus clustering a performance comparison usenix winter pages silvers c silvers ubc an efficient unified io and memory caching subsystem for netbsd usenix annual technical conference freenix track solomon d a solomon inside windows nt second edition microsoft press thekkath et al c a thekkath t mann and e k lee frangipani a scalable distributed file system symposium on operating systems principles pages vahalia u vahalia unix internals the new frontiers prentice hall overview the control of devices connected to the computer is a major concern of operatingsystem designers because io devices vary so widely in their function and speed consider a mouse a hard disk and a tape robot varied methods are needed to control them these methods form the io subsystem of the kernel which separates the rest of the kernel from the complexities of managing io devices io hardware computers operate a great many kinds of devices most fit into the general categories of storage devices disks tapes transmission devices network connections bluetooth and humaninterface devices screen keyboard mouse audio in and out other devices are more specialized such as those involved in the steering of a jet in these aircraft a human gives input to the flight computer via a joystick and foot pedals and the computer sends output commands that cause motors to move rudders and flaps and fuels to the engines despite the incredible variety of io devices though we need only a few concepts to understand how the devices are attached and how the software can control the hardware a device communicates with a computer system by sending signals over a cable or even through the air the device communicates with the machine via a connection point or port for example a serial port if devices share a common set of wires the connection is called a bus a bus is a set of wires and a rigidly defined protocol that specifies a set of messages that can be sent on the wires in terms of the electronics the messages are conveyed by patterns of electrical voltages applied to the wires with defined timings when device a has a cable that plugs into device b and device b has a cable that plugs into device c and device c plugs into a port on the computer this arrangement is called a daisy chain a daisy chain usually operates as a bus buses are used widely in computer architecture and vary in their signaling methods speed throughput and connection methods a typical pc bus structure appears in figure in the figure a pci bus the common pc system bus connects the processormemory subsystem to fast devices and an expansion bus connects relatively slow devices such as the keyboard and serial and usb ports in the upperright portion of the figure four disks are connected together on a small computer system interface scsi bus plugged into a scsi controller other common buses used to interconnect main parts of a computer include pci express pcie with throughput of up to gb per second and hypertransport with throughput of up to gb per second a controller is a collection of electronics that can operate a port a bus or a device a serialport controller is a simple device controller it is a single chip or portion of a chip in the computer that controls the signals on the application io interface it interferes with system security and stability the trend in generalpurpose operating systems is to protect memory and devices so that the system can try to guard against erroneous or malicious applications io hardware summary although the hardware aspects of io are complex when considered at the level of detail of electronicshardware design the concepts that we have just described are sufficient to enable us to understand many io features of operating systems lets review the main concepts a bus a controller an io port and its registers the handshaking relationship between the host and a device controller the execution of this handshaking in a polling loop or via interrupts the offloading of this work to a dma controller for large transfers we gave a basic example of the handshaking that takes place between a device controller and the host earlier in this section in reality the wide variety of available devices poses a problem for operatingsystem implementers each kind of device has its own set of capabilities controlbit definitions and protocols for interacting with the host and they are all different how can the operating system be designed so that we can attach new devices to the computer without rewriting the operating system and when the devices vary so widely how can the operating system give a convenient uniform io interface to applications we address those questions next kernel io subsystem kernels provide many services related to io several services scheduling buffering caching spooling device reservation and error handling are provided by the kernels io subsystem and build on the hardware and devicedriver infrastructure the io subsystem is also responsible for protecting itself from errant processes and malicious users io scheduling to schedule a set of io requests means to determine a good order in which to execute them the order in which applications issue system calls rarely is the best choice scheduling can improve overall system performance can share device access fairly among processes and can reduce the average waiting time for io to complete here is a simple example to illustrate suppose that a disk arm is near the beginning of a disk and that three applications issue blocking read calls to that disk application requests a block near the end of the disk application requests one near the beginning and application requests one in the middle of the disk the operating system can reduce the distance that the disk arm travels by serving the applications in the order rearranging the order of service in this way is the essence of io scheduling operatingsystem developers implement scheduling by maintaining a wait queue of requests for each device when an application issues a blocking io system call the request is placed on the queue for that device the io scheduler rearranges the order of the queue to improve the overall system efficiency and the average response time experienced by applications the operating system may also try to be fair so that no one application receives especially poor service or it may give priority service for delaysensitive requests for instance requests from the virtual memory subsystem may take priority over application requests several scheduling algorithms for disk io are detailed in section when a kernel supports asynchronous io it must be able to keep track of many io requests at the same time for this purpose the operating system might attach the wait queue to a devicestatus table the kernel manages this table which contains an entry for each io device as shown in figure transforming io requests to hardware operations streams otherwise a physical io must be performed the process is removed from the run queue and is placed on the wait queue for the device and the io request is scheduled eventually the io subsystem sends the request to the device driver depending on the operating system the request is sent via a subroutine call or an inkernel message the device driver allocates kernel buffer space to receive the data and schedules the io eventually the driver sends commands to the device controller by writing into the devicecontrol registers the device controller operates the device hardware to perform the data transfer the driver may poll for status and data or it may have set up a dma transfer into kernel memory we assume that the transfer is managed by a dma controller which generates an interrupt when the transfer completes the correct interrupt handler receives the interrupt via the interruptvector table stores any necessary data signals the device driver and returns from the interrupt the device driver receives the signal determines which io request has completed determines the requests status and signals the kernel io subsystem that the request has been completed the kernel transfers data or return codes to the address space of the requesting process and moves the process from the wait queue back to the ready queue moving the process to the ready queue unblocks the process when the scheduler assigns the process to the cpu the process resumes execution at the completion of the system call performance device typically resorts to dropping incoming messages consider a network card whose input buffer is full the network card must simply drop further messages until there is enough buffer space to store incoming messages the benefit of using streams is that it provides a framework for a modular and incremental approach to writing device drivers and network protocols modules may be used by different streams and hence by different devices for example a networking module may be used by both an ethernet network card and a wireless network card furthermore rather than treating characterdevice io as an unstructured byte stream streams allows support for message boundaries and control information when communicating between modules most unix variants support streams and it is the preferred method for writing protocols and device drivers for example system v unix and solaris implement the socket mechanism using streams summary the basic hardware elements involved in io are buses device controllers and the devices themselves the work of moving data between devices and main memory is performed by the cpu as programmed io or is offloaded to a dma controller the kernel module that controls a device is a device driver the systemcall interface provided to applications is designed to handle several basic categories of hardware including block devices character devices memorymapped files network sockets and programmed interval timers the system calls usually block the processes that issue them but nonblocking and asynchronous calls are used by the kernel itself and by applications that must not sleep while waiting for an io operation to complete the kernels io subsystem provides numerous services among these are io scheduling buffering caching spooling device reservation and error handling another service name translation makes the connections between hardware devices and the symbolic file names used by applications it involves several levels of mapping that translate from characterstring names to specific device drivers and device addresses and then to physical addresses of io ports or bus controllers this mapping may occur within the filesystem name space as it does in unix or in a separate device name space as it does in msdos streams is an implementation and methodology that provides a framework for a modular and incremental approach to writing device drivers and exercises network protocols through streams drivers can be stacked with data passing through them sequentially and bidirectionally for processing io system calls are costly in terms of cpu consumption because of the many layers of software between a physical device and an application these layers imply overhead from several sources context switching to cross the kernels protection boundary signal and interrupt handling to service the io devices and the load on the cpu and memory system to copy data between kernel buffers and application space practice exercises state three advantages of placing functionality in a device controller rather than in the kernel state three disadvantages the example of handshaking in section used two bits a busy bit and a commandready bit is it possible to implement this handshaking with only one bit if it is describe the protocol if it is not explain why one bit is insufficient why might a system use interruptdriven io to manage a single serial port and polling io to manage a frontend processor such as a terminal concentrator polling for an io completion can waste a large number of cpu cycles if the processor iterates a busywaiting loop many times before the io completes but if the io device is ready for service polling can be much more efficient than is catching and dispatching an interrupt describe a hybrid strategy that combines polling sleeping and interrupts for io device service for each of these three strategies pure polling pure interrupts hybrid describe a computing environment in which that strategy is more efficient than is either of the others how does dma increase system concurrency how does it complicate hardware design why is it important to scale up systembus and device speeds as cpu speed increases distinguish between a streams driver and a streams module exercises when multiple interrupts from different devices appear at about the same time a priority scheme could be used to determine the order in which the interrupts would be serviced discuss what issues need to be considered in assigning priorities to different interrupts what are the advantages and disadvantages of supporting memorymapped io to device control registers chapter io systems consider the following io scenarios on a singleuser pc a a mouse used with a graphical user interface b a tape drive on a multitasking operating system with no device preallocation available c a disk drive containing user files d a graphics card with direct bus connection accessible through memorymapped io for each of these scenarios would you design the operating system to use buffering spooling caching or a combination would you use polled io or interruptdriven io give reasons for your choices in most multiprogrammed systems user programs access memory through virtual addresses while the operating system uses raw physical addresses to access memory what are the implications of this design for the initiation of io operations by the user program and their execution by the operating system what are the various kinds of performance overhead associated with servicing an interrupt describe three circumstances under which blocking io should be used describe three circumstances under which nonblocking io should be used why not just implement nonblocking io and have processes busywait until their devices are ready typically at the completion of a device io a single interrupt is raised and appropriately handled by the host processor in certain settings however the code that is to be executed at the completion of the io can be broken into two separate pieces the first piece executes immediately after the io completes and schedules a second interrupt for the remaining piece of code to be executed at a later time what is the purpose of using this strategy in the design of interrupt handlers some dma controllers support direct virtual memory access where the targets of io operations are specified as virtual addresses and a translation from virtual to physical address is performed during the dma how does this design complicate the design of the dma controller what are the advantages of providing such functionality unix coordinates the activities of the kernel io components by manipulating shared inkernel data structures whereas windows uses objectoriented message passing between kernel io components discuss three pros and three cons of each approach write in pseudocode an implementation of virtual clocks including the queueing and management of timer requests for the kernel and applications assume that the hardware provides three timer channels discuss the advantages and disadvantages of guaranteeing reliable transfer of data between modules in the streams abstraction bibliography bibliographical notes vahalia provides a good overview of io and networking in unix mckusick and nevilleneil detail the io structures and methods employed in freebsd the use and programming of the various interprocesscommunication and network protocols in unix are explored in stevens hart covers windows programming intel provides a good source for intel processors rago provides a good discussion of streams hennessy and patterson describe multiprocessor systems and cacheconsistency issues bibliography hart j m hart windows system programming third edition addisonwesley hennessy and patterson j hennessy and d patterson computer architecture a quantitative approach fifth edition morgan kaufmann intel intel and ia architectures software developers manual combined volumes a b a and b intel corporation mckusick and nevilleneil m k mckusick and g v nevilleneil the design and implementation of the freebsd unix operating system addison wesley rago s rago unix system v network programming addisonwesley stevens r stevens advanced programming in the unix environment addisonwesley vahalia u vahalia unix internals the new frontiers prentice hall part five goals of protection as computer systems have become more sophisticated and pervasive in their applications the need to protect their integrity has also grown protection was originally conceived as an adjunct to multiprogramming operating systems so that untrustworthy users might safely share a common logical name space such as a directory of files or share a common physical name space such as memory modern protection concepts have evolved to increase the reliability of any complex system that makes use of shared resources we need to provide protection for several reasons the most obvious is the need to prevent the mischievous intentional violation of an access restriction principles of protection frequently a guiding principle can be used throughout a project such as the design of an operating system following this principle simplifies design decisions and keeps the system consistent and easy to understand a key timetested guiding principle for protection is the principle of least privilege it dictates that programs users and even systems be given just enough privileges to perform their tasks consider the analogy of a security guard with a passkey if this key allows the guard into just the public areas that she guards then misuse of the key will result in minimal damage if however the passkey allows access to all areas then damage from its being lost stolen misused copied or otherwise compromised will be much greater an operating system following the principle of least privilege implements its features programs system calls and data structures so that failure or compromise of a component does the minimum damage and allows the minimum damage to be done the overflow of a buffer in a system daemon might cause the daemon process to fail for example but should not allow the execution of code from the daemon processs stack that would enable a remote domain of protection user to gain maximum privileges and access to the entire system as happens too often today such an operating system also provides system calls and services that allow applications to be written with finegrained access controls it provides mechanisms to enable privileges when they are needed and to disable them when they are not needed also beneficial is the creation of audit trails for all privileged function access the audit trail allows the programmer system administrator or lawenforcement officer to trace all protection and security activities on the system managing users with the principle of least privilege entails creating a separate account for each user with just the privileges that the user needs an operator who needs to mount tapes and back up files on the system has access to just those commands and files needed to accomplish the job some systems implement rolebased access control rbac to provide this functionality computers implemented in a computing facility under the principle of least privilege can be limited to running specific services accessing specific remote hosts via specific services and doing so during specific times typically these restrictions are implemented through enabling or disabling each service and through using access control lists as described in sections section and section the principle of least privilege can help produce a more secure computing environment unfortunately it frequently does not for example windows has a complex protection scheme at its core and yet has many security holes by comparison solaris is considered relatively secure even though it is a variant of unix which historically was designed with little protection in mind one reason for the difference may be that windows has more lines of code and more services than solaris and thus has more to secure and protect another reason could be that the protection scheme in windows is incomplete or protects the wrong aspects of the operating system leaving other areas vulnerable access matrix our general model of protection can be viewed abstractly as a matrix called an access matrix the rows of the access matrix represent domains and the columns represent objects each entry in the matrix consists of a set of access rights because the column defines objects explicitly we can omit the object name from the access right the entry accessij defines the set of operations that a process executing in domain di can invoke on object oj to illustrate these concepts we consider the access matrix shown in figure there are four domains and four objects three files f f f and one laser printer a process executing in domain d can read files f and f a process executing in domain d has the same privileges as one executing in object f f f printer domain d read read d print d read execute d read read write write figure access matrix implementation of the access matrix how can the access matrix be implemented effectively in general the matrix will be sparse that is most of the entries will be empty although datastructure techniques are available for representing sparse matrices they are not particularly useful for this application because of the way in which the protection facility is used here we first describe several methods of implementing the access matrix and then compare the methods global table the simplest implementation of the access matrix is a global table consisting of a set of ordered triples domain object rightsset whenever an operation m is executed on an object oj within domain di the global table is searched for a triple di oj rk with m rk if this triple is found the operation is allowed to continue otherwise an exception or error condition is raised this implementation suffers from several drawbacks the table is usually large and thus can not be kept in main memory so additional io is needed virtual memory techniques are often used for managing this table in addition it is difficult to take advantage of special groupings of objects or domains for example if everyone can read a particular object this object must have a separate entry in every domain access lists for objects each column in the access matrix can be implemented as an access list for one object as described in section obviously the empty entries can be discarded the resulting list for each object consists of ordered pairs domain rightsset which define all domains with a nonempty set of access rights for that object this approach can be extended easily to define a list plus a default set of access rights when an operation m on an object oj is attempted in domain access control since access is checked when the file is opened protection is ensured this strategy is used in the unix system the right to access must still be checked on each access and the filetable entry has a capability only for the allowed operations if a file is opened for reading then a capability for read access is placed in the filetable entry if an attempt is made to write onto the file the system identifies this protection violation by comparing the requested operation with the capability in the filetable entry revocation of access rights in a dynamic protection system we may sometimes need to revoke access rights to objects shared by different users various questions about revocation may arise immediate versus delayed does revocation occur immediately or is it delayed if revocation is delayed can we find out when it will take place selective versus general when an access right to an object is revoked does it affect all the users who have an access right to that object or can we specify a select group of users whose access rights should be revoked partial versus total can a subset of the rights associated with an object be revoked or must we revoke all access rights for this object temporary versus permanent can access be revoked permanently that is the revoked access right will never again be available or can access be revoked and later be obtained again with an accesslist scheme revocation is easy the access list is searched for any access rights to be revoked and they are deleted from the list revocation is immediate and can be general or selective total or partial and permanent or temporary capabilities however present a much more difficult revocation problem as mentioned earlier since the capabilities are distributed throughout the system we must find them before we can revoke them schemes that implement revocation for capabilities include the following reacquisition periodically capabilities are deleted from each domain if a process wants to use a capability it may find that that capability has been deleted the process may then try to reacquire the capability if access has been revoked the process will not be able to reacquire the capability backpointers a list of pointers is maintained with each object pointing to all capabilities associated with that object when revocation is required we can follow these pointers changing the capabilities as necessary this scheme was adopted in the multics system it is quite general but its implementation is costly indirection the capabilities point indirectly not directly to the objects each capability points to a unique entry in a global table which in turn points to the object we implement revocation by searching the global table for the desired entry and deleting it then when an access is attempted the capability is found to point to an illegal table entry table entries can be reused for other capabilities without difficulty since both the capability and the table entry contain the unique name of the object the object for a capabilitybased systems capability and its table entry must match this scheme was adopted in the cal system it does not allow selective revocation keys a key is a unique bit pattern that can be associated with a capability this key is defined when the capability is created and it can be neither modified nor inspected by the process that owns the capability a master key is associated with each object it can be defined or replaced with the setkey operation when a capability is created the current value of the master key is associated with the capability when the capability is exercised its key is compared with the master key if the keys match the operation is allowed to continue otherwise an exception condition is raised revocation replaces the master key with a new value via the setkey operation invalidating all previous capabilities for this object this scheme does not allow selective revocation since only one master key is associated with each object if we associate a list of keys with each object then selective revocation can be implemented finally we can group all keys into one global table of keys a capability is valid only if its key matches some key in the global table we implement revocation by removing the matching key from the table with this scheme a key can be associated with several objects and several keys can be associated with each object providing maximum flexibility in keybased schemes the operations of defining keys inserting them into lists and deleting them from lists should not be available to all users in particular it would be reasonable to allow only the owner of an object to set the keys for that object this choice however is a policy decision that the protection system can implement but should not define languagebased protection to the degree that protection is provided in existing computer systems it is usually achieved through an operatingsystem kernel which acts as a security agent to inspect and validate each attempt to access a protected resource since comprehensive access validation may be a source of considerable overhead either we must give it hardware support to reduce the cost of each validation or we must allow the system designer to compromise the goals of protection satisfying all these goals is difficult if the flexibility to implement protection policies is restricted by the support mechanisms provided or if protection environments are made larger than necessary to secure greater operational efficiency as operating systems have become more complex and particularly as they have attempted to provide higherlevel user interfaces the goals of protection have become much more refined the designers of protection systems have drawn heavily on ideas that originated in programming languages and especially on the concepts of abstract data types and objects protection systems are now concerned not only with the identity of a resource to which access is attempted but also with the functional nature of that access in the newest protection systems concern for the function to be invoked extends beyond a set of systemdefined functions such as standard fileaccess methods to include functions that may be userdefined as well policies for resource use may also vary depending on the application and they may be subject to change over time for these reasons protection can no longer be considered a matter of concern only to the designer of an operating system it should also be available as a tool for use by the application designer so that resources of an application subsystem can be guarded against tampering or the influence of an error compilerbased enforcement at this point programming languages enter the picture specifying the desired control of access to a shared resource in a system is making a declarative statement about the resource this kind of statement can be integrated into a language by an extension of its typing facility when protection is declared along with data typing the designer of each subsystem can specify its requirements for protection as well as its need for use of other resources in a system such a specification should be given directly as a program is composed and in the language in which the program itself is stated this approach has several significant advantages opensource operating systems realtime embedded systems embedded computers are the most prevalent form of computers in existence these devices are found everywhere from car engines and manufacturing robots to dvds and microwave ovens they tend to have very specific tasks the systems they run on are usually primitive and so the operating systems provide limited features usually they have little or no user interface preferring to spend their time monitoring and managing hardware devices such as automobile engines and robotic arms these embedded systems vary considerably some are generalpurpose computers running standard operating systems such as linux with specialpurpose applications to implement the functionality others are hardware devices with a specialpurpose embedded operating system providing just the functionality desired yet others are hardware devices with applicationspecific integrated circuits asics that perform their tasks without an operating system the use of embedded systems continues to expand the power of these devices both as standalone units and as elements of networks and the web is sure to increase as well even now entire houses can be computerized so that a central computer either a generalpurpose computer or an embedded system can control heating and lighting alarm systems and even coffee makers web access can enable a home owner to tell the house to heat up before she arrives home someday the refrigerator can notify the grocery store when it notices the milk is gone embedded systems almost always run realtime operating systems a realtime system is used when rigid time requirements have been placed on the operation of a processor or the flow of data thus it is often used as a control device in a dedicated application sensors bring data to the computer the computer must analyze the data and possibly adjust controls to modify the sensor inputs systems that control scientific experiments medical imaging systems industrial control systems and certain display systems are realtime systems some automobileengine fuelinjection systems homeappliance controllers and weapon systems are also realtime systems a realtime system has welldefined fixed time constraints processing must be done within the defined constraints or the system will fail for instance it would not do for a robot arm to be instructed to halt after it had smashed into the car it was building a realtime system functions correctly only if it returns the correct result within its time constraints contrast this system with a timesharing system where it is desirable but not mandatory to respond quickly or a batch system which may have no time constraints at all in chapter we consider the scheduling facility needed to implement realtime functionality in an operating system in chapter we describe the design of memory management for realtime computing finally in chapters and we describe the realtime components of the linux and windows operating systems summary protection untrusted url loader networking domain applet socket none lucentcom connect any permission class gui geturl u openaddr a geturl doprivileged checkpermission openaddr openproxylucentcom a connect connect a request u from proxy figure stack inspection of course for stack inspection to work a program must be unable to modify the annotations on its own stack frame or to otherwise manipulate stack inspection this is one of the most important differences between java and many other languages including c a java program can not directly access memory it can manipulate only an object for which it has a reference references can not be forged and manipulations are made only through welldefined interfaces compliance is enforced through a sophisticated collection of loadtime and runtime checks as a result an object can not manipulate its runtime stack because it can not get a reference to the stack or other components of the protection system more generally javas loadtime and runtime checks enforce type safety of java classes type safety ensures that classes can not treat integers as pointers write past the end of an array or otherwise access memory in arbitrary ways rather a program can access an object only via the methods defined on that object by its class this is the foundation of java protection since it enables a class to effectively encapsulate and protect its data and methods from other classes loaded in the same jvm for example a variable can be defined as private so that only the class that contains it can access it or protected so that it can be accessed only by the class that contains it subclasses of that class or classes in the same package type safety ensures that these restrictions can be enforced the security problem in many applications ensuring the security of the computer system is worth considerable effort large commercial systems containing payroll or other financial data are inviting targets to thieves systems that contain data pertaining to corporate operations may be of interest to unscrupulous competitors furthermore loss of such data whether by accident or fraud can seriously impair the ability of the corporation to function in chapter we discussed mechanisms that the operating system can provide with appropriate aid from the hardware that allow users to protect chapter security their resources including programs and data these mechanisms work well only as long as the users conform to the intended use of and access to these resources we say that a system is secure if its resources are used and accessed as intended under all circumstances unfortunately total security can not be achieved nonetheless we must have mechanisms to make security breaches a rare occurrence rather than the norm security violations or misuse of the system can be categorized as intentional malicious or accidental it is easier to protect against accidental misuse than against malicious misuse for the most part protection mechanisms are the core of protection from accidents the following list includes several forms of accidental and malicious security violations we should note that in our discussion of security we use the terms intruder and cracker for those attempting to breach security in addition a threat is the potential for a security violation such as the discovery of a vulnerability whereas an attack is the attempt to break security breach of confidentiality this type of violation involves unauthorized reading of data or theft of information typically a breach of confidentiality is the goal of an intruder capturing secret data from a system or a data stream such as creditcard information or identity information for identity theft can result directly in money for the intruder breach of integrity this violation involves unauthorized modification of data such attacks can for example result in passing of liability to an innocent party or modification of the source code of an important commercial application breach of availability this violation involves unauthorized destruction of data some crackers would rather wreak havoc and gain status or bragging rights than gain financially website defacement is a common example of this type of security breach theft of service this violation involves unauthorized use of resources for example an intruder or intrusion program may install a daemon on a system that acts as a file server denial of service this violation involves preventing legitimate use of the system denialofservice dos attacks are sometimes accidental the original internet worm turned into a dos attack when a bug failed to delay its rapid spread we discuss dos attacks further in section attackers use several standard methods in their attempts to breach security the most common is masquerading in which one participant in a communication pretends to be someone else another host or another person by masquerading attackers breach authentication the correctness of identification they can then gain access that they would not normally be allowed or escalate their privileges obtain privileges to which they would not normally be entitled another common attack is to replay a captured exchange of data a replay attack consists of the malicious or fraudulent repeat of a valid data transmission sometimes the replay comprises the entire attack for example in a repeat of a request to transfer money but frequently it is done along with message modification again to escalate privileges consider program threats ways ranging from passwords for authentication through guarding against viruses to detecting intrusions we start with an exploration of security threats system and network threats multipartite a virus of this type is able to infect multiple parts of a system including boot sectors memory and files this makes it difficult to detect and contain armored an armored virus is coded to make it hard for antivirus researchers to unravel and understand it can also be compressed to avoid detection and disinfection in addition virus droppers and other full files that are part of a virus infestation are frequently hidden via file attributes or unviewable file names this vast variety of viruses has continued to grow for example in a new and widespread virus was detected it exploited three separate bugs for its operation this virus started by infecting hundreds of windows servers including many trusted sites running microsoft internet information server iis any vulnerable microsoft explorer web browser visiting those sites received a browser virus with any download the browser virus installed several backdoor programs including a keystroke logger which records everything entered on the keyboard including passwords and creditcard numbers it also installed a daemon to allow unlimited remote access by an intruder and another that allowed an intruder to route spam through the infected desktop computer generally viruses are the most disruptive security attacks and because they are effective they will continue to be written and to spread an active securityrelated debate within the computing community concerns the existence of a monoculture in which many systems run the same hardware operating system and application software this monoculture supposedly consists of microsoft products one question is whether such a monoculture even exists today another question is whether if it does it increases the threat of and damage caused by viruses and other security intrusions cryptography as a security tool there are many defenses against computer attacks running the gamut from methodology to technology the broadest tool available to system designers and users is cryptography in this section we discuss cryptography and its use in computer security note that the cryptography discussed here has been simplified for educational purposes readers are cautioned against using any user authentication domain name of the server with which it is communicating for applications in which the server also needs information about the client ssl supports an option by which a client can send a certificate to the server in addition to its use on the internet ssl is being used for a wide variety of tasks for example ipsec vpns now have a competitor in ssl vpns ipsec is good for pointtopoint encryption of traffic say between two company offices ssl vpns are more flexible but not as efficient so they might be used between an individual employee working remotely and the corporate office summary help find and fix bugs and otherwise explore mature fullfeatured operating systems compilers tools user interfaces and other types of programs the availability of source code for historic projects such as multics can help students to understand those projects and to build knowledge that will help in the implementation of new projects gnulinux and bsd unix are all opensource operating systems but each has its own goals utility licensing and purpose sometimes licenses are not mutually exclusive and crosspollination occurs allowing rapid improvements in operatingsystem projects for example several major components of opensolaris have been ported to bsd unix the advantages of free software and open sourcing are likely to increase the number and quality of opensource projects leading to an increase in the number of individuals and companies that use these projects implementing security defenses another variation on onetime passwords uses a code book or onetime pad which is a list of singleuse passwords each password on the list is used once and then is crossed out or erased the commonly used skey system uses either a software calculator or a code book based on these calculations as a source of onetime passwords of course the user must protect his code book and it is helpful if the code book does not identify the system to which the codes are authenticators biometrics yet another variation on the use of passwords for authentication involves the use of biometric measures palmor handreaders are commonly used to secure physical access for example access to a data center these readers match stored parameters against what is being read from handreader pads the parameters can include a temperature map as well as finger length finger width and line patterns these devices are currently too large and expensive to be used for normal computer authentication fingerprint readers have become accurate and costeffective and should become more common in the future these devices read finger ridge patterns and convert them into a sequence of numbers over time they can store a set of sequences to adjust for the location of the finger on the reading pad and other factors software can then scan a finger on the pad and compare its features with these stored sequences to determine if they match of course multiple users can have profiles stored and the scanner can differentiate among them a very accurate twofactor authentication scheme can result from requiring a password as well as a user name and fingerprint scan if this information is encrypted in transit the system can be very resistant to spoofing or replay attack multifactor authentication is better still consider how strong authentication can be with a usb device that must be plugged into the system a pin and a fingerprint scan except for having to place ones finger on a pad and plug the usb into the system this authentication method is no less convenient than that using normal passwords recall though that strong authentication by itself is not sufficient to guarantee the id of the user an authenticated session can still be hijacked if it is not encrypted firewalling to protect systems and networks we turn next to the question of how a trusted computer can be connected safely to an untrustworthy network one solution is the use of a firewall to separate trusted and untrusted systems a firewall is a computer appliance or router that sits between the trusted and the untrusted a network firewall limits network access between the two security domains and monitors and logs all connections it can also limit connections based on source or destination address source or destination port or direction of the connection for instance web servers use http to communicate with web browsers a firewall therefore may allow only http to pass from all hosts outside the firewall to the web server within the firewall the morris internet worm used the finger protocol to break into computers so finger would not be allowed to pass for example in fact a network firewall can separate a network into multiple domains a common implementation has the internet as the untrusted domain a semitrusted and semisecure network called the demilitarized zone dmz as another domain and a companys computers as a third domain figure computersecurity classifications the us department of defense trusted computer system evaluation criteria specify four security classifications in systems a b c and d this specification is widely used to determine the security of a facility and to model security solutions so we explore it here the lowestlevel classification is division d or minimal protection division d includes only one class and is used for systems that have failed to meet the requirements of any of the other security classes for instance msdos and windows are in division d division c the next level of security provides discretionary protection and accountability of users and their actions through the use of audit capabilities division c has two levels c and c a cclass system incorporates some form of controls that allow users to protect private information and to keep other users from accidentally reading or destroying their data a c environment is one in which cooperating users access data at the same levels of sensitivity most versions of unix are c class the total of all protection systems within a computer system hardware software firmware that correctly enforce a security policy is known as a trusted computer base tcb the tcb of a c system controls access between users and files by allowing the user to specify and control sharing of objects by named individuals or defined groups in addition the tcb requires that the users identify themselves before they start any activities that the tcb is expected to mediate this identification is accomplished via a protected mechanism or password the tcb protects the authentication data so that they are inaccessible to unauthorized users a cclass system adds an individuallevel access control to the requirements of a c system for example access rights of a file can be specified to the level of a single individual in addition the system administrator can selectively audit the actions of any one or more users based on individual identity the tcb also protects itself from modification of its code or data structures in addition no information produced by a prior user is available to another user who accesses a storage object that has been released back to the system some special secure versions of unix have been certified at the c level divisionb mandatoryprotection systems have all the properties of a classc system in addition they attach a sensitivity label to each object in the system the bclass tcb maintains these labels which are used for decisions pertaining to mandatory access control for example a user at the confidential level could not access a file at the more sensitive secret level the tcb also denotes the sensitivity level at the top and bottom of each an example windows page of any humanreadable output in addition to the normal username password authentication information the tcb also maintains the clearance and authorizations of individual users and will support at least two levels of security these levels are hierarchical so that a user may access any objects that carry sensitivity labels equal to or lower than his security clearance for example a secretlevel user could access a file at the confidential level in the absence of other access controls processes are also isolated through the use of distinct address spaces a bclass system extends the sensitivity labels to each system resource such as storage objects physical devices are assigned minimum and maximum security levels that the system uses to enforce constraints imposed by the physical environments in which the devices are located in addition a b system supports covert channels and the auditing of events that could lead to the exploitation of a covert channel a bclass system allows the creation of accesscontrol lists that denote users or groups not granted access to a given named object the tcb also contains a mechanism to monitor events that may indicate a violation of security policy the mechanism notifies the security administrator and if necessary terminates the event in the least disruptive manner the highestlevel classification is division a architecturally a classa system is functionally equivalent to a b system but it uses formal design specifications and verification techniques granting a high degree of assurance that the tcb has been implemented correctly a system beyond class a might be designed and developed in a trusted facility by trusted personnel the use of a tcb merely ensures that the system can enforce aspects of a security policy the tcb does not specify what the policy should be typically a given computing environment develops a security policy for certification and has the plan accredited by a security agency such as the national computer security center certain computing environments may require other certification such as that supplied by tempest which guards against electronic eavesdropping for example a tempestcertified system has terminals that are shielded to prevent electromagnetic fields from escaping this shielding ensures that equipment outside the room or building where the terminal is housed can not detect what information is being displayed by the terminal summary only by the posix subsystem a discretionary accesscontrol list that identifies which users or groups are allowed and which are explicitly denied access and a system accesscontrol list that controls which auditing messages the system will generate optionally the system accesscontrol list can set the integrity of the object and identify which operations to block from lowerintegrity subjects read write always enforced or execute for example the security descriptor of the file foobar might have owner avi and this discretionary accesscontrol list avi all access group cs readwrite access user cliff no access in addition it might have a system accesscontrol list that tells the system to audit writes by everyone along with an integrity label of medium that denies read write and execute to lowerintegrity subjects an accesscontrol list is composed of accesscontrol entries that contain the security id of the individual and an access mask that defines all possible actions on the object with a value of accessallowed or accessdenied for each action files in windows may have the following access types readdata writedata appenddata execute readextendedattribute writeextendedattribute readattributes and writeattributes we can see how this allows a fine degree of control over access to objects windows classifies objects as either container objects or noncontainer objects container objects such as directories can logically contain other objects by default when an object is created within a container object the new object inherits permissions from the parent object similarly if the user copies a file from one directory to a new directory the file will inherit the permissions of the destination directory noncontainer objects inherit no other permissions furthermore if a permission is changed on a directory the new permissions do not automatically apply to existing files and subdirectories the user may explicitly apply them if he so desires the system administrator can prohibit printing to a printer on the system for all or part of a day and can use the windows performance monitor to help her spot approaching problems in general windows does a good job of providing features to help ensure a secure computing environment many of these features are not enabled by default however which may be one reason for the myriad security breaches on windows systems another reason is the vast number of services windows starts at system boot time and the number of applications that typically are installed on a windows system for a real multiuser environment the system administrator should formulate a security plan and implement it using the features that windows provides and other security tools overview the fundamental idea behind a virtual machine is to abstract the hardware of a single computer the cpu memory disk drives network interface cards and so forth into several different execution environments thereby creating the illusion that each separate environment is running on its own private computer this concept may seem similar to the layered approach of operating system implementation see section and in some ways it is in the case of virtualization there is a layer that creates a virtual system on which operating systems or applications can run chapter virtual machines virtual machine implementations involve several components at the base is the host the underlying hardware system that runs the virtual machines the virtual machine manager vmm also known as a hypervisor creates and runs virtual machines by providing an interface that is identical to the host except in the case of paravirtualization discussed later each guest process is provided with a virtual copy of the host figure usually the guest process is in fact an operating system a single physical machine can thus run multiple operating systems concurrently each in its own virtual machine take a moment to note that with virtualization the definition of operating system once again blurs for example consider vmm software such as vmware esx this virtualization software is installed on the hardware runs when the hardware boots and provides services to applications the services include traditional ones such as scheduling and memory management along with new types such as migration of applications between systems furthermore the applications are in fact guest operating systems is the vmware esx vmm an operating system that in turn runs other operating systems certainly it acts like an operating system for clarity however we call the component that provides virtual environments a vmm the implementation of vmms varies greatly options include the following hardwarebased solutions that provide support for virtual machine creation and management via firmware these vmms which are commonly found in mainframe and large to midsized servers are generally known as type hypervisors ibm lpars and oracle ldoms are examples operatingsystemlike software built to provide virtualization including vmware esxmentioned above joyent smartos and citrix xenserver these vmms are known as type hypervisors processes processes processes processes programming kernel kernel kernel interface kernel vm vm vm virtual machine manager hardware hardware a b figure system models a nonvirtual machine b virtual machine history indirection all problems in computer science can be solved by another level of indirection david wheeler except for the problem of too many layers of indirection kevlin henney generalpurpose operating systems that provide standard functions as well as vmm functions including microsoft windows server with hyperv and redhat linux with the kvm feature because such systems have a feature set similar to type hypervisors they are also known as type applications that run on standard operating systems but provide vmm features to guest operating systems these applications which include vmware workstation and fusion parallels desktop and oracle virtualbox are type hypervisors paravirtualization a technique in which the guest operating system is modified to work in cooperation with the vmm to optimize performance programmingenvironment virtualization in which vmms do not virtualize real hardware but instead create an optimized virtual system this technique is used by oracle java and microsoftnet emulators that allow applications written for one hardware environment to run on a very different hardware environment such as a different type of cpu application containment which is not virtualization at all but rather provides virtualizationlike features by segregating applications from the operating system oracle solaris zones bsd jails and ibm aix wpars contain applications making them more secure and manageable the variety of virtualization techniques in use today is a testament to the breadth depth and importance of virtualization in modern computing virtualization is invaluable for datacenter operations efficient application development and software testing among many other uses benefits and features several advantages make virtualization attractive most of them are fundamentally related to the ability to share the same hardware yet run several different execution environments that is different operating systems concurrently one important advantage of virtualization is that the host system is protected from the virtual machines just as the virtual machines are protected from each other a virus inside a guest operating system might damage that operating system but is unlikely to affect the host or the other guests because each virtual machine is almost completely isolated from all other virtual machines there are almost no protection problems a potential disadvantage of isolation is that it can prevent sharing of resources two approaches to provide sharing have been implemented first it is possible to share a filesystem volume and thus to share files second it is possible to define a network of virtual machines each of which can building blocks a protocol such as rdp thus they need not be expensive highperformance components other uses of virtualization are sure to follow as it becomes more prevalent and hardware support continues to improve types of virtual machines and their implementations virtualized resource then control is passed to the vmm to manage that interaction the functionality in intel vtx is similar providing root and nonroot modes equivalent to host and guest modes both provide guest vcpu state data structures to load and save guest cpu state automatically during guest context switches in addition virtual machine control structures vmcss are provided to manage guest and host state as well as the various guest execution controls exit controls and information about why guests exit back to the host in the latter case for example a nested pagetable violation caused by an attempt to access unavailable memory can result in the guests exit amd and intel have also addressed memory management in the virtual environment with amds rvi and intels ept memory management enhancements vmms no longer need to implement software npts in essence these cpus implement nested page tables in hardware to allow the vmm to fully control paging while the cpus accelerate the translation from virtual to physical addresses the npts add a new layer one representing the guests view of logicaltophysical address translation the cpu pagetable walking function includes this new layer as necessary walking through the guest table to the vmm table to find the physical address desired a tlb miss results in a performance penalty because more tables must be traversed the guest and host page tables to complete the lookup figure shows the extra translation work performed by the hardware to translate from a guest virtual address to a final physical address io is another area improved by hardware assistance consider that the standard directmemoryaccess dma controller accepts a target memory address and a source io device and transfers data between the two without operatingsystem action without hardware assistance a guest might try to set up a dma transfer that affects the memory of the vmm or other guests in cpus that provide hardwareassisted dma such as intel cpus with vtd even dma has a level of indirection first the vmm sets up protection domains to tell the cpu which physical memory belongs to each guest next it assigns the io devices to the protection domains allowing them direct access to those memory regions and only those regions the hardware then transforms the address in a dma request issued by an io device to the host physical memory address associated with the io in this manner dma transfers are passed through between a guest and a device without vmm interference similarly interrupts must be delivered to the appropriate guest and must not be visible to other guests by providing an interrupt remapping feature cpus with virtualization hardware assistance automatically deliver an interrupt destined for a guest to a core that is currently running a thread of that guest that way the guest receives interrupts without the vmms needing to intercede in their delivery without interrupt remapping malicious guests can generate interrupts that can be used to gain control of the host system see the bibliographical notes at the end of this chapter for more details virtualization and operatingsystem components thus far we have explored the building blocks of virtualization and the various types of virtualization in this section we take a deeper dive into the operatingsystem aspects of virtualization including how the vmm provides core operatingsystem functions like scheduling io and memory management here we answer questions such as these how do vmms schedule cpu use when guest operating systems believe they have dedicated cpus how can memory management work when many guests require large amounts of memory examples address the hardware networking address can move between systems before virtualization this did not happen as the mac address was tied to physical hardware with virtualization the mac must be movable for existing networking connections to continue without resetting modern network switches understand this and route traffic wherever the mac address is even accommodating a move a limitation of live migration is that no disk state is transferred one reason live migration is possible is that most of the guests state is maintained within the guest for example open file tables systemcall state kernel state and so on because disk io is so much slower than memory access and used disk space is usually much larger than used memory disks associated with the guest can not be moved as part of a live migration rather the disk must be remote to the guest accessed over the network in that case disk access state is maintained within the guest and network connections are all that matter to the vmm the network connections are maintained during the migration so remote disk access continues typically nfs cifs or iscsi is used to store virtual machine images and any other storage a guest needs access to those networkbased storage accesses simply continue when the network connections are continued once the guest has been migrated live migration enables entirely new ways of managing data centers for example virtualization management tools can monitor all the vmms in an environment and automatically balance resource use by moving guests between the vmms they can also optimize the use of electricity and cooling by migrating all guests off selected servers if other servers can handle the load and powering down the selected servers entirely if the load increases these tools can power up the servers and migrate guests back to them summary java program class loader java api class files class files java interpreter host system windows linux etc figure the java virtual machine ensures that the bytecode does not perform pointer arithmetic which could provide illegal memory access if the class passes verification it is run by the java interpreter the jvm also automatically manages memory by performing garbage collection the practice of reclaiming memory from objects no longer in use and returning it to the system much research focuses on garbage collection algorithms for increasing the performance of java programs in the virtual machine the jvm may be implemented in software on top of a host operating system such as windows linux or mac os x or as part of a web browser alternatively the jvm may be implemented in hardware on a chip specifically designed to run java programs if the jvm is implemented in software the java interpreter interprets the bytecode operations one at a time a faster software technique is to use a justintime jit compiler here the first time a java method is invoked the bytecodes for the method are turned into native machine language for the host system these operations are then cached so that subsequent invocations of a method are performed using the native machine instructions and the bytecode operations need not be interpreted all over again running the jvm in hardware is potentially even faster here a special java chip executes the java bytecode operations as native code thus bypassing the need for either a software interpreter or a justintime compiler advantages of distributed systems a distributed system is a collection of loosely coupled nodes interconnected by a communication network from the point of view of a specific node in a distributed system the rest of the nodes and their respective resources are remote whereas its own resources are local the nodes in a distributed system may vary in size and function they may include small microprocessors personal computers and large generalpurpose computer systems these processors are referred to by a number of names such as processors sites machines and hosts depending on the context in which they are mentioned we mainly use site to indicate the location of a machine and node to refer to a specific system at a site generally one node at one site the server has a resource that another node at another site the client or user would like to use a general structure of a distributed system is shown in figure there are four major reasons for building distributed systems resource sharing computation speedup reliability and communication in this section we briefly discuss each of them chapter distributed systems site a site c server network resources communication client site b figure a distributed system resource sharing if a number of different sites with different capabilities are connected to one another then a user at one site may be able to use the resources available at another for example a user at site a may be using a laser printer located at site b meanwhile a user at b may access a file that resides at a in general resource sharing in a distributed system provides mechanisms for sharing files at remote sites processing information in a distributed database printing files at remote sites using remote specialized hardware devices such as a supercomputer and performing other operations computation speedup if a particular computation can be partitioned into subcomputations that can run concurrently then a distributed system allows us to distribute the subcomputations among the various sites the subcomputations can be run concurrently and thus provide computation speedup in addition if a particular site is currently overloaded with jobs some of them can be moved to other lightly loaded sites this movement of jobs is called load sharing or job migration automated load sharing in which the distributed operating system automatically moves jobs is not yet common in commercial systems reliability if one site fails in a distributed system the remaining sites can continue operating giving the system better reliability if the system is composed of multiple large autonomous installations that is generalpurpose computers the failure of one of them should not affect the rest if however the system is composed of small machines each of which is responsible for some crucial system function such as the web server or the file system then a single failure may halt the operation of the whole system in general with enough types of networkbased operating systems redundancy in both hardware and data the system can continue operation even if some of its sites have failed the failure of a site must be detected by the system and appropriate action may be needed to recover from the failure the system must no longer use the services of that site in addition if the function of the failed site can be taken over by another site the system must ensure that the transfer of function occurs correctly finally when the failed site recovers or is repaired mechanisms must be available to integrate it back into the system smoothly communication when several sites are connected to one another by a communication network users at the various sites have the opportunity to exchange information at a low level messages are passed between systems much as messages are passed between processes in the singlecomputer message system discussed in section given message passing all the higherlevel functionality found in standalone systems can be expanded to encompass the distributed system such functions include file transfer login mail and remote procedure calls rpcs the advantage of a distributed system is that these functions can be carried out over great distances two people at geographically distant sites can collaborate on a project for example by transferring the files of the project logging in to each others remote systems to run programs and exchanging mail to coordinate the work users minimize the limitations inherent in longdistance work we wrote this book by collaborating in such a manner the advantages of distributed systems have resulted in an industrywide trend toward downsizing many companies are replacing their mainframes with networks of workstations or personal computers companies get a bigger bang for the buck that is better functionality for the cost more flexibility in locating resources and expanding facilities better user interfaces and easier maintenance network structure process migration a logical extension of computation migration is process migration when a process is submitted for execution it is not always executed at the site at which it is initiated the entire process or parts of it may be executed at different sites this scheme may be used for several reasons load balancing the processes or subprocesses may be distributed across the network to even the workload computation speedup if a single process can be divided into a number of subprocesses that can run concurrently on different sites then the total process turnaround time can be reduced hardware preference the process may have characteristics that make it more suitable for execution on some specialized processor such as matrix inversion on an array processor rather than on a microprocessor software preference the process may require software that is available at only a particular site and either the software can not be moved or it is less expensive to move the process data access just as in computation migration if the data being used in the computation are numerous it may be more efficient to have a process run remotely than to transfer all the data we use two complementary techniques to move processes in a computer network in the first the system can attempt to hide the fact that the process has migrated from the client the client then need not code her program explicitly to accomplish the migration this method is usually employed for achieving load balancing and computation speedup among homogeneous systems as they do not need user input to help them execute programs remotely the other approach is to allow or require the user to specify explicitly how the process should migrate this method is usually employed when the process must be moved to satisfy a hardware or software preference you have probably realized that the world wide web has many aspects of a distributed computing environment certainly it provides data migration between a web server and a web client it also provides computation migration for instance a web client could trigger a database operation on a web server finally with java javascript and similar languages it provides a form of process migration java applets and javascript scripts are sent from the server to the client where they are executed a network operating system provides most of these features but a distributed operating system makes them seamless and easily accessible the result is a powerful and easytouse facility one of the reasons for the huge growth of the world wide web communication structure communication protocols when we are designing a communication network we must deal with the inherent complexity of coordinating asynchronous operations communicating in a potentially slow and errorprone environment in addition the systems on the network must agree on a protocol or a set of protocols for determining host names locating hosts on the network establishing connections and so on we can simplify the design problem and related implementation by partitioning the problem into multiple layers each layer on one system communicates with the equivalent layer on other systems typically each layer has its own protocols and communication takes place between peer layers using a specific protocol the protocols may be implemented in hardware or software for instance figure shows the logical communications between two computers with the three lowestlevel layers implemented in hardware the international standards organization created the osi model for describing the various layers of networking while these layers are not implemented in practice they are useful for understanding how networking logically works and we describe them below computer a computer b ap ap application layer al presentation layer pl session layer sl transport layer tl network layer nl link layer ll physical layer pl data network network environment osi environment real systems environment figure two computers communicating via the osi network model an example tcpip we now return to the nameresolution issue raised in section and examine its operation with respect to the tcpip protocol stack on the internet then we consider the processing needed to transfer a packet between hosts on different ethernet networks we base our description on the ipv protocols which are the type most commonly used today in a tcpip network every host has a name and an associated ip address or hostid both of these strings must be unique and so that the name space can be managed they are segmented the name is hierarchical as explained in section describing the host name and then the organization with which the host is associated the hostid is split into a network number and a host number the proportion of the split varies depending on the size of the network once the internet administrators assign a network number the site with that number is free to assign hostids the sending system checks its routing tables to locate a router to send the frame on its way the routers use the network part of the hostid to transfer the packet from its source network to the destination network the destination system then receives the packet the packet may be a complete message or it may just be a component of a message with more packets needed before the message can be reassembled and passed to the tcpudp layer for transmission to the destination process within a network how does a packet move from sender host or router to receiver every ethernet device has a unique byte number called the medium access control mac address assigned to it for addressing two devices on a lan communicate with each other only with this number if a system needs to send data to another system the networking software generates an address resolution protocol arp packet containing the ip address of the destination system this packet is broadcast to all other systems on that ethernet network a broadcast uses a special network address usually the maximum address to signal that all hosts should receive and process the packet the broadcast is not resent by gateways so only systems on the local network receive it only the system whose ip address matches the ip address of the arp request responds and sends back its mac address to the system that initiated the query for efficiency the host caches the ip mac address pair in an internal table the cache entries are aged so that an entry is eventually removed from the cache if an access to that system is not required within a given time in robustness a distributed system may suffer from various types of hardware failure the failure of a link the failure of a site and the loss of a message are the most common types to ensure that the system is robust we must detect any of these failures reconfigure the system so that computation can continue and recover when a site or a link is repaired failure detection in an environment with no shared memory we are generally unable to differentiate among link failure site failure and message loss we can usually detect only that one of these failures has occurred once a failure has been detected appropriate action must be taken what action is appropriate depends on the particular application to detect link and site failure we use a heartbeat procedure suppose that sites a and b have a direct physical link between them at fixed intervals the sites send each other an iamup message if site a does not receive this message within a predetermined time period it can assume that site b has failed that the link between a and b has failed or that the message from b has been lost at this point site a has two choices it can wait for another time period to receive an iamup message from b or it can send an areyouup message to b if time goes by and site a still has not received an iamup message or if site a has sent an areyouup message and has not received a reply the procedure can be repeated again the only conclusion that site a can draw safely is that some type of failure has occurred site a can try to differentiate between link failure and site failure by sending an areyouup message to b by another route if one exists if and when b receives this message it immediately replies positively this positive reply tells a that b is up and that the failure is in the direct link between them since we do not know in advance how long it will take the message to travel from a to b and back we must use a timeout scheme at the time a sends the areyouup message it specifies a time interval during which it is willing to wait for the reply from b if a receives the reply message within that time interval then it can safely conclude that b is up if not however that is if a timeout occurs then a may conclude only that one or more of the following situations has occurred site b is down the direct link if one exists from a to b is down the alternative path from a to b is down the message has been lost site a can not however determine which of these events has occurred reconfiguration suppose that site a has discovered through the mechanism just described that a failure has occurred it must then initiate a procedure that will allow the system to reconfigure and to continue its normal mode of operation design issues making the multiplicity of processors and storage devices transparent to the users has been a key challenge to many designers ideally a distributed system should look to its users like a conventional centralized system the user interface of a transparent distributed system should not distinguish between local and remote resources that is users should be able to access remote resources as though these resources were local and the distributed system should be responsible for locating the resources and for arranging for the appropriate interaction another aspect of transparency is user mobility it would be convenient to allow users to log into any machine in the system rather than forcing them to use a specific machine a transparent distributed system facilitates user mobility by bringing over the users environment for example home directory to wherever he logs in protocols like ldap provide an authentication system for distributed file systems local remote and mobile users once the authentication is complete facilities like desktop virtualization allow users to see their desktop sessions at remote facilities still another issue is scalability the capability of a system to adapt to increased service load systems have bounded resources and can become completely saturated under increased load for example with respect to a file system saturation occurs either when a servers cpu runs at a high utilization rate or when disks io requests overwhelm the io subsystem scalability is a relative property but it can be measured accurately a scalable system reacts more gracefully to increased load than does a nonscalable one first its performance degrades more moderately and second its resources reach a saturated state later even perfect design can not accommodate an evergrowing load adding new resources might solve the problem but it might generate additional indirect load on other resources for example adding machines to a distributed system can clog the network and increase service loads even worse expanding the system can call for expensive design modifications a scalable system should have the potential to grow without these problems in a distributed system the ability to scale up gracefully is of special importance since expanding the network by adding new machines or interconnecting two networks is commonplace in short a scalable design should withstand high service load accommodate growth of the user community and allow simple integration of added resources scalability is related to fault tolerance discussed earlier a heavily loaded component can become paralyzed and behave like a faulty component in addition shifting the load from a faulty component to that components backup can saturate the latter generally having spare resources is essential for ensuring reliability as well as for handling peak loads gracefully thus the multiple resources in a distributed system represent an inherent advantage giving the system a greater potential for fault tolerance and scalability however inappropriate design can obscure this potential faulttolerance and scalability considerations call for a design demonstrating distribution of control and data facilities like the hadoop distributed file system were created with this problem in mind hadoop is based on googles mapreduce and google file system projects that created a facility to track every web page on the internet hadoop is an opensource programming framework that supports the processing of large data sets in distributed computing environments traditional systems with traditional databases can not scale to the capacity and performance needed by big data projects at least not at reasonable prices examples of big data projects include mining twitter for information pertinent to a company and sifting financial data to look for trends in stock pricing with hadoop and its related tools thousands of systems can work together to manage a distributed database of petabytes of information operatingsystem services an operating system provides an environment for the execution of programs it provides certain services to programs and to the users of those programs the specific services provided of course differ from one operating system to another but we can identify common classes these operating system services are provided for the convenience of the programmer to make the programming chapter operatingsystem structures user and other system programs gui batch command line user interfaces system calls program io file communication resource accounting execution operations systems allocation error protection detection and services security operating system hardware figure a view of operating system services task easier figure shows one view of the various operatingsystem services and how they interrelate one set of operating system services provides functions that are helpful to the user user interface almost all operating systems have a user interface ui this interface can take several forms one is a commandline interface cli which uses text commands and a method for entering them say a keyboard for typing in commands in a specific format with specific options another is a batch interface in which commands and directives to control those commands are entered into files and those files are executed most commonly a graphical user interface gui is used here the interface is a window system with a pointing device to direct io choose from menus and make selections and a keyboard to enter text some systems provide two or all three of these variations program execution the system must be able to load a program into memory and to run that program the program must be able to end its execution either normally or abnormally indicating error io operations a running program may require io which may involve a file or an io device for specific devices special functions may be desired such as recording to a cd or dvd drive or blanking a display screen for efficiency and protection users usually can not control io devices directly therefore the operating system must provide a means to do io filesystem manipulation the file system is of particular interest obviously programs need to read and write files and directories they also need to create and delete them by name search for a given file and list file information finally some operating systems include permissions management to allow or deny access to files or directories based on file ownership many operating systems provide a variety of file systems sometimes to allow personal choice and sometimes to provide specific features or performance characteristics summary consistency a client machine is sometimes faced with the problem of deciding whether a locally cached copy of data is consistent with the master copy and hence can be used if the client machine determines that its cached data are out of date it must cache an uptodate copy of the data before allowing further accesses there are two approaches to verifying the validity of cached data clientinitiated approach the client initiates a validity check in which it contacts the server and checks whether the local data are consistent with the master copy the frequency of the validity checking is the crux of this approach and determines the resulting consistency semantics it can range from a check before every access to a check only on first access to a file on file open basically every access coupled with a validity check is delayed compared with an access served immediately by the cache alternatively checks can be initiated at fixed time intervals depending on its frequency the validity check can load both the network and the server serverinitiated approach the server records for each client the files or parts of files that it caches when the server detects a potential inconsistency it must react a potential for inconsistency occurs when two different clients in conflicting modes cache a file if unix semantics section is implemented we can resolve the potential inconsistency by having the server play an active role the server must be notified whenever a file is opened and the intended mode read or write must be indicated for every open the server can then act when it detects that a file has been opened simultaneously in conflicting modes by disabling caching for that particular file actually disabling caching results in switching to a remoteservice mode of operation distributed file systems are in common use today providing file sharing within lans and across wans as well the complexity of implementing such a system should not be underestimated especially considering that it must be operatingsystem independent for widespread adoption and must provide availability and good performance in the presence of long distances and sometimesfrail networking linux history linux looks and feels much like any other unix system indeed unix compatibility has been a major design goal of the linux project however linux is much younger than most unix systems its development began in when a finnish university student linus torvalds began developing a small but selfcontained kernel for the processor the first true bit processor in intels range of pccompatible cpus chapter the linux system early in its development the linux source code was made available free both at no cost and with minimal distributional restrictions on the internet as a result linuxs history has been one of collaboration by many developers from all around the world corresponding almost exclusively over the internet from an initial kernel that partially implemented a small subset of the unix system services the linux system has grown to include all of the functionality expected of a modern unix system in its early days linux development revolved largely around the central operatingsystem kernel the core privileged executive that manages all system resources and interacts directly with the computer hardware we need much more than this kernel of course to produce a full operating system we thus need to make a distinction between the linux kernel and a complete linux system the linux kernel is an original piece of software developed from scratch by the linux community the linux system as we know it today includes a multitude of components some written from scratch others borrowed from other development projects and still others created in collaboration with other teams the basic linux system is a standard environment for applications and user programming but it does not enforce any standard means of managing the available functionality as a whole as linux has matured a need has arisen for another layer of functionality on top of the linux system this need has been met by various linux distributions a linux distribution includes all the standard components of the linux system plus a set of administrative tools to simplify the initial installation and subsequent upgrading of linux and to manage installation and removal of other packages on the system a modern distribution also typically includes tools for management of file systems creation and management of user accounts administration of networks web browsers word processors and so on the linux kernel the first linux kernel released to the public was version dated may it had no networking ran only on compatible intel processors and pc hardware and had extremely limited devicedriver support the virtual memory subsystem was also fairly basic and included no support for memorymapped files however even this early incarnation supported shared pages with copyonwrite and protected address spaces the only file system supported was the minix file system as the first linux kernels were crossdeveloped on a minix platform the next milestone linux was released on march this release culminated three years of rapid development of the linux kernel perhaps the single biggest new feature was networking included support for unixs standard tcpip networking protocols as well as a bsdcompatible socket interface for networking programming devicedriver support was added for running ip over ethernet or via the ppp or slip protocols over serial lines or modems the kernel also included a new much enhanced file system without the limitations of the original minix file system and it supported a range of scsi controllers for highperformance disk access the developers extended the virtual memory subsystem to support paging to swap files and memory mapping design principles in its overall design linux resembles other traditional nonmicrokernel unix implementations it is a multiuser preemptively multitasking system with a full set of unixcompatible tools linuxs file system adheres to traditional unix semantics and the standard unix networking model is fully implemented the internal details of linuxs design have been influenced heavily by the history of this operating systems development although linux runs on a wide variety of platforms it was originally developed exclusively on pc architecture a great deal of that early development was carried out by individual enthusiasts rather than by wellfunded development or research facilities so from the start linux attempted to squeeze as much functionality as possible from limited resources today linux can run happily on a multiprocessor machine with many gigabytes of main memory and many terabytes of disk space but it is still capable of operating usefully in under mb of ram as pcs became more powerful and as memory and hard disks became cheaper the original minimalist linux kernels grew to implement more unix functionality speed and efficiency are still important design goals but much recent and current work on linux has concentrated on a third major design goal standardization one of the prices paid for the diversity of unix implementations currently available is that source code written for one may not necessarily compile or run correctly on another even when the same system calls are present on two different unix systems they do not necessarily behave in exactly the same way the posix standards comprise a set of specifications for different aspects of operatingsystem behavior there are posix documents for common operatingsystem functionality and for extensions such as process threads and realtime operations linux is designed to comply with the relevant posix documents and at least two linux distributions have achieved official posix certification because it gives standard interfaces to both the programmer and the user linux presents few surprises to anybody familiar with unix we do not detail these interfaces here the sections on the programmer interface section a and user interface section a of bsd apply equally well to linux by default however the linux programming interface adheres to svr unix semantics rather than to bsd behavior a separate set of libraries is available to implement bsd semantics in places where the two behaviors differ significantly many other standards exist in the unix world but full certification of linux with respect to these standards is sometimes slowed because certification is often available only for a fee and the expense involved in certifying an operating systems compliance with most standards is substantial however supporting a wide base of applications is important for any operating system so implementation of standards is a major goal for linux development even if the implementation is not formally certified in addition to the basic posix kernel modules of the most important user utilities is the shell the standard commandline interface on unix systems linux supports many shells the most common is the bourneagain shell bash process management a process is the basic context in which all userrequested activity is serviced within the operating system to be compatible with other unix systems linux must use a process model similar to those of other versions of unix linux operates differently from unix in a few key places however in this section we review the traditional unix process model section a and introduce linuxs threading model the fork and exec process model the basic principle of unix process management is to separate into two steps two operations that are usually combined into one the creation of a new process and the running of a new program a new process is created by the fork system call and a new program is run after a call to exec these are two distinctly separate functions we can create a new process with fork without running a new program the new subprocess simply continues to execute exactly the same program at exactly the same point that the first parent process was running in the same way running a new program does not require that a new process be created first any process may call exec at any time a new binary object is loaded into the processs address space and the new executable starts executing in the context of the existing process this model has the advantage of great simplicity it is not necessary to specify every detail of the environment of a new program in the system call that runs that program the new program simply runs in its existing environment if a parent process wishes to modify the environment in which a new program is to be run it can fork and then still running the original executable in a child process make any system calls it requires to modify that child process before finally executing the new program under unix then a process encompasses all the information that the operating system must maintain to track the context of a single execution of a single program under linux we can break down this context into a number of specific sections broadly process properties fall into three groups the process identity environment and context process identity a process identity consists mainly of the following items process id pid each process has a unique identifier the pid is used to specify the process to the operating system when an application makes a scheduling processes and threads linux provides the fork system call which duplicates a process without loading a new executable image linux also provides the ability to create threads via the clone system call linux does not distinguish between processes and threads however in fact linux generally uses the term task rather than process or thread when referring to a flow of control within a program the clone system call behaves identically to fork except that it accepts as arguments a set of flags that dictate what resources are shared between the parent and child whereas a process created with fork shares no resources with its parent the flags include flag meaning clonefs filesystem information is shared clonevm the same memory space is shared clonesighand signal handlers are shared clonefiles the set of open files is shared thus if clone is passed the flags clone fs clone vm clone sighand and clone files the parent and child tasks will share the same filesystem information such as the current working directory the same memory space the same signal handlers and the same set of open files using clone in this fashion is equivalent to creating a thread in other systems since the parent task shares most of its resources with its child task if none of these flags is set when clone is invoked however the associated resources are not shared resulting in functionality similar to that of the fork system call the lack of distinction between processes and threads is possible because linux does not hold a processs entire context within the main process data structure rather it holds the context within independent subcontexts thus a processs filesystem context filedescriptor table signalhandler table and virtual memory context are held in separate data structures the process data structure simply contains pointers to these other structures so any number of processes can easily share a subcontext by pointing to the same subcontext and incrementing a reference count the arguments to the clone system call tell it which subcontexts to copy and which to share the new process is always given a new identity and a new scheduling context these are the essentials of a linux process according to the arguments passed however the kernel may either create new subcontext data structures initialized so as to be copies of the parents or set up the new process to use the same subcontext data structures being used by the parent the fork system call is nothing more than a special case of clone that copies all subcontexts sharing none user and operatingsystem interface we mentioned earlier that there are several ways for users to interface with the operating system here we discuss two fundamental approaches one provides a commandline interface or command interpreter that allows users to directly enter commands to be performed by the operating system the other allows users to interface with the operating system via a graphical user interface or gui command interpreters some operating systems include the command interpreter in the kernel others such as windows and unix treat the command interpreter as a special program that is running when a job is initiated or when a user first logs on on interactive systems on systems with multiple command interpreters to choose from the interpreters are known as shells for example on unix and linux systems a user may choose among several different shells including the bourne shell c shell bourneagain shell korn shell and others thirdparty shells and free userwritten shells are also available most shells provide similar functionality and a users choice of which shell to use is generally based on personal preference figure shows the bourne shell command interpreter being used on solaris the main function of the command interpreter is to get and execute the next userspecified command many of the commands given at this level manipulate files create delete list print copy execute and so on the msdos and unix shells operate in this way these commands can be implemented in two general ways in one approach the command interpreter itself contains the code to execute the command for example a command to delete a file may cause the command interpreter to jump to a section of its code that sets up the parameters and makes the appropriate system call in this case the number of commands that can be given determines the size of the command interpreter since each command requires its own implementing code an alternative approach used by unix among other operating systems implements most commands through system programs in this case the command interpreter does not understand the command in any way it merely uses the command to identify a file to be loaded into memory and executed thus the unix command to delete a file rm filetxt would search for a file called rm load the file into memory and execute it with the parameter filetxt the function associated with the rm command would memory management memory management under linux has two components the first deals with allocating and freeing physical memory pages groups of pages and small blocks of ram the second handles virtual memory which is memorymapped into the address space of running processes in this section we describe these two components and then examine the mechanisms by which the loadable components of a new program are brought into a processs virtual memory in response to an exec system call file systems space however most programs also need to run functions from the system libraries and these library functions must also be loaded in the simplest case the necessary library functions are embedded directly in the programs executable binary file such a program is statically linked to its libraries and statically linked executables can commence running as soon as they are loaded the main disadvantage of static linking is that every program generated must contain copies of exactly the same common system library functions it is much more efficient in terms of both physical memory and diskspace usage to load the system libraries into memory only once dynamic linking allows that to happen linux implements dynamic linking in user mode through a special linker library every dynamically linked program contains a small statically linked function that is called when the program starts this static function just maps the link library into memory and runs the code that the function contains the link library determines the dynamic libraries required by the program and the names of the variables and functions needed from those libraries by reading the information contained in sections of the elf binary it then maps the libraries into the middle of virtual memory and resolves the references to the symbols contained in those libraries it does not matter exactly where in memory these shared libraries are mapped they are compiled into positionindependent code pic which can run at any address in memory input and output inode when data are read from one of these files the proc file system will collect the appropriate information format it into textual form and place it into the requesting processs read buffer the mapping from inode number to information type splits the inode number into two fields in linux a pid is bits in size but an inode number is bits the top bits of the inode number are interpreted as a pid and the remaining bits define what type of information is being requested about that process a pid of zero is not valid so a zero pid field in the inode number is taken to mean that this inode contains global rather than processspecific information separate global files exist in proc to report information such as the kernel version free memory performance statistics and drivers currently running not all the inode numbers in this range are reserved the kernel can allocate new proc inode mappings dynamically maintaining a bitmap of allocated inode numbers it also maintains a tree data structure of registered global proc filesystem entries each entry contains the files inode number file name and access permissions along with the special functions used to generate the files contents drivers can register and deregister entries in this tree at any time and a special section of the tree appearing under the procsys directory is reserved for kernel variables files under this tree are managed by a set of common handlers that allow both reading and writing of these variables so a system administrator can tune the value of kernel parameters simply by writing out the new desired values in ascii decimal to the appropriate file to allow efficient access to these variables from within applications the procsys subtree is made available through a special system call sysctl that reads and writes the same variables in binary rather than in text without the overhead of the file system sysctl is not an extra facility it simply reads the proc dynamic entry tree to identify the variables to which the application is referring interprocess communication linux provides a rich environment for processes to communicate with each other communication may be just a matter of letting another process know that some event has occurred or it may involve transferring data from one process to another synchronization and signals the standard linux mechanism for informing a process that an event has occurred is the signal signals can be sent from any process to any other process with restrictions on signals sent to processes owned by another user however a limited number of signals are available and they can not carry information only the fact that a signal has occurred is available to a process signals are not generated only by processes the kernel also generates signals internally for example it can send a signal to a server process when data arrive on a network channel to a parent process when a child terminates or to a waiting process when a timer expires internally the linux kernel does not use signals to communicate with processes running in kernel mode if a kernelmode process is expecting an event to occur it will not use signals to receive notification of that event rather communication about incoming asynchronous events within the kernel takes place through the use of scheduling states and wait queue structures these mechanisms allow kernelmode processes to inform one another about relevant events and they also allow events to be generated by device drivers or by the networking system whenever a process wants to wait for some event to complete it places itself on a wait queue associated with that event and tells the scheduler that it is no longer eligible for execution once the event has completed every process on the wait queue will be awoken this procedure allows multiple processes to wait for a single event for example if several processes are trying to read a file from a disk then they will all be awakened once the data have been read into memory successfully although signals have always been the main mechanism for communicating asynchronous events among processes linux also implements the semaphore mechanism of system v unix a process can wait on a semaphore as easily as it can wait for a signal but semaphores have two advantages large numbers of semaphores can be shared among multiple independent processes and operations on multiple semaphores can be performed atomically internally the standard linux wait queue mechanism synchronizes processes that are communicating with semaphores passing of data among processes linux offers several mechanisms for passing data among processes the standard unix pipe mechanism allows a child process to inherit a communication channel from its parent data written to one end of the pipe can be read at the other under linux pipes appear as just another type of inode to virtual file system software and each pipe has a pair of wait queues to synchronize the reader and writer unix also defines a set of networking facilities that can send streams of data to both local and remote processes networking is covered in section network structure another process communications method shared memory offers an extremely fast way to communicate large or small amounts of data any data written by one process to a shared memory region can be read immediately by any other process that has mapped that region into its address space the main disadvantage of shared memory is that on its own it offers no synchronization a process can neither ask the operating system whether a piece of shared memory has been written to nor suspend execution until such a write occurs shared memory becomes particularly powerful when used in conjunction with another interprocesscommunication mechanism that provides the missing synchronization a sharedmemory region in linux is a persistent object that can be created or deleted by processes such an object is treated as though it were a small independent address space the linux paging algorithms can elect to page sharedmemory pages out to disk just as they can page out a processs data pages the sharedmemory object acts as a backing store for sharedmemory regions just as a file can act as a backing store for a memorymapped memory region when a file is mapped into a virtual address space region then any page faults that occur cause the appropriate page of the file to be mapped into virtual memory similarly sharedmemory mappings direct page faults to map in pages from a persistent sharedmemory object also just as for files sharedmemory objects remember their contents even if no processes are currently mapping them into virtual memory security representing multiple destinations the fib is organized as a set of hash tables indexed by destination address the tables representing the most specific routes are always searched first successful lookups from this table are added to the routecaching table which caches routes only by specific destination no wildcards are stored in the cache so lookups can be made quickly an entry in the route cache expires after a fixed period with no hits at various stages the ip software passes packets to a separate section of code for firewall management selective filtering of packets according to arbitrary criteria usually for security purposes the firewall manager maintains a number of separate firewall chains and allows a skbuff to be matched against any chain chains are reserved for separate purposes one is used for forwarded packets one for packets being input to this host and one for data generated at this host each chain is held as an ordered list of rules where a rule specifies one of a number of possible firewalldecision functions plus some arbitrary data for matching purposes two other functions performed by the ip driver are disassembly and reassembly of large packets if an outgoing packet is too large to be queued to a device it is simply split up into smaller fragments which are all queued to the driver at the receiving host these fragments must be reassembled the ip driver maintains an ipfrag object for each fragment awaiting reassembly and an ipq for each datagram being assembled incoming fragments are matched against each known ipq if a match is found the fragment is added to it otherwise a new ipq is created once the final fragment has arrived for a ipq a completely new skbuff is constructed to hold the new packet and this packet is passed back into the ip driver packets identified by the ip as destined for this host are passed on to one of the other protocol drivers the udp and tcp protocols share a means of associating packets with source and destination sockets each connected pair of sockets is uniquely identified by its source and destination addresses and by the source and destination port numbers the socket lists are linked to hash tables keyed on these four address and port values for socket lookup on incoming packets the tcp protocol has to deal with unreliable connections so it maintains ordered lists of unacknowledged outgoing packets to retransmit after a timeout and of incoming outoforder packets to be presented to the socket when the missing data have arrived summary linux is a modern free operating system based on unix standards it has been designed to run efficiently and reliably on common pc hardware it also runs on a variety of other platforms such as mobile phones it provides a programming interface and user interface compatible with standard unix systems and can run a large number of unix applications including an increasing number of commercially supported applications linux has not evolved in a vacuum a complete linux system includes many components that were developed independently of linux the core linux operatingsystem kernel is entirely original but it allows much existing free unix software to run resulting in an entire unixcompatible operating system free from proprietary code the linux kernel is implemented as a traditional monolithic kernel for performance reasons but it is modular enough in design to allow most drivers to be dynamically loaded and unloaded at run time linux is a multiuser system providing protection between processes and running multiple processes according to a timesharing scheduler newly created processes can share selective parts of their execution environment with their parent processes allowing multithreaded programming interprocess communication is supported by both system v mechanisms message queues semaphores and shared memory and bsds socket interface multiple networking protocols can be accessed simultaneously through the socket interface the memorymanagement system uses page sharing and copyonwrite to minimize the duplication of data shared by different processes pages are loaded on demand when they are first referenced and are paged back out to backing store according to an lfu algorithm if physical memory needs to be reclaimed to the user the file system appears as a hierarchical directory tree that obeys unix semantics internally linux uses an abstraction layer to manage multiple file systems deviceoriented networked and virtual file systems are supported deviceoriented file systems access disk storage through a page cache that is unified with the virtual memory system practice exercises dynamically loadable kernel modules give flexibility when drivers are added to a system but do they have disadvantages too under what circumstances would a kernel be compiled into a single binary file and when would it be better to keep it split into modules explain your answer exercises multithreading is a commonly used programming technique describe three different ways to implement threads and compare these three methods with the linux clone mechanism when might using each alternative mechanism be better or worse than using clones the linux kernel does not allow paging out of kernel memory what effect does this restriction have on the kernels design what are two advantages and two disadvantages of this design decision discuss three advantages of dynamic shared linkage of libraries compared with static linkage describe two cases in which static linkage is preferable compare the use of networking sockets with the use of shared memory as a mechanism for communicating data between processes on a single computer what are the advantages of each method when might each be preferred at one time unix systems used disklayout optimizations based on the rotation position of disk data but modern implementations including linux simply optimize for sequential data access why do they do so of what hardware characteristics does sequential access take advantage why is rotational optimization no longer so useful exercises what are the advantages and disadvantages of writing an operating system in a highlevel language such as c in what circumstances is the systemcall sequence fork exec most appropriate when is vfork preferable what socket type should be used to implement an intercomputer filetransfer program what type should be used for a program that periodically tests to see whether another computer is up on the network explain your answer linux runs on a variety of hardware platforms what steps must linux developers take to ensure that the system is portable to different processors and memorymanagement architectures and to minimize the amount of architecturespecific kernel code what are the advantages and disadvantages of making only some of the symbols defined inside a kernel accessible to a loadable kernel module what are the primary goals of the conflictresolution mechanism used by the linux kernel for loading kernel modules discuss how the clone operation supported by linux is used to support both processes and threads would you classify linux threads as userlevel threads or as kernellevel threads support your answer with the appropriate arguments what extra costs are incurred in the creation and scheduling of a process compared with the cost of a cloned thread chapter the linux system how does linuxs completely fair scheduler cfs provide improved fairness over a traditional unix process scheduler when is the fairness guaranteed what are the two configurable variables of the completely fair scheduler cfs what are the pros and cons of setting each of them to very small and very large values the linux scheduler implements soft realtime scheduling what features necessary for certain realtime programming tasks are missing how might they be added to the kernel what are the costs downsides of such features under what circumstances would a user process request an operation that results in the allocation of a demandzero memory region what scenarios would cause a page of memory to be mapped into a user programs address space with the copyonwrite attribute enabled in linux shared libraries perform many operations central to the operating system what is the advantage of keeping this functionality out of the kernel are there any drawbacks explain your answer what are the benefits of a journaling file system such as linuxs ext what are the costs why does ext provide the option to journal only metadata the directory structure of a linux operating system could include files corresponding to several different file systems including the linux proc file system how might the need to support different filesystem types affect the structure of the linux kernel in what ways does the linux setuid feature differ from the setuid feature svr the linux source code is freely and widely available over the internet and from cdrom vendors what are three implications of this availability for the security of the linux system bibliographical notes the linux system is a product of the internet as a result much of the available documentation on linux is available in some form on the internet the following key sites reference most of the useful information available the linux crossreference page lxr httplxrlinuxno maintains current listings of the linux kernel browsable via the web and fully crossreferenced the kernel hackers guide provides a helpful overview of the linux kernel components and internals and is located at httptldporgldptlktlkhtml bibliography the linux weekly news lwn httplwnnet provides weekly linuxrelated news including a very well researched subsection on linux kernel news many mailing lists devoted to linux are also available the most important are maintained by a mailinglist manager that can be reached at the email address majordomovgerrutgersedu send email to this address with the single line help in the mails body for information on how to access the list server and to subscribe to any lists finally the linux system itself can be obtained over the internet complete linux distributions are available from the home sites of the companies concerned and the linux community also maintains archives of current system components at several places on the internet the most important is ftpftpkernelorgpublinux in addition to investigating internet resources you can read about the internals of the linux kernel in mauerer and love bibliography love r love linux kernel development third edition developers library mauerer w mauerer professional linux kernel architecture john wiley and sons history in the mids microsoft and ibm cooperated to develop the os operating system which was written in assembly language for singleprocessor intel systems in microsoft decided to end the joint effort with ibm and develop its own new technology or nt portable operating system to support both the os and posix applicationprogramming interfaces apis in chapter windows october dave cutler the architect of the dec vaxvms operating system was hired and given the charter of building microsofts new operating system originally the team planned to use the os api as nts native environment but during development nt was changed to use a new bit windows api called win based on the popular bit api used in windows the first versions of nt were windows nt and windows nt advanced server at that time bit windows was at version windows nt version adopted the windows user interface and incorporated internet webserver and webbrowser software in addition userinterface routines and all graphics code were moved into the kernel to improve performance with the side effect of decreased system reliability although previous versions of nt had been ported to other microprocessor architectures the windows version released in february supported only intel and compatible processors due to marketplace factors windows incorporated significant changes it added active directory an xbased directory service better networking and laptop support support for plugandplay devices a distributed file system and support for more processors and more memory in october windows xp was released as both an update to the windows desktop operating system and a replacement for windows in the server edition of windows xp became available called windows net server windows xp updated the graphical user interface gui with a visual design that took advantage of more recent hardware advances and many new easeofuse features numerous features were added to automatically repair problems in applications and the operating system itself as a result of these changes windows xp provided better networking and device experience including zeroconfiguration wireless instant messaging streaming media and digital photographyvideo dramatic performance improvements for both the desktop and large multiprocessors and better reliability and security than earlier windows operating systems the longawaited update to windows xp called windows vista was released in november but it was not well received although windows vista included many improvements that later showed up in windows these improvements were overshadowed by windows vistas perceived sluggishness and compatibility problems microsoft responded to criticisms of windows vista by improving its engineering processes and working more closely with the makers of windows hardware and applications the result was windows which was released in october along with corresponding server editions of windows among the significant engineering changes is the increased use of execution tracing rather than counters or profiling to analyze system behavior tracing runs constantly in the system watching hundreds of scenarios execute when one of these scenarios fails or when it succeeds but does not perform well the traces can be analyzed to determine the cause windows uses a clientserver architecture like mach to implement two operatingsystem personalities win and posix with userlevel processes called subsystems at one time windows also supported an os subsystem but it was removed in windows xp due to the demise of os the subsystem architecture allows enhancements to be made to one operatingsystem personality without affecting the application compatibility of the other although the posix subsystem continues to be available for windows the win api has become very popular and the posix apis are used by only a few sites the subsystem approach continues to be interesting to study from an operating design principles system perspective but machinevirtualization technologies are now becoming the dominant way of running multiple operating systems on a single machine windows is a multiuser operating system supporting simultaneous access through distributed services or through multiple instances of the gui via the windows terminal services the server editions of windows support simultaneous terminal server sessions from windows desktop systems the desktop editions of terminal server multiplex the keyboard mouse and monitor between virtual terminal sessions for each loggedon user this feature called fast user switching allows users to preempt each other at the console of a pc without having to log off and log on we noted earlier that some gui implementation moved into kernel mode in windows nt it started to move into user mode again with windows vista which included the desktop window manager dwm as a usermode process dwm implements the desktop compositing of windows providing the windows aero interface look on top of the windows directx graphic software directx continues to run in the kernel as does the code implementing windows previous windowing and graphics models wink and gdi windows made substantial changes to the dwm significantly reducing its memory footprint and improving its performance windows xp was the first version of windows to ship a bit version for the ia in and the amd in internally the native nt file system ntfs and many of the win apis have always used bit integers where appropriate so the major extension to bit in windows xp was support for large virtual addresses however bit editions of windows also support much larger physical memories by the time windows shipped the amd isa had become available on almost all cpus from both intel and amd in addition by that time physical memories on client systems frequently exceeded the gb limit of the ia as a result the bit version of windows is now commonly installed on larger client systems because the amd architecture supports highfidelity ia compatibility at the level of individual processes and bit applications can be freely mixed in a single system in the rest of our description of windows we will not distinguish between the client editions of windows and the corresponding server editions they are based on the same core components and run the same binary files for the kernel and most drivers similarly although microsoft ships a variety of different editions of each release to address different market price points few of the differences between editions are reflected in the core of the system in this chapter we focus primarily on the core components of windows system calls system calls provide an interface to the services made available by an operating system these calls are generally available as routines written in c and c although certain lowlevel tasks for example tasks where hardware must be accessed directly may have to be written using assemblylanguage instructions before we discuss how an operating system makes system calls available lets first use an example to illustrate how system calls are used writing a simple program to read data from one file and copy them to another file the first input that the program will need is the names of the two files the input file and the output file these names can be specified in many ways depending on the operatingsystem design one approach is for the program to ask the user for the names in an interactive system this approach will require a sequence of system calls first to write a prompting message on the screen and then to read from the keyboard the characters that define the two files on mousebased and iconbased systems a menu of file names is usually displayed in a window the user can then use the mouse to select the source name and a window can be opened for the destination name to be specified this sequence requires many io system calls once the two file names have been obtained the program must open the input file and create the output file each of these operations requires another system call possible error conditions for each operation can require additional system calls when the program tries to open the input file for example it may find that there is no file of that name or that the file is protected against access in these cases the program should print a message on the console another sequence of system calls and then terminate abnormally another system call if the input file exists then we must create a new output file we may find that there is already an output file with the same name this situation may cause the program to abort a system call or we may delete the existing file another system call and create a new one yet another system call another option in an interactive system is to ask the user via a sequence of system calls to output the prompting message and to read the response from the terminal whether to replace the existing file or to abort the program when both files are set up we enter a loop that reads from the input file a system call and writes to the output file another system call each read and write must return status information regarding various possible error conditions on input the program may find that the end of the file has been reached or that there was a hardware failure in the read such as a parity error the write operation may encounter various errors depending on the output device for example no more disk space system components the architecture of windows is a layered system of modules as shown in figure the main layers are the hal the kernel and the executive all of which run in kernel mode and a collection of subsystems and services that run in user mode the usermode subsystems fall into two categories the environmental subsystems which emulate different operating systems and the protection subsystems which provide security functions one of the chief advantages of this type of architecture is that interactions between modules are kept simple the remainder of this section describes these layers and subsystems hardwareabstraction layer the hal is the layer of software that hides hardware chipset differences from upper levels of the operating system the hal exports a virtual hardware terminal services and fast user switching windows supports a guibased console that interfaces with the user via keyboard mouse and display most systems also support audio and video audio input is used by windows voicerecognition software voice recognition makes the system more convenient and increases its accessibility for users with disabilities windows added support for multitouch hardware allowing users to input data by touching the screen and making gestures with one or more fingers eventually the videoinput capability which is currently used for communication applications is likely to be used for visually interpreting gestures as microsoft has demonstrated for its xbox kinect product other future input experiences may evolve from microsofts surface computer most often installed at public venues such as hotels and conference centers the surface computer is a table surface with special cameras underneath it can track the actions of multiple users at once and recognize objects that are placed on top the pc was of course envisioned as a personal computer an inherently singleuser machine modern windows however supports the sharing of a pc among multiple users each user that is logged on using the gui has a session created to represent the gui environment he will be using and to contain all the processes created to run his applications windows allows multiple sessions to exist at the same time on a single machine however windows only supports a single console consisting of all the monitors keyboards and mice connected to the pc only one session can be connected to the console at a time from the logon screen displayed on the console users can create new sessions or attach to an existing session that was previously created this allows multiple users to share a single pc without having to log off and on between users microsoft calls this use of sessions fast user switching file system users can also create new sessions or connect to existing sessions on one pc from a session running on another windows pc the terminal server ts connects one of the gui windows in a users local session to the new or existing session called a remote desktop on the remote computer the most common use of remote desktops is for users to connect to a session on their work pc from their home pc many corporations use corporate terminalserver systems maintained in data centers to run all user sessions that access corporate resources rather than allowing users to access those resources from the pcs in each users office each server computer may handle many dozens of remotedesktop sessions this is a form of thinclient computing in which individual computers rely on a server for many functions relying on datacenter terminal servers improves reliability manageability and security of the corporate computing resources the ts is also used by windows to implement remote assistance a remote user can be invited to share a session with the user logged on to the session on the console the remote user can watch the users actions and even be given control of the desktop to help resolve computing problems networking even across volumes ntfs also supports hard links where a single file has an entry in more than one directory of the same volume change journal ntfs keeps a journal describing all changes that have been made to the file system usermode services can receive notifications of changes to the journal and then identify what files have changed by reading from the journal the search indexer service uses the change journal to identify files that need to be reindexed the filereplication service uses it to identify files that need to be replicated across the network volume shadow copies windows implements the capability of bringing a volume to a known state and then creating a shadow copy that can be used to back up a consistent view of the volume this technique is known as snapshots in some other file systems making a shadow copy of a volume is a form of copyonwrite where blocks modified after the shadow copy is created are stored in their original form in the copy to achieve a consistent state for the volume requires the cooperation of applications since the system can not know when the data used by the application are in a stable state from which the application could be safely restarted the server version of windows uses shadow copies to efficiently maintain old versions of files stored on file servers this allows users to see documents stored on file servers as they existed at earlier points in time the user can use this feature to recover files that were accidentally deleted or simply to look at a previous version of the file all without pulling out backup media programmer interface the win api is the fundamental interface to the capabilities of windows this section describes five main aspects of the win api access to kernel objects sharing of objects between processes process management interprocess communication and memory management access to kernel objects the windows kernel provides many services that application programs can use application programs obtain these services by manipulating kernel objects a process gains access to a kernel object named xxx by calling the createxxx function to open a handle to an instance of xxx this handle is unique to the process depending on which object is being opened if the create function fails it may return or it may return a special constant named invalid handle value a process can close any handle by calling the closehandle function and the system may delete the object if the count of handles referencing the object in all processes drops to zero sharing objects between processes windows provides three ways to share objects between processes the first way is for a child process to inherit a handle to the object when the parent calls the createxxx function the parent supplies a securities attributes structure with the binherithandle field set to true this field creates an inheritable handle next the child process is created passing a value of true to the createprocess functions binherithandle argument figure shows a code sample that creates a semaphore handle inherited by a child process assuming the child process knows which handles are shared the parent and child can achieve interprocess communication through the shared objects in the example in figure the child process gets the value of the handle from the first commandline argument and then shares the semaphore with the parent process the second way to share objects is for one process to give the object a name when the object is created and for the second process to open the name this method has two drawbacks windows does not provide a way to check whether an object with the chosen name already exists and the object name space is global without regard to the object type for instance two applications types of system calls system calls can be grouped roughly into six major categories process control file manipulation device manipulation information maintenance communications and protection in sections through we briefly discuss the types of system calls that may be provided by an operating system most of these system calls support or are supported by concepts and functions that are discussed in later chapters figure summarizes the types of system calls normally provided by an operating system as mentioned in this text we normally refer to the system calls by generic names throughout the text however we provide examples of the actual counterparts to the system calls for windows unix and linux systems process control a running program needs to be able to halt its execution either normally end or abnormally abort if a system call is made to terminate the currently running program abnormally or if the program runs into a problem and causes an error trap a dump of memory is sometimes taken and an error message generated the dump is written to disk and may be examined by a debugger a system program designed to aid the programmer in finding and correcting errors or bugs to determine the cause of the problem under either normal or abnormal circumstances the operating system must transfer control to the invoking command interpreter the command interpreter then reads the next command in an interactive system the command interpreter simply continues with the next command it is assumed that the user will issue an appropriate command to respond to any error in a gui system a popup window might alert the user to the error and ask for guidance in a batch system the command interpreter usually terminates the entire job and continues with the next job some systems may allow for special recovery actions in case an error occurs if the program discovers an error in its input and wants to terminate abnormally it may also want to define an error level more severe errors can be indicated by a higherlevel error parameter it is then summary microsoft designed windows to be an extensible portable operating system one able to take advantage of new techniques and hardware windows supports multiple operating environments and symmetric multiprocessing including both bit and bit processors and numa computers the use of kernel objects to provide basic services along with support for client server computing enables windows to support a wide variety of application environments windows provides virtual memory integrated caching and preemptive scheduling it supports elaborate security mechanisms and includes internationalization features windows runs on a wide variety of computers so users can choose and upgrade hardware to match their budgets and performance requirements without needing to alter the applications they run practice exercises what type of operating system is windows describe two of its major features list the design goals of windows describe two in detail chapter windows describe the booting process for a windows system describe the three main architectural layers of the windows kernel what is the job of the object manager what types of services does the process manager provide what is a local procedure call what are the responsibilities of the io manager what types of networking does windows support how does windows implement transport protocols describe two networking protocols how is the ntfs namespace organized how does ntfs handle data structures how does ntfs recover from a system crash what is guaranteed after a recovery takes place how does windows allocate user memory describe some of the ways in which an application can use memory via the win api exercises under what circumstances would one use the deferred procedure calls facility in windows what is a handle and how does a process obtain a handle describe the management scheme of the virtual memory manager how does the vm manager improve performance describe a useful application of the noaccess page facility provided in windows describe the three techniques used for communicating data in a local procedure call what settings are most conducive to the application of the different messagepassing techniques what manages caching in windows how is the cache managed how does the ntfs directory structure differ from the directory structure used in unix operating systems what is a process and how is it managed in windows what is the fiber abstraction provided by windows how does it differ from the thread abstraction how does usermode scheduling ums in windows differ from fibers what are some tradeoffs between fibers and ums ums considers a thread to have two parts a ut and a kt how might it be useful to allow uts to continue executing in parallel with their kts what is the performance tradeoff of allowing kts and uts to execute on different processors bibliography why does the selfmap occupy large amounts of virtual address space but no additional virtual memory how does the selfmap make it easy for the vm manager to move the pagetable pages to and from disk where are the pagetable pages kept on disk when a windows system hibernates the system is powered off suppose you changed the cpu or the amount of ram on a hibernating system do you think that would work why or why not give an example showing how the use of a suspend count is helpful in suspending and resuming threads in windows bibliographical notes russinovich and solomon give an overview of windows and considerable technical detail about system internals and components brown presents details of the security architecture of windows the microsoft developer network library httpmsdnmicrosoftcom supplies a wealth of information on windows and other microsoft products including documentation of all the published apis iseminger provides a good reference on the windows active directory detailed discussions of writing programs that use the win api appear in richter silberschatz et al supply a good discussion of b trees the source code for a wrk version of the windows kernel together with a collection of slides and other crk curriculum materials is available from wwwmicrosoftcomwindowsacademic for use by universities bibliography brown k brown programming windows security addisonwesley iseminger d iseminger active directory services for microsoft windows technical reference microsoft press richter j richter advanced windows microsoft press russinovich and solomon m e russinovich and d a solomon windows internals including windows server and windows vista fifth edition microsoft press silberschatz et al a silberschatz h f korth and s sudarshan database system concepts sixth edition mcgrawhill feature migration one reason to study early architectures and operating systems is that a feature that once ran only on huge systems may eventually make its way into very small systems indeed an examination of operating systems for mainframes and microcomputers shows that many features once available only on mainframes have been adopted for microcomputers the same operatingsystem concepts are thus appropriate for various classes of computers mainframes minicomputers microcomputers and handhelds to understand modern operating systems then you need to recognize the theme of feature migration and the long history of many operatingsystem features as shown in figure a good example of feature migration started with the multiplexed information and computing services multics operating system multics was early systems we turn our attention now to a historical overview of early computer systems we should note that the history of computing starts far before computers with looms and calculators we begin our discussion however with the computers of the twentieth century before the s computing devices were designed and implemented to perform specific fixed tasks modifying one of those tasks required a great deal of effort and manual labor all that changed in the s when alan turing and john von neumann and colleagues both separately and together worked on the idea of a more generalpurpose stored program computer such a machine atlas as a huge buffer for reading as far ahead as possible on input devices and for storing output files until the output devices are able to accept them spooling is also used for processing data at remote sites the cpu sends the data via communication paths to a remote printer or accepts an entire input job from a remote card reader the remote processing is done at its own speed with no cpu intervention the cpu just needs to be notified when the processing is completed so that it can spool the next batch of data spooling overlaps the io of one job with the computation of other jobs even in a simple system the spooler may be reading the input of one job while printing the output of a different job during this time still another job or other jobs may be executed reading its cards from disk and printing its output lines onto the disk spooling has a direct beneficial effect on the performance of the system for the cost of some disk space and a few tables the computation of one job and the io of other jobs can take place at the same time thus spooling can keep both the cpu and the io devices working at much higher rates spooling leads naturally to multiprogramming which is the foundation of all modern operating systems xds the rc the the the operating system was designed at the technische hogeschool in eindhoven in the netherlands in the mids it was a batch system running on a dutch computer the el x with kb of bit words the system was mainly noted for its clean design particularly its layer structure and its use of a set of concurrent processes employing semaphores for synchronization unlike the processes in the xds system the set of processes in the the system was static the operating system itself was designed as a set of cooperating processes in addition five user processes were created that served as the active agents to compile execute and print user programs when one job was finished the process would return to the input queue to select another job a priority cpuscheduling algorithm was used the priorities were recomputed every seconds and were inversely proportional to the amount of cpu time used recently in the last to seconds this scheme gave higher priority to iobound processes and to new processes memory management was limited by the lack of hardware support however since the system was limited and user programs could be written only in algol a software paging scheme was used the algol compiler automatically generated calls to system routines which made sure the requested information was in memory swapping if necessary the backing store was a kbword drum a word page was used with an lru pagereplacement strategy another major concern of the the system was deadlock control the bankers algorithm was used to provide deadlock avoidance closely related to the the system is the venus system the venus system was also a layerstructured design using semaphores to synchronize processes the lower levels of the design were implemented in microcode however providing a much faster system pagedsegmented memory was used for memory management in addition the system was designed as a timesharing system rather than a batch system ctss the compatible timesharing system ibm os venient and practical mode of computing one result of ctss was increased development of timesharing systems another result was the development of multics multics the multics operating system was designed from to at mit as a natural extension of ctss ctss and other early timesharing systems were so successful that they created an immediate desire to proceed quickly to bigger and better systems as larger computers became available the designers of ctss set out to create a timesharing utility computing service would be provided like electrical power large computer systems would be connected by telephone wires to terminals in offices and homes throughout a city the operating system would be a timeshared system running continuously with a vast file system of shared programs and data multics was designed by a team from mit ge which later sold its computer department to honeywell and bell laboratories which dropped out of the project in the basic ge computer was modified to a new computer system called the ge mainly by the addition of pagedsegmentation memory hardware in multics a virtual address was composed of an bit segment number and a bit word offset the segments were then paged in kbword pages the secondchance pagereplacement algorithm was used the segmented virtual address space was merged into the file system each segment was a file segments were addressed by the name of the file the file system itself was a multilevel tree structure allowing users to create their own subdirectory structures like ctss multics used a multilevel feedback queue for cpu scheduling protection was accomplished through an access list associated with each file and a set of protection rings for executing processes the system which was written almost entirely in pl comprised about lines of code it was extended to a multiprocessor system allowing a cpu to be taken out of service for maintenance while the system continued running cpm and msdos minicomputers came along and decreased the need for large monolithic systems they were followed by workstations and then personal computers which put computing power closer and closer to the end users tops dec created many influential computer systems during its history probably the most famous operating system associated with dec is vms a popular businessoriented system that is still in use today as openvms a product of hewlettpackard but perhaps the most influential of decs operating systems was tops tops started life as a research project at bolt beranek and newman bbn around bbn took the businessoriented dec pdp computer running tops added a hardware memorypaging system to implement virtual memory and wrote a new operating system for that computer to take advantage of the new hardware features the result was tenex a generalpurpose timesharing system dec then purchased the rights to tenex and created a new computer with a builtin hardware pager the resulting system was the decsystem and the tops operating system tops had an advanced commandline interpreter that provided help as needed to users that in combination with the power of the computer and its reasonable price made the decsystem the most popular timesharing system of its time in dec stopped work on its line of bit pdp computers to concentrate on bit vax systems running vms mach the other systems there are of course other operating systems and most of them have interesting properties the mcp operating system for the burroughs computer family was the first to be written in a system programming language it supported segmentation and multiple cpus the scope operating system for the cdc was also a multicpu system the coordination and synchronization of the multiple processes were surprisingly well designed history is littered with operating systems that suited a purpose for a time be it a long or a short time and then when faded were replaced by operating systems that had more features supported newer hardware were easier to use or were better marketed we are sure this trend will continue in the future system programs another aspect of a modern system is its collection of system programs recall figure which depicted the logical computer hierarchy at the lowest level is hardware next is the operating system then the system programs and finally the application programs system programs also known as system utilities provide a convenient environment for program development and execution some of them are simply user interfaces to system calls others are considerably more complex they can be divided into these categories file management these programs create delete copy rename print dump list and generally manipulate files and directories status information some programs simply ask the system for the date time amount of available memory or disk space number of users or similar status information others are more complex providing detailed performance logging and debugging information typically these programs format and print the output to the terminal or other output devices or files or display it in a window of the gui some systems also support a registry which is used to store and retrieve configuration information file modification several text editors may be available to create and modify the content of files stored on disk or other storage devices there may also be special commands to search contents of files or perform transformations of the text programminglanguage support compilers assemblers debuggers and interpreters for common programming languages such as c c java and perl are often provided with the operating system or available as a separate download program loading and execution once a program is assembled or compiled it must be loaded into memory to be executed the system may provide absolute loaders relocatable loaders linkage editors and overlay loaders debugging systems for either higherlevel languages or machine language are needed as well communications these programs provide the mechanism for creating virtual connections among processes users and computer systems they allow users to send messages to one anothers screens to browse web pages to send email messages to log in remotely or to transfer files from one machine to another background services all generalpurpose systems have methods for launching certain systemprogram processes at boot time some of these processes terminate after completing their tasks while others continue to run until the system is halted constantly running systemprogram processes are known as services subsystems or daemons one example is the network daemon discussed in section in that example a system needed a service to listen for network connections in order to connect those requests to the correct processes other examples include process schedulers that start processes according to a specified schedule system error monitoring services and print servers typical systems have dozens operatingsystem design and implementation of daemons in addition operating systems that run important activities in user context rather than in kernel context may use daemons to run these activities along with system programs most operating systems are supplied with programs that are useful in solving common problems or performing common operations such application programs include web browsers word processors and text formatters spreadsheets database systems compilers plotting and statisticalanalysis packages and games the view of the operating system seen by most users is defined by the application and system programs rather than by the actual system calls consider a users pc when a users computer is running the mac os x operating system the user might see the gui featuring a mouseandwindows interface alternatively or even in one of the windows the user might have a commandline unix shell both use the same set of system calls but the system calls look different and act in different ways further confusing the user view consider the user dualbooting from mac os x into windows now the same user on the same hardware has two entirely different interfaces and two sets of applications using the same physical resources on the same hardware then a user can be exposed to multiple user interfaces sequentially or concurrently operation of an os when a computer is switched on the boot procedure analyzes its configuration cpu type memory size io devices and details of other hardware connected to the computer see section it then loads a part of the os in memory initializes its data structures with this information and hands over control of the computer system to it figure is a schematic diagram of os operation see section an event like io completion or end of a time slice causes an interrupt when a process makes a system call eg to request resources or start an io operation it too leads to an interrupt called a software interrupt the interrupt chapter structure of operating systems a condition in a request by a hardware causes a process causes a hardware interrupt software interrupt context save io memory event handler handler handlers scheduler cpu is switched to the scheduled process figure overview of os operation table functions of an os function description process management initiation and termination of processes scheduling memory management allocation and deallocation of memory swapping virtual memory management io management io interrupt servicing initiation of io operations optimization of io device performance file management creation storage and access of files security and protection preventing interference with processes and resources network management sending and receiving of data over the network action switches the cpu to an interrupt servicing routine the interrupt servicing routine performs a context save action to save information about the interrupted program and activates an event handler which takes appropriate actions to handle the event the scheduler then selects a process and switches the cpu to it cpu switching occurs twice during the processing of an event first to the kernel to perform event handling and then to the process selected by the scheduler the functions of an os are thus implemented by event handlers when they are activated by interrupt servicing routines table summarizes these functions which primarily concern management of processes and resources and prevention of interference with them structure of an operating system policies and mechanisms in determining how an operating system is to perform one of its functions the os designer needs to think at two distinct levels policy a policy is the guiding principle under which the operating system will perform the function mechanism a mechanism is a specific action needed to implement a policy a policy decides what should be done while a mechanism determines how something should be done and actually does it a policy is implemented as a decisionmaking module that decides which mechanism modules to call under what conditions a mechanism is implemented as a module that performs a specific action the following example identifies policies and mechanisms in roundrobin scheduling example policies and mechanisms in roundrobin scheduling in scheduling we would consider the roundrobin technique section to be a policy the following mechanisms would be needed to implement the roundrobin scheduling policy maintain a queue of ready processes switch the cpu to execution of the selected process this action is called dispatching the prioritybased scheduling policy which is used in multiprogramming systems see section would also require a mechanism for maintaining information about ready processes however it would be different from the mechanism used in roundrobin scheduling because it would organize information according to process priority the dispatching mechanism however would be common to all scheduling policies apart from mechanisms for implementing specific process or resource management policies the os also has mechanisms for performing housekeeping actions the context save action mentioned in section is implemented as a mechanism portability and extensibility of operating systems the design and implementation of operating systems involves huge financial investments to protect these investments an operating system design should have a lifetime of more than a decade since several changes will take place in computer architecture io device technology and application environments during chapter structure of operating systems this time it should be possible to adapt an os to these changes two features are important in this context portability and extensibility porting is the act of adapting software for use in a new computer system portability refers to the ease with which a software program can be ported it is inversely proportional to the porting effort extensibility refers to the ease with which new functionalities can be added to a software system porting of an os implies changing parts of its code that are architecturedependent so that the os can work with new hardware some examples of architecturedependent data and instructions in an os are an interrupt vector contains information that should be loaded in various fields of the psw to switch the cpu to an interrupt servicing routine see section this information is architecturespecific information concerning memory protection and information to be provided to the memory management unit mmu is architecturespecific see sections and io instructions used to perform an io operation are architecturespecific the architecturedependent part of an operating systems code is typically associated with mechanisms rather than with policies an os would have high portability if its architecturedependent code is small in size and its complete code is structured such that the porting effort is determined by the size of the architecturedependent code rather than by the size of its complete code hence the issue of os portability is addressed by separating the architecturedependent and architectureindependent parts of an os and providing welldefined interfaces between the two parts extensibility of an os is needed for two purposes for incorporating new hardware in a computer system typically new io devices or network adapters and for providing new functionalities in response to new user expectations early operating systems did not provide either kind of extensibility hence even addition of a new io device required modifications to the os later operating systems solved this problem by adding a functionality to the boot procedure it would check for hardware that was not present when the os was last booted and either prompt the user to select appropriate software to handle the new hardware typically a set of routines called a device driver that handled the new device or itself select such software the new software was then loaded and integrated with the kernel so that it would be invoked and used appropriately modern operating systems go a step further by providing a plugandplay capability whereby new hardware can be added even while an os is in operation the os handles the interrupt caused by addition of new hardware selects the appropriate software and integrates it with the kernel lack of extensibility leads to difficulties in adapting an os to new user expectations several examples of such difficulties can be found in the history of operating systems in s and s pc users desired a new feature for setting up several sessions with an operating system at the same time several wellknown operating systems of that time eg msdos had difficulties providing operating systems with monolithic structure an os is a complex software that has a large number of functionalities and may contain millions of instructions it is designed to consist of a set of software modules where each module has a welldefined interface that must be used to access any of its functions or data such a design has the property that a module can not see inner details of functioning of other modules this property simplifies design coding and testing of an os early operating systems had a monolithic structure whereby the os formed a single software layer between the user and the bare machine ie the computer systems hardware see figure the user interface was provided by a command interpreter the command interpreter organized creation of user processes both the command interpreter and user processes invoked os functionalities and services through system calls two kinds of problems with the monolithic structure were realized over a period of time the sole os layer had an interface with the bare machine hence architecturedependent code was spread throughout the os and so there was poor portability it also made testing and debugging difficult leading to high costs of maintenance and enhancement these problems led to the search for alternative ways to structure an os in the following sections we discuss three methods of structuring an os that have been implemented as solutions to these problems layered structure the layered structure attacks the complexity and cost of developing and maintaining an os by structuring it into a number of layers see section the the multiprogramming system of the s is a wellknown example of a layered os kernelbased structure the kernelbased structure confines architecture dependence to a small section of the os code that constitutes the kernel see section so that portability is increased the unix os has a kernelbased structure user user interface process os layer bare machine figure monolithic os layered design of operating systems the monolithic os structure suffered from the problem that all os components had to be able to work with the bare machine this feature increased the cost and effort in developing an os because of the large semantic gap between the operating system and the bare machine definition semantic gap the mismatch between the nature of operations needed in the application and the nature of operations provided in the machine the semantic gap can be illustrated as follows a machine instruction implements a machinelevel primitive operation like arithmetic or logical manipulation of operands an os module may contain an algorithm say that uses oslevel primitive operations like saving the context of a process and initiating an io operation these operations are more complex than the machinelevel primitive operations this difference leads to a large semantic gap which has to be bridged through programming each operation desired by the os now becomes a sequence of instructions possibly a routine see figure it leads to high programming costs the semantic gap between an os and the machine on which it operates can be reduced by either using a more capable machine a machine that provides instructions to perform some or all operations that operating systems have to perform or by simulating a more capable machine in the software the former approach is expensive in the latter approach however the simulator which is a operating program io system management management semantic gap bare arithmetic logical io machine instructions instructions instructions figure semantic gap part overview program executes on the bare machine and mimics a more powerful machine that has many features desired by the os this new machine is called an extended machine and its simulator is called the extended machine software now the os interfaces with the extended machine rather than with the bare machine the extended machine software forms a layer between the os and the bare machine the basic discipline in designing a layered os is that the routines of one layer must use only the facilities of the layer directly below it that is no layer in the structure can be bypassed further access to routines of a lower layer must take place strictly through the interface between layers thus a routine situated in one layer does not know addresses of data structures or instructions in the lower layer it only knows how to invoke a routine of the lower layer this property which we will call information hiding prevents misuse or corruption of one layers data by routines situated in other layers of the os during debugging localization of errors becomes easy since the cause of an error in a layer eg an incorrect value in its data element must lie within that layer itself information hiding also implies that an os layer may be modified without affecting other layers these features simplify testing and debugging of an os figure illustrates a twolayered os the extended machine provides operations like context save dispatching swapping and io initiation the operating system layer is located on top of the extended machine layer this arrangement considerably simplifies the coding and testing of os modules by separating the algorithm of a function from the implementation of its primitive operations it is now easier to test debug and modify an os module than in a monolithic os we say that the lower layer provides an abstraction that is the extended machine we call the operating system layer the top layer of the os the layered structures of operating systems have been evolved in various ways using different abstractions and a different number of layers example describes the the multiprogramming os which uses a multilayered structure and provides a process as an abstraction in the lowest layer operating process io system management management semantic gap extended context dispatch perform machine save a process io bare machine figure layered os design chapter structure of operating systems structure of the the multiprogramming system example the the multiprogramming system was developed at technische hogeschool eindhoven in the netherlands by dijkstra and others using a layered design table shows the hierarchy of layers in the the system layer of the system handles processor allocation to implement multiprogramming this function involves keeping track of process states and switching between processes using prioritybased scheduling layers above layer need not concern themselves with these issues in fact they can be oblivious to the presence of multiple processes in the system layer performs memory management it implements a memory hierarchy consisting of the memory and a drum which is a secondary storage device see section details of transfer between the memory and the drum need not concern the rest of the os layer implements communication between a process and the operators console by allocating a virtual console to each process layer performs io management intricacies of io programming see section are thus hidden from layer which is occupied by user processes the layered approach to os design suffers from three problems the operation of a system may be slowed down by the layered structure recall that each layer can interact only with adjoining layers it implies that a request for os service made by a user process must move down from the highest numbered layer to the lowest numbered layer before the required action is performed by the bare machine this feature leads to high overhead the second problem concerns difficulties in developing a layered design since a layer can access only the immediately lower layer all features and facilities needed by it must be available in lower layers this requirement poses a problem in the ordering of layers that require each others services this problem is often solved by splitting a layer into two and putting other layers between the two halves for example a designer may wish to put process handling functions in one layer and memory management in the next higher layer however memory allocation is required as a part of process creation to overcome this difficulty process handling can be split into two layers one layer would perform process management functions like context save switching scheduling and process synchronization table layers in the the multiprogramming system layer description layer processor allocation and multiprogramming layer memory and drum management layer operatorprocess communication layer io management layer user processes virtual machine operating systems different classes of users need different kinds of user service hence running a single os on a computer system can disappoint many users operating the computer under different oss during different periods is not a satisfactory solution because it would make accessible services offered under only one of the operating systems at any time this problem is solved by using a virtual machine operating system vm os to control the computer system the vm os creates several virtual machines each virtual machine is allocated to one user who can use any os of his own choice on the virtual machine and run his programs under this os this way users of the computer system can use different operating systems at the same time we call each of these operating systems a guest os and call the virtual machine os the host os the computer used by the vm os is called the host machine a virtual machine is a virtual resource see section let us consider a virtual machine that has the same architecture as the host machine ie it has a virtual cpu capable of executing the same instructions and similar memory and io devices it may however differ from the host machine in terms of some elements of its configuration like memory size and io devices because of the identical architectures of the virtual and host machines no semantic gap exists between them so operation of a virtual machine does not introduce any performance loss contrast this with the use of the extended machine layer described in section software intervention is also not needed to run a guest os on a virtual machine the vm os achieves concurrent operation of guest operating systems through an action that resembles process scheduling it selects a virtual machine and arranges to let the guest os running on it execute its instructions on the cpu the guest os in operation enjoys complete control over the host machines chapter structure of operating systems environment including interrupt servicing the absence of a software layer between the host machine and guest os ensures efficient use of the host machine a guest os remains in control of the host machine until the vm os decides to switch to another virtual machine which typically happens in response to an interrupt the vm os can employ the timer to implement timeslicing and roundrobin scheduling of guest oss a somewhat complex arrangement is needed to handle interrupts that arise when a guest os is in operation some of the interrupts would arise in its own domain eg an io interrupt from a device included in its own virtual machine while others would arise in the domains of other guest oss the vm os can arrange to get control when an interrupt occurs find the guest os whose domain the interrupt belongs to and schedule that guest os to handle it however this arrangement incurs high overhead because of two context switch operations the first context switch passes control to the vm os and the second passes control to the correct guest os hence the vm os may use an arrangement in which the guest os in operation would be invoked directly by interrupts arising in its own domain it is implemented as follows while passing control to a guest operating system the vm os replaces its own interrupt vectors see section by those defined in the guest os this action ensures that an interrupt would switch the cpu to an interrupt servicing routine of the guest os if the guest os finds that the interrupt did not occur in its own domain it passes control to the vm os by making a special system call invoke vm os the vm os now arranges to pass the interrupt to the appropriate guest os when a large number of virtual machines exists interrupt processing can cause excessive shuffling between virtual machines hence the vm os may not immediately activate the guest os in whose domain an interrupt occurred it may simply note occurrence of interrupts that occurred in the domain of a guest os and provide this information to the guest os the next time it is scheduled example describes how ibm vm a wellknown vm os of the s operates structure of vm example figure shows three of the guest oss supported by vm the conversational monitor system cms is a singleuser operating system while the os and dos are multiprogramming operating systems a user process is unaware of the presence of the vm it sees only the guest os that it uses to prevent interference between the guest oss the cpu is put in the user mode while executing a guest os initiation of io operations which involves use of privileged instructions is handled as follows when the kernel of a guest os executes an io instruction it appears as an attempt to execute a privileged instruction while the cpu is in the user mode so it causes a program interrupt the interrupt is directed to the vm rather than to the guest os the vm now initiates the io operation by executing the io instruction that had caused the interrupt part overview cms os dos vm figure virtual machine operating system vm distinction between kernel and user modes of the cpu causes some difficulties in the use of a vm os the vm os must protect itself from guest oss so it must run guest oss with the cpu in the user mode however this way both a guest os and user processes under it run in the user mode which makes the guest os vulnerable to corruption by a user process the intel x family of computers has a feature that provides a way out of this difficulty the x computers support four execution modes of the cpu hence the host os can run with the cpu in the kernel mode a guest os can execute processes running under it with the cpu in the user mode but can itself run with the cpu in one of the intermediate modes virtualization is the process of mapping the interfaces and resources of a virtual machine into the interfaces and resources of the host machine full virtualization would imply that the host machine and a virtual machine have identical capabilities hence an os can operate identically while running on a bare machine and on a virtual machine supported by a vm os however full virtualization may weaken security in example we saw how vm lets a guest os execute a privileged instruction but its execution causes an interrupt and vm itself executes the instruction on behalf of the guest os this arrangement is insecure because vm can not determine whether use of the privileged instruction is legitimate it would be legitimate if a guest os used it but illegitimate if a user process used it modern virtual machine environments employ the technique of paravirtualization to overcome the problems faced in full virtualization paravirtualization replaces a nonvirtualizable instruction ie an instruction that can not be made available in a vm by easily virtualized instructions for example the security issue in vm could be resolved through paravirtualization as follows the privileged instructions would not be included in a virtual machine instead the virtual machine would provide a special instruction for use by a guest os that wished to execute a privileged instruction the special instruction would cause a software interrupt and pass information about the privileged instruction the guest os wished to execute to the vm os and the vm os would execute the privileged instruction on behalf of the guest os the host os guest os and user processes would use different execution modes of the cpu so that the host os would know whether the special instruction in the virtual machine was used by a guest os or by a user process the latter usage would be considered illegal paravirtualization has also been used to enhance performance of a host os chapter structure of operating systems the kernel of an os typically puts the cpu into an idle loop when none of the user processes in the os wishes to use the cpu however cpu time of the host machine would be wasted when a guest os enters into an idle loop hence paravirtualization could be employed to provide a special instruction in the virtual machine to notify this condition to the host os so that the host os could take away the cpu from the guest os for a specified period of time use of paravirtualization implies that a virtual machine would differ from the host machine so the code of a guest os would have to be modified to avoid use of nonvirtualizable instructions it can be done by porting a guest os to operate under the vm os alternatively it can be achieved by employing the technique of dynamic binary translation for the kernel of a guest os which replaces a portion of kernel code that contains nonvirtualizable instructions by code that does not contain such instructions to reduce the overhead of this arrangement the modified kernel code is cached so that binary translation does not have to be repeated often virtual machines are employed for diverse purposes to use an existing server for a new application that requires use of a different operating system this is called workload consolidation it reduces the hardware and operational cost of computing by reducing the number of servers needed in an organization to provide security and reliability for applications that use the same host and the same os this benefit arises from the fact that virtual machines of different applications can not access each others resources to test a modified os or a new version of application code on a server concurrently with production runs of that os to provide disaster management capabilities by transferring a virtual machine from a server that has to shut down because of an emergency to another server available on the network a vm os is large complex and expensive to make the benefits of virtual machines available widely at a lower cost virtual machines are also used without a vm os two such arrangements are described in the following virtual machine monitors vmms a vmm also called a hypervisor is a software layer that operates on top of a host os it virtualizes the resources of the host computer and supports concurrent operation of many virtual machines when a guest os is run in each virtual machine provided by a vmm the host os and the vmm together provide a capability that is equivalent of a vm os vmware and xen are two vmms that aim at implementing hundreds of guest oss on a host computer while ensuring that a guest os suffers only a marginal performance degradation when compared to its implementation on a bare machine programming language virtual machines programming languages have used virtual machines to obtain some of the benefits discussed earlier in the s the kernelbased operating systems figure is an abstract view of a kernelbased os the kernel is the core of the os it provides a set of functions and services to support various os functionalities the rest of the os is organized as a set of nonkernel routines which implement operations on processes and resources that are of interest to users and a user user interface nonkernel routines kernel bare machine figure structure of a kernelbased os chapter structure of operating systems interface recall from section and figure that the operation of the kernel is interruptdriven the kernel gets control when an interrupt such as a timer interrupt or an io completion interrupt notifies occurrence of an event to it or when the softwareinterrupt instruction is executed to make a system call when the interrupt occurs an interrupt servicing routine performs the context save function and invokes an appropriate event handler which is a nonkernel routine of the os a system call may be made by the user interface to implement a user command by a process to invoke a service in the kernel or by a nonkernel routine to invoke a function of the kernel for example when a user issues a command to execute the program stored in some file say file alpha the user interface makes a system call and the interrupt servicing routine invokes a nonkernel routine to set up execution of the program the nonkernel routine would make system calls to allocate memory for the programs execution open file alpha and load its contents into the allocated memory area followed by another system call to initiate operation of the process that represents execution of the program if a process wishes to create a child process to execute the program in file alpha it too would make a system call and identical actions would follow the historical motivations for the kernelbased os structure were portability of the os and convenience in the design and coding of nonkernel routines portability of the os is achieved by putting architecturedependent parts of os code which typically consist of mechanisms in the kernel and keeping architectureindependent parts of code outside it so that the porting effort is limited only to porting of the kernel the kernel is typically monolithic to ensure efficiency the nonkernel part of an os may be monolithic or it may be further structured into layers table contains a sample list of functions and services offered by the kernel to support various os functionalities these functions and services provide a set of abstractions to the nonkernel routines their use simplifies design and coding of nonkernel routines by reducing the semantic gap faced by them see section for example the io functions of table collectively implement the abstraction of virtual devices see section a process is another abstraction provided by the kernel a kernelbased design may suffer from stratification analogous to the layered os design see section because the code to implement an os command may contain an architecturedependent part which is typically a mechanism that would be included in the kernel and an architectureindependent part which is typically the implementation of a policy that would be kept outside the kernel these parts would have to communicate with one another through system calls which would add to os overhead because of interrupt servicing actions consider the command to initiate execution of the program in a file named alpha as discussed earlier the nonkernel routine that implements the command would make four system calls to allocate memory open file alpha load the program contained in it into memory and initiate its execution which would incur considerable overhead some operating system designs reduce os overhead by including the architectureindependent part of a functions code also in the kernel part overview table typical functions and services offered by the kernel os functionality examples of kernel functions and services process management save context of the interrupted program dispatch a process manipulate scheduling lists process communication send and receive interprocess messages memory management set memory protection information swapin swapout handle page fault that is missing from memory interrupt of section io management initiate io process io completion interrupt recover from io errors file management open a file readwrite data security and protection add authentication information for a new user maintain information for file protection network management sendreceive data through a message thus the nonkernel routine that initiated execution of a program would become a part of the kernel other such examples are process scheduling policies io scheduling policies of device drivers and memory management policies these inclusions reduce os overhead however they also reduce portability of the os kernelbased operating systems have poor extensibility because addition of a new functionality to the os may require changes in the functions and services offered by the kernel evolution of kernelbased structure of operating systems the structure of kernelbased operating systems evolved to offset some of its drawbacks two steps in this evolution were dynamically loadable kernel modules and userlevel device drivers to provide dynamically loadable kernel modules the kernel is designed as a set of modules that interact among themselves through wellspecified interfaces a base kernel consisting of a core set of modules is loaded when the system is booted other modules which conform to interfaces of the base kernel are loaded when their functionalities are needed and are removed from memory when they are no longer needed use of loadable modules conserves memory during os operation because only required modules of the kernel are in memory at any time it also provides extensibility as kernel modules can be modified separately and new modules can be added to the kernel easily use of loadable kernel modules has a few drawbacks too loading and removal of modules fragments memory so the kernel has to perform memory management actions to reduce its memory requirement a buggy module can also crash a system loadable kernel modules are used to implement device drivers for new io devices network adapters or microkernelbased operating systems putting all architecturedependent code of the os into the kernel provides good portability however in practice kernels also include some architectureindependent code this feature leads to several problems it leads to a large kernel size which detracts from the goal of portability it may also necessitate kernel modification to incorporate new features which causes low extensibility a large kernel supports a large number of system calls some of these calls may be used rarely and so their implementations across different versions of the kernel may not be tested thoroughly this compromises reliability of the os the microkernel was developed in the early s to overcome the problems concerning portability extensibility and reliability of kernels a microkernel is an essential core of os code thus it contains only a subset of the mechanisms typically included in a kernel and supports only a small number of system calls which are heavily tested and used this feature enhances portability and reliability part overview servers user processes roundrobin memory process handler scheduler microkernel bare machine figure structure of microkernelbased operating systems of the microkernel less essential parts of os code are outside the microkernel and use its services hence these parts could be modified without affecting the kernel in principle these modifications could be made without having to reboot the os the services provided in a microkernel are not biased toward any specific features or policies in an os so new functionalities and features could be added to the os to suit specific operating environments figure illustrates the structure of a microkernelbased os the microkernel includes mechanisms for process scheduling and memory management etc but does not include a scheduler or memory handler these functions are implemented as servers which are simply processes that never terminate the servers and user processes operate on top of the microkernel which merely performs interrupt handling and provides communication between the servers and user processes the small size and extensibility of microkernels are valuable properties for the embedded systems environment because operating systems need to be both small and finetuned to the requirements of an embedded application extensibility of microkernels also conjures the vision of using the same microkernel for a wide spectrum of computer systems from palmheld systems to large parallel and distributed systems this vision has been realized to some extent the mach microkernel has been used to implement several different versions of unix the distributed operating system amoeba uses an identical microkernel on all computers in a distributed system ranging from workstations to large multiprocessors just what is the essential core of os code has been a matter of some debate and as a result considerable variation exists in the services included in a microkernel for example ibms implementation of the mach microkernel leaves the process scheduling policy and device drivers outside the kernel these functions run as servers the qnx microkernel includes interrupt servicing routines process scheduling interprocess communication and core network services the l microkernel includes memory management and supports only seven system calls both qnx and l are only kb in size where kb is bytes despite such variation it can be argued that certain services must be provided by a microkernel these include memory management support interprocess communication and interrupt servicing memory management and interprocess communication case studies previous sections discussed the structure of an operating system that is arrangement of its parts and properties of these arrangements in this section we discuss both structure and architecture of some modern operating systems where architecture concerns the structure of the operating system as well as functionalities of its components and relationships between them design and implementation features of specific os components are described in relevant chapters of parts of this text part overview architecture of unix unix is a kernelbased operating system figure is a schematic diagram of the unix kernel it consists of two main components process management and file management the process management component consists of a module for interprocess communication which implements communication and synchronization between processes and the memory management and scheduling modules the file management component performs io through device drivers each device driver handles a specific class of io devices and uses techniques like disk scheduling to ensure good throughput of an io device the buffer cache is used to reduce both the time required to implement a data transfer between a process and an io device and the number of io operations performed on devices like disks see section the process management and file management components of the kernel are activated through interrupts raised in the hardware and system calls made by processes and nonkernel routines of the os the user interface of the os is a command interpreter called a shell that runs as a user process the unix kernel can not be interrupted at any arbitrary moment of time it can be interrupted only when a process executing kernel code exits or when its execution reaches a point at which it can be safely interrupted this feature ensures that the kernel data structures are not in an inconsistent state when an interrupt occurs and another process starts executing the kernel code which considerably simplifies coding of the kernel see section the unix kernel has a long history of over four decades the original kernel was small and simple it provided a small set of abstractions simple but powerful features like the pipe mechanism which enabled users to execute several programs concurrently and a small file system that supported only one file organization called the byte stream organization all devices were represented as files which unified the management of io devices and files the kernel was nonkernel routines system calls interrupts kernel file management interprocess scheduler communimemory buffer cache cation management device drivers process management hardware figure kernel of the unix operating system chapter structure of operating systems written in the c language and had a size of less than kb hence it was easily portable however the unix kernel was monolithic and not very extensible so it had to be modified as new computing environments like the clientserver environment evolved interprocess communication and threads were added to support clientserver computing networking support similarly required kernel modification a major strength of unix was its use of open standards it enabled a large number of organizations ranging from the academia to the industry to participate in its development which led to widespread use of unix but also led to the development of a large number of variants because of concurrent and uncoordinated development the kernel became bulky growing to a few million bytes in size which affected its portability around this time a feature was added to dynamically load kernel modules in memory it enabled kernel modules to be loaded only when needed this feature reduced the memory requirement of the kernel but not its code size hence it did not enhance its portability several efforts have been made to redesign the unix kernel to make it modular and extensible the mach kernel which has a specific emphasis on multiprocessor systems is an example of this trend later mach developed into a microkernelbased operating system the kernel of linux the linux operating system provides the functionalities of unix system v and unix bsd it is also compliant with the posix standard it was initially implemented on the intel and has since been implemented on later intel processors and several other architectures linux has a monolithic kernel the kernel is designed to consist of a set of individually loadable modules each module has a wellspecified interface that indicates how its functionalities can be invoked and its data can be accessed by other modules conversely the interface also indicates the functions and data of other modules that are used by this module each module can be individually loaded into memory or removed from it depending on whether it is likely to be used in near future in principle any component of the kernel can be structured as a loadable module but typically device drivers become separate modules a few kernel modules are loaded when the system is booted a new kernel module is loaded dynamically when needed however it has to be integrated with the kernel modules that already existed in memory so that the modules can collectively function as a monolithic kernel this integration is performed as follows the kernel maintains a table in which it records the addresses of functions and data that are defined in the modules existing in memory while loading a new module the kernel analyzes its interface and finds which functions and data of other modules it uses obtains their addresses from the table and inserts them in appropriate instructions of the new module at the end of this step the kernel part overview updates its table by adding the addresses of functions and data defined in the new module use of kernel modules with wellspecified interfaces provides several advantages existence of the module interface simplifies testing and maintenance of the kernel an individual module can be modified to provide new functionalities or enhance existing ones this feature overcomes the poor extensibility typically associated with monolithic kernels use of loadable modules also limits the memory requirement of the kernel because some modules may not be loaded during an operation of the system to enhance this advantage the kernel has a feature to automatically remove unwanted modules from memory it produces an interrupt periodically and checks which of its modules in memory have not been used since the last such interrupt these modules are delinked from the kernel and removed from memory alternatively modules can be individually loaded and removed from memory through system calls the linux kernel which was released in removed many of the limitations of the linux kernel and also enhanced its capabilities in several ways two of the most prominent improvements were in making the system more responsive and capable of supporting embedded systems kernels up to linux were nonpreemptible so if the kernel was engaged in performing a lowpriority task higherpriority tasks of the kernel were delayed the linux kernel is preemptible which makes it more responsive to users and application programs however the kernel should not be preempted when it is difficult to save its state or when it is performing sensitive operations so the kernel disables and enables its own preemptibility through special functions the linux kernel can also support architectures that do not possess a memory management unit mmu which makes it suitable for embedded systems thus the same kernel can now be used in embedded systems desktops and servers the other notable feature in the linux kernel is better scalability through an improved model of threads an improved scheduler and fast synchronization between processes these features are described in later chapters the kernel of solaris early operating systems for sun computer systems were based on bsd unix however later development was based on unix svr the presvr versions of the os are called sunos while the svrbased and later versions are called solaris since the s sun has focused on networking and distributed computing several networking and distributed computing features of its operating systems have become industry standards eg remote procedure calls rpc and a file system for distributed environments nfs later sun also focused on multiprocessor systems which resulted in an emphasis on multithreading the kernel making it preemptible see section and employing fast synchronization techniques in the kernel the solaris kernel has an abstract machine layer that supports a wide range of processor architectures of the sparc and intel x family including multiprocessor architectures the kernel is fully preemptible and provides realtime chapter structure of operating systems capabilities solaris employs the kerneldesign methodology of dynamically loadable kernel modules see section the kernel has a core module that is always loaded it contains interrupt servicing routines system calls process and memory management and a virtual file system framework that can support different file systems concurrently other kernel modules are loaded and unloaded dynamically each module contains information about other modules on which it depends and about other modules that depend on it the kernel maintains a symbol table containing information about symbols defined in currently loaded kernel modules this information is used while loading and linking a new module new information is added to the symbol table after a module is loaded and some information is deleted after a module is deleted the solaris kernel supports seven types of loadable modules scheduler classes file systems loadable system calls loaders for different formats of executable files streams modules bus controllers and device drivers miscellaneous modules use of loadable kernel modules provides easy extensibility thus new file systems new formats of executable files new system calls and new kinds of buses and devices can be added easily an interesting feature in the kernel is that when a new module is to be loaded the kernel creates a new thread for loading linking and initializing working of the new module this arrangement permits module loading to be performed concurrently with normal operation of the kernel it also permits loading of several modules to be performed concurrently architecture of windows figure shows architecture of the windows os the hardware abstraction layer hal interfaces with the bare machine and provides abstractions of the io interfaces interrupt controllers and interprocessor communication mechanisms in a multiprocessor system the kernel uses the abstractions provided by the hal to provide basic services such as interrupt processing and multiprocessor synchronization this way the kernel is shielded from peculiarities of a specific architecture which enhances its portability the hal and the kernel are together equivalent to a conventional kernel see figure a device driver also uses the abstractions provided by the hal to manage io operations on a class of devices the kernel performs the process synchronization and scheduling functions the executive comprises nonkernel routines of the os its code uses facilities in the kernel to provide services such as process creation and termination virtual memory management an interprocess message passing facility for clientserver communication called the local procedure call lpc io management and a file cache to provide efficient file io and a security reference monitor that performs summary portability of an operating system refers to the ease portability and extensibility have become crucial with which the os can be implemented on a comrequirements because of long lifespans of modputer having a different architecture extensibility ern operating systems in this chapter we discussed of an operating system refers to the ease with which different ways of structuring operating systems to its functionalities can be modified or enhanced meet these requirements to adapt it to a new computing environment chapter structure of operating systems an os functionality typically contains a polthe virtual machine operating system vm os icy which specifies the principle that is to be used supported operation of several operating systems to perform the functionality and a few mechanisms on a computer simultaneously by creating a virtual that perform actions to implement the functionalmachine for each user and permitting the user to ity mechanisms such as dispatching and context run an os of his choice in the virtual machine save interact closely with the computer so their the vm os interleaved operation of the users code is inherently architecturedependent polivirtual machines on the host computer through cies are architectureindependent hence portaa procedure analogous to scheduling when a bility and extensibility of an os depends on virtual machine was scheduled its os would how the code of its policies and mechanisms is organize execution of user applications running structured under it early operating systems had a monolithic in a kernelbased design of operating systems structure these operating systems had poor portathe kernel is the core of the operating system which bility because architecturedependent code was invokes the nonkernel routines to implement operaspread throughout the os they also suffered tions on processes and resources the architecturefrom high design complexity the layered design dependent code in an os typically resides in the of operating systems used the principle of abstrackernel this feature enhances portability of the tion to control complexity of designing the os operating system it viewed the os as a hierarchy of layers in a microkernel is the essential core of os code which each layer provided a set of services to it is small in size contains a few mechanisms the layer above it and itself used the services and does not contain any policies policy modin the layer below it architecture dependencies ules are implemented as server processes they were often restricted to lower layers in the hiercan be changed or replaced without affecting the archy however the design methodology did not microkernel thus providing high extensibility of guarantee it the os test your concepts classify each of the following statements as true refer to relevant sections of chapters and or false a preempting a program a mechanisms of the os are typically b prioritybased scheduling used in multiproarchitectureindependent gramming systems b a layered os organization reduces the semc loading a swappedout program into antic gap between the top layer of the os and memory the bare machine d checking whether a user program can be c in a virtual machine os each user can run permitted to access a file an os of his choice which of the following operating systems has d a kernelbased os structure provides the highest portability extensibility a an os with a monolithic structure e in a microkernelbased os the process b an os with a layered structure scheduler may run as a user process c a virtual machine os classify each of the following functions perd a kernelbased os formed by an os as a policy or a mechanism part overview exercises the scheduling mechanism manipulate schrequires lessthanfull virtualization of its reeduling lists see table is invoked to modify sources however it may degrade efficiency of scheduling lists in response to events in the sysoperation of a guest os tem and actions of the scheduler describe the what are the consequences of merging nonkerfunctions this mechanism should perform for a nel routines with a the user interface b the roundrobin scheduling and b prioritybased kernel hint refer to section scheduling as used in a multiprogramming os list the differences between a kernel employ justify the following statement secure opering dynamically loadable modules and a a ation of a virtual machine operating system monolithic kernel and b a microkernel bibliography dijkstra describes the structure of the the multi beck m h bohme m dziadzka u kunitz programming system the virtual machine operating r magnus c schroter and d verworner system vm is based on cp and is described in linux kernel programming rd ed creasy the xen and vmware virtual machine pearson education new york products are described in barham et al and bovet d p and m cesati understanding sugarman et al respectively the may issue the linux kernel rd ed oreilly sebastopol of ieee computer is a special issue on virtualization creasy r j the origin of the vm technologies rosenblum and garfinkel discusses timesharing system ibm journal of research trends in the design of virtual machine monitors and development warhol discusses the strides made by micro dijkstra e w the structure of the kernels in the early s while liedtke describes multiprogramming system communications of the principles of microkernel design hartig et al the acm describes porting and performance of the linux os engler d r m f kasshoek and j otoole on the l microkernel engler et al discusses exokernel an operating system design of an exokernel bach vahalia and architecture for applicationlevel resource mckusick et al describe the unix kernel beck management symposium on os principles et al bovet and cesati and love describe the linux kernel while mauro and mcdougall hartig h m hohmuth j liedtke describes the kernel of solaris tanenbaum s schonberg and j wolter the describes microkernels of the amoeba and mach operatperformance of microkernelbased systems ing systems russinovich and solomon describes th acm symposium on operating system architecture of windows principles bach m j the design of the unix liedtke j towards real microkernels operating system prentice hall englewood communications of the acm cliffs nj love r linux kernel development barham p b dragovic k fraser s hand nd ed novell press t harris a ho r neugebauer i pratt and mauro j and r mcdougall solaris a warfield xen and the art of internals nd ed prentice hall englewood virtualization acm symposium on operating cliffs nj system principles chapter structure of operating systems mckusick m k k bostic m j karels sugarman j g venkitachalam and and j s quarterman the design and b h lim virtualizing io devices on implementation of the bsd operating system vmware workstations hosted virtual machine addisonwesley reading mass monitor usenix annual technical meyer j and l h seawright a virtual conference machine timesharing system ibm systems tanenbaum a s modern operating journal systems nd ed prentice hall englewood rosenblum m and t garfinkel virtual cliffs nj machine monitors current technology and future vahalia u unix internals the new trends ieee computer frontiers prenticehall englewood cliffs nj russinovich m e and d a solomon warhol p d small kernels hit it big microsoft windows internals th ed microsoft byte january press redmond wash part process management a process is an execution of a program an application may be designed to have many processes that operate concurrently and interact among themselves to jointly achieve a goal this way the application may be able to provide a quicker response to the user an os contains a large number of processes at any time process management involves creating processes fulfilling their resource requirements scheduling them for use of a cpu implementing process synchronization to control their interactions avoiding deadlocks so that they do not wait for each other indefinitely and terminating them when they complete their operation the manner in which an os schedules processes for use of a cpu determines the response times of processes resource efficiency and system performance a thread uses the resources of a process but resembles a process in all other respects an os incurs less overhead in managing threads than in managing processes we use the term process as generic to both processes and threads road map for part processes and programs a program is a passive entity that does not perform any actions by itself it has to be executed if the actions it calls for are to take place a process is an execution of a program it actually performs the actions specified in a program an operating system shares the cpu among processes this is how it gets user programs to execute what is a process to understand what is a process let us discuss how the os executes a program program p shown in figure a contains declarations of a file info and a variable item and statements that read values from info use them to perform some calculations and print a result before coming to a halt during execution part process management figure a program and an abstract view of its execution instructions of this program use values in its data area and the stack to perform the intended calculations figure b shows an abstract view of its execution the instructions data and stack of program p constitute its address space to realize execution of p the os allocates memory to accommodate ps address space allocates a printer to print its results sets up an arrangement through which p can access file info and schedules p for execution the cpu is shown as a lightly shaded box because it is not always executing instructions of p the os shares the cpu between execution of p and executions of other programs following the above discussion we can define a process as follows definition process an execution of a program using resources allocated to it when a user initiates execution of a program the os creates a new process and assigns a unique id to it it now allocates some resources to the process sufficient memory to accommodate the address space of the program and some devices such as a keyboard and a monitor to facilitate interaction with the user the process may make system calls during its operation to request additional resources such as files we refer to the address space of the program and resources allocated to it as the address space and resources of the process respectively accordingly a process comprises six components id code data stack resources cpu state where id is the unique id assigned by the os code is the code of the program it is also called the text of a program data is the data used in the execution of the program including data from files stack contains parameters of functions and procedures called during execution of the program and their return addresses chapter processes and threads resources is the set of resources allocated by the os cpu state is composed of contents of the psw and the generalpurpose registers gprs of the cpu we assume that the stack pointer is maintained in a gpr the cpu state section contains information that indicates which instruction in the code would be executed next and other information such as contents of the condition code field also called the flags field of the psw that may influence its execution the cpu state changes as the execution of the program progresses we use the term operation of a process for execution of a program thus a process operates when it is scheduled relationships between processes and programs a program consists of a set of functions and procedures during its execution control flows between the functions and procedures according to the logic of the program is an execution of a function or procedure a process this doubt leads to the obvious question what is the relationship between processes and programs the os does not know anything about the nature of a program including functions and procedures in its code it knows only what it is told through system calls the rest is under control of the program thus functions of a program may be separate processes or they may constitute the code part of a single process we discuss examples of these situations in the following table shows two kinds of relationships that can exist between processes and programs a onetoone relationship exists when a single execution of a sequential program is in progress for example execution of program p in figure a manytoone relationship exists between many processes and a program in two cases many executions of a program may be in progress at the same time processes representing these executions have a manytoone relationship with the program during execution a program may make a system call to request that a specific part of its code should be executed concurrently ie as a separate activity occurring at the same time the kernel sets up execution of the specified part of the code and treats it as a separate process the new process and the process representing execution of the program have a manytoone relationship with the program we call such a program a concurrent program processes that coexist in the system at some time are called concurrent processes concurrent processes may share their code data and resources with other table relationships between processes and programs relationship examples onetoone a single execution of a sequential program manytoone many simultaneous executions of a program execution of a concurrent program part process management processes they have opportunities to interact with one another during their execution child processes the kernel initiates an execution of a program by creating a process for it for lack of a technical term for this process we will call it the primary process for the program execution the primary process may make system calls as described in the previous section to create other processes these processes become its child processes and the primary process becomes their parent a child process may itself create other processes and so on the parentchild relationships between these processes can be represented in the form of a process tree which has the primary process as its root a child process may inherit some of the resources of its parent it could obtain additional resources during its operation through system calls typically a process creates one or more child processes and delegates some of its work to each of them it is called multitasking within an application it has the three benefits summarized in table creation of child processes has the same benefits as the use of multiprogramming in an os the kernel may be able to interleave operation of iobound and cpubound processes in the application which may lead to a reduction in the duration ie running time of an application it is called computation speedup most operating systems permit a parent process to assign priorities to child processes a realtime application can assign a high priority to a child process that performs a critical function to ensure that its response requirement is met we shall elaborate on this aspect later in example the third benefit namely guarding a parent process against errors in a child process arises as follows consider a process that has to invoke an untrusted code table benefits of child processes benefit explanation computation speedup actions that the primary process of an application would have performed sequentially if it did not create child processes would be performed concurrently when it creates child processes it may reduce the duration ie running time of the application priority for critical a child process that performs a critical function functions may be assigned a high priority it may help to meet the realtime requirements of an application guarding a parent process the kernel aborts a child process if an error against errors arises during its operation the parent process is not affected by the error it may be able to perform a recovery action chapter processes and threads if the untrusted code were to be included in the code of the process an error in the untrusted code would compel the kernel to abort the process however if the process were to create a child process to execute the untrusted code the same error would lead to the abort of the child process so the parent process would not come to any harm the os command interpreter uses this feature to advantage the command interpreter itself runs as a process and creates a child process whenever it has to execute a user program this way its own operation is not harmed by malfunctions in the user program example illustrates how the data logging system of section benefits from use of child processes child processes in a realtime application example the realtime data logging application of section receives data samples from a satellite at the rate of samples per second and stores them in a file we assume that each sample arriving from the satellite is put into a special register of the computer the primary process of the application which we will call the datalogger process has to perform the following three functions copy the sample from the special register into memory copy the sample from memory into a file perform some analysis of a sample and record its results into another file used for future processing it creates three child processes named copysample recordsample and housekeeping leading to the process tree shown in figure a note that a process is depicted by a circle and a parentchild relationship is depicted by an arrow as shown in figure b copysample copies the sample from the register into a memory area named bufferarea that can hold say samples recordsample writes a sample from bufferarea into a file housekeeping analyzes a sample from bufferarea and records its results in another file arrival of a new sample causes an interrupt and a programmerdefined interrupt servicing routine is associated with this interrupt the kernel executes this routine whenever a new sample arrives it activates copysample operation of the three processes can overlap as follows copysample can copy a sample into bufferarea recordsample can write a previous sample to the file while housekeeping can analyze it and write its results into the other file this arrangement provides a smaller worstcase response time of the application than if these functions were to be executed sequentially so long as bufferarea has some free space only copysample has to complete before the next sample arrives the other processes can be executed later this possibility is exploited by assigning the highest priority to copysample part process management register buffer copy area sample data logger record sample housekeeping copy record housekeeping sample sample memory a b figure realtime application of section a process tree b processes to facilitate use of child processes the kernel provides operations for creating a child process and assigning a priority to it terminating a child process determining the status of a child process sharing communication and synchronization between processes their use can be described as follows in example the datalogger process creates three child processes the copysample and recordsample processes share bufferarea they need to synchronize their operation such that process recordsample would copy a sample out of bufferarea only after process copysample has written it there the datalogger process could be programmed to either terminate its child processes before itself terminating or terminate itself only after it finds that all its child processes have terminated concurrency and parallelism parallelism is the quality of occurring at the same time two events are parallel if they occur at the same time and two tasks are parallel if they are performed at the same time concurrency is an illusion of parallelism thus two tasks are concurrent if there is an illusion that they are being performed in parallel whereas in reality only one of them may be performed at any time in an os concurrency is obtained by interleaving operation of processes on the cpu which creates the illusion that these processes are operating at the implementing processes in the operating systems view a process is a unit of computational work hence the kernels primary task is to control operation of processes to provide effective utilization of the computer system accordingly the kernel allocates resources to a process protects the process and its resources from interference by other processes and ensures that the process gets to use the cpu until it completes its operation part process management event context save event handling scheduling dispatching exit from kernel figure fundamental functions of the kernel for controlling processes the kernel is activated when an event which is a situation that requires the kernels attention leads to either a hardware interrupt or a system call see section the kernel now performs four fundamental functions to control operation of processes see figure context save saving cpu state and information concerning resources of the process whose operation is interrupted event handling analyzing the condition that led to an interrupt or the request by a process that led to a system call and taking appropriate actions scheduling selecting the process to be executed next on the cpu dispatching setting up access to resources of the scheduled process and loading its saved cpu state in the cpu to begin or resume its operation the kernel performs the context save function to save information concerning the interrupted process it is followed by execution of an appropriate event handling routine which may inhibit further operation of the interrupted process eg if this process has made a system call to start an io operation or may enable operation of some other process eg if the interrupt was caused by completion of its io operation the kernel now performs the scheduling function to select a process and the dispatching function to begin or resume its operation as discussed earlier in sections and to perform scheduling an operating system must know which processes require the cpu at any moment hence the key to controlling operation of processes is to monitor all processes and know what each process is doing at any moment of time whether executing on the cpu waiting for the cpu to be allocated to it waiting for an io operation to complete or waiting to be swapped into memory the operating system monitors the process state to keep track of what a process is doing at any moment chapter processes and threads here in section we will see what is meant by a process state and we will look at the different states of a process and the arrangements by which the operating system maintains information about the state of a process we do not discuss scheduling in this chapter it is discussed later in chapter process states and state transitions an operating system uses the notion of a process state to keep track of what a process is doing at any moment definition process state the indicator that describes the nature of the current activity of a process the kernel uses process states to simplify its own functioning so the number of process states and their names may vary across oss however most oss use the four fundamental states described in table the kernel considers a process to be in the blocked state if it has made a resource request and the request is yet to be granted or if it is waiting for some event to occur a cpu should not be allocated to such a process until its wait is complete the kernel would change the state of the process to ready when the request is granted or the event for which it is waiting occurs such a process can be considered for scheduling the kernel would change the state of the process to running when it is dispatched the state would be changed to terminated when execution of the process completes or when it is aborted by the kernel for some reason a conventional computer system contains only one cpu and so at most one process can be in the running state there can be any number of processes in the blocked ready and terminated states an os may define more process states to simplify its own functioning or to support additional functionalities like swapping we discuss this aspect in section table fundamental process states state description running a cpu is currently executing instructions in the process code blocked the process has to wait until a resource request made by it is granted or it wishes to wait until a specific event occurs ready the process wishes to use the cpu to continue its operation however it has not been dispatched terminated the operation of the process ie the execution of the program represented by it has completed normally or the os has aborted it part process management process state transitions a state transition for a process pi is a change in its state a state transition is caused by the occurrence of some event such as the start or end of an io operation when the event occurs the kernel determines its influence on activities in processes and accordingly changes the state of an affected process when a process pi in the running state makes an io request its state has to be changed to blocked until its io operation completes at the end of the io operation pis state is changed from blocked to ready because it now wishes to use the cpu similar state changes are made when a process makes some request that can not immediately be satisfied by the os the process state is changed to blocked when the request is made ie when the request event occurs and it is changed to ready when the request is satisfied the state of a ready process is changed to running when it is dispatched and the state of a running process is changed to ready when it is preempted either because a higherpriority process became ready or because its time slice elapsed see sections and table summarizes causes of state transitions figure diagrams the fundamental state transitions for a process a new process is put in the ready state after resources required by it have been allocated it may enter the running blocked and ready states a number of times as a result of events described in table eventually it enters the terminated state example process state transitions consider the timesharing system of example which uses a time slice of ms it contains two processes p and p p has a cpu burst of ms followed by an io operation that lasts for ms while p has a cpu burst of ms followed by an io operation that lasts for ms execution of p and p was described in figure table illustrates the state transitions during operation of the system actual execution of programs proceeds as follows system operation starts with both processes in the ready state at time the scheduler selects process p for execution and changes its state to running at ms p is preempted and p is dispatched hence ps state is changed to ready and ps state is changed to running at ms p is preempted and p is dispatched p enters the blocked state at ms because of an io operation p is dispatched because it is in the ready state at ms p is preempted because its time slice elapses however it is dispatched again since no other process is in the ready state p initiates an io operation at ms now both processes are in the blocked state suspended processes a kernel needs additional states to describe the nature of the activity of a process that is not in one of the four fundamental states described earlier consider a chapter processes and threads table causes of fundamental state transitions for a process state transition description ready running the process is dispatched the cpu begins or resumes execution of its instructions blocked ready a request made by the process is granted or an event for which it was waiting occurs running ready the process is preempted because the kernel decides to schedule some other process this transition occurs either because a higherpriority process becomes ready or because the time slice of the process elapses running blocked the process in operation makes a system call to indicate that it wishes to wait until some resource request made by it is granted or until a specific event occurs in the system five major causes of blocking are process requests an io operation process requests a resource process wishes to wait for a specified interval of time process waits for a message from another process process waits for some action by another process running terminated execution of the program is completed five primary reasons for process termination are selftermination the process in operation either completes its task or realizes that it can not operate meaningfully and makes a terminate me system call examples of the latter condition are incorrect or inconsistent data or inability to access data in a desired manner eg incorrect file access privileges termination by a parent a process makes a terminate pi system call to terminate a child process pi when it finds that execution of the child process is no longer necessary or meaningful exceeding resource utilization an os may limit the resources that a process may consume a process exceeding a resource limit would be aborted by the kernel abnormal conditions during operation the kernel aborts a process if an abnormal condition arises due to the instruction being executed eg execution of an invalid instruction execution of a privileged instruction arithmetic conditions like overflow or memory protection violation incorrect interaction with other processes the kernel may abort a process if it gets involved in a deadlock part process management running completion terminated dispatching resource or wait request preemption new ready blocked process resource granted or wait completed figure fundamental state transitions for a process table process state transitions in a timesharing system new states time event remarks p p p is scheduled running ready p is preempted p is scheduled ready running p is preempted p is scheduled running ready p starts io p is scheduled blocked running p is preempted blocked ready p is scheduled blocked running p starts io blocked blocked process that was in the ready or the blocked state when it got swapped out of memory the process needs to be swapped back into memory before it can resume its activity hence it is no longer in the ready or blocked state the kernel must define a new state for it we call such a process a suspended process if a user indicates that his process should not be considered for scheduling for a specific period of time it too would become a suspended process when a suspended process is to resume its old activity it should go back to the state it was in when it was suspended to facilitate this state transition the kernel may define many suspend states and put a suspended process into the appropriate suspend state we restrict the discussion of suspended processes to swapped processes and use two suspend states called ready swapped and blocked swapped accordingly figure shows process states and state transitions the transition ready ready swapped or blocked blocked swapped is caused by a swapout action the reverse state transition takes place when the process is swapped back into memory the blocked swapped ready swapped transition takes place if the request for which the process was waiting is granted even while the process is in a suspended state for example if a resource for which it was blocked is granted to it however the process continues to be swapped out when it is swapped back into memory its state changes to ready and it competes with other ready processes for chapter processes and threads running completion terminated dispatching resource or wait request preemption new resource granted process ready or wait completed blocked swapout swapin swapout swapin ready resource granted blocked swapped or wait completed swapped figure process states and state transitions using two swapped states the cpu a new process is put either in the ready state or in the ready swapped state depending on availability of memory process context and the process control block the kernel allocates resources to a process and schedules it for use of the cpu accordingly the kernels view of a process consists of two parts code data and stack of the process and information concerning memory and other resources such as files allocated to it information concerning execution of a program such as the process state the cpu state including the stack pointer and some other items of information described later in this section these two parts of the kernels view are contained in the process context and the process control block pcb respectively see figure this arrangement enables different os modules to access relevant processrelated information conveniently and efficiently process context the process context consists of the following address space of the process the code data and stack components of the process see definition memory allocation information information concerning memory areas allocated to a process this information is used by the memory management unit mmu during operation of the process see section status of file processing activities information about files being used such as current positions in the files part process management memory resource file info info pointers processid process state gpr contents code data stack pc value process context process control block pcb figure kernels view of a process process interaction information information necessary to control interaction of the process with other processes eg ids of parent and child processes and interprocess messages sent to it that have not yet been delivered to it resource information information concerning resources allocated to the process miscellaneous information miscellaneous information needed for operation of a process the os creates a process context by allocating memory to the process loading the process code in the allocated memory and setting up its data space information concerning resources allocated to the process and its interaction with other processes is maintained in the process context throughout the life of the process this information changes as a result of actions like file open and close and creation and destruction of data by the process during its operation process control block pcb the process control block pcb of a process contains three kinds of information concerning the process identification information such as the process id id of its parent process and id of the user who created it process state information such as its state and the contents of the psw and the generalpurpose registers gprs and information that is useful in controlling its operation such as its priority and its interaction with other processes it also contains a pointer field that is used by the kernel to form pcb lists for scheduling eg a list of ready processes table describes the fields of the pcb data structure the priority and state information is used by the scheduler it passes the id of the selected process to the dispatcher for a process that is not in the running state the psw and gprs fields together contain the cpu state of the process when it last got blocked or was preempted see section operation of the process can be resumed by simply loading this information from its pcb into the cpu this action would be performed when this process is to be dispatched when a process becomes blocked it is important to remember the reason it is done by noting the cause of blocking such as a resource request or an chapter processes and threads table fields of the process control block pcb pcb field contents process id the unique id assigned to the process at its creation parent child ids these ids are used for process synchronization typically for a process to check if a child process has terminated priority the priority is typically a numeric value a process is assigned a priority at its creation the kernel may change the priority dynamically depending on the nature of the process whether cpubound or iobound its age and the resources consumed by it typically cpu time process state the current state of the process psw this is a snapshot ie an image of the psw when the process last got blocked or was preempted loading this snapshot back into the psw would resume operation of the process see fig for fields of the psw gprs contents of the generalpurpose registers when the process last got blocked or was preempted event information for a process in the blocked state this field contains information concerning the event for which the process is waiting signal information information concerning locations of signal handlers see section pcb pointer this field is used to form a list of pcbs for scheduling purposes io operation in the event information field of the pcb consider a process pi that is blocked on an io operation on device d the event information field in pis pcb indicates that it awaits end of an io operation on device d when the io operation on device d completes the kernel uses this information to make the transition blocked ready for process pi context save scheduling and dispatching the context save function performs housekeeping whenever an event occurs it saves the cpu state of the interrupted process in its pcb and saves information concerning its context see section recall that the interrupted process would have been in the running state before the event occurred the context save function changes its state to ready the event handler may later change the interrupted processs state to blocked eg if the current event was a request for io initiation by the interrupted process itself the scheduling function uses the process state information from pcbs to select a ready process for execution and passes its id to the dispatching function the dispatching function sets up the context of the selected process changes its state to running and loads the saved cpu state from its pcb into the cpu part process management to prevent loss of protection it flushes the address translation buffers used by the memory management unit mmu example illustrates the context save scheduling and dispatching functions in an os using prioritybased scheduling example context save scheduling and dispatching an os contains two processes p and p with p having a higher priority than p let p be blocked on an io operation and let p be running the following actions take place when the io completion event occurs for the io operation of p the context save function is performed for p and its state is changed to ready using the event information field of pcbs the event handler finds that the io operation was initiated by p so it changes the state of p from blocked to ready scheduling is performed p is selected because it is the highestpriority ready process ps state is changed to running and it is dispatched process switching functions and of example collectively perform switching between processes p and p switching between processes also occurs when a running process becomes blocked as a result of a request or gets preempted at the end of a time slice an event does not lead to switching between processes if occurrence of the event either causes a state transition only in a process whose priority is lower than that of the process whose operation is interrupted by the event or does not cause any state transition eg if the event is caused by a request that is immediately satisfied in the former case the scheduling function selects the interrupted process itself for dispatching in the latter case scheduling need not be performed at all the dispatching function could simply change the state of the interrupted process back to running and dispatch it switching between processes involves more than saving the cpu state of one process and loading the cpu state of another process the process context needs to be switched as well we use the term state information of a process to refer to all the information that needs to be saved and restored during process switching process switching overhead depends on the size of the state information of a process some computer systems provide special instructions to reduce the process switching overhead eg instructions that save or load the psw and all generalpurpose registers or flush the address translation buffers used by the memory management unit mmu process switching has some indirect overhead as well the newly scheduled process may not have any part of its address space in the cache and so it may perform poorly until it builds sufficient information in the cache see section virtual memory operation is also poorer initially because address chapter processes and threads translation buffers in the mmu do not contain any information relevant to the newly scheduled process event handling the following events occur during the operation of an os process creation event a new process is created process termination event a process completes its operation timer event the timer interrupt occurs resource request event process makes a resource request resource release event a process releases a resource io initiation request event process wishes to initiate an io operation io completion event an io operation completes message send event a message is sent by one process to another message receive event a message is received by a process signal send event a signal is sent by one process to another signal receive event a signal is received by a process a program interrupt the current instruction in the running process malfunctions a hardware malfunction event a unit in the computers hardware malfunctions the timer io completion and hardware malfunction events are caused by situations that are external to the running process all other events are caused by actions in the running process we group events into two broad classes for discussing actions of event handlers and discuss events and in section the kernel performs a standard action like aborting the running process when events or occur events pertaining to process creation termination and preemption when a user issues a command to execute a program the command interpreter of the user interface makes a create process system call with the name of the program as a parameter when a process wishes to create a child process to execute a program it itself makes a create process system call with the name of the program as a parameter the event handling routine for the create process system call creates a pcb for the new process assigns a unique process id and a priority to it and puts this information and id of the parent process into relevant fields of the pcb it now determines the amount of memory required to accommodate the address space of the process ie the code and data of the program to be executed and its stack and arranges to allocate this much memory to the process memory allocation techniques are discussed later in chapters and in most operating systems some standard resources are associated with each process eg a keyboard and standard input and output files the kernel allocates these standard resources to the process at this time it now enters information about allocated memory and resources into the context of the new process after completing these chores part process management it sets the state of the process to ready in its pcb and enters this process in an appropriate pcb list when a process makes a system call to terminate itself or terminate a child process the kernel delays termination until the io operations that were initiated by the process are completed it now releases the memory and resources allocated to it this function is performed by using the information in appropriate fields of the process context the kernel now changes the state of the process to terminated the parent of the process may wish to check its status sometime in future so the pcb of the terminated process is not destroyed now it will be done sometime after the parent process has checked its status or has itself terminated if the parent of the process is already waiting for its termination the kernel must activate the parent process to perform this action the kernel takes the id of the parent process from the pcb of the terminated process and checks the event information field of the parent processs pcb to find whether the parent process is waiting for termination of the child process see section the process in the running state should be preempted if its time slice elapses the context save function would have already changed the state of the running process to ready before invoking the event handler for timer interrupts so the event handler simply moves the pcb of the process to an appropriate scheduling list preemption should also occur when a higherpriority process becomes ready but that is realized implicitly when the higherpriority process is scheduled so an event handler need not perform any explicit action for it events pertaining to resource utilization when a process requests a resource through a system call the kernel may be able to allocate the resource immediately in which case event handling does not cause any process state transitions so the kernel can skip scheduling and directly invoke the dispatching function to resume operation of the interrupted process if the resource can not be allocated the event handler changes the state of the interrupted process to blocked and notes the id of the required resource in the event information field of the pcb when a process releases a resource through a system call the event handler need not change the state of the process that made the system call however it should check whether any other processes were blocked because they needed the resource and if so it should allocate the resource to one of the blocked processes and change its state to ready this action requires a special arrangement that we will discuss shortly a system call to request initiation of an io operation and an interrupt signaling end of the io operation lead to analogous event handling actions the state of the process is changed to blocked when the io operation is initiated and the cause of blocking is noted in the event information field of its pcb its state is changed back to ready when the io operation completes a request to receive a message from another process and a request to send a message to another process also lead to analogous actions event control block ecb when an event occurs the kernel must find the process whose state is affected by it for example when an io completion interrupt occurs the kernel must identify the process awaiting its completion it can achieve this by searching the event information field of the pcbs of all chapter processes and threads event description process id ecb pointer figure event control block ecb processes this search is expensive so operating systems use various schemes to speed it up we discuss a scheme that uses event control blocks ecbs as shown in figure an ecb contains three fields the event description field describes an event and the process id field contains the id of the process awaiting the event when a process pi gets blocked for occurrence of an event ei the kernel forms an ecb and puts relevant information concerning ei and pi into it the kernel can maintain a separate ecb list for each class of events like interprocess messages or io operations so the ecb pointer field is used to enter the newly created ecb into an appropriate list of ecbs when an event occurs the kernel scans the appropriate list of ecbs to find an ecb with a matching event description the process id field of the ecb indicates which process is waiting for the event to occur the state of this process is changed to reflect the occurrence of the event the following example illustrates use of ecbs for handling an io completion event their use in handling interprocess messages is described in section the event information field of the pcb now appears redundant however we retain it because the kernel may need to know which event a process is blocked on for example while aborting the process use of ecb for handling io completion example the actions of the kernel when process pi requests an io operation on some device d and when the io operation completes are as follows the kernel creates an ecb and initializes it as follows a event description end of io on device d b process awaiting the event pi the newly created ecb let us call it ecb j is added to a list of ecbs the state of pi is changed to blocked and the address of ecb j is put into the event information field of pis pcb see figure when the interrupt end of io on device d occurs ecb j is located by searching for an ecb with a matching event description field the id of the affected process ie pi is extracted from ecb j the pcb of pi is located and its state is changed to ready summary of event handling figure illustrates event handling actions of the kernel described earlier the block action always changes the state of the process that made a system call from ready to blocked the unblock action finds a process whose request can be fulfilled now and changes its state from blocked part process management pcb pi ecbj end of io on d blocked pi event information figure pcbecb interrelationship resource or message request io block request create or terminate process timer schedule dispatch interrupt io completion send unblock message resource release figure event handling actions of the kernel to ready a system call for requesting a resource leads to a block action if the resource can not be allocated to the requesting process this action is followed by scheduling and dispatching because another process has to be selected for use of the cpu the block action is not performed if the resource can be allocated straightaway in this case the interrupted process is simply dispatched again when a process releases a resource an unblock action is performed if some other process is waiting for the released resource followed by scheduling and dispatching because the unblocked process may have a higher priority than the process that released the resource again scheduling is skipped if no process is unblocked because of the event chapter processes and threads sharing communication and synchronization between processes processes of an application need to interact with one another because they work toward a common goal table describes four kinds of process interaction we summarize their important features in the following data sharing a shared variable may get inconsistent values if many processes update it concurrently for example if two processes concurrently execute the statement a a where a is a shared variable the result may depend on the way the kernel interleaves their execution the value of a may be incremented by only we discuss this problem later in section to avoid this problem only one process should access shared data at any time so a data access in one process may have to be delayed if another process is accessing the data this is called mutual exclusion thus data sharing by concurrent processes incurs the overhead of mutual exclusion message passing a process may send some information to another process in the form of a message the other process can copy the information into its own data structures and use it both the sender and the receiver process must anticipate the information exchange ie a process must know when it is expected to send or receive a message so the information exchange becomes a part of the convention or protocol between processes synchronization the logic of a program may require that an action ai should be performed only after some action aj has been performed synchronization between processes is required if these actions are performed in different processes the process that wishes to perform action ai is made to wait until another process performs action aj signals a signal is used to convey an exceptional situation to a process so that it may handle the situation through appropriate actions the code that a process wishes to execute on receiving a signal is called a signal handler the signal mechanism is modeled along the lines of interrupts thus when a signal table four kinds of process interaction kind of interaction description data sharing shared data may become inconsistent if several processes modify the data at the same time hence processes must interact to decide when it is safe for a process to modify or use shared data message passing processes exchange information by sending messages to one another synchronization to fulfill a common goal processes must coordinate their activities and perform their actions in a desired order signals a signal is used to convey occurrence of an exceptional situation to a process part process management is sent to a process the kernel interrupts operation of the process and executes a signal handler if one has been specified by the process otherwise it may perform a default action operating systems differ in the way they resume a process after executing a signal handler example illustrates sharing communication and synchronization between processes in the realtime application of example implementation of signals is described in section example process interaction in a realtime data logging application in the realtime data logging application of example bufferarea is shared by processes copysample and recordsample if a variable noofsamples inbuffer is used to indicate how many samples are currently in the buffer both these processes would need to update noofsamplesinbuffer so its consistency should be protected by delaying a process that wishes to update it if another process is accessing it these processes also need to synchronize their activities such that a new sample is moved into an entry in bufferarea only after the previous sample contained in the entry is written into the file and contents of a buffer entry are written into the file only after a new sample is moved into it these processes also need to know the size of the buffer ie how many samples it can hold like noofsamplesinbuffer a variable size could be used as shared data however use as shared data would incur the overhead of mutual exclusion which is not justified because the buffer size is not updated regularly it changes only in exceptional situations hence these processes could be coded to use the size of the buffer as a local data item bufsize its value would be sent to them by the process datalogger through messages process datalogger would also need to send signals to these processes if the size of the buffer has to be changed signals a signal is used to notify an exceptional situation to a process and enable it to attend to it immediately a list of exceptional situations and associated signal names or signal numbers are defined in an os eg cpu conditions like overflows and conditions related to child processes resource utilization or emergency communications from a user to a process the kernel sends a signal to a process when the corresponding exceptional situation occurs some kinds of signals may also be sent by processes a signal sent to a process because of a condition in its own activity such as an overflow condition in the cpu is said to be a synchronous signal whereas that sent because of some other condition is said to be an asynchronous signal to utilize signals a process makes a registerhandler system call specifying a routine that should be executed when a specific signal is sent to it this routine is chapter processes and threads called a signal handler if a process does not specify a signal handler for a signal the kernel executes a default handler that performs some standard actions like dumping the address space of the process and aborting it a process pi wishing to send a signal to another process pj invokes the library function signal with two parameters id of the destination process ie pj and the signal number this function uses the software interrupt instruction siinstrn interruptcode to make a system call named signal the event handling routine for the signal call extracts the parameters to find the signal number it now makes a provision to pass the signal to pj and returns it does not make any change in the state of the sender process ie pi signal handling in a process is implemented along the same lines as interrupt handling in an os in section we described how the interrupt hardware employs one interrupt vector for each class of interrupts which contains the address of a routine that handles interrupts of that class a similar arrangement can be used in each process the signal vectors area would contain a signal vector for each kind of signal which would contain the address of a signal handler when a signal is sent to a process the kernel accesses its signal vectors area to check whether it has specified a signal handler for that signal if so it would arrange to pass control to the handler otherwise it would execute its own default handler for that signal signal handling becomes complicated if the process to which a signal is sent is in the blocked state the kernel would have to change its state temporarily to ready so that it could execute a signal handler after which it would have to change the state back to blocked some operating systems prefer a simpler approach that merely notes the arrival of a signal if the destination process is in the blocked state and arranges to execute the signal handler when the process becomes ready and gets scheduled example illustrates how a signal is handled by a process signal handling example figure illustrates the arrangement used for handling signals the code of process pi contains a function named sh whose last instruction is a return from function instruction which pops an address off the stack and passes control to the instruction with this address process pi makes a library call registerhandlersigsh to register sh as the signal handler for signal sig the library routine registerhandler makes the system call registerhandler while handling this call the kernel accesses the pcb of pi obtains the start address of the signal vectors area and enters the address sh in the signal vector of signal sig control now returns to pi the solid arrows in figure a indicate addresses in the kernels data structures while the dashed arrows indicate how the cpu is switched to the kernel when the system call is made and how it is switched back to pi threads applications use concurrent processes to speed up their operation however switching between processes within an application incurs high process switching overhead because the size of the process state information is large see section so operating system designers developed an alternative model of chapter processes and threads execution of a program called a thread that could provide concurrency within an application with less overhead to understand the notion of threads let us analyze process switching overhead and see where a saving can be made process switching overhead has two components execution related overhead the cpu state of the running process has to be saved and the cpu state of the new process has to be loaded in the cpu this overhead is unavoidable resourceuse related overhead the process context also has to be switched it involves switching of the information about resources allocated to the process such as memory and files and interaction of the process with other processes the large size of this information adds to the process switching overhead consider child processes pi and pj of the primary process of an application these processes inherit the context of their parent process if none of these processes have allocated any resources of their own their context is identical their state information differs only in their cpu states and contents of their stacks consequently while switching between pi and pj much of the saving and loading of process state information is redundant threads exploit this feature to reduce the switching overhead definition thread an execution of a program that uses the resources of a process a process creates a thread through a system call the thread does not have resources of its own so it does not have a context it operates by using the context of the process and accesses the resources of the process through it we use the phrases threads of a process and parent process of a thread to describe the relationship between a thread and the process whose context it uses note that threads are not a substitute for child processes an application would create child processes to execute different parts of its code and each child process can create threads to achieve concurrency figure illustrates the relationship between threads and processes in the abstract view of figure a process pi has three threads which are represented by wavy lines inside the circle representing process pi figure b shows an implementation arrangement process pi has a context and a pcb each thread of pi is an execution of a program so it has its own stack and a thread control block tcb which is analogous to the pcb and stores the following information thread scheduling information thread id priority and state cpu state ie contents of the psw and gprs pointer to pcb of parent process tcb pointer which is used to make lists of tcbs for scheduling use of threads effectively splits the process state into two parts the resource state remains with the process while an execution state which is the cpu state is part process management threads process pi stacks memory resource file info info pointers code data stack context of context of pcb thread control process pi process pi blocks tcbs a b figure threads in process pi a concept b implementation associated with a thread the cost of concurrency within the context of a process is now merely replication of the execution state for each thread the execution states need to be switched during switching between threads the resource state is neither replicated nor switched during switching between threads of the process thread states and state transitions barring the difference that threads do not have resources allocated to them threads and processes are analogous hence thread states and thread state transitions are analogous to process states and process state transitions when a thread is created it is put in the ready state because its parent process already has the necessary resources allocated to it it enters the running state when it is dispatched it does not enter the blocked state because of resource requests because it does not make any resource requests however it can enter the blocked state because of process synchronization requirements for example if threads were used in the realtime data logging application of example thread recordsample would have to enter the blocked state if no data samples exist in bufferarea advantages of threads over processes table summarizes the advantages of threads over processes of which we have already discussed the advantage of lower overhead of thread creation and switching unlike child processes threads share the address space of the parent process so they can communicate through shared data rather than through messages thereby eliminating the overhead of system calls applications that service requests received from users such as airline reservation systems or banking systems are called servers their users are called clients clientserver computing is discussed in section performance of servers can be improved through concurrency or parallelism see section ie either through interleaving of requests that involve io operations or through use of many cpus to service different requests use of threads simplifies their design we discuss it with the help of figure figure a is a view of an airline reservation server the server enters requests made by its clients in a queue and serves them one after another if chapter processes and threads table advantages of threads over processes advantage explanation lower overhead of creation thread state consists only of the state of a and switching computation resource allocation state and communication state are not a part of the thread state so creation of threads and switching between them incurs a lower overhead more efficient communication threads of a process can communicate with one another through shared data thus avoiding the overhead of system calls for communication simplification of design use of threads can simplify design and coding of applications that service requests concurrently server server server s s s pending pending requests requests clients clients clients a b c figure use of threads in structuring a server a server using sequential code b multithreaded server c server using a thread pool several requests are to be serviced concurrently the server would have to employ advanced io techniques such as asynchronous io and use complex logic to switch between the processing of requests by contrast a multithreaded server could create a new thread to service each new request it receives and terminate the thread after servicing the request this server would not have to employ any special techniques for concurrency because concurrency is implicit in its creation of threads figure b shows a multithreaded server which has created three threads because it has received three requests creation and termination of threads is more efficient than creation and termination of processes however its overhead can affect performance of the server if clients make requests at a very high rate an arrangement called thread pool is used to avoid this overhead by reusing threads instead of destroying them after servicing requests the thread pool consists of one primary thread that performs housekeeping tasks and a few worker threads that are used repetitively the primary thread maintains a list of pending requests and a list of idle worker threads when a new request is made it assigns the request to an idle worker thread if one exists otherwise it enters the request in the list of pending requests when a worker thread completes servicing of a request the primary thread either assigns a new request to the worker thread to service or enters it in the list of idle part process management worker threads figure c illustrates a server using a thread pool it contains three worker threads that are busy servicing three service requests while three service requests are pending if the thread pool facility is implemented in the os the os would provide the primary thread for the pool which would simplify coding of the server because it would not have to handle concurrency explicitly the os could also vary the number of worker threads dynamically to provide adequate concurrency in the application and also reduce commitment of os resources to idle worker threads coding for use of threads threads should ensure correctness of data sharing and synchronization see section section describes features in the posix threads standard that can be used for this purpose correctness of data sharing also has another facet functions or subroutines that use static or global data to carry values across their successive activations may produce incorrect results when invoked concurrently because the invocations effectively share the global or static data concurrently without mutual exclusion such routines are said to be thread unsafe an application that uses threads must be coded in a thread safe manner and must invoke routines only from a thread safe library signal handling requires special attention in a multithreaded application recall that the kernel permits a process to specify signal handlers see section when several threads are created in a process which thread should handle a signal there are several possibilities the kernel may select one of the threads for signal handling this choice can be made either statically eg either the first or the last thread created in the process or dynamically eg the highestpriority thread alternatively the kernel may permit an application to specify which thread should handle signals at any time a synchronous signal arises as a result of the activity of a thread so it is best that the thread itself handles it ideally each thread should be able to specify which synchronous signals it is interested in handling however to provide this feature the kernel would have to replicate the signal handling arrangement of figure for each thread so few operating systems provide it an asynchronous signal can be handled by any thread in a process to ensure prompt attention to the condition that caused the signal the highestpriority thread should handle such a signal posix threads the ansiieee portable operating system interface posix standard defines the pthreads application program interface for use by c language programs popularly called posix threads this interface provides routines that perform the following tasks thread management threads are managed through calls on thread library routines for creation of threads querying status of threads normal or abnormal termination of threads waiting for termination of a thread setting of scheduling attributes and specifying thread stack size assistance for data sharing data shared by threads may attain incorrect values if two or more threads update it concurrently a feature called mutex is chapter processes and threads provided to ensure mutual exclusion between threads while accessing shared data ie to ensure that only one thread is accessing shared data at any time routines are provided to begin use of shared data in a thread and indicate end of use of shared data if threads are used in example threads copysample and recordsample would use a mutex to ensure that they do not access and update noofsamplesinbuffer concurrently assistance for synchronization condition variables are provided to facilitate coordination between threads so that they perform their actions in the desired order if threads are used in example condition variables would be used to ensure that thread copysample would copy a sample into bufferarea before recordsample would write it from there into the file figure illustrates use of pthreads in the realtime data logging application of example a pthread is created through the call pthreadcreate data structure attributes start routine arguments where the thread data structure becomes the de facto thread id and attributes indicate scheduling priority and synchronization options a thread terminates through a pthreadexit call which takes a thread status as a parameter synchronization between the parent thread and a child thread is performed through the pthreadjoin call which takes a thread id and some attributes as parameters on issuing this call the parent thread is blocked until the thread indicated in the call has terminated an error is raised if the termination status of the thread does not match the attributes indicated in the pthreadjoin call some thread implementations require a thread to be created with the attribute joinable to qualify for such synchronization the code in figure creates three threads to perform the functions performed by processes in example as mentioned above and indicated through comments in figure the threads would use the mutex bufmutex to ensure mutually exclusive access to the buffer and use condition variables buffull and bufempty to ensure that they deposit samples into the buffer and take them out of the buffer in the correct order we do not show details of mutexes and condition variables here they are discussed later in chapter kernellevel userlevel and hybrid threads these three models of threads differ in the role of the process and the kernel in the creation and management of threads this difference has a significant impact on the overhead of thread switching and the concurrency and parallelism within a process kernellevel threads a kernellevel thread is implemented by the kernel hence creation and termination of kernellevel threads and checking of their status is performed part process management include pthreadh include stdioh int size buffer noofsamplesinbuffer int main pthreadt id id id pthreadmutext bufmutex conditionmutex pthreadcondt buffull bufempty pthreadcreateid null movetobuffer null pthreadcreateid null writeintofile null pthreadcreateid null analysis null pthreadjoinid null pthreadjoinid null pthreadjoinid null pthreadexit void movetobuffer repeat until all samples are received if no space in buffer wait on buffull use bufmutex to access the buffer increment no of samples signal bufempty pthreadexit void writeintofile repeat until all samples are written into the file if no data in buffer wait on bufempty use bufmutex to access the buffer decrement no of samples signal buffull pthreadexit void analysis repeat until all samples are analyzed read a sample from the buffer and analyze it pthreadexit figure outline of the data logging application using posix threads through system calls figure shows a schematic of how the kernel handles kernellevel threads when a process makes a createthread system call the kernel creates a thread assigns an id to it and allocates a thread control block tcb the tcb contains a pointer to the pcb of the parent process of the thread when an event occurs the kernel saves the cpu state of the interrupted thread in its tcb after event handling the scheduler considers tcbs of all threads and selects one ready thread the dispatcher uses the pcb pointer in its chapter processes and threads pi pcb pj pcb thread control blocks tcbs pcb pointer scheduler selected tcb figure scheduling of kernellevel threads tcb to check whether the selected thread belongs to a different process than the interrupted thread if so it saves the context of the process to which the interrupted thread belongs and loads the context of the process to which the selected thread belongs it then dispatches the selected thread however actions to save and load the process context are skipped if both threads belong to the same process this feature reduces the switching overhead hence switching between kernellevel threads of a process could be as much as an order of magnitude faster ie times faster than switching between processes advantages and disadvantages of kernellevel threads a kernellevel thread is like a process except that it has a smaller amount of state information this similarity is convenient for programmers programming for threads is no different from programming for processes in a multiprocessor system kernellevel threads provide parallelism see section as many threads belonging to a process can be scheduled simultaneously which is not possible with the userlevel threads described in the next section so it provides better computation speedup than userlevel threads however handling threads like processes has its disadvantages too switching between threads is performed by the kernel as a result of event handling hence it incurs the overhead of event handling even if the interrupted thread and the selected thread belong to the same process this feature limits the savings in the thread switching overhead userlevel threads userlevel threads are implemented by a thread library which is linked to the code of a process the library sets up the thread implementation arrangement shown in figure b without involving the kernel and itself interleaves operation of threads in the process thus the kernel is not aware of presence of userlevel threads in a process it sees only the process most oss implement the part process management pthreads application program interface provided in the ieee posix standard see section in this manner an overview of creation and operation of threads is as follows a process invokes the library function createthread to create a new thread the library function creates a tcb for the new thread and starts considering the new thread for scheduling when the thread in the running state invokes a library function to perform synchronization say wait until a specific event occurs the library function performs scheduling and switches to another thread of the process thus the kernel is oblivious to switching between threads it believes that the process is continuously in operation if the thread library can not find a ready thread in the process it makes a block me system call the kernel now blocks the process it will be unblocked when some event activates one of its threads and will resume execution of the thread library function which will perform scheduling and switch to execution of the newly activated thread scheduling of userlevel threads figure is a schematic diagram of scheduling of userlevel threads the thread library code is a part of each process it performs scheduling to select a thread and organizes its execution we view this operation as mapping of the tcb of the selected thread into the pcb of the process the thread library uses information in the tcbs to decide which thread should operate at any time to dispatch the thread the cpu state of the thread should become the cpu state of the process and the process stack pointer should point to the threads stack since the thread library is a part of a process the cpu is in the user mode hence a thread can not be dispatched by loading new information into the psw the thread library has to use nonprivileged instructions to change psw contents accordingly it loads the address of the threads stack pi pj process context thread library thread control blocks tcbs mapping performed by thread library process control blocks pcbs scheduler selected pcb figure scheduling of userlevel threads chapter processes and threads into the stack address register obtains the address contained in the program counter pc field of the threads cpu state found in its tcb and executes a branch instruction to transfer control to the instruction which has this address the next example illustrates interesting situations during scheduling of userlevel threads scheduling of userlevel threads example figure illustrates how the thread library manages three threads in a process pi the codes n r and b in the tcbs represent the states running ready and blocked respectively process pi is in the running state and the thread library is executing it dispatches thread h so hs state is shown as n ie running process pi is preempted sometime later by the kernel figure a illustrates states of the threads and of process pi thread h is in the running state and process pi is in the ready state thread h would resume its operation when process pi is scheduled next the line from hs tcb to pis pcb indicates that hs tcb is currently mapped into pis pcb this fact is important for the dispatching and context save actions of the thread library thread h is in the ready state in figure a so its tcb contains the code r thread h awaits a synchronization action by h so it is in the blocked state its tcb contains the code b and h to indicate that it is awaiting an event that is a synchronization action by h figure b shows the situation when the kernel dispatches pi and changes its state to running the thread library overlaps operation of threads using the timer while scheduling h the library would have requested an interrupt after a small interval of time when the timer interrupt occurs it gets control through the event handling routine of the kernel for timer interrupts and decides to preempt h so it saves the cpu state in hs tcb and schedules h hence the state codes in the tcbs of h and h change to r and n respectively figure c note that thread scheduling performed by the thread library is invisible to the kernel all through these events the kernel sees process pi in the running state a user thread should not make a blocking system call however let us see what would happen if h made a system call to initiate an io operation on device d which is a blocking system call the kernel would change the state of process pi to blocked and note that it is blocked because of an io operation on device d figure d some time after the io operation completes the kernel would schedule process pi and operation of h would resume note that the state code in hs tcb remains n signifying the running state all through its io operation advantages and disadvantages of userlevel threads thread synchronization and scheduling is implemented by the thread library this arrangement avoids part process management h h h h h h h h h h h h pi pi pi pi tcbs n r b n r b r n b r n b h h h h pcb ready running running blocked of pi d a b c d figure actions of the thread library n r b indicate running ready and blocked the overhead of a system call for synchronization between threads so the thread switching overhead could be as much as an order of magnitude smaller than in kernellevel threads this arrangement also enables each process to use a scheduling policy that best suits its nature a process implementing a realtime application may use prioritybased scheduling of its threads to meet its response requirements whereas a process implementing a multithreaded server may perform roundrobin scheduling of its threads however performance of an application would depend on whether scheduling of userlevel threads performed by the thread library is compatible with scheduling of processes performed by the kernel for example roundrobin scheduling in the thread library would be compatible with either roundrobin scheduling or prioritybased scheduling in the kernel whereas prioritybased scheduling would be compatible only with prioritybased scheduling in the kernel managing threads without involving the kernel also has a few drawbacks first the kernel does not know the distinction between a thread and a process so if a thread were to block in a system call the kernel would block its parent process in effect all threads of the process would get blocked until the cause of the blocking was removed in figure d of example thread h can not be scheduled even though it is in the ready state because thread h made a blocking system call hence threads must not make system calls that can lead to blocking to facilitate this an os would have to make available a nonblocking version of each system call that would otherwise lead to blocking of a process second since the kernel schedules a process and the thread library schedules the threads within a process at most one thread of a process can be in operation at any time thus userlevel threads can not provide parallelism see section and the concurrency provided by them is seriously impaired if a thread makes a system call that leads to blocking chapter processes and threads hybrid thread models a hybrid thread model has both userlevel threads and kernellevel threads and a method of associating userlevel threads with kernellevel threads different methods of associating userand kernellevel threads provide different combinations of the low switching overhead of userlevel threads and the high concurrency and parallelism of kernellevel threads figure illustrates three methods of associating userlevel threads with kernellevel threads the thread library creates userlevel threads in a process and associates a thread control block tcb with each userlevel thread the kernel creates kernellevel threads in a process and associates a kernel thread control block ktcb with each kernellevel thread in the manytoone association method a single kernellevel thread is created in a process by the kernel and all userlevel threads created in a process by the thread library are associated with this kernellevel thread this method of association provides an effect similar to mere userlevel threads userlevel threads can be concurrent without being parallel thread switching incurs low overhead and blocking of a userlevel thread leads to blocking of all threads in the process in the onetoone method of association each userlevel thread is permanently mapped into a kernellevel thread this association provides an effect similar to mere kernellevel threads threads can operate in parallel on different cpus of a multiprocessor system however switching between threads is performed at the kernel level and incurs high overhead blocking of a userlevel thread does not block other userlevel threads of the process because they are mapped into different kernellevel threads the manytomany association method permits a userlevel thread to be mapped into different kernellevel threads at different times see figure c it provides parallelism between userlevel threads that are mapped into different kernellevel threads at the same time and provides low overhead of switching pcb pcb pcb tcbs tcbs tcbs ktcbs ktcbs ktcbs a b c figure a manytoone b onetoone c manytomany associations in hybrid threads case studies of processes and threads processes in unix data structures unix uses two data structures to hold control data about processes proc structure contains process id process state priority information about relationships with other processes a descriptor of the event for which a blocked process is waiting signal handling mask and memory management information u area stands for user area contains a process control block which stores the cpu state for a blocked process pointer to proc structure user and group ids and information concerning the following signal handlers open files and the current directory terminal attached to the process and cpu usage by the process these data structures together hold information analogous to the pcb data structure discussed in section the proc structure mainly holds scheduling related data while the u area contains data related to resource allocation and signal handling the proc structure of a process is always held in memory the u area needs to be in memory only when the process is in operation types of processes two types of processes exist in unix user processes and kernel processes a user process executes a user computation it is associated with the users terminal when a user initiates a program the kernel creates the primary process for it which can create child processes see section a daemon process is one that is detached from the users terminal it runs in the background and typically performs functions on a systemwide basis eg print spooling and network management once created daemon processes can exist throughout the lifetime of the os kernel processes execute code of the kernel they are concerned with background activities of the kernel like swapping they are created automatically when the system is booted and they can invoke kernel functionalities or refer to kernel data structures without having to perform a system call process creation and termination the system call fork creates a child process and sets up its context called the userlevel context in unix literature it allocates a proc structure for the newly created process and marks its state as ready and also allocates a u area for the process the kernel keeps track of the parentchild relationships using the proc structure fork returns the id of the child process chapter processes and threads the userlevel context of the child process is a copy of the parents userlevel context hence the child executes the same code as the parent at creation the program counter of the child process is set to contain the address of the instruction at which the fork call returns the fork call returns a in the child process which is the only difference between parent and child processesa child process can execute the same program as its parent or it can use a system call from the exec family of system calls to load some other program for execution although this arrangement is cumbersome it gives the child process an option of executing the parents code in the parents context or choosing its own program for execution the former alternative was used in older unix systems to set up servers that could service many user requests concurrently the complete view of process creation and termination in unix is as follows after booting the system creates a process init this process creates a child process for every terminal connected to the system after a sequence of exec calls each child process starts running the login shell when a programmer indicates the name of a file from the command line the shell creates a new process that executes an exec call for the named file in effect becoming the primary process of the program thus the primary process is a child of the shell process the shell process now executes the wait system call described later in this section to wait for end of the primary process of the program thus it becomes blocked until the program completes and becomes active again to accept the next user command if a shell process performs an exit call to terminate itself init creates a new process for the terminal to run the login shell a process pi can terminate itself through the exit system call exit statuscode where statuscode is a code indicating the termination status of the process on receiving the exit call the kernel saves the status code in the proc structure of pi closes all open files releases the memory allocated to the process and destroys its u area however the proc structure is retained until the parent of pi destroys it this way the parent of pi can query its termination status any time it wishes in essence the terminated process is dead but it exists hence it is called a zombie process the exit call also sends a signal to the parent of pi the child processes of pi are made children of the kernel process init this way init receives a signal when a child of pi say pc terminates so that it can release pcs proc structure waiting for process termination a process pi can wait for the termination of a child process through the system call wait addr where addr is the address of a variable say variable xyz within the address space of pi if process pi has child processes and at least one of them has already terminated the wait call stores the termination status of a terminated child process in xyz and immediately returns with the id of the terminated child process if more terminated child processes exist their termination status will be made available to pi only when it repeats the wait call the state of process pi is changed to blocked if it has children but none of them has terminated it will be unblocked when one of the child processes terminates the wait call returns with a if pi has no children the following example illustrates benefits of these semantics of the wait call part process management example child processes in unix figure shows the c code of a process that creates three child processes in the for loop and awaits their completion this code can be used to set up processes of the realtime data logging system of example note that the fork call returns to the calling process with the id of the newly created child process whereas it returns to the child process with a because of this peculiarity child processes execute the code in the if statement while the parent process skips the if statement and executes a wait statement the wait is satisfied whenever a child process terminates through the exit statement however the parent process wishes to wait until the last process finishes so it issues another wait if the value returned is anything other than the fourth wait call returns with a which brings the parent process out of the loop the parent process code does not contain an explicit exit call the language compiler automatically adds this at the end of main waiting for occurrence of events a process that is blocked on an event is said to sleep on it eg a process that initiates an io operation would sleep on its completion event unix uses an interesting arrangement to activate processes sleeping on an event it does not use event control blocks ecbs described earlier in section instead it uses event addresses a set of addresses is reserved in the kernel and every event is mapped into one of these addresses when a process wishes to sleep on an event the address of the event is computed the state of the process is changed to blocked and the address of the event is put in its process structure this address serves as the description of the event awaited by the process when the event occurs the kernel computes its event address and activates all processes sleeping on it main int savedstatus for i i i if fork code for child processes exit while waitsavedstatus loop till all child processes terminate figure process creation and termination in unix chapter processes and threads this arrangement incurs unnecessary overhead in some situations for example consider several processes sleeping on the same event as a result of data access synchronization when the event occurs all these processes are activated but only one process gains access to the data and the other processes go back to sleep this is analogous to the busy wait situation which we will discuss in the next chapter the method of mapping events into addresses adds to this problem a hashing scheme is used for mapping and so two or more events may map into the same event address now occurrence of any one of these events will activate all processes sleeping on all these events each activated process would now have to check whether the event on which it is sleeping has indeed occurred and go back to sleep if this is not the case interrupt servicing unix avoids interrupts during sensitive kernellevel actions by assigning each interrupt an interrupt priority level ipl depending on the program being executed by the cpu an interrupt priority level is also associated with the cpu when an interrupt at a priority level l arises it is handled only if l is larger than the interrupt priority level of the cpu otherwise it is kept pending until the cpus interrupt priority level becomes l the kernel uses this feature to prevent inconsistency of the kernel data structures by raising the ipl of the cpu to a high value before starting to update its data structures and lowering it after the update is completed system calls when a system call is made the system call handler uses the system call number to determine which system functionality is being invoked from its internal tables it knows the address of the handler for this functionality it also knows the number of parameters this call is supposed to take however these parameters exist on the user stack which is a part of the process context of the process making the call so these parameters are copied from the process stack into some standard place in the u area of the process before control is passed to the handler for the specific call this action simplifies operation of individual event handlers signals a signal can be sent to a process or to a group of processes this action is performed by the kill system call kill pid signum where pid is an integer value that can be positive zero or negative a positive value of pid is the id of a process to which the signal is to be sent a value of pid implies that the signal is to be sent to some processes within the same process tree as the sender process ie some processes that share an ancestor with the sender process this feature is implemented as follows at a fork call the newly created process is assigned a group id that is the same as the process group number of its parent process a process may change its group number by using the setpgrp system call when pid the signal is sent to all processes with the same group number as the sender a negative value of pid is used to reach processes outside the process tree of the sender we will not elaborate on this feature here a process specifies a signal handler by executing the statement oldfunction signal signum function part process management where signal is a function in the c library that makes a signal system call signum is an integer and function is the name of a function within the address space of the process this call specifies that the function function should be executed on occurrence of the signal signum the signal call returns with the previous action specified for the signal signum a user can specify sigdfl as function to indicate that the default action defined in the kernel such as producing a core dump and aborting the process is to be executed on occurrence of the signal or specify sigign as function to indicate that the occurrence of the signal is to be ignored the kernel uses the u area of a process to note the signal handling actions specified by it and a set of bits in the proc structure to register the occurrence of signals whenever a signal is sent to a process the bit corresponding to the signal is set to in the proc structure of the destination process the kernel now determines whether the signal is being ignored by the destination process if not it makes provision to deliver the signal to the process if a signal is ignored it remains pending and is delivered when the process specifies its interest in receiving the signal either by specifying an action or by specifying that the default action should be used for it a signal remains pending if the process for which it is intended is in a blocked state the signal is delivered when the process comes out of the blocked state in general the kernel checks for pending signals when a process returns from a system call or interrupt after a process gets unblocked and before a process gets blocked on an event invocation of the signal handling action is implemented as described earlier in section a few anomalies exist in the way signals are handled if a signal occurs repeatedly the kernel simply notes that it has occurred but does not count the number of its occurrences hence the signal handler may be executed once or several times depending on when the process gets scheduled to execute the signal handler another anomaly concerns a signal sent to a process that is blocked in a system call after executing the signal handler such a process does not resume its execution of the system call instead it returns from the system call if necessary it may have to repeat the system call table lists some interesting unix signals table interesting signals in unix signal description sigchld child process died or suspended sigfpe arithmetic fault sigill illegal instruction sigint tty interrupt controlc sigkill kill process sigsegv segmentation fault sigsys invalid system call sigxcpu cpu time limit is exceeded sigxfsz file size limit is exceeded chapter processes and threads process states and state transitions there is one conceptual difference between the process model described in section and that used in unix in the model of section a process in the running state is put in the ready state the moment its execution is interrupted a system process then handles the event that caused the interrupt if the running process had itself caused a software interrupt by executing an siinstrn its state may further change to blocked if its request can not be granted immediately in this model a user process executes only user code it does not need any special privileges a system process may have to use privileged instructions like io initiation and setting of memory protection information so the system process executes with the cpu in the kernel mode processes behave differently in the unix model when a process makes a system call the process itself proceeds to execute the kernel code meant to handle the system call to ensure that it has the necessary privileges it needs to execute with the cpu in the kernel mode a mode change is thus necessary every time a system call is made the opposite mode change is necessary after processing a system call similar mode changes are needed when a process starts executing the interrupt servicing code in the kernel because of an interrupt and when it returns after servicing an interrupt the unix kernel code is made reentrant so that many processes can execute it concurrently this feature takes care of the situation where a process gets blocked while executing kernel code eg when it makes a system call to initiate an io operation or makes a request that can not be granted immediately to ensure reentrancy of code every process executing the kernel code must use its own kernel stack this stack contains the history of function invocations since the time the process entered the kernel code if another process also enters the kernel code the history of its function invocations will be maintained on its own kernel stack thus their operation would not interfere in principle the kernel stack of a process need not be distinct from its user stack however distinct stacks are used in practice because most computer architectures use different stacks when the cpu is in the kernel and user modes unix uses two distinct running states these states are called user running and kernel running states a user process executes user code while in the user running state and kernel code while in the kernel running state it makes the transition from user running to kernel running when it makes a system call or when an interrupt occurs it may get blocked while in the kernel running state because of an io operation or nonavailability of a resource when the io operation completes or its resource request is granted the process returns to the kernel running state and completes the execution of the kernel code that it was executing it now leaves the kernel mode and returns to the user mode accordingly its state is changed from kernel running to user running because of this arrangement a process does not get blocked or preempted in the user running state it first makes a transition to the kernel running state and then gets blocked or preempted in fact user running kernel running is the only transition out of the user running state figure illustrates fundamental process states and state transitions in unix as shown there even process termination occurs when a process is in the kernel running state this happens because the part process management user running interrupt return from system call interrupt system call kernel exit zombie running dispatching resource or wait request preemption ready resource granted blocked or wait completed figure process state transitions in unix process executes the system call exit while in the user running state this call changes its state to kernel running the process actually terminates and becomes a zombie process as a result of processing this call processes and threads in linux data structures the linux kernel supports the threading model ie kernellevel threads it uses a process descriptor which is a data structure of type taskstruct to contain all information pertaining to a process or thread for a process this data structure contains the process state information about its parent and child processes the terminal used by the process its current directory open files the memory allocated to it signals and signal handlers the kernel creates substructures to hold information concerning the terminal directory files memory and signals and puts pointers to them in the process descriptor this organization saves both memory and overhead when a thread is created creation and termination of processes and threads both processes and threads are created through the system calls fork and vfork whose functionalities are identical to the corresponding unix calls these functionalities are actually implemented by the system call clone which is hidden from the view of programs the clone system call takes four parameters start address of the process or thread parameters to be passed to it flags and a child stack specification some of the important flags are clonevm shares the memory management information used by the mmu clonefs shares the information about root and current working directory chapter processes and threads clonefiles shares the information about open files clonesighand shares the information about signals and signal handlers the organization of taskstruct facilitates selective sharing of this information since it merely contains pointers to the substructures where the actual information is stored at a clone call the kernel makes a copy of taskstruct in which some of these pointers are copied and others are changed a thread is created by calling clone with all flags set so that the new thread shares the address space files and signal handlers of its parent a process is created by calling clone with all flags cleared the new process does not share any of these components the linux kernel also includes support for the native posix threading library nptl which provides a number of enhancements that benefit heavily threaded applications it can support up to billion threads whereas the linux kernel could support only up to threads per cpu a new system call exitgroup has been introduced to terminate a process and all its threads it can terminate a process having a hundred thousand threads in about seconds as against about minutes in the linux kernel signal handling is performed in the kernel space and a signal is delivered to one of the available threads in a process stop and continue signals affect an entire process while fatal signals terminate the entire process these features simplify handling of multithreaded processes the linux kernel also supports a fast userspace mutex called futex that reduces the overhead of thread synchronization through a reduction in the number of system calls parent child relationships information about parent and child processes or threads is stored in a taskstruct to maintain awareness of the process tree taskstruct contains a pointer to the parent and to the deemed parent which is a process to which termination of this process should be reported if its parent process has terminated a pointer to the youngest child and pointers to the younger and older siblings of a process thus the process tree of figure would be represented as shown in figure datalogger copysample recordsample housekeeping figure linux process tree for the processes of figure a part process management process states the state field of a process descriptor contains a flag indicating the state of a process a process can be in one of five states at any time taskrunning the process is either scheduled or waiting to be scheduled taskinterruptible the process is sleeping on an event but may receive a signal taskuninterruptible the process is sleeping on an event but may not receive a signal taskstopped the operation of the process has been stopped by a signal taskzombie the process has completed but the parent process has not yet issued a system call of the waitfamily to check whether it has terminated the taskrunning state corresponds to one of running or ready states described in section the taskinterruptible and taskuninterruptible states both correspond to the blocked state splitting the blocked state into two states resolves the dilemma faced by an os in handling signals sent to a process in the blocked state see section a process can decide whether it wants to be activated by a signal while waiting for an event to occur or whether it wants the delivery of a signal to be deferred until it comes out of the blocked state a process enters the taskstopped state when it receives a sigstop or sigtstp signal to indicate that its execution should be stopped or a sigttin or sigttou signal to indicate that a background process requires input or output threads in solaris solaris which is a unix based operating system originally provided a hybrid thread model that actually supported all three association methods of hybrid threads discussed in section namely manytoone onetoone and manytomany association methods this model has been called the m n model in sun literature solaris continued to support this model and also provided an alternative implementation which is equivalent to kernellevel threads the support for the m n model was discontinued in solaris in this section we discuss the m n model and the reasons why it was discontinued the m n model employs three kinds of entities to govern concurrency and parallelism within a process user threads user threads are analogous to userlevel threads discussed in section they are created and managed by a thread library so they are not visible to the kernel lightweight processes a lightweight process lwp is an intermediary between user threads and a kernel thread many lwps may be created for chapter processes and threads a process each lwp is a unit of parallelism within a process user threads are mapped into lwps by the thread library this mapping can be onetoone manytoone manytomany or a suitable combination of all three the number of lwps for a process and the nature of the mapping between user threads and lwps is decided by the programmer who makes it known to the thread library through appropriate function calls kernel threads a kernel thread is a kernellevel thread the kernel creates one kernel thread for each lwp in a process it also creates some kernel threads for its own use eg a thread to handle disk io in the system figure illustrates an arrangement of user threads lwps and kernel threads process pi has three user threads and one lwp so a manytoone mapping exists between them process pj has four user threads and three lwps one of these user threads is exclusively mapped into one of the lwps the remaining three user threads and two lwps have a manytomany mapping this way each of the three threads can operate in any of the two lwps lwps can operate in parallel because each of them has a kernel thread associated with it the kernel creates an lwp control block for each lwp and a kernel thread control block ktcb for each kernel thread in addition the thread library maintains a thread control block for each user thread the information in this control block is analogous to that described in section the scheduler examines the ktcbs and for each cpu in the system selects a kernel thread that is in the ready state the dispatcher dispatches the lwp corresponding to this kernel thread the thread library can switch between user threads mapped into this lwp to achieve concurrency between user threads the number of lwps threads pi pcb pj pcb process context thread thread control blocks library mapping performed by thread library lwp control blocks kernel thread control blocks scheduler selected ktcb figure threads in solaris part process management per process and the association of user threads with lwps is decided by the programmer thus both parallelism and concurrency within a process are under the programmers control an nway parallelism would be possible within a process if the programmer created n lwps for a process n p where p is the number of cpus however the degree of parallelism would reduce if a user thread made a blocking system call during its operation because the call would block the lwp in which it is mapped solaris provides scheduler activations described later in this section to overcome this problem a complex arrangement of control blocks is used to control switching between kernel threads the kernel thread control block contains the kernel registers of the cpu stack pointer priority scheduling information and a pointer to the next ktcb in a scheduling list in addition it contains a pointer to the lwp control block the lwp control block contains saved values of user registers of the cpu signal handling information and a pointer to the pcb of the owner process signal handling signals generated by operation of a thread such as an arithmetic condition or a memory protection violation are delivered to the thread itself signals generated by external sources such as a timer have to be directed to a thread that has enabled its handling the m n model provided each process with an lwp that was dedicated to signal handling when a signal was generated the kernel would keep it pending and notify this lwp which would wait until it found that some thread that had enabled handling of that specific signal was running on one of the other lwps of the process and would ask the kernel to direct the pending signal to that lwp states of processes and kernel threads the kernel is aware only of states of processes and kernel threads it is oblivious to existence of user threads a process can be in one of the following states sidl a transient state during creation srun runnable process sonproc running on a processor ssleep sleeping sstop stopped szomb terminated process the srun and ssleep states correspond to the ready and blocked states of section a kernel thread has states tsrun tsonproc tssleep tsstopped and tszomb that are analogous to the corresponding process states a kernel thread that is free is in the tsfree state scheduler activations a scheduler activation is like a kernel thread the kernel uses scheduler activations to perform two auxiliary functions when some lwp of the process becomes blocked the kernel uses a scheduler activation to create a new lwp so that other runnable threads of the process could operate when an event related to the operation of the thread library occurs the kernel uses a scheduler activation to notify the thread library chapter processes and threads consider a manytoone mapping between many user threads and an lwp and a user thread that is currently mapped into the lwp a kernel thread is associated with the lwp so the user thread operates when the kernel thread is scheduled if the user thread makes a blocking system call the kernel thread would block effectively the lwp with which it is associated would block if some of the other threads that are mapped into the same lwp are runnable we have a situation where a runnable user thread can not be scheduled because the lwp has become blocked in such situations the kernel creates a scheduler activation when the user thread is about to block provides the activation to the thread library and makes an upcall to it the upcall is implemented as a signal sent to the thread library the thread library now executes its signal handler using the activation provided by the kernel the signal handler saves the state of the user thread that is about to block releases the lwp that was used by it and hands it over to the kernel for reuse it now schedules a new user thread on the new activation provided by the kernel in effect the user thread that was about to block is removed from an lwp and a new user thread is scheduled in a new lwp of the process when the event for which the user thread had blocked occurs the kernel makes another upcall to the thread library with a scheduler activation so that it can preempt the user thread currently mapped into the lwp return the lwp to the kernel and schedule the newly activated thread on the new activation provided by the kernel switchover to the implementation the m n model was developed in the expectation that because a context switch by the thread library incurred significantly less overhead than a context switch by the kernel userlevel scheduling of threads in the thread library would provide good application performance however as mentioned in section it is possible only when schedulers in the thread library and in the kernel work harmoniously the implementation in solaris provided efficient kernellevel context switching use of the model led to simpler signal handling as threads could be dedicated to handling of specific signals it also eliminated the need for scheduler activations and provided better scalability hence the m n model was discontinued in solaris processes and threads in windows the flavor of processes and threads in windows differs somewhat from that presented earlier in this chapter windows treats a process as a unit for resource allocation and uses a thread as a unit for concurrency accordingly a windows process does not operate by itself it must have at least one thread inside it a resource can be accessed only through a resource handle a process inherits some resource handles from its parent process it can obtain more resource handles by opening new resources the kernel stores all these handles in a handles table for each process this way a resource can be accessed by simply specifying an offset into the handles table part process management windows uses three control blocks to manage a process an executive process block contains fields that store the process id memory management information address of the handle table a kernel process block for the process and address of the process environment block the kernel process block contains scheduling information for threads of the process such as the processor affinity for the process the state of the process and pointers to the kernel thread blocks of its threads the executive process block and the kernel process block are situated in the system address space the process environment block contains information that is used by the loader to load the code to be executed and by the heap manager it is situated in the user address space the control blocks employed to manage a thread contain information about its operation and about the process containing it the executive thread block of a thread contains a kernel thread block a pointer to the executive process block of its parent process and impersonation information the kernel thread block contains information about the kernel stack of the thread and the threadlocal storage scheduling information for the thread and a pointer to its thread environment block which contains its id and information about its synchronization requirements windows supports the notion of a job as a method of managing a group of processes a job is represented by a job object which contains information such as handles to processes in it the jobwide cpu time limit per process cpu time limit job scheduling class that sets the time slice for the processes of the job processor affinity for processes of the job and their priority class a process can be a member of only one job all processes created by it automatically belong to the same job process creation the create call takes a parameter that is a handle to the parent of the new process or thread this way a create call need not be issued by the parent of a process or thread a server process uses this feature to create a thread in a client process so that it can access resources with the clients access privileges rather than its own privileges recall from section that the environment subsystems provide support for execution of programs developed for other oss like msdos win and os the semantics of process creation depend on the environment subsystem used by an application process in the win and os operating environments a process has one thread in it when it is created it is not so in other environments supported by the windows os hence process creation is actually handled by an environment subsystem dll that is linked to an application process after creating a process it passes the id of the new process or thread to the environment subsystem process so that it can manage the new process or thread appropriately creation of a child process by an application process in the win environment proceeds as follows the environment subsystem dll linked to the application process makes a system call to create a new process this call is handled by the executive it creates a process object initializes it by loading the image of the code to be executed and returns a handle to the process object the chapter processes and threads environment subsystem dll now makes a second system call to create a thread and passes the handle to the new process as a parameter the executive creates a thread in the new process and returns a handle to it the dll now sends a message to the environment subsystem process passing it the process and thread handles and the id of their parent process the environment subsystem process enters the process handle in the table of processes that currently exist in the environment and enters the thread handle in the scheduling data structures control now returns to the application process thread states and state transitions figure shows the state transition diagram for threads a thread can be in one of following six states ready the thread can be executed if a cpu is available standby this is a thread that has been selected to run next on a specific processor if its priority is high the thread currently running on the processor would be preempted and this thread would be scheduled running a cpu is currently allocated to the thread and the thread is in operation waiting the thread is waiting for a resource or event or has been suspended by the environment subsystem transition the threads wait was satisfied but meanwhile its kernel stack was removed from memory because it had been waiting for long the thread would enter the ready state when the kernel stack is brought back into memory terminated the thread has completed its operation thread pools windows provides a thread pool in every process as described in section the pool contains a set of worker threads and an arrangement standby dispatch running completion terminated select preemption resource for or wait execution request resource granted ready or wait completed waiting kernel stack reloaded kernel stack transiremoved tion figure thread state transitions in windows summary a computer user and the operating system have with one another they achieve this by employing different views of execution of programs the user the process synchronization means provided in the is concerned with achieving execution of a prooperating system gram in a sequential or concurrent manner as the operating system allocates resources to a desired whereas the os is concerned with alloprocess and stores information about them in the cation of resources to programs and servicing of process context of the process to control operaseveral programs simultaneously so that a suittion of the process it uses the notion of a process able combination of efficient use and user service state the process state is a description of the curmay be obtained in this chapter we discussed varrent activity within the process the process state ious aspects of these two views of execution of changes as the process operates the fundamental programs process states are ready running blocked termiexecution of a program can be speeded up nated and suspended the os keeps information through either parallelism or concurrency paralconcerning each process in a process control block lelism implies that several activities are in progress pcb the pcb of a process contains the process within the program at the same time concurrency state and the cpu state associated with the process is an illusion of parallelism activities appear to if the cpu is not currently executing its instrucbe parallel but may not be actually so tions the scheduling function of the kernel selects a process is a model of execution of a proone of the ready processes and the dispatching gram when the user issues a command to execute function switches the cpu to the selected process a program the os creates the primary process through information found in its process context for it this process can create other processes by and the pcb making requests to the os through system calls a thread is an alternative model of execueach of these processes is called its child process tion of a program a thread differs from a process the os can service a process and some of its in that no resources are allocated to it this difchild processes concurrently by letting the same ference makes the overhead of switching between cpu execute instructions of each one of them threads much less than the overhead of switchfor some time or service them in parallel by exeing between processes three models of threads cuting their instructions on several cpus at the called kernellevel threads userlevel threads and same time the processes within a program must hybrid threads are used they have different impliwork harmoniously toward a common goal by cations for switching overhead concurrency and sharing data or by coordinating their activities parallelism chapter processes and threads test your concepts an application comprises several processes e when a userlevel thread of a process makes a a primary process and some child prosystem call that leads to blocking all threads cesses this arrangement provides computation of the process become blocked speedup if f kernellevel threads provide more concura the computer system contains many cpus rency than userlevel threads in both uniprob some of the processes are io bound cessor and multiprocessor systems c some of the processes are cpu bound g when a process terminates its termination d none of the above code should be remembered until its parent classify each of the following statements as true process terminates or false which of the following state transitions for a a the os creates a single process if two users process can cause the state transition blocked execute the same program ready for one or more other processes b the state of a process that is blocked on a a a process starts an io operation and resource request changes to running when the becomes blocked resource is granted to it b a process terminates c there is no distinction between a terminated c a process makes a resource request and process and a suspended process becomes blocked d after handling an event the kernel need d a process sends a message not perform scheduling before dispatching if e a process makes the state transition blocked none of the process states has changed blocked swapped exercises describe the actions of the kernel when prob give a sequence of state transitions through cesses make system calls for the following which it can reach the ready state purposes is more than one sequence of state transitions a request to receive a message possible in each of these cases b request to perform an io operation the designer of a kernel has decided to use a sinc request for status information concerning a gle swapped state give a diagram analogous to process figure showing process states and state trand request to create a process sitions describe how the kernel would perform e request to terminate a child process swapping and comment on the effectiveness of describe the conditions under which a kernel swapping may perform dispatching without performing compare and contrast inherent parallelism in scheduling the following applications give an algorithm to implement a unixlike a an online banking application which perwait call using the pcb data structure shown mits users to perform banking transactions in table comment on comparative lifetimes through a webbased browser of a process and its pcb b a webbased airline reservation system describe how each signal listed in table is an airline reservation system using a centralized raised and handled in unix database services user requests concurrently is a process is in the blocked swapped state it preferable to use threads rather than proa give a sequence of state transitions through cesses in this system give reasons for your which it could have reached this state answer part process management name two system calls a thread should avoid comment on computation speedup of the folusing if threads are implemented at the user level lowing applications in computer systems having and explain your reasons i a single cpu and ii many cpus as described in example and illustrated in a many threads are created in a server that figure if a process has userlevel threads handles user requests at a large rate where its own state depends on states of all of its servicing of a user request involves both cpu threads list the possible causes of each of the and io activities fundamental state transitions for such a process b computation of an expression z a b explain whether you agree with the following cd is performed by spawning two child statement on the basis of what you read in this processes to evaluate a b and c d chapter concurrency increases the scheduling c a server creates a new thread to handle every overhead without providing any speedup of an user request received and servicing of each application program user request involves accesses to a database on the basis of the solaris case study write a d two matrices contain m rows and n columns short note on how to decide the number of user each where m and n are both very large an threads and lightweight processes lwps that application obtains the result of adding the should be created in an application two matrices by creating m threads each of an os supports both userlevel threads and which performs addition of one row of the kernellevel threads do you agree with the folmatrices lowing recommendations about when to use compute the best computation speedup in the userlevel threads and when kernellevel threads realtime data logging application of examwhy or why not ple under the following conditions the overa if a candidate for a thread is a cpubound head of event handling and process switching computation make it a kernellevel thread is negligible for each sample the copysample if the system contains multiple processors process requires microseconds s of cpu otherwise make it a userlevel thread time and does not involve any io operation b if a candidate for a thread is an iobound recordsample requires ms to record the samcomputation make it a userlevel thread if ple and consumes only s of cpu time while the process containing it does not contain housekeeping consumes s of cpu time and a kernellevel thread otherwise make it a its write operation requires ms kernellevel thread class project implementing a shell write a program in cc which will act as a shell in ls lists information about files a unix or linux system when invoked the program in the current directory will display its own prompt to the user accept the users rm deletes indicated files command from the keyboard classify it and invoke an supports options r f v appropriate routine to implement it the command syshistory n prints the most recent n tem should not be used in implementing any command commands issued by the user other than the ls command the shell must support the along with their serial numbers following commands if n is omitted prints all commands issued by the user issue n issues the nth command in the command description history once again programname creates a child process to run cd directoryname changes current directory programname supports if user has appropriate the redirection operators and permissions to redirect the input and chapter processes and threads output of the program to rmexcept listoffiles which removes all files indicated files except those in listoffiles from the current programname the child process for directory programname should be support a command programname m that crerun in the background ates a child process to execute programname but quit quits the shell aborts the process if it does not complete its operaafter implementing a basic shell supporting these comtion in m seconds hint use an appropriate routine mands you should add two advanced features to the from the library to deliver a sigalrm signal after shell m seconds and use a signal handler to perform appropriate actions design a new command that provides a useful facility as an example consider a command bibliography the process concept is discussed in dijkstra management of parallelism acm transactions brinch hansen and bic and shaw brinch on computer systems hansen describes implementation of processes in bach m j the design of the unix the rc system operating system prentice hall englewood marsh et al discusses userlevel threads and cliffs nj issues concerning thread libraries anderson et al beck m h bohme m dziadzka u kunitz discusses use of scheduler activations for communicar magnus c schroter and d verworner tion between the kernel and a thread library engelschall linux kernel programming rd ed discusses how userlevel threads can be implepearson education new york mented in unix by using standard unix facilities and bic l and a c shaw the logical also summarizes properties of other multithreading design of operating systems nd ed prentice packages hall englewood cliffs nj kleiman butenhof lewis and berg brinch hansen p the nucleus of a and nichols et al discuss programming multiprogramming system communications of with posix threads lewis and berg discusses the acm multithreading in java brinch hansen p operating system bach mckusick and vahalia principles prentice hall englewood discuss processes in unix beck et al and bovet cliffs nj and cesati describes processes and threads in bovet d p and m cesati understanding linux stevens and rago describes processes and the linux kernel rd ed oreilly sebastopol threads in unix linux and bsd it also discusses dae butenhof d programming with posix mon processes in unix ogorman discusses threads addisonwesley reading implementation of signals in linux eykholt et al mass describes threads in sunos while vahalia custer h inside windowsnt microsoft and mauro and mcdougall describe threads press redmond wash and lwps in solaris custer richter and dijkstra e w the structure of the russinovich and solomon describe processes and multiprogramming system communications of threads in windows vahalia and tanenbaum the acm discuss threads in mach engelschall r s portable multithreading the signal stack trick for anderson t e b n bershad e d lazowska userspace thread creation proceedings of the and h m levy scheduler activations usenix annual technical conference effective kernel support for the userlevel san diego part process management eykholt j r s r kleiman s barton implementation of the bsd operating system s faulkner a shivalingiah m smith d stein addison wesley reading mass j voll m weeks and d williams nichols b d buttlar and j p farrell beyond multiprocessing multithreading the pthreads programming oreilly sebastopol sunos kernel proceedings of the summer ogorman j linux process manager usenix conference the internals of scheduling interrupts and kleiman s d shah and b smaalders signals john wiley new york programming with threads prentice hall richter j programming applications for englewood cliffs nj microsoft windows th ed microsoft press lewis b and d berg multithreaded redmond wash programming with pthreads prentice hall russinovich m e and d a solomon englewood cliffs nj microsoft windows internals th ed microsoft lewis b and d berg multithreaded press redmond wash programming with java technology sun silberschatz a p b galvin and g gagne microsystems operating system principles th ed john mauro j and r mcdougall solaris wiley new york internals nd ed prentice hall englewood stevens w r and s a rago advanced cliffs nj programming in the unix environment nd ed marsh b d m l scott t j leblanc and addisonwesley reading mass e p markatos firstclass userlevel tanenbaum a s modern operating threads proceedings of the thirteenth acm systems nd ed prentice hall englewood symposium on operating systems principles cliffs nj october vahalia u unix internals the mckusick m k k bostic m j karels new frontiers prentice hall englewood and j s quarterman the design and cliffs nj what is process synchronization in this chapter we use the term process as a generic term for both a process and a thread applications employ concurrent processes either to achieve computation speedup see table or to simplify their own design as in multithreaded servers see section as summarized in table processes of an application interact among themselves to share data coordinate their activities and exchange messages or signals we use the following notation to formally define the term interacting processes readseti set of data items read by process pi and interprocess messages or signals received by it writeseti set of data items modified by process pi and interprocess messages or signals sent by it race conditions in section we mentioned that uncoordinated accesses to shared data may affect consistency of data to see this problem consider processes pi and pj that update the value of a shared data item ds through operations ai and aj respectively operation ai ds ds operation aj ds ds chapter process synchronization the control structure parbegin list of statements parend encloses code that is to be executed in parallel parbegin stands for parallelbegin and parend for parallelend if list of statements contains n statements execution of the parbeginparend control structure spawns n processes each process consisting of the execution of one statement in list of statements for example parbegin s s s s parend initiates four processes that execute s s s and s respectively the statement grouping facilities of a language such as beginend can be used if a process is to consist of a block of code instead of a single statement for visual convenience we depict concurrent processes created in a parbeginparend control structure as follows parbegin s s sn sm sm snm parend process p process p process pn where statements s sm form the code of process p etc declarations of shared variables are placed before a parbegin declarations of local variables are placed at the start of a process comments are enclosed within braces indentation is used to show nesting of control structures figure pseudocode conventions for concurrent programs let dsinitial be the initial value of ds and let process pi be the first one to perform its operation the value of ds after operation ai will be dsinitial if process pj performs operation aj now the resulting value of ds will be dsnew dsinitial ie dsinitial if the processes perform their operations in the reverse order the new value of ds would be identical if processes pi and pj perform their operations concurrently we would expect the result to be dsinitial however it is not guaranteed to be so this situation is called a race condition this term is borrowed from electronics where it refers to the principle that an attempt to examine a value or make measurements on a waveform while it is changing can lead to wrong results the race condition can be explained as follows operation ai is typically implemented by using three machine instructions the first instruction loads the value of ds in a data register say register r the second instruction adds to the contents of r and the third instruction stores the contents of r back into the location assigned to ds we call this sequence of instructions the loadaddstore sequence operation aj is similarly implemented by a loadaddstore sequence the result of performing operations ai and aj would be wrong if both ai and aj operated on the old value of ds this could happen if one process were engaged in performing the loadaddstore sequence but the other process was performing a load instruction before this sequence was completed in such a case the value of ds at the end of both the operations would be either dsinitial or dsinitial depending on which of the operations completed later part process management we define a race condition formally as follows let function fids represent the operation ai on ds ie for a given value of ds fids indicates the value ds would have after executing operation ai function fjds analogously represents the operation aj on ds let process pi be the first one to perform its operation the value of ds after the operation would be fids if process pj performs operation aj now operation aj will operate on fids so the resulting value of ds will be fj fids if the processes perform their operations in the reverse order the new value of ds will be fi fj ds definition race condition a condition in which the value of a shared data item ds resulting from execution of operations ai and aj on ds in interacting processes may be different from both fi fj ds and fj fids the next example illustrates a race condition in an airline reservation application and its consequences example race condition in an airline reservation application the left column in the upper half of figure shows the code used by processes in an airline reservation application the processes use identical code hence ai and aj the operations performed by processes pi and pj are identical each of these operations examines the value of nextseatno and updates it by if a seat is available the right column of figure shows the machine instructions corresponding to the code statement s corresponds to three instructions s s and s that form a loadaddstore sequence of instructions for updating the value of nextseatno the lower half of figure is a timing diagram for the applications it shows three possible sequences in which processes pi and pj could execute their instructions when nextseatno and capacity in case process pi executes the if statement that compares values of nextseatno with capacity and proceeds to execute instructions s s s and s that allocate a seat and increment nextseatno when process pj executes the if statement it finds that no seats are available so it does not allocate a seat in case process pi executes the if statement and finds that a seat can be allocated however it gets preempted before it can execute instruction s process pj now executes the if statement and finds that a seat is available it allocates a seat by executing instructions s s s and s and exits nextseatno is now when process pi is resumed it proceeds to execute instruction s which allocates a seat thus seats are allocated to both requests this is a race condition because when nextseatno only one seat should be allocated in case process pi gets preempted after it loads in regj through instruction s now again both pi and pj allocate a seat each which is a race condition chapter process synchronization code of processes corresponding machine instructions s if nextseatno capacity s load nextseatno in regk s if regk capacity goto s then s allotednonextseatno s move nextseatno to allotedno s nextseatnonextseatno s load nextseatno in regj s add to regj s store regj in nextseatno s go to s else s display sorry no seats s display sorry available s s some execution cases pi s s s s s s s case pj s s s pi s s s s s s s case pj s s s s s s s pi s s s s s s s case pj s s s s s s s execution of instructions by processes time figure data sharing by processes of a reservation application a program containing a race condition may produce correct or incorrect results depending on the order in which instructions of its processes are executed this feature complicates both testing and debugging of concurrent programs so race conditions should be prevented data access synchronization race conditions are prevented if we ensure that operations ai and aj of definition do not execute concurrently that is only one of the operations can access shared data ds at any time this requirement is called mutual exclusion when mutual exclusion is ensured we can be sure that the result of executing operations ai and aj would be either fi fj ds or fj fids data access synchronization is coordination of processes to implement mutual exclusion over shared data a technique of data access synchronization is used to delay a process that wishes to access ds if another process is currently accessing ds and to resume its operation when the other process finishes using ds to prevent race conditions we first check if the logic of processes in an application causes a race condition we use the following notation for this purpose critical sections mutual exclusion between actions of concurrent processes is implemented by using critical sections of code a critical section is popularly known by its acronym cs definition critical section a critical section for a data item ds is a section of code that is designed so that it can not be executed concurrently either with itself or with other critical sections for ds if some process pi is executing a critical section for ds another process wishing to execute a critical section for ds will have to wait until pi finishes executing its critical section thus a critical section for a data item ds is a mutual exclusion region with respect to accesses to ds we mark a critical section in a segment of code by a dashed rectangular box note that processes may share a single copy of the segment of code that contains one critical section in which case only a single critical section for ds exists in the application in all other cases many critical sections for ds may exist in the application definition covers both situations a process that is executing a critical section is said to be in a critical section we also use the terms enter a critical section and exit a critical section for situations where a process starts and completes an execution of a critical section figure a shows the code of a process that contains several critical sections the process has a cyclic behavior due to the statement repeat forever in each iteration it enters a critical section when it needs to access a shared data item at other times it executes other parts of code in its logic which together constitute remainder of the cycle for simplicity whenever possible we use the simple process form shown in figure b to depict a process the following example illustrates the use of a critical section to avoid race conditions chapter process synchronization repeat forever repeat forever critical section critical section remainder of critical section the cycle critical section remainder of the cycle end end a b figure a a process with many critical sections b a simpler way of depicting this process if nextseatno capacity if nextseatno capacity then then allotednonextseatno allotednonextseatno nextseatnonextseatno nextseatnonextseatno else else display sorry no seats display sorry no seats available available process pi process pj figure use of critical sections in an airline reservation system preventing a race condition through a critical section example figure shows use of critical sections in the airline reservation system of figure each process contains a critical section in which it accesses and updates the shared variable nextseatno let finextseatno and fjnextseatno represent the operations performed in critical sections of pi and pj respectively if pi and pj attempt to execute their critical sections concurrently one of them will be delayed hence the resulting value of nextseatno will be either fi fj nextseatno or fj finextseatno from definition a race condition does not arise use of critical sections causes delays in operation of processes both processes and the kernel must cooperate to reduce such delays a process must not execute for too long inside a critical section and must not make system calls that might put it in the blocked state the kernel must not preempt a process that is engaged in executing a critical section this condition requires the kernel to know whether a process is inside a critical section at any moment and it can not be met if processes implement critical sections on their own ie without involving the kernel nevertheless in this chapter we shall assume that a process spends only a short time inside a critical section control synchronization and indivisible operations interacting processes need to coordinate their execution with respect to one another so that they perform their actions in a desired order this requirement is met through control synchronization chapter process synchronization perform operation ai only after pj perform operation aj performs operation aj process pi process pj figure processes requiring control synchronization var operationaj performed boolean piblocked boolean begin operationaj performed false piblocked false parbegin if operationaj performed false perform operation aj then if piblocked true piblocked true then block pi piblocked false perform operation ai activate pi else operationaj performed true parend end process pi process pj figure a naive attempt at signaling through boolean variables figure shows a pseudocode for processes pi and pj wherein process pi would perform an operation ai only after process pj has performed an operation aj signaling is a general technique of control synchronization it can be used to meet the synchronization requirement of figure as follows when process pi reaches the point where it wishes to perform operation ai it checks whether process pj has performed operation aj if it is so pi would perform operation ai right away otherwise it would block itself waiting for process pj to perform operation aj after performing operation aj process pj would check whether pi is waiting for it if so it would signal process pi to resume its operation figure shows a naive attempt at signaling the synchronization data consists of two boolean variables operationajperformed is a flag that indicates whether process pj has performed operation aj and piblocked is a flag which indicates whether process pi has blocked itself waiting for process pj to execute operation aj both these flags are initialized to false the code makes system calls to block and activate processes to achieve the desired control synchronization before performing operation ai process pi consults the variable operationaj performed to check whether process pj has already performed operation aj if so it goes ahead to perform operation ai otherwise it sets part process management table race condition in process synchronization time actions of process p i actions of process pj t if actionajperformed false t perform action aj t if piblocked true t actionajperformed true t piblocked true t block pi piblocked to true and makes a system call to block itself process pj performs operation aj and checks whether process pi has already become blocked to wait until it has performed operation aj if so it makes a system call to activate pi otherwise it sets operationajperformed to true so that process pi would know that it has performed operation aj however this naive signaling arrangement does not work because process pi may face indefinite blocking in some situations table shows such a situation process pi checks the value of operationajperformed and finds that operation aj has not been performed at time t it is poised to set the variable piblocked to true but at this time it is preempted process pj is now scheduled it performs operation aj and checks whether process pi is blocked however piblocked is false so pj simply sets operationajperformed to true and continues its execution pi is scheduled at time t it sets piblocked to true and makes a system call to block itself process pi will sleep for ever in the notation of section consider the if statements in processes pi and pj to represent the operations fi and fj on the state of the system the result of their execution should have been one of the following process pi blocks itself gets activated by pj and performs operation ai or process pi finds that pj has already performed aj and goes ahead to perform operation ai however in the execution shown in table process pi blocks itself and is never activated from definition this is a race condition the race condition has two causes process pi can be preempted after finding operationajperformed false but before setting piblocked to true and process pj can be preempted after finding piblocked false but before setting operationajperformed to true the race condition can be prevented if we could ensure that processes pi and pj would not be preempted before they set the respective flags to true an indivisible operation also called an atomic operation is the device that ensures that processes can execute a sequence of actions without being preempted definition indivisible operation an operation on a set of data items that can not be executed concurrently either with itself or with any other operation on a data item included in the set synchronization approaches in this section we discuss how the critical sections and indivisible operations required for process synchronization can be implemented looping versus blocking a critical section for ds and an indivisible signaling operation on ds have the same basic requirement processes should not be able to execute some sequences part process management of instructions concurrently or in parallel hence both could be implemented through mutual exclusion as follows while some process is in a critical section on ds or is executing an indivisible operation using ds do nothing critical section or indivisible operation using ds in the while loop the process checks if some other process is in a critical section for the same data or is executing an indivisible operation using the same data if so it keeps looping until the other process finishes this situation is called a busy wait because it keeps the cpu busy in executing a process even as the process does nothing the busy wait ends only when the process finds that no other process is in a critical section or executing an indivisible operation a busy wait in a process has several adverse consequences an implementation of critical sections employing busy waits can not provide the bounded wait property because when many processes are in a busy wait for a cs the implementation can not control which process would gain entry to a cs when the process currently in cs exits in a timesharing os a process that gets into a busy wait to gain entry to a cs would use up its time slice without entering the cs which would degrade the system performance in an os using prioritybased scheduling a busy wait can result in a situation where processes wait for each other indefinitely consider the following situation a highpriority process pi is blocked on an io operation and a lowpriority process pj enters a critical section for data item ds when pis io operation completes pj is preempted and pi is scheduled if pi now tries to enter a critical section for ds using the while loop described earlier it would face a busy wait this busy wait denies the cpu to pj hence it is unable to complete its execution of the critical section and exit in turn this situation prevents pi from entering its critical section processes pi and pj now wait for each other indefinitely because a highpriority process waits for a process with a low priority this situation is called priority inversion the priority inversion problem is typically addressed through the priority inheritance protocol wherein a lowpriority process that holds a resource temporarily acquires the priority of the highestpriority process that needs the resource in our example process pj would temporarily acquire the priority of process pi which would enable it to get scheduled and exit from its critical section however use of the priority inheritance protocol is impractical in these situations because it would require the kernel to know minute details of the operation of processes to avoid busy waits a process waiting for entry to a critical section should be put into the blocked state its state should be changed to ready only when it can chapter process synchronization be allowed to enter the cs this approach can be realized through the following outline if some process is in a critical section on ds or is executing an indivisible operation using ds then make a system call to block itself critical section or indivisible operation using ds in this approach the kernel must activate the blocked process when no other process is operating in a critical section on ds or executing an indivisible operation using ds when a critical section or an indivisible operation is realized through any of the above outlines a process wishing to enter a cs has to check whether any other process is inside a cs and accordingly decide whether to loop or block this action itself involves executing a few instructions in a mutually exclusive way to avoid a race condition see section so how is that to be done actually it can be done in two ways in the first approach called the algorithmic approach a complex arrangement of checks is used in concurrent processes to avoid race conditions we shall discuss the features of this approach and its drawbacks in section the second approach uses some features in computer hardware to simplify this check we discuss this approach in the next section hardware support for process synchronization process synchronization involves executing some sequences of instructions in a mutually exclusive manner on a uniprocessor system this can be achieved by disabling interrupts while a process executes such a sequence of instructions so that it will not be preempted however this approach involves the overhead of system calls to disable interrupts and enable them again and also delays processing of interrupts which can lead to undesirable consequences for system performance or user service it is also not applicable to multiprocessor systems for these reasons operating systems implement critical sections and indivisible operations through indivisible instructions provided in computers together with shared variables called lock variables in this section we use illustrations of the looping approach to process synchronization however the techniques discussed here are equally applicable to the blocking approach to process synchronization note that indivisible instructions merely assist in implementing critical sections the properties of cs implementation summarized in table have to be ensured separately by enabling processes to enter cs in an appropriate manner see exercise indivisible instructions since the mids computer systems have provided special features in their hardware to prevent race conditions while accessing a memory location containing shared data the basic theme is that all accesses to a memory location made by one instruction should be implemented without permitting another cpu to access the same location two popular techniques part process management used for this purpose are locking the memory bus during an instruction eg in intel x processors and providing special instructions that perform some specific operations on memory locations in a racefree manner eg in ibm and m processors we will use the term indivisible instruction as a generic term for all such instructions use of a lock variable a lock variable is a twostate variable that is used to bridge the semantic gap see definition between critical sections or indivisible operations on the one hand and indivisible instructions provided in a computer system on the other to implement critical sections for a data item ds an application associates a lock variable with ds the lock variable has only two possible values open and closed when a process wishes to execute a critical section for ds it tests the value of the lock variable if the lock is open it closes the lock executes the critical section and opens the lock while exiting from the critical section to avoid race conditions in setting the value of the lock variable an indivisible instruction is used to test and close the lock lock variables assist in implementing indivisible operations in a similar manner figure illustrates how a critical section or an indivisible operation is implemented by using an indivisible instruction and a lock variable the indivisible instruction performs the actions indicated in the dashed box if the lock is closed it loops back to itself otherwise it closes the lock in the following we illustrate use of two indivisible instructions called testandset and swap instructions to implement critical sections and indivisible operations testandset ts instruction this indivisible instruction performs two actions it tests the value of a memory byte and sets the condition code field ie the flags field of the psw to indicate whether the value was zero or nonzero it also sets all bits in the byte to s no other cpu can access the memory byte until both actions are complete this instruction can be used to implement the statements enclosed in the dashed box in figure figure is a segment of an ibm assembly language program for implementing a critical section or an indivisible operation lock is a lock variable used with the convention that a nonzero value implies that the lock is closed and a zero implies that it is open the first line in the assembly language program declares lock and initializes it to the ts instruction sets the condition code according to the value of lock and then sets the value of lock to closed thus the condition code indicates if the lock was closed before the ts instruction was executed the branch instruction bc entrytest checks the condition code and loops entrytest if lock closed performed by then goto entrytest an indivisible lock closed instruction critical section or indivisible operation lock open figure implementing a critical section or indivisible operation by using a lock variable chapter process synchronization lock dc x lock is initialized to open entrytest ts lock testandset lock bc entrytest loop if lock was closed critical section or indivisible operation mvi lock x open the lockby moving s figure implementing a critical section or indivisible operation by using testandset temp ds reserve one byte for temp lock dc x lock is initialized to open mvi temp xff xff is used to close the lock entrytest swap lock temp comp temp x test old value of lock bc entrytest loop if lock was closed critical section or indivisible operation mvi lock x open the lock figure implementing a critical section or indivisible operation by using a swap instruction back to the ts instruction if the lock was closed this way a process that finds the lock closed would execute the loop in a busy wait until lock was opened the mvi instruction puts s in all bits of lock ie it opens the lock this action would enable only one of the processes looping at entrytest to proceed swap instruction the swap instruction exchanges contents of two memory locations it is an indivisible instruction no other cpu can access either of the locations during swapping figure shows how a critical section or an indivisible operation can be implemented by using the swap instruction for convenience we use the same coding conventions as used for the ts instruction the temporary location temp is initialized to a nonzero value the swap instruction swaps its contents with lock this action closes the lock the old value of lock is now available in temp it is tested to find whether the lock was already closed if so the process loops on the swap instruction until the lock is opened the process executing the critical section or indivisible operation opens the lock at the end of the operation this action enables one process to get past the bc instruction and enter the critical section or the indivisible operation many computers provide a compareandswap instruction this instruction has three operands if the first two operands are equal it copies the third operands value into the second operands location otherwise it copies the second operands value into the first operands location it is easy to rewrite the program of figure by using the instruction compareandswap firstopd lock thirdopd where the values of firstopd and thirdopd correspond to the open and closed values of the lock in effect this instruction closes the lock and puts its old value in firstopd part process management algorithmic approaches synchronization primitives and concurrent programming constructs historically implementation of process synchronization has gone through three important stages algorithmic approaches synchronization primitives and concurrent programming constructs each stage in its history solved practical difficulties that were faced in the previous stage algorithmic approaches were largely confined to implementing mutual exclusion they did not use any special features in computer architecture programming languages or the kernel to achieve mutual exclusion instead they depended on a complex arrangement of checks to ensure that processes accessed shared data in a mutually exclusive manner thus the algorithmic approaches were independent of hardware and software platforms however correctness of mutual exclusion depended on correctness of these checks and was hard to prove because of logical complexity of the checks this problem inhibited development of large applications since the algorithmic approaches worked independently of the kernel they could not employ the blocking approach to process synchronization see section so they used the looping approach and suffered from all its drawbacks a set of synchronization primitives were developed to overcome deficiencies of the algorithmic approach each primitive was a simple operation that contributed to process synchronization it was implemented by using indivisible instructions in the hardware and support from the kernel for blocking and activation of processes the primitives possessed useful properties for implementing both mutual exclusion and indivisible operations and it was hoped that these properties could be used to construct proofs of correctness of a concurrent program however experience showed that these primitives could be used haphazardly a property that caused its own difficulties with correctness of programs most modern operating systems provide the wait and signal primitives of semaphores however they are employed only by system programmers because of the problems mentioned above the next important step in the history of process synchronization was the development of concurrent programming constructs which provided data abstraction and encapsulation features specifically suited to the construction of concurrent programs they had welldefined semantics that were enforced by the language compiler effectively concurrent programming constructs incorporated functions that were analogous to those provided by the synchronization primitives but they also included features to ensure that these functions could not be used in a haphazard or indiscriminate manner these properties helped in ensuring correctness of programs which made construction of large applications practical most modern programming languages provide a concurrent programming construct called a monitor we discuss algorithmic approaches to process synchronization in section and semaphores and synchronization primitives for mutual exclusion in section section describes monitors structure of concurrent systems a concurrent system consists of three key components shared data operations on shared data interacting processes shared data include two kinds of data application data used and manipulated by processes and synchronization data ie data used for synchronization between processes an operation is a convenient unit of code typically a function or a procedure in a programming language which accesses and manipulates shared data a synchronization operation is an operation on synchronization data a snapshot of a concurrent system is a view of the system at a specific time instant it shows relationships between shared data operations and processes at that instant of time we use the pictorial conventions shown in figure to depict a snapshot a process is shown as a circle a circle with a cross in it indicates a blocked process a data item or a set of data items is represented by a rectangular box the values of data if known are shown inside the box operations on data are shown as connectors or sockets joined to the data an oval shape enclosing a data item indicates that the data item is shared a dashed line connects a process and an operation on data if the process is currently engaged in executing the operation recall that a dashed rectangular box encloses code executed as a critical section we extend this convention to operations on data hence mutually exclusive operations on data are enclosed in a dashed rectangular box a queue of blocked processes is associated with the dashed box to show the processes waiting to perform one of the operations the execution of a concurrent system is represented by a series of snapshots pi process pi d shared data d pi blocked process pi d op queue of blocked pi processes process pi performs op op on shared data d d data d op d d op op op are mutually operations exclusive operations op on data d op figure pictorial conventions for snapshots of concurrent systems part process management example snapshots of a concurrent system consider the system of figure where process pi performs action ai only after process pj performs action aj we assume that operations ai and aj operate on shared data items x and y respectively let the system be implemented using the operations checkaj and postaj of figure this system comprises the following components shared data boolean variables operationajperformed and piblocked both initialized to false and data items x and y operations on application data operations ai and aj synchronization operations operations checkaj and postaj processes processes pi and pj figure shows three snapshots of this system t and f indicate values true and false respectively operations checkaj and postaj both use the boolean variables operationajperformed and piblocked these operations are indivisible operations so they are mutually exclusive accordingly they are enclosed in a dashed box figure a shows the situation when process pj is engaged in performing operation aj and process pi wishes to perform operation ai so it invokes operation checkaj operation checkaj finds that operationajperformed is false so it sets piblocked to true blocks process pi and exits when pj finishes performing operation aj it invokes operation postaj see figure b this operation finds that piblocked is true so it sets piblocked to false activates process pi and exits process pi now performs operation ai see figure c ai ai ai pi x x x aj pj aj aj y y y checkaj pi checkaj pi checkaj piblocked f t f operation f f f ajperformed postaj postaj pj postaj pj a b c figure snapshots of the system of example classic process synchronization problems a solution to a process synchronization problem should meet three important criteria correctness data access synchronization and control synchronization should be performed in accordance with synchronization requirements of the problem maximum concurrency a process should be able to operate freely except when it needs to wait for other processes to perform synchronization actions no busy waits to avoid performance degradation synchronization should be performed through blocking rather than through busy waits see section as discussed in sections and critical sections and signaling are the key elements of process synchronization so a solution to a process synchronization problem should incorporate a suitable combination of these elements in this section we analyze some classic problems in process synchronization which are representative of synchronization problems in various application domains and discuss issues and common mistakes in designing their solutions in later sections we implement their solutions using various synchronization features provided in programming languages producers consumers with bounded buffers a producersconsumers system with bounded buffers consists of an unspecified number of producer and consumer processes and a finite pool of buffers see figure each buffer is capable of holding one item of information it is said to become full when a producer writes a new item into it and become empty when a consumer copies out an item contained in it it is empty when the producers consumers system starts its operation a producer process produces one item of information at a time and writes it into an empty buffer a consumer process consumes information one item at a time from a full buffer a producersconsumers system with bounded buffers is a useful abstraction for many practical synchronization problems a print service is a good example producers consumers buffer pool figure a producersconsumers system with bounded buffers part process management in the os domain a fixedsize queue of print requests is the bounded buffer a process that adds a print request to the queue is a producer process and a print daemon is a consumer process the data logging application of example would also be an instance of the producersconsumers problem if the housekeeping process is omitted the copysample process is the producer since it writes a data sample into a buffer the recordsample process is a consumer since it removes a data sample from the buffer and writes it into the disk file a solution to the producersconsumers problem must satisfy the following conditions a producer must not overwrite a full buffer a consumer must not consume an empty buffer producers and consumers must access buffers in a mutually exclusive manner the following condition is also sometimes imposed information must be consumed in the same order in which it is put into the buffers ie in fifo order figure shows an outline for the producersconsumers problem producer and consumer processes access a buffer inside a critical section a producer enters its critical section and checks whether an empty buffer exists if so it produces into that buffer otherwise it merely exits from its critical section this sequence is repeated until it finds an empty buffer the boolean variable produced is used to break out of the while loop after the producer produces into an empty buffer analogously a consumer makes repeated checks until it finds a full buffer to consume from this outline suffers from two problems poor concurrency and busy waits the pool contains many buffers and so it should be possible for producers and consumers to concurrently access empty and full buffers respectively however begin parbegin var produced boolean var consumed boolean repeat repeat produced false consumed false while produced false while consumed false if an empty buffer exists if a full buffer exists then then produce in a buffer consume a buffer produced true consumed true remainder of the cycle remainder of the cycle forever forever parend end producer consumer figure an outline for producersconsumers using critical sections chapter process synchronization both produce and consume actions take place in critical sections for the entire buffer pool and so only one process whether producer or consumer can access a buffer at any time busy waits exist in both producers and consumers a producer repeatedly checks for an empty buffer and a consumer repeatedly checks for a full buffer to avoid busy waits a producer process should be blocked if an empty buffer is not available when a consumer consumes from a buffer it should activate a producer that is waiting for an empty buffer similarly a consumer should be blocked if a full buffer is not available a producer should activate such a consumer after producing in a buffer when we reanalyze the producersconsumers problem in this light we notice that though it involves mutual exclusion between a producer and a consumer that use the same buffer it is really a signaling problem after producing an item of information in a buffer a producer should signal a consumer that wishes to consume the item from that buffer similarly after consuming an item in a buffer a consumer should signal a producer that wishes to produce an item of information in that buffer these requirements can be met by using the signaling arrangement discussed in section an improved outline using this approach is shown in figure for a simple producersconsumers system that consists of a single producer a single consumer and a single buffer the operation checkbempty performed by the producer blocks it if the buffer is full while the operation postb full sets buffer full to true and activates the consumer if the consumer is blocked for the buffer to become full analogous operations checkb full and postbempty are defined for use by the consumer process the boolean flags producerblocked and consumerblocked are used by these operations to note whether the producer or consumer process var buffer buffer full boolean producerblocked consumerblocked boolean begin buffer full false producerblocked false consumerblocked false parbegin repeat repeat checkbempty checkb full produce in the buffer consume from the buffer postb full postbempty remainder of the cycle remainder of the cycle forever forever parend end producer consumer figure an improved outline for a single buffer producersconsumers system using signaling part process management procedure checkbempty procedure checkbfull begin begin if bufferfull true if bufferfull false then then producerblocked true consumerblocked true block producer block consumer end end procedure postbfull procedure postbempty begin begin bufferfull true bufferfull false if consumerblocked true if producerblocked true then then consumerblocked false producerblocked false activate consumer activate producer end end operations of producer operations of consumer figure indivisible operations for the producersconsumers problem is blocked at any moment figure shows details of the indivisible operations this outline will need to be extended to handle multiple buffers or multiple producerconsumer processes we discuss this aspect in section readers and writers a readerswriters system consists of shared data an unspecified number of reader processes that only read the data and an unspecified number of writer processes that modify or update the data we use the terms reading and writing for accesses to the shared data made by reader and writer processes respectively a solution to the readerswriters problem must satisfy the following conditions many readers can perform reading concurrently reading is prohibited while a writer is writing only one writer can perform writing at any time conditions do not specify which process should be preferred if a reader and a writer process wish to access the shared data at the same time the following additional condition is imposed if it is important to give a higher priority to readers in order to meet some business goals a reader has a nonpreemptive priority over writers ie it gets access to the shared data ahead of a waiting writer but it does not preempt an active writer this system is called a readers preferred readerswriters system a writers preferred readerswriters system is analogously defined chapter process synchronization bank account print credit statement stat debit analysis readers writers figure readers and writers in a banking system figure illustrates an example of a readerswriters system the readers and writers share a bank account the reader processes print statement and stat analysis merely read the data from the bank account hence they can execute concurrently credit and debit modify the balance in the account clearly only one of them should be active at any moment and none of the readers should be concurrent with it in an airline reservation system processes that merely query the availability of seats on a flight are reader processes while processes that make reservations are writer processes since they modify parts of the reservation database we determine the synchronization requirements of a readerswriters system as follows conditions permit either one writer to perform writing or many readers to perform concurrent reading hence writing should be performed in a critical section for the shared data when a writer finishes writing it should either enable another writer to enter its critical section or activate all waiting readers using a signaling arrangement and a count of waiting readers if readers are reading a waiting writer should be enabled to perform writing when the last reader finishes reading this action would require a count of concurrent readers to be maintained figure is an outline for a readerswriters system writing is performed in a critical section a critical section is not used in a reader because that would prevent concurrency between readers a signaling arrangement is used to handle blocking and activation of readers and writers for simplicity details of maintaining and using counts of waiting readers and readers reading concurrently are not shown in the outline we shall discuss these in section the outline of figure does not provide bounded waits for readers and writers however it provides maximum concurrency this outline does not prefer either readers or writers dining philosophers five philosophers sit around a table pondering philosophical issues a plate of spaghetti is kept in front of each philosopher and a fork is placed between each pair of philosophers see figure to eat a philosopher must pick up the two forks placed between him and the neighbors on either side one at a time the problem is to design processes to represent the philosophers such that each philosopher can eat when hungry and none dies of hunger part process management parbegin repeat repeat if a writer is writing if readers are reading or a then writer is writing wait then read wait if no other readers reading write then if readers or writers waiting if writers waiting then then activate either one waiting activate one waiting writer writer or all waiting readers forever forever parend end readers writers figure an outline for a readerswriters system p p p p p figure dining philosophers the correctness condition in the dining philosophers system is that a hungry philosopher should not face indefinite waits when he decides to eat the challenge is to design a solution that does not suffer from either deadlocks where processes become blocked waiting for each other see section or livelocks where processes are not blocked but defer to each other indefinitely consider the outline of a philosopher process pi shown in figure where details of process synchronization have been omitted a philosopher picks up the forks one at a time say first the left fork and then the right fork this solution is prone to deadlock because if all philosophers simultaneously lift their left forks none will be able to lift the right fork it also contains race conditions because neighbors might fight over a shared fork we can avoid deadlocks by modifying the philosopher process so that if the right fork is not available the philosopher would defer to his left neighbor by putting down the left fork and repeating the attempt to take the forks sometime later however this approach suffers from livelocks because the same situation may recur chapter process synchronization repeat if left fork is not available then block pi lift left fork if right fork is not available then block pi lift right fork eat put down both forks if left neighbor is waiting for his right fork then activate left neighbor if right neighbor is waiting for his left fork then activate right neighbor think forever figure outline of a philosopher process pi var successful boolean repeat successful false while not successful if both forks are available then lift the forks one at a time successful true if successful false then eat block pi put down both forks if left neighbor is waiting for his right fork then activate left neighbor if right neighbor is waiting for his left fork then activate right neighbor think forever figure an improved outline of a philosopher process an improved outline for the dining philosophers problem is given in figure a philosopher checks availability of forks in a cs and also picks up the forks in the cs hence race conditions can not arise this arrangement ensures that at least some philosophers can eat at any time and deadlocks can not arise a philosopher who can not get both forks at the same time blocks algorithmic approach to implementing critical sections the algorithmic approach to implementing critical sections did not employ either the process blocking and activation services of the kernel to delay a process or indivisible instructions in a computer to avoid race conditions consequently process synchronization implemented through this approach was independent of both the os and the computer however these features required the approach to use a busy wait to delay a process at a synchronization point see section and use a complex arrangement of logical conditions to ensure absence of race conditions which complicated proofs of correctness the algorithmic approach was not widely used in practice due to these weaknesses this section describes the algorithmic approach to implementing critical sections which as we saw in section can be used for both data access synchronization and control synchronization this study provides an insight into how to ensure mutual exclusion while avoiding both deadlocks and livelocks we begin by discussing critical section implementation schemes for use by two processes later we see how to extend some of these schemes for use by more than two processes twoprocess algorithms algorithm first attempt var turn begin turn parbegin repeat repeat while turn while turn do nothing do nothing critical section critical section turn turn remainder of the cycle remainder of the cycle forever forever parend end process p process p chapter process synchronization the variable turn is a shared variable the notation in its declaration indicates that it takes values in the range ie its value is either or it is initialized to before processes p and p are created each process contains a critical section for some shared data ds the shared variable turn indicates which process can enter its critical section next suppose process p wishes to enter its critical section if turn p can enter right away after exiting its critical section p sets turn to so that p can enter its critical section if p finds turn when it wishes to enter its critical section it waits in the while turn do nothing loop until p exits from its critical section and executes the assignment turn thus the correctness condition is satisfied algorithm violates the progress condition of critical section implementation described in table because of the way it uses shared variable turn let process p be in its critical section and process p be in the remainder of the cycle when p exits from its critical section it would set turn to if it finishes the remainder of its cycle and wishes to enter its critical section once again it will encounter a busy wait until after p uses its critical section and sets turn to thus p is not granted entry to its critical section even though no other process is interested in using its critical section algorithm is an attempt to eliminate this problem algorithm second attempt var c c begin c c parbegin repeat repeat while c while c do nothing do nothing c c critical section critical section c c remainder of the cycle remainder of the cycle forever forever parend end process p process p the algorithm uses two shared variables c and c whose values are restricted to either a or a these variables can be looked upon as status flags for processes p and p respectively p sets c to while entering its critical section and sets it back to after exiting from its critical section thus c indicates that p is in its critical section and c indicates that it is not in its critical section similarly the value of c indicates whether p is in its critical section before entering its critical section each process checks whether the other process is in its critical section if not it enters its own critical section right away otherwise it loops until the other process exits its critical section and then enters its own part process management critical section the progress violation of algorithm is eliminated because processes are not forced to take turns using their critical sections algorithm violates the mutual exclusion condition when both processes try to enter their critical sections at the same time both c and c will be since none of the processes is in its critical section and so both processes will enter their critical sections to avoid this problem the statements while c do nothing and c in process p could be interchanged and the statements while c do nothing and c could be interchanged in process p this way c will be set to before p checks the value of c and hence both processes will not be able to be in their critical sections at the same time however if both processes try to enter their critical sections at the same time both c and c will be and so both processes will wait for each other indefinitely this is a deadlock situation see section both the correctness violation and the deadlock possibility can be eliminated if a process defers to the other process when it finds that the other process also wishes to enter its critical section this can be achieved as follows if p finds that p is also trying to enter its critical section it can set c to this will permit p to enter its critical section p can wait for some time and make another attempt to enter its critical section after setting c to similarly p can set c to if it finds that p is also trying to enter its critical section however this approach may lead to a situation in which both processes defer to each other indefinitely this is a livelock situation we discussed earlier in the context of dining philosophers see section dekkers algorithm dekkers algorithm combines the useful features of algorithms and to avoid a livelock situation if both processes try to enter their critical sections at the same time turn indicates which of the processes should be allowed to enter it has no effect at other times algorithm dekkers algorithm var turn c c begin c c turn parbegin repeat repeat c c while c do while c do if turn then if turn then begin begin c c while turn while turn do nothing do nothing c c end end chapter process synchronization critical section critical section turn turn c c remainder of the cycle remainder of the cycle forever forever parend end process p process p variables c and c are used as status flags of the processes as in algorithm the statement while c do in p checks if it is safe for p to enter its critical section to avoid the correctness problem of algorithm the statement c in p precedes the while statement if c when p wishes to enter a critical section p skips the while loop and enters its critical section right away if both processes try to enter their critical sections at the same time the value of turn will force one of them to defer to the other for example if p finds c it defers to p only if turn otherwise it simply waits for c to become before entering its critical section process p which is also trying to enter its critical section at the same time is forced to defer to p only if turn in this manner the algorithm satisfies mutual exclusion and also avoids deadlock and livelock conditions the actual value of turn at any time is immaterial to correctness of the algorithm petersons algorithm petersons algorithm is simpler than dekkers algorithm it uses a boolean array flag that contains one flag for each process these flags are equivalent to the status variables c c of dekkers algorithm a process sets its flag to true when it wishes to enter a critical section and sets it back to false when it exits from the critical section processes are assumed to have the ids p and p a process id is used as a subscript to access the status flag of a process in the array flag the variable turn is used for avoiding livelocks however it is used differently than in dekkers algorithm algorithm petersons algorithm var flag array of boolean turn begin flag false flag false parbegin repeat repeat flag true flag true turn turn while flag and turn while flag and turn do nothing do nothing critical section critical section flag false flag false remainder of the cycle remainder of the cycle part process management forever forever parend end process p process p a process wishing to enter a critical section begins by deferring to another process by setting turn to point to the other process however it goes ahead and enters its critical section if it finds that the other process is not interested in using its own critical section if both processes try to enter their critical sections at the same time the value of turn decides which process may enter as an example consider process p it sets flag to true and turn to when it wishes to enter its critical section if process p is not interested in using its critical section flag will be false and so p will come out of the while loop to enter its critical section right away if p is also interested in entering its critical section flag will be true in that case the value of turn decides which process may enter its critical section it is interesting to consider operation of petersons algorithm for different relative speeds of p and p consider the situation when both p and p wish to use their critical sections and p is slightly ahead of p if both processes execute at the same speed p will enter its critical section ahead of p because p will have changed turn to by the time p reaches the while statement p now waits in the while loop until p exits from its critical section if however p is slower than p it will set turn to sometime after p sets it to hence p will wait in the while loop and p will enter its critical section nprocess algorithms in an algorithmic implementation of a critical section the algorithm has to know the number of processes that use a critical section for the same data item this awareness is reflected in many features of its code the size of the array of status flags the checks to determine whether any other process wishes to enter a critical section and the arrangement for one process to defer to another each of these features has to change if the number of processes to be handled by the critical section implementation changes for example in a twoprocess critical section implementation any process needs to check the status of only one other process and possibly defer to it to ensure correctness and absence of deadlocks and livelocks in an nprocess critical section implementation a process must check the status of n other processes and do it in a manner that prevents race conditions it makes an nprocess algorithm more complex we see this in the context of the algorithm by eisenberg and mcguire which extends the twoprocess solution of dekkers algorithm to n processes algorithm an nprocess algorithm eisenberg and mcguire const n var flag array n of idle wantin incs chapter process synchronization turn n begin for j to n do flagj idle parbegin process pi repeat repeat flagi wantin j turn while j i do if flagj idle then j turn loop here else j j mod n flagi incs j while j n and j i or flagj incs do j j until j n and turn i or flagturn idle turn i critical section j turn mod n while flagj idle do j j mod n turn j flagi idle remainder of the cycle forever process pk parend end the variable turn indicates which process may enter its critical section next its initial value is immaterial to correctness of the algorithm each process has a way status flag that takes the values idle wantin and incs it is initialized to the value idle a process sets its flag to wantin whenever it wishes to enter a critical section it now has to decide whether it may change the flag to incs to make this decision it checks the flags of other processes in an order that we call the modulo n order the modulo n order is pturn pturn pn p p pturn in the first while loop the process checks whether any process ahead of it in the modulo n order wishes to use its own critical section if not it turns its flag to incs since processes make this check concurrently more than one process may simultaneously reach the same conclusion hence another check is made to ensure correctness the second while loop checks whether any other process has turned its flag to incs if so the process changes its flag back to wantin and repeats all the checks all other processes that had changed their flags to incs also change their flags back to wantin and repeat the checks these processes will not tie for part process management entry to a critical section again because they have all turned their flags to wantin and so only one of them will be able to get past the first while loop this feature avoids the livelock condition the process earlier in the modulo n order from pturn will get in and enter its critical section ahead of other processes it changes its flag to idle when it leaves its critical section thus the flag has the value idle whenever a process is in the remainder of its cycle this solution contains a certain form of unfairness since processes do not enter their critical sections in the same order in which they requested entry to a critical section this unfairness is eliminated in the bakery algorithm by lamport bakery algorithm when a process wishes to enter a critical section it chooses a number that is larger than any number chosen by any process earlier choosing is an array of boolean flags choosingi is used to indicate whether process pi is currently engaged in choosing a number numberi contains the number chosen by process pi numberi if pi has not chosen a number since the last time it entered the critical section the basic idea of the algorithm is that processes should enter their critical sections in the order of increasing numbers chosen by them we discuss the operation of the algorithm in the following algorithm bakery algorithm lamport const n var choosing array n of boolean number array n of integer begin for j to n do choosingj false numberj parbegin process pi repeat choosingi true numberi max number numbern choosingi false for j to n do begin while choosingj do nothing while numberj and numberj j numberii do nothing end critical section numberi remainder of the cycle forever process pj parend end semaphores as mentioned in section synchronization primitives were developed to overcome the limitations of algorithmic implementations the primitives are simple operations that can be used to implement both mutual exclusion and control synchronization a semaphore is a special kind of synchronization data that can be used only through specific synchronization primitives definition semaphore a shared integer variable with nonnegative values that can be subjected only to the following operations initialization specified as part of its declaration the indivisible operations wait and signal the wait and signal operations on a semaphore were originally called the p and v operations respectively by dijkstra their semantics are shown in figure when a process performs a wait operation on a semaphore the operation checks whether the value of the semaphore is if so it decrements the value of the semaphore and lets the process continue its execution otherwise it blocks the process on the semaphore a signal operation on a semaphore activates a process blocked on the semaphore if any or increments the value of the semaphore by due to these semantics semaphores are also called counting semaphores indivisibility of the wait and signal operations is ensured by the programming language or the operating system that implements it it ensures that race conditions can not arise over a semaphore see section processes use wait and signal operations to synchronize their execution with respect to one another the initial value of a semaphore determines how many processes can get past the wait operation a process that does not get past a wait operation is blocked on the semaphore this feature avoids busy waits section describes uses of semaphores sections and discuss implementation of the producersconsumers and readerswriters problems using semaphores part process management procedure wait s begin if s then s s else block the process on s end procedure signal s begin if some processes are blocked on s then activate one blocked process else s s end figure semantics of the wait and signal operations on a semaphore table uses of semaphores in implementing concurrent systems use description mutual mutual exclusion can be implemented by using a semaphore exclusion that is initialized to a process performs a wait operation on the semaphore before entering a cs and a signal operation on exiting from it a special kind of semaphore called a binary semaphore further simplifies cs implementation bounded bounded concurrency implies that a function may be executed concurrency or a resource may be accessed by n processes concurrently n c where c is a constant a semaphore initialized to c can be used to implement bounded concurrency signaling signaling is used when a process pi wishes to perform an operation ai only after process pj has performed an operation aj it is implemented by using a semaphore initialized to pi performs a wait on the semaphore before performing operation ai pj performs a signal on the semaphore after it performs operation aj uses of semaphores in concurrent systems table summarizes three uses of semaphores in implementing concurrent systems mutual exclusion is useful in implementing critical sections bounded concurrency is important when a resource can be shared by up to c processes where c is a constant signaling is useful in control synchronization we discuss details of these uses in this section mutual exclusion figure shows implementation of a critical section in processes pi and pj by using a semaphore named semcs semcs is initialized to each process performs a wait operation on semcs before entering its critical section and a signal operation after exiting from its critical section the first process to perform waitsemcs finds that semcs is hence it decrements semcs chapter process synchronization var semcs semaphore parbegin repeat repeat wait semcs wait semcs critical section critical section signal semcs signal semcs remainder of the cycle remainder of the cycle forever forever parend end process pi process pj figure cs implementation with semaphores by and goes on to enter its critical section when the second process performs waitsemcs it is blocked on semcs because its value is it is activated when the first process performs signalsemcs after exiting from its own critical section the second process then enters its critical section if no process is blocked on semcs when a signalsemcs operation is performed the value of semcs becomes this value of semcs permits a process that is performing a wait operation at some later time to immediately enter its critical section more processes using similar code can be added to the system without causing correctness problems the next example illustrates operation of this system using snapshots critical sections through semaphores example figure shows snapshots taken during operation of the system shown in figure the wait and signal operations on semcs are enclosed in a dashed rectangular box because they are mutually exclusive refer to the pictorial conventions of figure let process pi perform waitsemcs figure a illustrates the situation at the start of pis wait operation figure b shows the situation after pi completes the wait operation and pj executes a wait operation pis waitsemcs operation has reduced the value of semcs to so pj becomes blocked on the wait operation figure c shows the situation after process pi performs a signal operation the value of semcs remains but process pj has been activated process pj performs a signal operation on exiting from its critical section since no process is currently blocked on semcs pjs signal operation simply results in increasing the value of semcs by see figure d it is interesting to check which properties of critical section implementations mentioned in table are satisfied by the implementation of figure mutual exclusion follows from the fact that semcs is initialized to the implementation possesses the progress property because a process performing the wait operation gets to enter its critical section if no other process is in its critical part process management wait semcs wait semcs pi pi pj signal pj signal a b wait semcs wait semcs pi pi pj signal pj signal c d figure snapshots of the concurrent system of figure section however the bounded wait property does not hold because the order in which blocked processes are activated by signal operations is not defined in the semantics of semaphores hence a blocked process may starve if other processes perform wait and signal operations repeatedly correctness problems can arise because the wait and signal operations are primitives and so a program can use them in a haphazard manner for example process pi of figure could have been erroneously written as repeat signal semcs critical section signal semcs remainder of the cycle forever where a signalsemcs has been used instead of a waitsemcs at pis entry to its critical section now the critical section would not be implemented correctly because many processes would be able to enter their critical sections at the same time as another example consider what would happen if the code of process pi erroneously uses a waitsemcs operation in place of the signalsemcs operation following its critical section when pi executes its critical section it will be blocked on the wait operation after exiting from its critical section because the value of semcs will be other processes wishing to enter the critical section will be blocked on the wait operation preceding their critical sections since no process performs a signal operation on semcs all these processes will remain blocked indefinitely which is a deadlock situation binary semaphores a binary semaphore is a special kind of semaphore used for implementing mutual exclusion hence it is often called a mutex a binary semaphore is initialized to and takes only the values and during execution of a program the wait and signal operations on a binary semaphore are slightly different from those shown in figure the statement s s in the wait operation is replaced by the statement s and the statement s s in the signal operation is replaced by the statement s chapter process synchronization var sync semaphore parbegin wait sync performaction aj performaction ai signal sync parend end process pi process pj figure signaling using semaphores bounded concurrency we use the term bounded concurrency for the situation in which up to c processes can concurrently perform an operation opi where c is a constant bounded concurrency is implemented by initializing a semaphore semc to c every process wishing to perform opi performs a waitsemc before performing opi and a signalsemc after performing it from the semantics of the wait and signal operations it is clear that up to c processes can concurrently perform opi signaling between processes consider the synchronization requirements of processes pi and pj shown in figure process pi should perform an operation ai only after process pj performs an operation aj a semaphore can be used to achieve this synchronization as shown in figure here process pi performs a waitsync before executing operation ai and pj performs a signalsync after executing operation aj the semaphore sync is initialized to and so pi will be blocked on waitsync if pj has not already performed a signalsync it will proceed to perform operation ai only after process pj performs a signal unlike the solution of figure race conditions can not arise because the wait and signal operations are indivisible the signaling arrangement can be used repetitively as the wait operation makes the value of sync once again producers consumers using semaphores as discussed in section the producersconsumers problem is a signaling problem after producing an item of information in a buffer a producer signals to a consumer that is waiting to consume from the same buffer analogously a consumer signals to a waiting producer hence we should implement producers consumers using the signaling arrangement shown in figure for simplicity we first discuss the solution for the single buffer case shown in figure the buffer pool is represented by an array of buffers with a single element in it two semaphores full and empty are declared they are used to indicate the number of full and empty buffers respectively a producer performs a waitempty before starting the produce operation and a consumer performs a waitfull before a consume operation part process management type item var full semaphore initializations empty semaphore buffer array of item begin parbegin repeat repeat wait empty wait full buffer x buffer ie produce ie consume signal full signal empty remainder of the cycle remainder of the cycle forever forever parend end producer consumer figure producersconsumers with a single buffer initially the semaphore full has the value hence consumers will be blocked on waitfull empty has the value and so one producer will get past the waitempty operation after completing the produce operation it performs signalfull this enables one consumer to enter either immediately or later when the consumer finishes a consume operation it performs a signalempty that enables a producer to perform a produce operation this solution avoids busy waits since semaphores are used to check for empty or full buffers and so a process will be blocked if it can not find an empty or full buffer as required the total concurrency in this system is sometimes a producer executes and sometimes a consumer executes example describes the operation of this solution example producersconsumers with a single buffer through semaphores the snapshot of figure a shows the initial situation in the producers consumers system of figure figure b shows the situation when the producer and consumer processes attempt to produce and consume respectively the producer process has got past its wait operation on empty since empty was initialized to the value of semaphore empty becomes and the producer starts producing in the buffer the consumer process is blocked on the wait full operation because full is when the producer performs a signalfull after the produce operation the consumer process is activated and starts consuming from the buffer figure c shows this situation figure shows how semaphores can be used to implement a solution of the nbuffer producersconsumers problem n containing one producer and one chapter process synchronization buffer producer buffer producer buffer producer produce produce produce consume consumer consume consume consumer full wait full wait wait consumer signal signal signal empty wait empty wait empty wait signal signal signal a b c figure snapshots of single buffer producersconsumers using semaphores const n type item var buffer array n of item full semaphore initializations empty semaphore n prod ptr cons ptr integer begin prod ptr cons ptr parbegin repeat repeat wait empty wait full buffer prod ptr x buffer cons ptr ie produce ie consume prod ptr prod ptr mod n cons ptr cons ptr mod n signal full signal empty remainder of the cycle remainder of the cycle forever forever parend end producer consumer figure bounded buffers using semaphores consumer process this solution is a simple extension of the singlebuffer solution shown in figure the values of the semaphores empty and full indicate the number of empty and full buffers respectively hence they are initialized to n and respectively prodptr and consptr are used as subscripts of the array buffer the part process management producer produces in bufferprodptr and increments prodptr the consumer consumes from bufferconsptr and increments consptr in the same manner this feature ensures that buffers are consumed in fifo order a producer and a consumer can operate concurrently so long as some full and some empty buffers exist in the system it is easy to verify that this solution implements the correctness conditions of the bounded buffer problem described in section however if many producer and consumer processes exist in the system we need to provide mutual exclusion among producers to avoid race conditions on prodptr analogously mutual exclusion should be provided among consumers to avoid race conditions on consptr readers writers using semaphores a key feature of the readerswriters problem is that readers and writers must wait while a writer is writing and when the writer exits either all waiting readers should be activated or one waiting writer should be activated see the outline of figure to implement this feature we use four counters as follows runread count of readers currently reading totread count of readers waiting to read or currently reading runwrite count of writers currently writing totwrite count of writers waiting to write or currently writing with these counters the outline of figure is refined as shown in figure we do not show details of how the counters are updated a reader is allowed to begin reading when runwrite and a writer is allowed to begin writing when runread and runwrite the value of totread is used to activate all waiting readers when a writer finishes writing this solution does not use an explicit critical section for writers instead writers are blocked until they can be allowed to start writing parbegin repeat repeat if runwrite if runread or then runwrite wait then wait read write if runread and if totread or totwrite totwrite then then activate either one waiting writer activate one waiting writer or all waiting readers forever forever parend readers writers figure refined solution outline for readerswriters chapter process synchronization blocking of readers and writers resembles blocking of producers and consumers in the producersconsumers problem hence it is best handled by using semaphores for signaling we introduce two semaphores named reading and writing a reader process would perform waitreading before starting to read this operation should block the reader process if conditions permitting it to read are not currently satisfied otherwise the reader should be able to get past it and start reading similarly a writer process would perform a waitwriting before writing and it would get blocked if appropriate conditions are not satisfied the conditions on which readers and writers are blocked may change when any of the counter values change ie when a reader finishes reading or a writer finishes writing hence the reader and writer processes should themselves perform appropriate signal operations after completing a read or a write operation this solution is implemented as follows see figure to avoid race conditions all counter values are examined and manipulated inside critical sections implemented by using a binary semaphore named semcs when a reader wishes to start reading it enters a critical section for semcs to check whether runwrite if so it increments runread exits the critical section and starts reading if not it must perform waitreading however performing a waitreading operation inside the critical section for semcs may cause a deadlock so it performs a waitreading after exiting the critical section if conditions permitting the start of a read operation were satisfied when it examined the counter values inside its critical section it would have itself performed a signalreading inside the critical section such a reader will get past the waitreading operation a writer will similarly perform a signalwriting inside its critical section for semcs under the correct set of conditions and waitwriting after exiting from the critical section readers and writers that get blocked on their respective wait operations are activated as follows when a reader finishes reading it performs a signal operation to activate a writer if no readers are active and a writer is waiting when a writer finishes writing it performs signal operations to activate all waiting readers if any otherwise it performs a signal operation to wake a waiting writer if any hence the resulting system is a readerspreferred readerswriters system the solution appears to have two redundant features see exercise first it uses two semaphores reading and writing even though only one resource the shared data is to be controlled second every reader performs a waitreading operation even though the operation is clearly redundant when some other readers are already engaged in reading however both features are needed to implement a writerspreferred readerswriters system see exercise implementation of semaphores figure shows a scheme for implementing semaphores a semaphore type is defined it has fields for the value of a semaphore a list that is used to store part process management var totread runread totwrite runwrite integer reading writing semaphore semcs semaphore begin totread runread totwrite runwrite parbegin repeat repeat wait semcs wait semcs totread totread totwrite totwrite if runwrite then if runread and runwrite then runread runread runwrite signal reading signal writing signal semcs signal semcs wait reading wait writing read write wait semcs wait semcs runread runread runwrite runwrite totread totread totwrite totwrite if runread and while runread totread do totwrite runwrite begin then runread runread runwrite signal reading signal writing end signal semcs if runread and forever totwrite runwrite then runwrite signal writing signal semcs forever parend end readers writers figure a readerspreferred readerswriters system using semaphores ids of processes blocked on the semaphore and a lock variable that is used to ensure indivisibility of the wait and signal operations on the semaphore the wait and signal operations on semaphores are implemented as procedures that take a variable of the semaphore type as a parameter a concurrent program declares semaphores as variables of the semaphore type and its processes invoke the wait and signal procedures to operate on them to avoid race conditions while accessing the value of the semaphore procedures wait and signal first invoke the function closelock to set the lock variable semlock closelock uses an indivisible instruction and a busy wait however the busy waits are short since the wait and signal operations are themselves short the procedures invoke the function openlock to reset the lock after completing chapter process synchronization type declaration for semaphore type semaphore record value integer value of the semaphore list list of blocked processes lock boolean lock variable for operations on this semaphore end procedures for implementing wait and signal operations procedure wait sem begin closelock semlock if semvalue then semvalue semvalue openlock semlock else add id of the process to list of processes blocked on sem blockme semlock end procedure signal sem begin closelock semlock if some processes are blocked on sem then procid id of a process blocked on sem activate procid else semvalue semvalue openlock semlock end figure a scheme for implementing wait and signal operations on a semaphore their execution recall from section that a busy wait may lead to priority inversion in an os using prioritybased scheduling we assume that a priority inheritance protocol is used to avoid this problem in a timesharing system a busy wait can cause delays in synchronization but does not cause more serious problems the wait procedure checks whether the value of sem is if so it decrements the value and returns if the value is the wait procedure adds the id of the process to the list of processes blocked on sem and makes a block me system call with the lock variable as a parameter this call blocks the process that invoked the wait procedure and also opens the lock passed to it as a parameter note that the wait procedure could not have performed these actions itself race conditions would arise if it opened the lock before making a blockme call and a deadlock would arise if it made made a blockme call before opening the lock monitors recall from section that a concurrent programming construct provides data abstraction and encapsulation features specifically suited to the construction of concurrent programs a monitor type resembles a class in a language like c or chapter process synchronization java it contains declarations of shared data it may also contain declarations of special synchronization data called condition variables on which only the builtin operations wait and signal can be performed these operations provide convenient means of setting up signaling arrangements for process synchronization procedures of the monitor type encode operations that manipulate shared data and perform process synchronization through condition variables thus the monitor type provides two of the three components that make up a concurrent system see section a concurrent system is set up as follows a concurrent program has a monitor type the program creates an object of the monitor type during its execution we refer to the object as a monitor variable or simply as a monitor the monitor contains a copy of the shared and synchronization data declared in the monitor type as its local data the procedures defined in the monitor type become operations of the monitor they operate on its local data the concurrent program creates processes through system calls these processes invoke operations of the monitor to perform data sharing and control synchronization they become blocked or activated when the monitor operations perform wait or signal operations the data abstraction and encapsulation features of the monitor assist in synchronization as follows only the operations of a monitor can access its shared and synchronization data to avoid race conditions the compiler of the programming language implements mutual exclusion over operations of a monitor by ensuring that at most one process can be executing a monitor operation at any time invocations of the operations are serviced in a fifo manner to satisfy the bounded wait property condition variables a condition is some situation of interest in a monitor a condition variable which is simply a variable with the attribute condition is associated with a condition in the monitor only the builtin operations wait and signal can be performed on a condition variable the monitor associates a queue of processes with each condition variable if a monitor operation invoked by a process performs a wait operation on a condition variable the monitor blocks the process enters its id in the process queue associated with the condition variable and schedules one of the processes if any waiting to begin or resume execution of a monitor operation if a monitor operation performs the signal operation on a condition variable the monitor activates the first process in the process queue associated with the condition variable when scheduled this process would resume execution of the monitor operation in which it was blocked the signal operation has no effect if the process queue associated with a condition variable is empty when the condition is signaled implementation of a monitor maintains several process queues one for each condition variable and one for processes waiting to execute monitor operations to ensure that processes do not get stuck halfway through execution of an operation the monitor favors processes that were activated by signal operations over those wishing to begin execution of monitor operations the following example describes use of a monitor to implement a binary semaphore we discuss an interesting implementation issue after the example part process management example monitor implementation of a binary semaphore the upper half of figure shows a monitor type semmontype that implements a binary semaphore and the lower half shows three processes that use a monitor variable binarysem recall from section that a binary semaphore takes only values and and is used to implement a critical section the boolean variable busy is used to indicate whether any process is currently using the critical section thus its values true and false correspond to the values and of the binary semaphore respectively the condition variable nonbusy corresponds to the condition that the critical section is not busy it is used to block processes that try to enter a critical section while busy true the procedures semwait and semsignal implement the wait and signal operations on the binary semaphore binarysem is a monitor variable the initialization part of the monitor type which contains the statement busy false is invoked when binarysem is created hence variable busy of binarysem is initialized to false type semmontype monitor var busy boolean nonbusy condition procedure semwait begin if busy true then nonbusywait busy true end procedure semsignal begin busy false nonbusysignal end begin initialization busy false end var binarysem semmontype begin parbegin repeat repeat repeat binarysemsemwait binarysemsemwait binarysemsemwait critical section critical section critica lsection binarysemsemsignal binarysemsemsignal binarysemsemsignal remainder of remainder of remainder of the cycle the cycle the cycle forever forever forever parend end process p process p process p figure monitor implementation of a binary semaphore chapter process synchronization queue busy data nonbusy procedure semwait queue nonbusywait operations procedure semsignal nonbusysignal busy false initializations figure a monitor implementing a binary semaphore figure depicts the monitor semmontype the monitor maintains two queues of processes queue contains processes waiting to execute operation semwait or semsignal of the monitor while queue contains processes waiting for a nonbusysignal statement to be executed let p be the first process to perform binarysemsemwait since busy is false it changes busy to true and enters its critical section if p performs binarysemsemwait while p is still inside its critical section it will be blocked on the statement nonbusywait it will wait in queue now let p start executing binarysemsemsignal and let p try to perform binarysemsemwait before p finishes executing binarysemsemsignal due to mutual exclusion over monitor operations p will be blocked and put in the queue associated with entry to the monitor ie in queue figure shows a snapshot of the system at this instant when process p executes the statement nonbusysignal and exits from the monitor p will be activated ahead of p because queues associated with condition variables enjoy priority over the queue associated with entry to the monitor process p will start executing binarysemsemwait only when process p completes execution of binarysemsemwait exits the monitor and enters its critical section p will now block itself on the condition nonbusy it will be activated when p executes the binarysemsemsignal operation if procedure semsignal of example contained some statements following the signal statement an interesting synchronization problem would arise when process p invokes binarysemsemsignal and executes the statement nonbusysignal the signal statement is expected to activate process p which part process management p semwait busy queue nonbusy p semsignal queue p figure a snapshot of the system of example should resume its execution of binarysemsemwait at the same time process p should continue its execution of binarysemsemsignal by executing statements that follow the nonbusysignal statement since monitor operations are performed in a mutually exclusive manner only one of them can execute and the other one will have to wait so which of them should be selected for execution selecting process p for execution would delay the signaling process p which seems unfair selecting p would imply that p is not really activated until p leaves the monitor hoare proposed the first alternative brinch hansen proposed that a signal statement should be the last statement of a monitor procedure so that the process executing signal exits the monitor procedure immediately and the process activated by the signal statement can be scheduled we will follow this convention in our examples example producersconsumers using monitors figure shows a solution to the producersconsumers problem that uses monitors it follows the same approach as the solution of figure using semaphores the upper half of figure shows a monitor type boundedbuffertype variable full is an integer that indicates the number of full buffers in the procedure produce a producer executes a bufferemptywait if full n it would be activated only when at least one empty buffer exists in the pool similarly the consumer executes a buffer fullwait if full waiting consumers and producers are activated by the statements buff fullsignal and buffemptysignal in the procedures produce and consume respectively the lower half of figure shows a system containing two producer processes p p and a consumer process p operation of a single buffer system ie n in figure can be depicted as shown in figure let processes p and p try to produce and let process p try to consume all at the same time let us assume that process p enters the procedure produce gets past the wait statement and starts producing while processes p case studies of process synchronization synchronization of posix threads as mentioned in section posix threads provide mutexes for mutual exclusion and condition variables for control synchronization between processes a mutex is a binary semaphore an os may implement posix threads part process management type boundedbuffertype monitor const n number of buffers type item var buffer array n of item full prodptr consptr integer bufffull condition buffempty condition procedure produce producedinfo item begin if full n then buffemptywait buffer prodptr producedinfo ie produce prodptr prodptr mod n full full bufffullsignal end procedure consume forconsumption item begin if full then bufffullwait forconsumption bufferconsptr ie consume consptr consptr mod n full full buffemptysignal end begin initialization full prodptr consptr end begin var bbuf boundedbuffertype parbegin var info item var info item var area item repeat repeat repeat info info bbufconsume area bbufproduce info bbufproduce info consume area remainder of remainder of remainder of the cycle the cycle the cycle forever forever forever parend end producer p producer p consumer p figure producersconsumers using monitors chapter process synchronization p produce p produce p buffempty buffempty buff full buff full consume p consume p p a b figure snapshots of the monitor of example with a single buffer as kernellevel threads or userlevel threads accordingly mutexes would be implemented through either a kernellevel implementation or a hybrid implementation described in section when threads are implemented as kernellevel threads and through the userlevel implementation when threads are implemented through userlevel threads analogously condition variables are also implemented through a kernellevel hybrid or userlevel implementation scheme process synchronization in unix unix system v provides a kernellevel implementation of semaphores the name of a semaphore is called a key the key is actually associated with an array of semaphores and individual semaphores in the array are distinguished with the help of subscripts processes share a semaphore by using the same key a process wishing to use a semaphore obtains access to it by making a semget system call with a key as a parameter if a semaphore array with matching key already exists the kernel makes that array accessible to the process making the semget call otherwise it creates a new semaphore array assigns the key to it and makes it accessible to the process the kernel provides a single system call semop for wait and signal operations it takes two parameters a key ie the name of a semaphore array and a list of subscript op specifications where subscript identifies a semaphore in the semaphore array and op is a wait or signal operation to be performed the entire set of operations defined in the list is performed in an atomic manner that is either all the operations are performed and the process is free to continue its execution or none of the operations is performed and the process is blocked a blocked process is activated only when all operations indicated in semop can succeed the semantics of semop can be used to prevent deadlocks consider the following example semaphores sem and sem are associated with resources r and r respectively a process performs a waitsemi before using a resource ri and a signalsemi after finishing with it if each of processes p and p require both resources simultaneously it is possible that p will obtain access to r but will become blocked on waitsem and process p will obtain access to r part process management but will become blocked on waitsem this is a deadlock situation because both processes wait for each other indefinitely such a deadlock would not arise if processes performed both wait operations through a single semop since a process would be either allocated both resources or it would not be allocated any of the resources the situation now resembles the all resources together approach to deadlock prevention described later in section unix svr provides an interesting feature to make programs using semaphores more reliable it keeps track of all operations performed by a process on each semaphore used by it and performs an undo on these operations when the process terminates this action helps to prevent disruptions in a concurrent application due to misbehavior of some process for example if a process pi performed more wait operations than signal operations on semaphore semi and terminated it could cause indefinite waits for other processes in the application performing an undo operation on all wait and signal operations performed by pi might prevent such disasters to perform undo operations efficiently the kernel maintains a cumulative count of changes in the value of a semaphore caused by the operations in a process and subtracts it from the value of the semaphore when the process terminates if a process pi performed more wait operations than signal operations on semaphore semi its cumulative count for semi would be negative subtracting this count would nullify the effect of pi on semi pis cumulative count would be if it had performed an equal number of wait and signal operations on semi thus the undo operation does not interfere with normal operation of processes using semaphores unix bsd places a semaphore in memory areas shared by a set of processes and provides a hybrid implementation of semaphores along the lines discussed in section this way it avoids making system calls in cases where a wait operation does not lead to blocking of a process and a signal operation does not lead to activation of a process which provides fast synchronization process synchronization in linux linux provides a unixlike semaphore see section for use by user processes it also provides two kinds of semaphores for use by the kernel a conventional semaphore and a readerwriter semaphore the conventional semaphore is implemented by a kernellevel scheme that is more efficient than the kernellevel scheme discussed in section it uses a data structure that contains the value of a semaphore a flag to indicate whether any processes are blocked on it and the actual list of such processes unlike the scheme of section a lock is not used to avoid race conditions on the value of the semaphore instead the wait and signal operations use indivisible instructions to decrement or increment the value of the semaphore these operations lock the list of blocked processes only if they find that processes are to be added to it or removed from it the wait operation locks the list only if the process that performed the wait operation is to be blocked whereas the signal operation locks it only if the semaphores flag indicates that the list is nonempty chapter process synchronization the readerwriter semaphore provides capabilities that can be used to implement the readerswriters problem of section within a kernel so that many processes can read a kernel data structure concurrently but only one process can update it at a time its implementation does not favor either readers or writers it permits processes to enter their critical sections in fifo order except that consecutive readers can read concurrently it is achieved by simply maintaining a list of processes waiting to perform a read or write operation which is organized in the chronological order kernels older than the linux kernel implemented mutual exclusion in the kernel space through system calls however as mentioned in section a wait operation has a low failure rate ie a process is rarely blocked on a wait call so many of the system calls are actually unnecessary the linux kernel provides a fast user space mutex called futex a futex is an integer in shared memory on which only certain operations can be performed the wait operation on a futex makes a system call only when a process needs to be blocked on the futex and the signal operation on a futex makes a system call only when a process is to be activated the wait operation also provides a parameter through which a process can indicate how long it is prepared to be blocked on the wait when this time elapses the wait operation fails and returns an error code to the process that made the call process synchronization in solaris process synchronization in the sun solaris operating system contains three interesting features readerwriter semaphores and adaptive mutexes a data structure called a turnstile and use of the priority inversion protocol the readerwriter semaphore is analogous to the readerwriter semaphore in linux an adaptive mutex is useful in a multiprocessor os hence it is discussed in chapter only an overview is included here recall from section that the solaris kernel provides parallelism through kernel threads when a thread ti performs a wait operation on a semaphore that is currently used by another thread tj the kernel can either block ti or let it spin the blocking approach involves the overhead of blocking thread ti scheduling another thread and activating thread ti when tj releases the semaphore spinning on the other hand incurs the overhead of a busy wait until tj releases the semaphore if tj is currently operating on another cpu it may release the semaphore before either ti or tj is preempted so it is better to let ti spin if tj is not operating currently ti may spin for long so it is better to conserve cpu time by blocking it the adaptive mutex uses this method the solaris kernel uses a data structure called a turnstile to hold information concerning threads that are blocked on a mutex or readerwriter semaphore this information is used for both synchronization and priority inheritance to minimize the number of turnstiles needed at any time the kernel of solaris attaches a turnstile with every new thread it creates it performs the following actions when a kernel thread is to be blocked on a mutex if no threads part process management are already blocked on the mutex it detaches the turnstile from the thread associates it with the mutex and enters the threads id in the turnstile if a turnstile is already associated with the mutex ie if some other threads are already blocked on it the kernel detaches the turnstile of the thread and returns it to the pool of free turnstiles and enters the threads id into the turnstile that is already associated with the mutex when a thread releases a mutex or a reader writer semaphore the kernel obtains information about threads blocked on the mutex or readerwriter semaphore and decides which threads to activate it now attaches a turnstile from the pool of free turnstiles with the activated thread a turnstile is returned to the pool of free turnstiles when the last thread in it wakes up the solaris kernel uses a priority inheritance protocol to reduce synchronization delays consider a thread ti that is blocked on a semaphore because thread tj is in a critical section implemented through the semaphore thread ti might suffer a long synchronization delay if tj is not scheduled for a long time which would happen if tj has a lower priority than ti to reduce the synchronization delay for ti the kernel raises the priority of tj to that of ti until tj exits the critical section if many processes become blocked on the semaphore being used by tj tjs priority should be raised to that of the highestpriority process blocked on the semaphore it is implemented by obtaining priorities of the blocked processes from the turnstile associated with the semaphore process synchronization in windows windows is an objectoriented system hence processes files and events are represented by objects the kernel provides a uniform interface for thread synchronization over different kinds of objects as follows a dispatcher object is a special kind of object that is either in the signaled state or in the nonsignaled state a dispatcher object is embedded in every object over which synchronization may be desired eg an object representing a process file event mutex or semaphore any thread that wishes to synchronize with an object would be put in the waiting state if the dispatcher object embedded in the object is in the nonsignaled state table describes the semantics of various kinds of objects which determine when the state of an object would change and which of the threads waiting on it would be activated when it is signaled a thread object enters the signaled state when the thread terminates whereas a process object enters the signaled state when all threads in the process terminate in both cases all threads waiting on the object are activated the file object enters the signaled state when an io operation on the file completes if any threads are waiting on it all of them are activated and its synchronization state is changed back to nonsignaled if no threads are waiting on it a thread that waits on it sometime in future will get past the wait operation and the synchronization state of the file object would be changed to nonsignaled the console input object has an analogous behavior except that only one waiting thread is activated when it chapter process synchronization table windows objects used for synchronization object nonsignaled state signaled state signal time action process not terminated last thread activate all threads terminates thread not terminated the thread activate all threads terminates file io request io completed activate all threads pending console input input not input provided activate one thread provided file change no changes change noticed activate one thread notify event not yet set set event executed activate all threads synchronization reset set event executed activate one thread event and reset event semaphore successful wait released activate one thread mutex successful wait released activate one thread condition initially and after wake or wakeall activate one thread or variable a wake or function is all threads wakeall performed function call timer reinitialization set time arrives or same as notify interval elapses and synchronization events is signaled the file change object is signaled when the system detects changes in the file it behaves like the file object in other respects threads use the event semaphore mutex and condition variable objects for mutual synchronization they signal these objects by executing library functions that lead to appropriate system calls an event object is signaled at a set event system call if it is a notification event all threads waiting on it are activated if it is a synchronization event only one thread is activated and the event is reset the timer object is also designed for use in the notification and synchronization modes the kernel changes the state of the object to signaled when the specified time arrives or the specified interval elapses its signal time actions are similar to those of the notify and synchronization events the semaphore object implements a counting semaphore which can be used to control a set of resources the number of resources is specified as the initial value of the semaphore a count in the semaphore object indicates how many of these resources are currently available for use by threads the semaphore object is in the nonsignaled state when the count is so any process performing a wait on it would be put in the waiting state when a thread releases a resource the kernel increments the number of available resources which puts the semaphore in the signaled state consequently some thread waiting on it would be activated summary process synchronization is a generic term for control synchronization processes may not wait data access synchronization which is used to for each others actions as expected hence avoidupdate shared data in a mutually exclusive manance of race conditions is a primary issue in process ner and control synchronization which is used synchronization to ensure that processes perform their actions in the computer provides indivisible instructions a desired order classic process synchronization which access memory locations in a mutually problems such as producersconsumers readers exclusive manner a process may use an indiviswriters and dining philosophers represent imporible instruction on a lock variable to implement tant classes of process synchronization problems a critical section however this approach suffers in this chapter we discussed the fundamental issues from busy waits because a process that can not in process synchronization and the support for enter the critical section keeps looping until it process synchronization provided by the computer may do so hence the kernel provides a facility the kernel and programming languages we also to block such a process until it may be permitanalyzed classic process synchronization problems ted to enter a critical section compilers of proand demonstrated use of various synchronization gramming languages implement process synchrofacilities of programming languages and operating nization primitives and constructs by using this systems in implementing them facility a semaphore is a primitive that facilitates a race condition is a situation in which actions blocking and activation of processes without race of concurrent processes may have unexpected conconditions a monitor is a construct that provides sequences such as incorrect values of shared data two facilities it implements operations on shared or faulty interaction among processes a race condata as critical sections over the data and it prodition exists when concurrent processes update vides statements for control synchronization shared data in an uncoordinated manner it is operating systems provide features for effiavoided through mutual exclusion which ensures cient implementation of process synchronization that only one process updates shared data at any eg linux provides readerswriters semaphores time a critical section on a shared data d is a solaris provides priority inheritance to avoid some section of code that accesses d in a mutually excluof the problems related to busy waits and windows sive manner a race condition may also exist in provides dispatcher objects chapter process synchronization test your concepts classify each of the following statements as true figure is modified to remove the action or false lift the forks one at a time from the while a an application can contain a race condition loop and put it following the while loop only if the computer system servicing the a semaphore is initialized to twelve wait application contains more than one cpu operations and seven signal operations are perb control synchronization is needed when proformed on it what is the number of processes cesses generate and analyze of figure b waiting on this semaphore share the variable sample a b c d c a process may be starved of entry to a a binary semaphore is initialized to wait critical section if the critical section impleoperations are performed on it in a row folmentation does not satisfy the bounded wait lowed by signal operations now more condition wait operations are performed on it what d a process may be starved of entry to a critical is the number of processes waiting on this section if the critical section implementation semaphore does not satisfy the progress condition a b c d e a busy wait is unavoidable unless a system ten processes share a critical section implecall is made to block a process mented by using a counting semaphore named f indefinite busy waits are possible in an os x nine of these processes use the code waitx using prioritybased scheduling but not poscritical section signalx however one prosible in an os using roundrobin scheduling cess erroneously uses the code signalx critical g algorithm can be used to implement a section signalx what is the maximum numsinglebuffer producersconsumers system if ber of processes that can be in the critical section process p is a producer and p is a consumer at the same time h when a lock variable is used an indivisia b c d ble instruction is not needed to implement a critical section in a readerswriters system a read operation i in a producersconsumers system consisting consumes time units and a write operation conof many producer processes many consumer sumes time units no readers or writers exist processes and many buffers in the bufferin the system at time ti one reader arrives at pool it is possible for many producer protime ti and readers and writer arrive at time cesses to be producing and many consumer ti if no more readers or writers arrive when processes to be consuming at the same time will the writer finish writing j in a writerspreferred readerswriters sysa ti tem some reader processes wishing to read b ti the shared data may become blocked even c ti while some other reader processes are reading d none of ac the shared data a producer process produces a new item of infork a deadlock can not occur in the dining mation in seconds and a consumer process philosophers problem if one of the philosoconsumes an item in seconds in a producers phers can eat with only one fork consumers system consisting of a single prol a critical section implemented using ducer process a single consumer process and semaphores would satisfy the bounded wait a single buffer both the producer and the conproperty only if the signal operation activates sumer processes start their operation at time processes in fifo order at what time will the consumer process finish m a race condition can occur over forks if the consuming items outline of the dining philosophers problem in a b c d e none of ad part process management exercises a concurrent program contains a few updates of and activate for process synchronization it has a a shared variable x which occur inside critical race condition describe how this race condition sections variable x is also used in the following arises section of code which is not enclosed in a critical the readerswriters solution of figure uses section two semaphores even though a single entity if x c the shared data is to be controlled modify this then y x solution to use a single semaphore rwpermielse y x ssion instead of semaphores reading and print x y writing hint perform a waitrwpermission in the reader only if reading is not already in does this program have a race condition progress two concurrent processes share a data item sum modify the readerswriters solution of which is initialized to however they do not use figure to implement a writerpreferred mutual exclusion while accessing its value each readerswriters system process contains a loop that executes times implement a critical section using the testand contains the single statement sum sum andset or swap instructions of section if no other operations are performed on sum use ideas from section to ensure that the indicate the lower bound and upper bound on bounded wait condition is satisfied the value of sum when both processes terminate a resource is to be allocated to requesting analyze algorithms and and comment processes in a fifo manner each process is on the critical section properties violated by coded as them give examples illustrating the violations answer the following in context of dekkers repeat algorithm a does the algorithm satisfy the progress requestresourceprocessid resourceid condition use resource b can a deadlock condition arise releaseresourceprocessid resourceid c can a livelock condition arise remainder of the cycle is the bounded wait condition satisfied by forever petersons algorithm the following changes are made in petersons develop the procedures requestresource and algorithm see algorithm the statements releaseresource using semaphores flag true and flag false in process p can one or more of the following features elimare interchanged and analogous changes are inate deficiencies of the outline of the dining made in process p discuss which properties philosophers problem shown in figure of the implementation of critical sections are a if n philosophers exist in the system have violated by the resulting system seats for at least n philosophers at the the statement while flag and turn in peterdining table sons algorithm is changed to while flag or turn b make sure that at least one lefthanded and analogous changes are made in process philosopher and at least one righthanded p which properties of critical section implephilosopher sit at the table at any time mentation are violated by the resulting system in figure producers and consumers comment on the effect of deleting the statement always execute the statements buffull signal while choosingj do nothing on working of and bufemptysignal suggest and implement lamports bakery algorithm a method of reducing the number of signal the solution of the producersconsumers probstatements executed during the operation of lem shown in figure uses kernel calls block the system chapter process synchronization type item var buffer item bufferfull boolean producerblocked boolean consumerblocked boolean begin bufferfull false producerblocked false consumerblocked false parbegin repeat repeat if bufferfull false then if bufferfull true then produce in buffer consume from buffer bufferfull true bufferfull false if consumerblocked true then if producerblocked true then activateconsumer activateproducer remainder of the cycle remainder of the cycle else else producerblocked true consumerblocked true blockproducer blockconsumer consumerblocked false producerblocked false forever forever parend producer consumer figure the producerconsumer problem with a synchronization error due to a race condition implement the dining philosophers problem of them and starts serving him otherwise he using monitors minimize the number of execugoes to sleep in the barbers chair a customer tions of signal statements in your solution and enters the waiting room only if there is at least observe its effect on the logical complexity of one vacant seat and either waits for the barber your solution to call him if the barber is busy or wakes the a customer gives the following instructions to a barber if he is asleep identify the synchronizabank manager do not credit any funds to my tion requirements between the barber and cusaccount if the balance in my account exceeds tomer processes code the barber and customer n and hold any debits until the balance in the processes such that deadlocks do not arise account is large enough to permit the debit a monitor is to be written to simulate a clock design a monitor to implement the customers manager used for realtime control of concurbank account rent processes the clock manager uses a variable the synchronization problem called sleeping named clock to maintain the current time the barber is described as follows a barber shop has os supports a signal called elapsedtime that is a single barber a single barbers chair in a small generated every ms the clock manager proroom and a large waiting room with n seats the vides a signal handling action for elapsedtime barber and the barbers chair are visible from the see section that updates clock at every waiting room after servicing one customer the occurrence of the signal this action is coded as barber checks whether any customers are waita procedure of the monitor a typical request ing in the waiting room if so he admits one made to the clock manager is wake me up at part process management am the clock manager blocks the prothis system using any synchronization primitive cesses making such requests and arranges to or control structure of your choice to prevent activate them at the designated times implement starvation of queries it is proposed to handle a this monitor maximum of queries on a part of the data nesting of monitor calls implies that a proceat any time modify the monitor to incorporate dure in monitor a calls a procedure of another this feature monitor say monitor b during execution of the a bridge on a busy highway is damaged by a nested call the procedure of monitor a conflood oneway traffic is to be instituted on the tinues to hold its mutual exclusion show that bridge by permitting vehicles traveling in opponested monitor calls can lead to deadlocks site directions to use the bridge alternately the write a short note on the implementation of following rules are formulated for use of the monitors your note must discuss bridge a how to achieve mutual exclusion between the a at any time the bridge is used by vehicles monitor procedures traveling in one direction only b whether monitor procedures need to b if vehicles are waiting to cross the bridge at be coded in a reentrant manner see both ends only one vehicle from one end is section allowed to cross the bridge before a vehicle a large data collection d is used merely to from the other end starts crossing the bridge answer queries ie no updates are carried out c if no vehicles are waiting at one end then on d so queries can be processed concurrently any number of vehicles from the other end because of the large size of d it is split into sevare permitted to cross the bridge eral parts d d dn and at any time only develop a concurrent system to implement these one of these parts say d is loaded in memrules ory to handle queries related to it if no queries when vehicles are waiting at both ends the rules are active on d and queries exist on some other of exercise a lead to poor use of the bridge part of data say d d is loaded in memory and hence up to vehicles should be allowed to queries on it are processed concurrently when cross the bridge in one direction even if vehid is split into two parts d and d this system cles are waiting at the other end implement the is called a readersreaders system implement modified rules class project interprocess communication an interprocess message communication system uses message buffers the system is to operate as the asymmetric naming convention described later in follows section which uses the following rules to send a message a sender provides the id of the each process has a cyclic behavior its operation is destination process to which it is to be delivered governed by commands in a command file that is and the text of the message to receive a mesused exclusively by it in each iteration it reads a sage a process simply provides the name of a varicommand from the file and invokes an appropriable in which the message should be deposited the ate operation of the monitor three commands are system provides it with a message sent to it by supported some process a send processid messagetext the prothe system consists of a monitor named comcess should send a message municationmanager and four processes the monib receive variablename the process should tor provides the operations send and receive which receive a message implement message passing using a global pool of c quit the process should complete its operation chapter process synchronization when a process invokes a send operation the monprocess performing the receive operation is blocked itor copies the text of the message in a free message if no message exists for it it would be activated buffer from the global pool of message buffers if when a message is sent to it the destination process of the message is currently after performing a send or receive operation the blocked on a receive operation the message is delivmonitor writes details of the actions performed by ered to it as described in item and the process it in a log file is activated in either case control is returned to the monitor detects a deadlock situation in which the process executing the send operation if none of some of the processes are blocked indefinitely it the message buffers in the global pool of meswrites details of the deadlock situation in the log sage buffers is free the process performing the send file and terminates itself operation is blocked until a message buffer becomes the interprocess message communication system free terminates itself when all processes have completed when a process invokes a receive operation it is their operation given a message sent to it in fifo order the monitor finds the message buffer that contains the first undelivered message that was sent to the process write the monitor communicationmanager and copies the text of the message into the variable test its operation with several sets of sample command mentioned by the process and frees the message files for the processes that create various interesting sitbuffer if a process executing the send operation was uations in message passing including some deadlock blocked as mentioned in item it is activated the situations class project disk scheduler a disk scheduler is that part of an os which decides the exclusively by it each command is for performing a order in which io operations should be performed on a read or write operation on a disk block in each iteradisk to achieve high disk throughput see section tion a process reads a command from its command file processes that wish to perform io operations on the disk and invokes the monitor operation iorequest to pass use a monitor named diskscheduler and the following details of the io operation to the monitor iorequest pseudocode blocks the process until its io operation is scheduled var diskscheduler diskmontype when the process is activated it returns from iorequest parbegin and performs its io operation after completing the io begin user process pi operation it invokes the monitor operation iocomplete var diskblockaddress integer so that the monitor can schedule the next io operarepeat tion the monitor writes details of its actions in a log file every time the iorequest or iocomplete operation is read a command from file fi invoked diskscheduler iorequest code the monitor type diskmontype for simpi iooperation plicity you may assume that io operations are scheddiskblockaddress uled in fifo order and that the number of processes perform io operation does not exceed hint note the process id of a prodiskscheduler iocomplete pi cess along with details of its io operation in a list in remainder of the cycle the monitor decide how many condition variables you forever would need to block and activate the processes end modify diskmontype such that io operations other user processes would be performed by the monitor itself rather than by parend user processes hint operation iocomplete would no each process has cyclic behavior its operation is longer be needed governed by commands in a command file that is used part process management bibliography dijkstra discusses the mutual exclusion problem ben ari m principles of concurrent describes dekkers algorithm and presents a mutual programming prentice hall englewood cliffs exclusion algorithm for n processes lamport nj describes and proves the bakery algorithm ben ben ari m principles of concurrent and ari describes the evolution of mutual exclusion distributed programming nd ed prentice hall algorithms and provides a proof of dekkers algorithm englewood cliffs nj ben ari discusses concurrent and distributed pro bovet d p and m cesati understanding gramming peterson lamport and the linux kernel rd ed oreilly sebastopol raynal are other sources on mutual exclusion brinch hansen p structured algorithms multiprogramming communications of the dijkstra proposed semaphores hoare acm and brinch hansen discuss the critical brinch hansen p operating system and conditional critical regions which are synchronizaprinciples prentice hall englewood cliffs nj tion constructs that preceded monitors brinch hansen brinch hansen p the programming and hoare describe the monitor concept language concurrent pascal ieee transactions buhr et al describes different monitor implemenon software engineering tations richter describes thread synchronization brinch hansen p the architecture of in cc programs under windows christopher and concurrent programs prentice hall englewood thiruvathukal describes the concept of monitors cliffs nj in java compares it with the monitors of brinch hansen buhr m m fortier and m h coffin and hoare and concludes that java synchronization is monitor classification computing surveys not as well developed as the brinch hansen and hoare monitors chandy k m and j misra parallel a synchronization primitive or construct is program design a foundation addisonwesley complete if it can be used to implement all process synreading mass chronization problems the completeness of semaphores christopher t w and g k thiruvathukal is discussed in patil lipton and kosaraju multithreaded and networked programming sun microsystems brinch hansen and ben ari courtois p j f heymans and d l parnas discuss the methodology for building concurrent concurrent control with readers and programs owicki and gries and francez and writers communications of the acm pneuli deal with the methodology of proving the correctness of concurrent programs dijkstra e w cooperating sequential vahalia and stevens and rago disprocesses technical report ewd cuss process synchronization in unix beck et al technological university eindhoven bovet and cesati and love discuss syn eisenberg m a and m r mcguire chronization in linux mauro and mcdougall further comments on dijkstras concurrent discusses synchronization in solaris while richter programming control problem communications and russinovich and solomon discuss of the acm synchronization features in windows francez n and a pneuli a proof method for cyclic programs acta informatica beck m h bohme m dziadzka u kunitz hoare c a r towards a theory of r magnus c schroter and d verworner parallel programming in operating systems linux kernel programming pearson techniques car hoare and rh perrot eds education new york academic press london chapter process synchronization hoare c a r monitors an operating owicki s and d gries verifying system structuring concept communications of properties of parallel programs an axiomatic the acm approach communications of the acm kosaraju s limitations of dijkstras semaphore primitives and petri nets operating patil s limitations and capabilities of systems review dijkstras semaphore primitives for coordination lamport l a new solution of dijkstras among processes technical report mit concurrent programming problem communica peterson g l myths about the mutual tions of the acm exclusion problem information processing lamport l a new approach to proving letters the correctness of multiprocess programs acm raynal m algorithms for mutual transactions on programming languages and exclusion mit press cambridge mass systems richter j programming applications for lamport l the mutual exclusion microsoft windows th ed microsoft press problem communications of the acm redmond wash russinovich m e and d a solomon lamport l the mutual exclusion microsoft windows internals th ed microsoft problem has been solved acm transactions on press redmond wash programming languages and systems stevens w r and s a rago advanced programming in the unix environment nd ed lipton r on synchronization primitive addison wesley reading mass systems phd thesis carnegiemellon vahalia u unix internals the new university frontiers prentice hall englewood love r linux kernel development nd cliffs nj ed novell press mauro j and r mcdougall solaris internals nd ed prentice hall englewood cliffs nj scheduling terminology and concepts scheduling very generally is the activity of selecting the next request to be serviced by a server figure is a schematic diagram of scheduling the scheduler actively considers a list of pending requests for servicing and selects one of them the server services the request selected by the scheduler this request leaves the server either when it completes or when the scheduler preempts it and puts it back into the list of pending requests in either situation the scheduler selects the request that should be serviced next from time to time the scheduler admits one of the arrived requests for active consideration and enters it into the list of pending requests actions of the scheduler are shown by the dashed arrows chapter scheduling request is preempted scheduler request request is request is arrives admitted scheduled server request is completed arrived pending requests requests figure a schematic of scheduling in figure events related to a request are its arrival admission scheduling preemption and completion in an operating system a request is the execution of a job or a process and the server is the cpu a job or a process is said to arrive when it is submitted by a user and to be admitted when the scheduler starts considering it for scheduling an admitted job or process either waits in the list of pending requests uses the cpu or performs io operations eventually it completes and leaves the system the schedulers action of admitting a request is important only in an operating system with limited resources for simplicity in most of our discussions we assume that a request is admitted automatically on arrival in chapter we discussed how use of priorities in the scheduler provides good system performance while use of roundrobin scheduling provides good user service in the form of fast response modern operating systems use more complex scheduling policies to achieve a suitable combination of system performance and user service table lists the key terms and concepts related to scheduling the service time of a job or a process is the total of cpu time and io time required by it to complete its execution and the deadline which is specified only in realtime systems see section is the time by which its servicing should be completed both service time and deadline are an inherent property of a job or a process the completion time of a job or a process depends on its arrival and service times and on the kind of service it receives from the os we group scheduling concepts into usercentric concepts and systemcentric concepts to characterize the oss concern for either user service or system performance usercentric scheduling concepts in an interactive environment a user interacts with a process during its operation the user makes a subrequest to a process and the process responds by performing actions or by computing results response time is the time since submission of a subrequest to the time its processing is completed it is an absolute measure of service provided to a subrequest turnaround time is an analogous absolute measure of service provided to a job or process part process management table scheduling terms and concepts term or concept definition or description request related arrival time time when a user submits a job or process admission time time when the system starts considering a job or process for scheduling completion time time when a job or process is completed deadline time by which a job or process must be completed to meet the response requirement of a realtime application service time the total of cpu time and io time required by a job process or subrequest to complete its operation preemption forced deallocation of cpu from a job or process priority a tiebreaking rule used to select a job or process when many jobs or processes await service user service related individual request deadline overrun the amount of time by which the completion time of a job or process exceeds its deadline deadline overruns can be both positive or negative fair share a specified share of cpu time that should be devoted to execution of a process or a group of processes response ratio the ratio time since arrival service time of a job or process service time of the job or process response time rt time between the submission of a subrequest for processing to the time its result becomes available this concept is applicable to interactive processes turnaround time ta time between the submission of a job or process and its completion by the system this concept is meaningful for noninteractive jobs or processes only weighted turnaround w ratio of the turnaround time of a job or process to its own service time user service related average service mean response time rt average of the response times of all subrequests serviced by the system mean turnaround average of the turnaround times of all jobs or time ta processes serviced by the system performance related schedule length the time taken to complete a specific set of jobs or processes throughput the average number of jobs processes or subrequests completed by a system in one unit of time chapter scheduling turnaround time differs from the service time of a job or process because it also includes the time when the job or process is neither executing on the cpu nor performing io operations we are familiar with these two measures from the discussions in chapter several other measures of user service are defined the weighted turnaround relates the turnaround time of a process to its own service time for example a weighted turnaround of indicates that the turnaround received by a request is times its own service time comparison of weighted turnarounds of different jobs or processes indicates the comparative service received by them fair share is the share of cpu time that should be alloted to a process or a group of processes response ratio of a job or process is the ratio time since arrival service timeservice time it relates the delay in the servicing of a job or process to its own service time it can be used in a scheduling policy to avoid starvation of processes see section the deadline overrun is the difference between the completion time and deadline of a job or process in a realtime application a negative value of deadline overrun indicates that the job or process was completed before its deadline whereas a positive value indicates that the deadline was missed the mean response time and mean turnaround time are measures of average service provided to subrequests and processes or jobs respectively systemcentric scheduling concepts throughput and schedule length are measures of system performance throughput indicates the average number of requests or subrequests completed per unit of time see section it provides a basis for comparing performance of two or more scheduling policies or for comparing performance of the same scheduling policy over different periods of time schedule length indicates the total amount of time taken by a server to complete a set of requests throughput and schedule length are related consider servicing of five requests r r let mina and maxc be the earliest of the arrival times and the latest of the completion times respectively the schedule length for these five requests is maxc mina and the throughput is maxc mina however it is typically not possible to compute schedule length and throughput in this manner because an os may also admit and service other requests in the interval from mina to maxc to achieve good system performance nevertheless schedule length is an important basis for comparing the performance of scheduling policies when the scheduling overhead is not negligible throughput is related to the mean response time and mean turnaround time in an obvious way fundamental techniques of scheduling schedulers use three fundamental techniques in their design to provide good user service or high performance of the system prioritybased scheduling the process in operation should be the highestpriority process requiring use of the cpu it is ensured by scheduling the highestpriority ready process at any time and preempting it when a process with a higher priority becomes ready recall from section that a part process management multiprogramming os assigns a high priority to iobound processes this assignment of priorities provides high throughput of the system reordering of requests reordering implies servicing of requests in some order other than their arrival order reordering may be used by itself to improve user service eg servicing short requests before long ones reduces the average turnaround time of requests reordering of requests is implicit in preemption which may be used to enhance user service as in a timesharing system or to enhance the system throughput as in a multiprogramming system variation of time slice when timeslicing is used from eq of section where is the cpu efficiency is the time slice and is the os overhead per scheduling decision better response times are obtained when smaller values of the time slice are used however it lowers the cpu efficiency because considerable process switching overhead is incurred to balance cpu efficiency and response times an os could use different values of for different requests a small value for iobound requests and a large value for cpubound requests or it could vary the value of for a process when its behavior changes from cpubound to iobound or from iobound to cpubound in sections and we discuss how the techniques of prioritybased scheduling and reordering of requests are used in classical nonpreemptive and preemptive scheduling policies in sections and we discuss how schedulers in modern oss combine these three fundamental techniques to provide a combination of good performance and good user service the role of priority priority is a tiebreaking rule that is employed by a scheduler when many requests await attention of the server the priority of a request can be a function of several parameters each parameter reflecting either an inherent attribute of the request or an aspect concerning its service it is called a dynamic priority if some of its parameters change during the operation of the request otherwise it called a static priority some process reorderings could be obtained through priorities as well for example short processes would be serviced before long processes if priority is inversely proportional to the service time of a process and processes that have received less cpu time would be processed first if priority is inversely proportional to the cpu time consumed by a process however complex priority functions may be needed to obtain some kinds of process reorderings such as those obtained through timeslicing their use would increase the overhead of scheduling in such situations schedulers employ algorithms that determine the order in which requests should be serviced if two or more requests have the same priority which of them should be scheduled first a popular scheme is to use roundrobin scheduling among such requests this way processes with the same priority share the cpu among nonpreemptive scheduling policies in nonpreemptive scheduling a server always services a scheduled request to completion thus scheduling is performed only when servicing of a previously scheduled request is completed and so preemption of a request as shown in figure never occurs nonpreemptive scheduling is attractive because of its simplicity the scheduler does not have to distinguish between an unserviced request and a partially serviced one since a request is never preempted the schedulers only function in improving user service or system performance is reordering of requests we discuss three nonpreemptive scheduling policies in this section firstcome firstserved fcfs scheduling shortest request next srn scheduling highest response ratio next hrn scheduling we illustrate the operation and performance of various scheduling policies with the help of the five processes shown in table for simplicity we assume that these processes do not perform io operations fcfs scheduling requests are scheduled in the order in which they arrive in the system the list of pending requests is organized as a queue the scheduler always schedules the first request in the list an example of fcfs scheduling is a batch processing system in which jobs are ordered according to their arrival times or arbitrarily table processes for scheduling process p p p p p admission time service time part process management completed process processes in system scheduled time id ta w in fcfs order process p p p p p p p p p p p p p p p p p p ta seconds w p p p p p time figure scheduling using the fcfs policy if they arrive at exactly the same time and results of a job are released to the user immediately on completion of the job the following example illustrates operation of an fcfs scheduler example fcfs scheduling figure illustrates the scheduling decisions made by the fcfs scheduling policy for the processes of table process p is scheduled at time the pending list contains p and p when p completes at seconds so p is scheduled the completed column shows the id of the completed process and its turnaround time ta and weighted turnaround w the mean values of ta and w ie ta and w are shown below the table the timing chart of figure shows how the processes operated from example it is seen that considerable variation exists in the weighted turnarounds provided by fcfs scheduling this variation would have been larger if processes subject to large turnaround times were short eg the weighted turnaround of p would have been larger if its execution requirement had been second or second shortest request next srn scheduling the srn scheduler always schedules the request with the smallest service time thus a request remains pending until all shorter requests have been serviced chapter scheduling completed process processes scheduled time id ta w in system process p p p p p p p p p p p p p p p p p p ta seconds w p p p p p time figure scheduling using the shortest request next srn policy shortest request next srn scheduling example figure illustrates the scheduling decisions made by the srn scheduling policy for the processes of table and the operation of the processes at time p is the only process in the system so it is scheduled it completes at time seconds at this time processes p and p exist in the system and p is shorter than p so p is scheduled and so on the mean turnaround time and the mean weighted turnaround are better than in fcfs scheduling because short requests tend to receive smaller turnaround times and weighted turnarounds than in fcfs scheduling this feature degrades the service that long requests receive however their weighted turnarounds do not increase much because their service times are large the throughput is higher than in fcfs scheduling in the first seconds of the schedule because short processes are being serviced however it is identical at the end of the schedule because the same processes have been serviced use of the srn policy faces several difficulties in practice service times of processes are not known to the operating system a priori hence the os may expect users to provide estimates of service times of processes however scheduling performance would be erratic if users do not possess sufficient experience in estimating service times or they manipulate the system to obtain better service by giving low service time estimates for their processes the srn policy offers preemptive scheduling policies in preemptive scheduling the server can be switched to the processing of a new request before completing the current request the preempted request is put back into the list of pending requests see figure its servicing is resumed when it is scheduled again thus a request might have to be scheduled many times before it completed this feature causes a larger scheduling overhead than when nonpreemptive scheduling is used we discussed preemptive scheduling in multiprogramming and timesharing operating systems earlier in chapter chapter scheduling completed process response ratios of processes time id ta w p p p p p scheduled process p p p p p p p p p p ta seconds w p p p p p time figure operation of highest response ratio hrn policy we discuss three preemptive scheduling policies in this section roundrobin scheduling with timeslicing rr least completed next lcn scheduling shortest time to go stg scheduling the rr scheduling policy shares the cpu among admitted requests by servicing them in turn the other two policies take into account the cpu time required by a request or the cpu time consumed by it while making their scheduling decisions roundrobin scheduling with timeslicing rr the rr policy aims at providing good response times to all requests the time slice which is designated as is the largest amount of cpu time a request may use when scheduled a request is preempted at the end of a time slice to facilitate this the kernel arranges to raise a timer interrupt when the time slice elapses the rr policy provides comparable service to all cpubound processes this feature is reflected in approximately equal values of their weighted turnarounds the actual value of the weighted turnaround of a process depends on the number of processes in the system weighted turnarounds provided to processes that perform io operations would depend on the durations of their io operations the rr policy does not fare well on measures of system performance like throughput because it does not give a favored treatment to short processes the following example illustrates the performance of rr scheduling part process management example roundrobin rr scheduling a roundrobin scheduler maintains a queue of processes in the ready state and simply selects the first process in the queue the running process is preempted when the time slice elapses and it is put at the end of the queue it is assumed that a new process that was admitted into the system at the same instant a process was preempted will be entered into the queue before the preempted process figure summarizes operation of the rr scheduler with second for the five processes shown in table the scheduler makes scheduling decisions every second the time when a decision is made is shown in the first row of the table in the top half of figure the next five rows show positions of the five processes in the ready queue a blank entry indicates that the process is not in the system at the designated time the last row shows the process selected by the scheduler it is the process occupying the first position in the ready queue consider the situation at seconds the scheduling queue contains p followed by p hence p is scheduled process p arrives at seconds and is entered in the queue p is also preempted at seconds and it is entered in the queue hence the queue has process p followed by p and p so p is scheduled time of scheduling c ta w position of p processes in p ready queue p implies p head of queue p process scheduled p p p p p p p p p p p p p p p p ta seconds w c completion time of a process p p p p p time figure scheduling using the roundrobin policy with timeslicing rr chapter scheduling the turnaround times and weighted turnarounds of the processes are as shown in the right part of the table the c column shows completion times the turnaround times and weighted turnarounds are inferior to those given by the nonpreemptive policies discussed in section because the cpu time is shared among many processes because of timeslicing it can be seen that processes p p and p which arrive at around the same time receive approximately equal weighted turnarounds p receives the worst weighted turnaround because through most of its life it is one of three processes present in the system p receives the best weighted turnaround because no other process exists in the system during the early part of its execution thus weighted turnarounds depend on the load in the system as discussed in chapter if a system contains n processes each subrequest by a process consumes exactly seconds and the overhead per scheduling decision is the response time rt for a subrequest is n however the relation between and rt is more complex than this first some processes will be blocked for io or waiting for user actions so the response time will be governed by the number of active processes rather than by n second if a request needs more cpu time than seconds it will have to be scheduled more than once before it can produce a response hence at small values of rt for a request may be higher for smaller values of the following example illustrates this aspect variation of response time in rr scheduling example an os contains identical processes that were initiated at the same time each process receives identical subrequests and each subrequest consumes ms of cpu time a subrequest is followed by an io operation that consumes ms the system consumes ms in cpu scheduling for ms the first subrequest by the first process receives a response time of ms and the first subrequest by the last process receives a response time of ms hence the average response time is ms a subsequent subrequest by any process receives a response time of ms ms because the process spends ms in an io wait before receiving the next subrequest for ms a subrequest would be preempted after ms when scheduled again it would execute for ms and produce results hence the response time for the first process is ms and that for the last process is ms a subsequent subrequest receives a response time of ms figure summarizes performance of the system for different values of as expected the schedule length and the overhead are higher for smaller values of the graph in figure illustrates the variation of average response time to second and subsequent subrequests for different values of note that the response time is larger when is ms than when it is ms part process management time slice ms ms ms ms average rt for first subrequest ms average rt for subsequent subrequest ms number of scheduling decisions schedule length ms overhead percent response time msecs time slice ms figure performance of rr scheduling for different values of least completed next lcn scheduling the lcn policy schedules the process that has so far consumed the least amount of cpu time thus the nature of a process whether cpubound or iobound and its cpu time requirement do not influence its progress in the system under the lcn policy all processes will make approximately equal progress in terms of the cpu time consumed by them so this policy guarantees that short processes will finish ahead of long processes ultimately however this policy has the familiar drawback of starving long processes of cpu attention it also neglects existing processes if new processes keep arriving in the system so even notsolong processes tend to suffer starvation or large turnaround times example least completed next lcn scheduling implementation of the lcn scheduling policy for the five processes shown in table is summarized in figure the middle rows in the table in the upper half of the figure show the amount of cpu time already consumed by a process the scheduler analyzes this information and selects the process that has consumed the least amount of cpu time in case of a tie it selects the process that has not been serviced for the longest period of time the turnaround times and weighted turnarounds of the processes are shown in the right half of the table chapter scheduling time of scheduling c ta w p cpu time p consumed by p processes p p process scheduled p p p p p p p p p p p p p p p p ta seconds w c completion time of a process p p p p p time figure scheduling using the least completed next lcn policy it can be seen that servicing of p p and p is delayed because new processes arrive and obtain cpu service before these processes can make further progress the lcn policy provides poorer turnaround times and weighted turnarounds than those provided by the rr policy see example and the stg policy to be discussed next because it favors newly arriving processes over existing processes in the system until the new processes catch up in terms of cpu utilization eg it favors p over p p and p shortest time to go stg scheduling the shortest time to go policy schedules a process whose remaining cpu time requirements are the smallest in the system it is a preemptive version of the shortest request next srn policy of section so it favors short processes over long ones and provides good throughput additionally the stg policy also favors a process that is nearing completion over short processes entering the system this feature helps to improve the turnaround times and weighted turnarounds of processes since it is analogous to the srn policy long processes might face starvation abstract views of an operating system a question such as what is an os is likely to evoke different answers depending on the users interest for example to a school or college student the os is the software that permits access to the internet to a programmer the os is the software that makes it possible to develop programs on a computer system part overview to a user of an application package the os is simply the software that makes it possible to use the package to a technician in say a computerized chemical plant the os is the invisible component of a computer system that controls the plant a user perceives an os as simply a means of achieving an intended use of a computer system for the student the sole purpose of the computer system is to get onto the internet the os helps in achieving this hence the student thinks of the operating system as the means for internet browsing the programmer the user of a package and the technician similarly identify the os with their particular purposes in using the computer since their purposes are different their perceptions of the os are also different figure illustrates the four views of an os we have just considered they are abstract views because each focuses on those characteristics considered essential from the perspective of the individual viewer it includes some elements of reality but ignores other elements the student the application user and the technician are end users of the os their views do not contain any features of the os the programmers view is that of a software developer it includes features of the os for software development an os designer has his own abstract view of the os which shows the structure of an os and the relationship between its component parts figure internet a b stock quotes c d figure abstract views of an os a students a programmers an application users and a technicians chapter introduction user user interface nonkernel routines kernel computer hardware figure a designers abstract view of an os illustrates this view each part consists of a number of routines the typical functionalities of these parts are as follows user interface the user interface accepts commands to execute programs and use resources and services provided by the operating system it is either a command line interface as in unix or linux which displays a command prompt to the user and accepts a user command or is a graphical user interface gui as in the windows operating system which interprets mouse clicks on icons as user commands nonkernel routines these routines implement user commands concerning execution of programs and use of the computers resources they are invoked by the user interface kernel the kernel is the core of the os it controls operation of the computer and provides a set of functions and services to use the cpu memory and other resources of the computer the functions and services of the kernel are invoked by the nonkernel routines and by user programs two features of an os emerge from the designers view of an os shown in figure the os is actually a collection of routines that facilitate execution of user programs and use of resources in a computer system it contains a hierarchical arrangement of layers in which routines in a higher layer use the facilities provided by routines in the layer below it in fact each layer takes an abstract view of the layer below it in which the next lower layer is a machine that can understand certain commands the fact that the lower layer is a set of routines rather than a whole computer system makes no difference to the higher layer each higher layer acts as a more capable machine than the layer below it to the user the user interface appears like a machine that understands commands in the command language of the os throughout this book we will use abstract views to present the design of os components this has two key benefits managing complexity an abstract view of a system contains only selected features of the system this property is useful in managing complexity during design or study of a system for example an abstract view of how an os scheduling in practice to provide a suitable combination of system performance and user service an operating system has to adapt its operation to the nature and number of user requests and availability of resources a single scheduler using a classical scheduling policy can not address all these issues effectively hence a modern os employs several schedulers up to three schedulers as we shall see later and some of the schedulers may use a combination of different scheduling policies chapter scheduling long medium and shortterm schedulers these schedulers perform the following functions longterm scheduler decides when to admit an arrived process for scheduling depending on its nature whether cpubound or iobound and on availability of resources like kernel data structures and disk space for swapping mediumterm scheduler decides when to swapout a process from memory and when to load it back so that a sufficient number of ready processes would exist in memory shortterm scheduler decides which ready process to service next on the cpu and for how long thus the shortterm scheduler is the one that actually selects a process for operation hence it is also called the process scheduler or simply the scheduler figure shows an overview of scheduling and related actions as discussed in sections and the operation of the kernel is interruptdriven every event that requires the kernels attention causes an interrupt the interrupt processing interrupts interrupt processing routine pcb lists start memory suspend create event ecb lists io handler resume terminate handlers process process longterm scheduler mediumterm schedulers scheduler shortterm scheduler control flow data flow dispatcher figure event handling and scheduling part process management routine performs a context save function and invokes an event handler the event handler analyzes the event and changes the state of the process if any affected by it it then invokes the longterm mediumterm or shortterm scheduler as appropriate for example the event handler that creates a new process invokes the longterm scheduler event handlers for suspension and resumption of processes see section invoke the mediumterm scheduler and the memory handler may invoke the mediumterm scheduler if it runs out of memory most other event handlers directly invoke the shortterm scheduler longterm scheduling the longterm scheduler may defer admission of a request for two reasons it may not be able to allocate sufficient resources like kernel data structures or io devices to a request when it arrives or it may find that admission of a request would affect system performance in some way eg if the system currently contained a large number of cpubound requests the scheduler might defer admission of a new cpubound request but it might admit a new iobound request right away longterm scheduling was used in the s and s for job scheduling because computer systems had limited resources so a longterm scheduler was required to decide whether a process could be initiated at the present time it continues to be important in operating systems where resources are limited it is also used in systems where requests have deadlines or a set of requests are repeated with a known periodicity to decide when a process should be initiated to meet response requirements of applications longterm scheduling is not relevant in other operating systems mediumterm scheduling mediumterm scheduling maps the large number of requests that have been admitted to the system into the smaller number of requests that can fit into the memory of the system at any time thus its focus is on making a sufficient number of ready processes available to the shortterm scheduler by suspending or reactivating processes the mediumterm scheduler decides when to swap out a process from memory and when to swap it back into memory changes the state of the process appropriately and enters its process control block pcb in the appropriate list of pcbs the actual swappingin and swappingout operations are performed by the memory manager the kernel can suspend a process when a user requests suspension when the kernel runs out of free memory or when it finds that the cpu is not likely to be allocated to the process in the near future in timesharing systems processes in blocked or ready states are candidates for suspension see figure the decision to reactivate a process is more involved the mediumterm scheduler considers the position occupied by a process in the scheduling list estimates when it is likely to be scheduled next and swaps it in ahead of this time shortterm scheduling shortterm scheduling is concerned with effective use of the cpu it selects one process from a list of ready processes and hands it to the dispatching mechanism it may also decide how long the process should chapter scheduling lists of processes arrived processes longterm scheduler ready swapped blocked swapped processes swapout swapin mediumterm blocked scheduler processes ready processes shortterm scheduler cpu figure long medium and shortterm scheduling in a timesharing system be allowed to use the cpu and instruct the kernel to produce a timer interrupt accordingly example illustrates long medium and shortterm scheduling in a timesharing os long medium and shortterm scheduling in timesharing example figure illustrates scheduling in a timesharing operating system the longterm scheduler admits a process when kernel resources like control blocks swap space on a disk and other resources like io devices whether real or virtual can be allocated to it the kernel copies the code of the process into the swap space and adds the process to the list of swappedout processes the mediumterm scheduler controls swapping of processes and decides when to move processes between the ready swapped and ready lists and between the blocked swapped and blocked lists see figure whenever the cpu is free the shortterm scheduler selects one process from the ready list for execution the dispatching mechanism initiates or resumes operation of the selected process on the cpu a process may shuttle between the medium and shortterm schedulers many times as a result of swapping part process management process pcb scheduler lists scheduling process context priority mechanisms dispatching save computation reordering control flow hardware data flow figure a schematic of the process scheduler scheduling data structures and mechanisms figure is a schematic diagram of the process scheduler it uses several lists of pcbs whose organization and use depends on the scheduling policy the process scheduler selects one process and passes its id to the process dispatching mechanism the process dispatching mechanism loads contents of two pcb fields the program status word psw and generalpurpose registers gprs fields into the cpu to resume operation of the selected process thus the dispatching mechanism interfaces with the scheduler on one side and the hardware on the other side the context save mechanism is a part of the interrupt processing routine when an interrupt occurs it is invoked to save the psw and gprs of the interrupted process the priority computation and reordering mechanism recomputes the priority of requests and reorders the pcb lists to reflect the new priorities this mechanism is either invoked explicitly by the scheduler when appropriate or invoked periodically its exact actions depend on the scheduling policy in use one question faced by all schedulers is what should the scheduler do if there are no ready processes it has no work for the cpu to perform however the cpu must remain alert to handle any interrupts that might activate one of the blocked processes a kernel typically achieves it by executing an idle loop which is an endless loop containing noop instructions when an interrupt causes a blocked ready transition for some process scheduling would be performed again and that process would get scheduled however execution of the idle loop wastes power in section we discuss alternative arrangements that conserve power when there are no ready processes in the system prioritybased scheduling figure shows an efficient arrangement of scheduling data for prioritybased scheduling a separate list of ready processes is maintained for each priority value chapter scheduling p p p highestpriority queue p p lowerthanhighest priority queue p p lowestpriority queue figure ready queues in prioritybased scheduling this list is organized as a queue of pcbs in which a pcb points to the pcb of the next process in the queue the header of a queue contains two pointers one points to the pcb of the first process in the queue and the other points to the header of the queue for the next lower priority the scheduler scans the headers in the order of decreasing priority and selects the first process in the first nonempty queue it can find this way the scheduling overhead depends on the number of distinct priorities rather than on the number of ready processes prioritybased scheduling can lead to starvation of lowpriority processes as discussed in section the technique of aging of processes which increases the priority of a ready process if it does not get scheduled within a certain period of time can be used to overcome starvation in this scheme process priorities would be dynamic so the pcb of a process would be moved between the different ready queues shown in figure starvation in prioritybased scheduling can also lead to an undesirable situation called priority inversion consider a highpriority process that needs a resource that is currently allocated to a lowpriority process if the lowpriority process faces starvation it can not use and release the resource consequently the highpriority process remains blocked indefinitely this situation is addressed through the priority inheritance protocol which temporarily raises the priority of the lowpriority process holding the resource to the priority value of the highpriority process that needs the resource the process holding the resource can now obtain the cpu use the resource and release it the kernel changes its priority back to the earlier value when it releases the resource roundrobin scheduling with timeslicing roundrobin scheduling can be implemented through a single list of pcbs of ready processes this list is organized as a queue the scheduler always removes the first pcb from the queue and schedules the process described by it if the time slice elapses the pcb of the process is put at the end of the queue if a process starts an io operation its pcb is added at the end of the queue when its io operation completes thus the pcb of a ready process moves toward the head of the queue until the process is scheduled part process management multilevel scheduling the multilevel scheduling policy combines prioritybased scheduling and roundrobin scheduling to provide a good combination of system performance and response times a multilevel scheduler maintains a number of ready queues a priority and a time slice are associated with each ready queue and roundrobin scheduling with timeslicing is performed within it the queue at a high priority level has a small time slice associated with it which ensures good response times for processes in this queue while the queue at a low priority level has a large time slice which ensures low process switching overhead a process at the head of a queue is scheduled only if the queues for all higher priority levels are empty scheduling is preemptive so a process is preempted when a new process is added to a queue at a higher priority level as in roundrobin scheduling with timeslicing when a process makes an io request or is swapped out its pcb is removed from the ready queue when the io operation completes or the process is swapped in its pcb is added at the end of that ready queue where it existed earlier to benefit from the features of multilevel scheduling the kernel puts highly interactive processes in the queue at the highest priority level the small time slice associated with this queue is adequate for these processes so they receive good response times see eq moderately interactive processes are put in a ready queue at a medium priority level where they receive larger time slices noninteractive processes are put in a ready queue at one of the low priority levels these processes receive a large time slice which reduces the scheduling overhead example multilevel scheduling figure illustrates ready queues in a multilevel scheduler processes p and p have a larger time slice than processes p p and p however they get a chance to execute only when p p and p are blocked processes p and p can execute only when all other processes in the system are blocked thus these two processes would face starvation if this situation is rare the multilevel scheduling policy uses static priorities hence it inherits the fundamental shortcoming of prioritybased scheduling employed in multiprogramming systems a process is classified a priori into a cpubound process or an iobound process for assignment of priority if wrongly classified an iobound process may receive a low priority which would affect both user service and system performance or a cpubound process may receive a high priority which would affect system performance as a result of static priorities the multilevel scheduling policy also can not handle a change in the computational or io behavior of a process can not prevent starvation of processes in low priority levels see example and can not employ the priority inheritance protocol to overcome priority inversion see section all these problems are addressed by the multilevel adaptive scheduling policy chapter scheduling multilevel adaptive scheduling in multilevel adaptive scheduling which is also called multilevel feedback scheduling the scheduler varies the priority of a process such that the process receives a time slice that is consistent with its requirement for cpu time the scheduler determines the correct priority level for a process by observing its recent cpu and io usage and moves the process to this level this way a process that is iobound during one phase in its operation and cpubound during another phase will receive an appropriate priority and time slice at all times this feature eliminates the problems of multilevel scheduling described earlier ctss a timesharing os for the ibm in the s is a wellknown example of multilevel adaptive scheduling the system used an eightlevel priority structure with the levels numbered through being the highestpriority level and being the lowestpriority level level number n had a time slice of n cpu seconds associated with it at initiation each user process was placed at level or depending on its memory requirement it was promoted or demoted in the priority structure according to the following rules if a process completely used up the time slice at its current priority level ie it did not initiate an io operation it was demoted to the next higher numbered level whereas if a process spent more than a minute in ready state in its current priority level without obtaining any cpu service it was promoted to the next lower numbered level further any process performing io on the user terminal was promoted to level subsequently it would be moved to the correct priority level through possible demotions fair share scheduling a common criticism of all scheduling policies discussed so far is that they try to provide equitable service to processes rather than to users or their applications if applications create different numbers of processes an application employing more processes is likely to receive more cpu attention than an application employing fewer processes the notion of a fair share addresses this issue a fair share is the fraction of cpu time that should be devoted to a group of processes that belong to the same user or the same application it ensures an equitable use of the cpu by users or applications the actual share of cpu time received by a group of processes may differ from the fair share of the group if all processes in some of the groups are inactive for example consider five groups of processes gg each having a percent share of cpu time if all processes in g are blocked processes of each of the other groups should be given percent of the available cpu time so that cpu time is not wasted what should the scheduler do when processes of g become active after some time should it give them only percent of cpu time after they wake up because that is their fair share of cpu time or should it give them all the available cpu time until their actual cpu consumption since inception becomes percent lottery scheduling which we describe in the following and the scheduling policies used in the unix and solaris operating systems see section differ in the way they handle this situation part process management lottery scheduling is a novel technique proposed for sharing a resource in a probabilistically fair manner lottery tickets are distributed to all processes sharing a resource in such a manner that a process gets as many tickets as its fair share of the resource for example a process would be given five tickets out of a total of tickets if its fair share of the resource is percent when the resource is to be allocated a lottery is conducted among the tickets held by processes that actively seek the resource the process holding the winning ticket is then allocated the resource the actual share of the resources allocated to the process depends on contention for the resource lottery scheduling can be used for fair share cpu scheduling as follows tickets can be issued to applications or users on the basis of their fair share of cpu time an application can share its tickets among its processes in any manner it desires to allocate a cpu time slice the scheduler holds a lottery in which only tickets of ready processes participate when the time slice is a few milliseconds this scheduling method provides fairness even over fractions of a second if all groups of processes are active kernel preemptibility kernel preemptibility plays a vital role in ensuring effectiveness of a scheduler a noninterruptible kernel can handle an event without getting further interrupted so event handlers have a mutually exclusive access to the kernel data structures without having to use data access synchronization however if event handlers have large running times noninterruptibility also causes a large kernel latency as the kernel can not respond readily to interrupts this latency which could be as much as ms in computers with slow cpus causes a significant degradation of response times and a slowdown of the os operation when the scheduling of a highpriority process is delayed because the kernel is handling an event concerning a lowpriority process it even causes a situation analogous to priority inversion making the kernel preemptible would solve this problem now scheduling would be performed more often so a highpriority process that is activated by an interrupt would get to execute sooner scheduling heuristics schedulers in modern operating systems use many heuristics to reduce their overhead and to provide good user service these heuristics employ two main techniques use of a time quantum variation of process priority a time quantum is the limit on cpu time that a process may be allowed to consume over a time interval it is employed as follows each process is assigned a priority and a time quantum a process is scheduled according to its priority provided it has not exhausted its time quantum as it operates the amount of cpu time used by it is deducted from its time quantum after a process has exhausted its time quantum it would not be considered for scheduling unless chapter scheduling the kernel grants it another time quantum which would happen only when all active processes have exhausted their quanta this way the time quantum of a process would control the share of cpu time used by it so it can be employed to implement fair share scheduling process priority could be varied to achieve various goals the priority of a process could be boosted while it is executing a system call so that it would quickly complete execution of the call release any kernel resources allocated to it and exit the kernel this technique would improve response to other processes that are waiting for the kernel resources held by the process executing the system call priority inheritance could be implemented by boosting the priority of a process holding a resource to that of the highestpriority process waiting for the resource process priority may also be varied to more accurately characterize the nature of a process when the kernel initiates a new process it has no means of knowing whether the process is iobound or cpubound so it assigns a default priority to the process as the process operates the kernel adjusts its priority in accordance with its behavior using a heuristic of the following kind when the process is activated after some period of blocking its priority may be boosted in accordance with the cause of blocking for example if it was blocked because of an io operation its priority would be boosted to provide it a better response time if it was blocked for a keyboard input it would have waited for a long time for the user to respond so its priority may be given a further boost if a process used up its time slice completely its priority may be reduced because it is more cpubound than was previously assumed power management when no ready processes exist the kernel puts the cpu into an idle loop see section this solution wastes power in executing useless instructions in powerstarved systems such as embedded and mobile systems it is essential to prevent this wastage of power to address this requirement computers provide special modes in the cpu when put in one of these modes the cpu does not execute instructions which conserves power however it can accept interrupts which enables it to resume normal operation when desired we will use the term sleep mode of the cpu generically for such modes some computers provide several sleep modes in the light sleep mode the cpu simply stops executing instructions in a heavy sleep mode the cpu not only stops executing instructions but also takes other steps that reduce its power consumption eg slowing the clock and disconnecting the cpu from the system bus ideally the kernel should put the cpu into the deepest sleep mode possible when the system does not have processes in the ready state however a cpu takes a longer time to wake up from a heavy sleep mode than it would from a light sleep mode so the kernel has to make a tradeoff here it starts by putting the cpu in the light sleep mode if no processes become ready for some more time it puts the cpu into a heavier sleep mode and so on this way it provides a tradeoff between the need for power saving and responsiveness of the system realtime scheduling realtime scheduling must handle two special scheduling constraints while trying to meet the deadlines of applications first the processes within a realtime application are interacting processes so the deadline of an application should be translated into appropriate deadlines for the processes second processes may be periodic so different instances of a process may arrive at fixed intervals and all of them have to meet their deadlines example illustrates these constraints in this section we discuss techniques used to handle them example dependences and periods in a realtime application consider a restricted form of the realtime data logging application of example in which the bufferarea can accommodate a single data sample since samples arrive at the rate of samples per second the response requirement of the application is ms hence processes copysample and recordsample must operate one after another and complete their operation within ms if process recordsample requires ms for its operation process copysample has a deadline of ms after arrival of a message since a new sample arrives every ms each of the processes has a period of ms process precedences and feasible schedules processes of a realtime application interact among themselves to ensure that they perform their actions in a desired order see section we make the simplifying assumption that such interaction takes place only at the start or end of a process it causes dependences between processes which must be taken into account while determining deadlines and while scheduling we use a process precedence graph ppg to depict such dependences between processes process pi is said to precede process pj if execution of pi must be completed before pj can begin its execution the notation pi pj shall indicate that process pi directly precedes process pj the precedence relation is transitive ie pi pj and pj pk implies that pi precedes pk the notation pi pk is used to indicate that process pi directly or indirectly precedes pk a process precedence graph is a directed graph g n e such that pi n represents a process and an edge pi pj e implies pi pj thus a path pi pk in ppg implies pi pk a process pk is a descendant of pi if pi pk chapter scheduling in section we defined a hard realtime system as one that meets the response requirement of a realtime application in a guaranteed manner even when fault tolerance actions are required this condition implies that the time required by the os to complete operation of all processes in the application does not exceed the response requirement of the application on the other hand a soft realtime system meets the response requirement of an application only in a probabilistic manner and not necessarily at all times the notion of a feasible schedule helps to differentiate between these situations definition feasible schedule a sequence of scheduling decisions that enables the processes of an application to operate in accordance with their precedences and meet the response requirement of the application realtime scheduling focuses on implementing a feasible schedule for an application if one exists consider an application for updating airline departure information on displays at second intervals it consists of the following independent processes where process p handles an exceptional situation that seldom occurs process p p p p p service time a feasible schedule does not exist for completing all five processes in seconds so a deadline overrun would occur however several schedules are possible when process p is not active the scheduler in a soft realtime system can use any one of them table summarizes three main approaches to realtime scheduling we discuss the features and properties of these scheduling approaches in the following table approaches to realtime scheduling approach description static scheduling a schedule is prepared before operation of the realtime application begins process interactions periodicities resource constraints and deadlines are considered in preparing the schedule prioritybased the realtime application is analyzed to assign scheduling appropriate priorities to processes in it conventional prioritybased scheduling is used during operation of the application dynamic scheduling scheduling is performed when a request to create a process is made process creation succeeds only if response requirement of the process can be satisfied in a guaranteed manner part process management static scheduling as the name indicates a schedule is prepared before the system is put into operation the schedule considers process precedences periodicities resource constraints and possibilities of overlapping io operations in some processes with computations in other processes this schedule is represented in the form of a table whose rows indicate when operation of different processes should begin no scheduling decisions are made during operation of the system the realtime os simply consults the table and starts operation of processes as indicated in it static scheduling leads to negligible scheduling overhead during system operation however it is inflexible and can not handle issues like fault tolerance the size of the scheduling table will depend on periods of processes if all processes have the same period or if processes are nonperiodic the scheduling table will have only as many rows as the number of processes in the application this schedule is used repeatedly during operation of the system if periodicities of processes are different the length of the schedule that needs to be represented in the scheduling table will be the least common multiple of periodicities of all processes in the application prioritybased scheduling a system analyst uses two considerations while assigning priorities to processes criticality of processes and periodicity of processes a process with a smaller period must complete its operation earlier than a process with a larger period so it must have a higher priority this approach has the benefits and drawbacks normally associated with the use of priorities it provides graceful degradation capabilities because critical functions would continue to be performed even when failures occur however it incurs scheduling overhead during operation dynamic scheduling in systems using the dynamic scheduling approach scheduling is performed during the systems operation multimedia systems like video on demand use a dynamic scheduling approach in which a scheduling decision is performed when a process arrives a request to initiate a process contains information such as the processs resource requirement service time and a deadline or a specification of service quality on receiving such a request the scheduler checks whether it is possible to assign the resources needed by the process and meet its deadline or provide it the desired quality of service it creates the process only if these checks succeed another approach to dynamic scheduling is to optimistically admit processes for execution in this approach there is no guarantee that the deadline or service quality requirements can be met soft realtime systems often follow this approach deadline scheduling two kinds of deadlines can be specified for a process a starting deadline ie the latest instant of time by which operation of the process must begin and a completion deadline ie the time by which operation of the process must complete we consider only completion deadlines in the following chapter scheduling p p p p p p figure the process precedence graph ppg for a realtime system deadline estimation a system analyst performs an indepth analysis of a realtime application and its response requirements deadlines for individual processes are determined by considering process precedences and working backward from the response requirement of the application accordingly di the completion deadline of a process pi is di dapplication k descendanti xk where dapplication is the deadline of the application xk is the service time of process pk and descendanti is the set of descendants of pi in the ppg ie the set of all processes that lie on some path between pi and the exit node of the ppg thus the deadline for a process pi is such that if it is met all processes that directly or indirectly depend on pi can also finish by the overall deadline of the application this method is illustrated in example determining process deadlines example figure shows the ppg of a realtime application containing processes each circle is a node of the graph and represents a process the number in a circle indicates the service time of a process an edge in the ppg shows a precedence constraint thus process p can be initiated only after process p completes process p can be initiated only after processes p and p complete etc we assume that processes do not perform io operations and are serviced in a nonpreemptive manner the total of the service times of the processes is seconds if the application has to produce a response in seconds the deadlines of the processes would be as follows process p p p p p p deadline a practical method of estimating deadlines will have to incorporate several other constraints as well for example processes may perform io if an io part process management operation of one process can be overlapped with execution of some independent process the deadline of its predecessors and ancestors in the ppg can be relaxed by the amount of io overlap independent processes were formally defined in section for example processes p and p in figure are independent of one another if the service time of p includes second of io time the deadline of p can be made seconds instead of seconds if the io operation of p can overlap with ps processing however overlapped execution of processes must consider resource availability as well hence determination of deadlines is far more complex than described here earliest deadline first edf scheduling as its name suggests this policy always selects the process with the earliest deadline consider a set of realtime processes that do not perform io operations if seq is the sequence in which processes are serviced by a deadline scheduling policy and pospi is the position of process pi in seq a deadline overrun does not occur for process pi only if the sum of its own service time and service times of all processes that precede it in seq does not exceed its own deadline ie kpospk pospi xk di where xk is the service time of process pk and di is the deadline of process pi if this condition is not satisfied a deadline overrun will occur for process pi when a feasible schedule exists it can be shown that condition holds for all processes ie a deadline overrun will not occur for any process table illustrates operation of the edf policy for the deadlines of example the notation p in the column processes in system indicates that process p has the deadline processes p p and p p have identical deadlines so three schedules other than the one shown in table are possible with edf scheduling none of them would incur deadline overruns the primary advantages of edf scheduling are its simplicity and nonpreemptive nature which reduces the scheduling overhead edf scheduling is a good policy for static scheduling because existence of a feasible schedule which can be checked a priori ensures that deadline overruns do not occur it is also table operation of earliest deadline first edf scheduling process deadline process time completed overrun processes in system scheduled p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p chapter scheduling a good dynamic scheduling policy for use in soft realtime system however the number of processes that miss their deadlines is unpredictable the next example illustrates this aspect of edf scheduling problems of edf scheduling example consider the ppg of figure with the edge p p removed it contains two independent applications one contains the processes pp and p while the other contains p alone if all processes are to complete by seconds a feasible schedule does not exist now deadlines of the processes determined by using eq are as follows process p p p p p p deadline edf scheduling may schedule the processes either in the sequence p p p p p p which is the same as in table or in the sequence p p p p p p processes p and p miss their deadlines in the first sequence whereas only process p misses its deadline in the second sequence we can not predict which sequence will be chosen by an implementation of edf scheduling so the number of processes that miss their deadlines is unpredictable rate monotonic scheduling when processes in an application are periodic the existence of a feasible schedule can be determined in an interesting way consider three independent processes that do not perform io operations process p p p time period ms service time ms process p repeats every ms and needs ms of cpu time so the fraction of the cpus time that it uses is ie the fractions of cpu time used by p and p are analogously and ie and they add up to so if the cpu overhead of os operation is negligible it is feasible to service these three processes in general a set of periodic processes p pn that do not perform io operations can be serviced by a hard realtime system that has a negligible overhead if xi in ti where ti is the period of pi and xi is its service time part process management p p p time ms figure operation of realtime processes using rate monotonic scheduling we still have to schedule these processes so that they can all operate without missing their deadlines the rate monotonic rm scheduling policy does it as follows it determines the rate at which a process has to repeat ie the number of repetitions per second and assigns the rate itself as the priority of the process it now employs a prioritybased scheduling technique to perform scheduling this way a process with a smaller period has a higher priority which would enable it to complete its operation early in the above example priorities of processes p p and p would be and ie and respectively figure shows how these processes would operate process p would be scheduled first it would execute once and become dormant after ms because x ms now p would be scheduled and would complete after ms p would be scheduled now but it would be preempted after ms because p becomes ready for the second time and so on as shown in figure process p would complete at ms by this time p has executed three times and p has executed two times rate monotonic scheduling is not guaranteed to find a feasible schedule in all situations for example if process p had a time period of ms its priority would be different however relative priorities of the processes would be unchanged so p would complete at ms as before thereby suffering a deadline overrun of ms a feasible schedule would have been obtained if p had been scheduled at ms and p at ms however it is not possible under rm scheduling because processes are scheduled in a prioritybased manner liu and layland have shown that rm scheduling may not be able to avoid deadline overruns if the total fraction of cpu time used by the processes according to eq exceeds mm where m is the number of processes this expression has a lower bound of which implies that if an application has a large number of processes rm scheduling may not be able to achieve more than percent cpu utilization if it is to meet deadlines of processes liu and layland also report a deadlinedriven scheduling algorithm that dynamically assigns priorities to processes based on their current deadlines a process with an earlier deadline is assigned a higher priority than a process with a later deadline it can avoid deadline overruns even when the fraction of eq has the value that is it can achieve percent cpu utilization however case studies scheduling in unix unix is a pure timesharing operating system it uses a multilevel adaptive scheduling policy in which process priorities are varied to ensure good system performance and also to provide good user service processes are allocated numerical priorities where a larger numerical value implies a lower effective priority in unix bsd the priorities are in the range to processes in the user mode have priorities between and while those in the kernel mode have priorities between and when a process is blocked in a system call its priority is changed to a value in the range depending on the cause of blocking when it becomes active again it executes the remainder of the system call with this priority this arrangement ensures that the process would be scheduled as soon as possible complete the task it was performing in the kernel mode and release kernel resources when it exits the kernel mode its priority reverts to its previous value which was in the range unix uses the following formula to vary the priority of a process process priority base priority for user processes f cpu time used recently nice value it is implemented as follows the scheduler maintains the cpu time used by a process in its process table entry this field is initialized to the realtime clock raises an interrupt times a second and the clock handler increments the count in the cpu usage field of the running process the scheduler recomputes process priorities every second in a loop for each process it divides the value in the cpu usage field by stores it back and also uses it as the value of f recall that a large numerical value implies a lower effective priority so the second factor in eq lowers the priority of a process the division by ensures that the effect of cpu time used by a process decays ie it wears off over a period of time to avoid the problem of starvation faced in the least completed next lcn policy see section a process can vary its own priority through the last factor in eq the system call nicepriority value sets the nice value of a user process it takes a zero or positive value as its argument thus a process can only decrease its effective priority to be nice to other processes it would typically do this when it enters a cpubound phase part process management table operation of a unixlike scheduling policy when processes perform io p p p p p scheduled time p t p t p t p t p t process p p p p p p p p p example process scheduling in unix table summarizes operation of the unix scheduling policy for the processes in table it is assumed that process p is an io bound process that initiates an io operation lasting seconds after using the cpu for seconds and none of the other processes perform io the t field indicates the cpu time consumed by a process and the p field contains its priority the scheduler updates the t field of a process times a second and recomputes process priorities once every second the time slice is second and the base priority of user processes is the first line of table shows that at second only p is present in the system its t field contains hence its priority is two lines are shown for the time second the first line shows the t fields of processes at second while the second line shows the p and t fields after the priority computation actions at second at the end of the time slice the contents of the t field of p are the decaying action of dividing the cpu time by reduces it to and so the priority of p becomes at seconds the effective priority of p is smaller than that of p because their t fields contain and respectively and so p is scheduled similarly p is scheduled at seconds since p uses the cpu for only second before starting an io operation it has a higher priority than p when scheduling is performed at seconds hence it is scheduled ahead of process p it is again scheduled at seconds this feature corrects the bias against iobound processes exhibited by pure roundrobin scheduling chapter scheduling table operation of fair share scheduling in unix p p p p p scheduled time p c g p c g p c g p c g p c g process p p p p p p p p p p p p p p p p fair share scheduling to ensure a fair share of cpu time to groups of processes unix schedulers add the term f cpu time used by processes in the group to eq thus priorities of all processes in a group reduce when any of them consumes cpu time this feature ensures that processes of a group would receive favored treatment if none of them has consumed much cpu time recently the effect of the new factor also decays over time fair share scheduling in unix example table depicts fair share scheduling of the processes of table fields p t and g contain process priority cpu time consumed by a process and cpu time consumed by a group of processes respectively two process groups exist the first group contains processes p p p and p while the second group contains process p all by itself at seconds process p has just arrived its effective priority is low because process p which is in the same group has executed for seconds however p does not have a low priority when it arrives because the cpu time already consumed by its group is as expected process p receives a favored treatment compared to other processes in fact it receives every alternate time slice processes p p and p suffer because they belong to the same process group these facts are reflected in the turnaround times and weighted part process management turnarounds of the processes which are as follows process p p p p p completion time turnaround time weighted turnaround mean turnaround time ta seconds mean weighted turnaround w scheduling in solaris solaris supports four classes of processes timesharing processes interactive processes system processes and realtime processes a time slice is called a time quantum in solaris terminology timesharing and interactive processes have priorities between and where a larger number implies a higher priority system processes have priorities between and they are not timesliced realtime processes have priorities between and and are scheduled by a roundrobin policy within a priority level threads used for interrupt servicing have priorities between and scheduling of timesharing and interactive processes is governed by a dispatch table for each priority level the table specifies how the priority of a process should change to suit its nature whether cpubound or iobound and also to prevent starvation use of the table rather than a priority computation rule as in unix provides finegrained tuning possibilities to the system administrator the dispatch table entry for each priority level contains the following values tsquantum the time quantum for processes of this priority level tstqexp the new priority of a process that uses its entire time quantum tsslpret the new priority of a process that blocks before using its complete time quantum tsmaxwait the maximum amount of time for which a process can be allowed to wait without getting scheduled tslwait the new priority of a process that does not get scheduled within tsmaxwait time a process that blocks before its time quantum elapses is assumed to be an iobound process its priority is changed to tsslpret which is a higher priority than its present priority analogously a process that uses its entire time quantum is assumed to be a cpubound process so tstqexp is a lower priority tsmaxwait is used to avoid starvation hence tslwait is a higher priority in addition to these changes in priority effected by the kernel a process can change its own priority through the nice system call with a number in the range to as a parameter solaris also supports a fair share scheduling class a group of processes is called a project and is assigned a few shares of cpu time the fair share of chapter scheduling a project at any time depends on the shares of other projects that are active concurrently it is the quotient of the shares of the project and the sum of the shares of all those projects that have at least one process active in multiprocessor systems shares are defined independently for each cpu solaris added the notion of zones on top of projects cpu shares are now assigned for both zones and projects to provide twolevel scheduling scheduling in linux linux supports both realtime and nonrealtime applications accordingly it has two classes of processes the realtime processes have static priorities between and where is the highest priority realtime processes can be scheduled in two ways fifo or roundrobin within each priority level the kernel associates a flag with each process to indicate how it should be scheduled nonrealtime processes have lower priorities than all realtime processes their priorities are dynamic and have numerical values between and where is the highest priority effectively the kernel has priority levels to start with each nonrealtime process has the priority the priority can be varied by the process itself through the nice or setpriority system calls however special privileges are needed to increase the priority through the nice system call so processes typically use this call to lower their priorities when they wish to be nice to other processes in addition to such priority variation the kernel varies the priority of a process to reflect its iobound or cpubound nature to implement this the kernel maintains information about how much cpu time the process has used recently and for how long it was in the blocked state and adds a bonus between and to the nice value of the process thus a highly interactive process would have an effective priority of nice while a cpubound process would have an effective priority of nice because of the multilevel priority structure the linux kernel organizes its scheduling data as shown in figure of section to limit the scheduling overhead linux uses a scheduler schematic analogous to figure thus scheduling is not performed after every event handling action it is performed when the currently executing process has to block due to a system call or when the needresched flag has been set by an event handling action this is done while handling expiry of the time slice or while handling an event that activates a process whose priority is higher than that of the currently executing process nonrealtime processes are scheduled by using the notion of a time slice however the linux notion of a time slice is actually a time quantum that a process can use over a period of time in accordance with its priority see section a process that exhausts its time slice would receive a new time slice only after all processes have exhausted their time slices linux uses time slices in the range of to ms to ensure that a higherpriority process would receive more cpu attention than a lowerpriority process linux assigns a larger time slice to a higherpriority process this assignment of time slices does not affect response part process management times because a highpriority process would be interactive in nature hence it would perform an io operation before using much cpu time the linux scheduler uses two lists of processes an active list and an exhausted list both lists are ordered by priorities of processes and use the data structure described earlier the scheduler schedules a process from the active list which uses time from its time slice when its time slice is exhausted it is put into the exhausted list schedulers in linux kernel and earlier kernels executed a priority recomputation loop when the active list became empty the loop computed a new time slice for each process based on its dynamic priority at the end of the loop all processes were transferred to the active list and normal scheduling operation was resumed the linux kernel uses a new scheduler that incurs less overhead and scales better with the number of processes and cpus the scheduler spreads the priority recomputation overhead throughout the schedulers operation rather than lump it in the recomputation loop it achieves this by recomputing the priority of a process when the process exhausts its time slice and gets moved to the exhausted list when the active list becomes empty the scheduler merely interchanges the active and exhausted lists the scalability of the scheduler is ensured in two ways the scheduler has a bit flag to indicate whether the list of processes for a priority level is empty when invoked the scheduler tests the flags of the process lists in the order of reducing priority and selects the first process in the first nonempty process list it finds this procedure incurs a scheduling overhead that does not depend on the number of ready processes it depends only on the number of scheduling levels hence it is bound by a constant this scheduling is called o ie order scheduling schedulers in older linux kernels used a synchronization lock on the active list of processes to avoid race conditions when many cpus were supported the linux kernel maintains active lists on a percpu basis which eliminates the synchronization lock and associated delays this arrangement also ensures that a process operates on the same cpu every time it is scheduled it helps to ensure better cache hit ratios scheduling in windows windows scheduling aims at providing good response times to realtime and interactive threads scheduling is prioritydriven and preemptive scheduling within a priority level is performed through a roundrobin policy with timeslicing a time slice is called a quantum in windows terminology priorities of nonrealtime threads are dynamically varied to favor interactive threads this aspect is analogous to multilevel adaptive scheduling see section realtime threads are given higher priorities than other threads they have priorities in the range while other threads have priorities in the range priorities of nonrealtime threads can vary during their lifetime hence this class of threads is also called the variable priority class the effective priority of a thread in this class at any moment is a combination of three factors the base priority of the process to which the thread belongs the base priority of the thread which chapter scheduling is in the range to and a dynamic component assigned by the kernel to favor interactive threads the kernel varies a threads dynamic component of priority as follows if the thread uses up its complete time slice when scheduled its priority is reduced by when a waiting ie blocked thread is activated it is given a priority increase based on the nature of the event on which it was blocked if it was blocked on input from the keyboard its priority is boosted by to deny an unfair advantage to an iobound thread the remaining time of its current quantum is reduced by one clock tick every time it makes an io request to guard against starvation the priority of a ready thread that has not received cpu time for more than seconds is raised to and its quantum is increased to twice its normal value when this quantum expires its priority and quantum revert back to their old values the scheduler uses a data structure resembling that shown in figure except for two refinements that provide efficiency since priority values lie in the range with priority reserved for a system thread an array of pointers is used to point at the queues of ready threads at different priority levels a vector of bit flags is used to indicate whether a ready thread exists at each of the priority levels this arrangement enables the scheduler to speedily locate the first thread in the highestpriority nonempty queue when none of the system or user threads is in the ready state the scheduler schedules a special idle thread on the cpu that continually executes an idle loop until a thread is scheduled on it in the loop it activates functions in the hardware abstraction layer hal at appropriate times to perform power management in a multiprocessor system the scheduler operating on one cpu may schedule a thread on another cpu that is idle see section to facilitate such scheduling the idle loop also examines the scheduling data structures to check whether a thread has been scheduled on the cpu that is executing the idle loop and switches the cpu to the scheduled thread if this is the case to conserve power when the computer is idle windows provides a number of system states wherein the computer operates in a mode that consumes low power in the hibernate state the states of running applications are stored on the disk and the system is turned off when the system is activated application states are restored from the disk before operation is resumed use of the disk to store application states leads to slow resumption however it provides reliability because operation of the computer is immune to loss or depletion of power while the computer is in hibernation in the standby state states of running applications are saved in memory and the computer enters a lowpower mode of operation resumption using the application states stored in memory is faster however the state information would be lost if power is lost or depleted while the system is in the standby state so computer operation is not reliable hence windows vista introduced a new hybrid state called the sleep state wherein the application states are stored both in memory and on the disk system operation is resumed as in the standby state if application states are available in memory otherwise it is resumed as in the hibernate state using the application states stored on the disk performance analysis of scheduling policies performance analysis of a scheduling policy is a study of its performance using measures such as response time of a process efficiency of use of the cpu and throughput of the system performance analysis can be used to compare performance of alternative scheduling policies and to determine good values of key system parameters like the time slice number of active users and the size of the list of ready processes performance of a scheduling policy is sensitive to the nature of requests directed at it and so performance analysis should be conducted in the environment in which the policy is to be put into effect the set of requests directed at a scheduling policy is called its workload the first step in performance analysis of a policy is to accurately characterize its typical workload in the following we discuss some issues involved in this step as mentioned in section in the context of the srn policy user estimates of service times are not reliable either because users lack the experience to provide good estimates of service time or because knowledgeable users may provide misleading estimates to obtain a favored treatment from the system some users may even resort to changes in their requests to obtain better service for instance a user who knows that the srn policy is being used may split a longrunning program into several programs with short service times all these factors distort the workload hence the characterization of a typical workload should be developed without involving the users three approaches could be used for performance analysis of scheduling policies implementation of a scheduling policy in an os simulation mathematical modeling both simulation and mathematical modeling avoid the need for implementing a scheduling policy in an os thereby avoiding the cost complexity and delays involved in implementing the policy however to produce the same results as an implementation these approaches require a very detailed characterization of requests in the workload which is generally not feasible in practice hence performance aspects like the scheduling overhead or service to individual requests are best studied through implementation whereas simulation and mathematical modeling are well suited for studying performance of a scheduling policy and for determining good values of system parameters like the time slice number of users or the size of the list of ready processes performance analysis through implementation the scheduling policy to be evaluated is implemented in a real operating system that is used in the target operating environment the os receives real user chapter scheduling data simulated collection clock scheduling module requests to lists scheduler completed be serviced requests pcb io simulator figure simulation of a scheduling policy requests services them using the scheduling policy and collects data for statistical analysis of the policys performance this approach to performance analysis is disruptive because a real os has to be decommissioned modified and recommissioned for every scheduling policy that is to be analyzed this disruption could be avoided by using virtual machine software which permits a guest kernel to be modified without affecting operation of the host kernel however the overhead introduced by use of the virtual machine would cause inaccuracies in the performance measurement simulation simulation is achieved by coding the scheduling policy and relevant os functions as a program the simulator program and using a typical workload as its input the workload is a recording of some real workload directed at the os during a sample period analysis may be repeated with many workloads to eliminate the effect of variations across workloads figure shows a schematic of a simulator the simulator operates as follows it maintains the data structures that are used by the simulated scheduling policy in which it puts information concerning user requests as they arrive in the system get admitted and receive service it also maintains a clock to keep track of the simulated time from time to time it mimics the scheduling action and selects a request for processing it estimates the length of time for which the request would use the cpu before an event like the initiation of an io operation or completion of a request occurs it now advances the simulated clock by the amount of time for which the request would have used the cpu before the event occurred and moves the request out of the scheduling queue it then performs scheduling once again and so on it may contain other modules like an io simulator module which would predict when the io operation initiated by a request would complete when the simulated clock shows this time it adds the request to a scheduling queue the data collection module collects useful data for performance analysis the level of detail handled in a simulator governs the cost of simulation and the quality of its results part process management mathematical modeling a mathematical model consists of two components a model of the server and a model of the workload being processed the model provides a set of mathematical expressions for important performance characteristics like service times of requests and overhead these expressions provide insights into the influence of various parameters on system performance the workload model differs from workloads used in simulations in that it is not a recording of actual workload in any specific time period it is a statistical distribution that represents the workload that is it is a function that generates fictitious requests that have the same statistical properties as the actual workload during any period queuing theory widespread use of mathematical models to analyze performance of various systems led to development of a separate branch of mathematics known as queuing theory performance analysis using queuing theory is called queuing analysis the earliest wellknown application of queuing analysis was by erlang in evaluating the performance of a telephone exchange with the number of trunk lines as the controlling parameter the fundamental queuing theory model of a system is identical with the simple scheduler model discussed at the start of this chapter see figure this is known as the singleserver model queuing analysis is used to develop mathematical expressions for server efficiency mean queue length and mean wait time a request arriving at time ai with service time xi is completed at time ci the elapsed time ci ai depends on two factors arrival times and service times of requests that are either in execution or in the scheduling queue at some time during the interval ci ai and the scheduling policy used by the server it is reasonable to assume that arrival times and service times of requests entering the system are not known in advance ie these characteristics of requests are nondeterministic in nature although characteristics of individual requests are unknown they are customarily assumed to conform to certain statistical distributions a computing environment is thus characterized by two parameters a statistical distribution governing arrival times of requests and a statistical distribution governing their service times we give a brief introduction to statistical distributions and their use in mathematical modeling using the following notation mean arrival rate requests per second mean execution rate requests per second is called the utilization factor of the server when the work being directed at the system exceeds its capacity in this case the number of requests in the system increases indefinitely performance evaluation of such a system is of little practical relevance since turnaround times can be arbitrarily large when the system capacity exceeds the total work directed at it however this is true only as a longterm average it may not hold in an arbitrary interval of time chapter scheduling hence the server may be idle once in a while and a few requests may exist in the queue at certain times most practical systems satisfy even when we consider a slow server does not exceed because most practical systems are selfregulatory in nature the number of users is finite and the arrival rate of requests slackens when the queue length is large because most users requests are locked up in the queue a system reaches a steady state when all transients in the system induced due to its abrupt initiation at time t die down in the steady state values of mean queue lengths mean wait times mean turnaround times etc reflect performance of the scheduling policy for obtaining these values we start by assuming certain distributions for arrival and servicing of requests in the system arrival times the time between arrival of two consecutive requests is called interarrival time since is the arrival rate the mean interarrival time is a statistical distribution that has this mean interarrival time and that fits empirical data reasonably well can be used for workload characterization arrival of requests in the system can be regarded as random events totally independent of each other two assumptions leading to a poisson distribution of arrivals are now made first the number of arrivals in an interval t to t dt is assumed to depend only on the value of dt and not on past history of the system during the interval t second for small values of dt probability of more than one arrival in the interval t to t dt is assumed to be negligible the first assumption is known as the memoryless property of the arrival times distribution an exponential distribution function giving the probability of an arrival in the interval to t for any t has the form f t e t this distribution has the mean interarrival time since t df t it is found that the exponential distribution fits the interarrival times in empirical data reasonably well however a hyperexponential distribution with the same mean of is found to be a better approximation for the experimental data coffman and wood service times the function st gives the probability that the service time of a request is less than or equal to t st e t as in the case of arrival times we make two assumptions that lead to a poisson distribution of service times hence the probability that a request that has already consumed t units of service time will terminate in the next dt seconds depends only on the value of dt and not on t in preemptive scheduling it applies every time a request is scheduled to run after an interruption the memoryless property of service times implies that a scheduling algorithm can not make any predictions based on past history of a request in the system thus any preemptive scheduling policy that requires knowledge of future behavior of requests must depend on estimates of service times supplied by a programmer the scheduling performance will then critically depend on user inputs part process management and may be manipulated by users in a practical situation a system must strive to achieve the opposite effect that is system performance should be immune to user specification or misspecification of the service time of a request this requirement points toward roundrobin scheduling with timeslicing as a practical scheduling policy performance analysis the relation between l the mean queue length and w the mean wait time for a request before its servicing begins is given by littles formula l w this relation follows from the fact that while a request waits in the queue w new requests join the queue when a new request arrives it is added to the request queue in nonpreemptive scheduling the new request would be considered only after the server completes the request it is servicing let w be the expected time to complete the current we ttwdf request natually w is independent of a scheduling policy t and has the value for an exponential distribution f t the mean wait time for a request when a specific scheduling policy is used is computed from w and features of the scheduling policy we outline how the mean wait times for fcfs and srn policies are derived derivations for hrn and roundrobin policies are more complex and can be found in brinch hansen table summarizes the mean wait time for a request whose service time is t when different scheduling policies are used w the waiting time for some request r is the amount of time r spends in the queue before its service begins hence in fcfs scheduling w w i xi table summary of performance analysis scheduling policy mean wait time for a request with service time t fcfs w srn w where t t y dsy t o hrn for small t w t for large t w w t roundrobin n where p p n n j j nj p is the probability that no terminal awaits a response note w t df t for an exponential distribution f t e t it is chapter scheduling where request i is ahead of request r in the scheduling queue since the system is in the steady state we can replace the i term by n where n is the number of requests ahead of r and is the mean service time since n is the mean queue length n w from littles formula hence w w w w w therefore w w thus the mean wait time in fcfs scheduling rises sharply for high values of in srn scheduling requests whose service times xr where xr is the service time of r are serviced before request r hence the waiting time for request r is w w i xi where xi xr w r r where r y dsy capacity planning performance analysis can be used for capacity planning for example the formulae shown in table can be used to determine values of important parameters like the size of the list of ready processes used by the kernel as an example consider an os in which the mean arrival rate of requests is requests per second and the mean response time for requests is seconds the mean queue length is computed by littles formula eq as note that queues will exceed this length from time to time the following example provides a basis for deciding the capacity of the ready queue capacity planning using queuing analysis example a kernel permits up to n entries in the queue of ready requests if the queue is full when a new request arrives the request is rejected and leaves the os pi the probability that the ready queue contains i processes at any time can be shown to be pi i n for and n p p p and p hence percent of requests are lost a higher value of n should be used to reduce the number of lost requests goals of an os the fundamental goals of an operating system are efficient use ensure efficient use of a computers resources user convenience provide convenient methods of using a computer system noninterference prevent interference in the activities of its users the goals of efficient use and user convenience sometimes conflict for example emphasis on quick service could mean that resources like memory have to remain allocated to a program even when the program is not in execution however it would lead to inefficient use of resources when such conflicts arise the designer has to make a tradeoff to obtain the combination of efficient use and user convenience that best suits the environment this is the notion of effective utilization of the computer system we find a large number of operating systems in use because each one of them provides a different flavor of effective utilization at one extreme we have oss that provide fast service required by command and control applications at the other extreme we have oss that make efficient use of computer resources to provide lowcost computing while in the middle we have oss that provide different combinations of the two interference with a users activities may take the form of illegal use or modification of a users programs or data or denial of resources and services to a user such interference could be caused by both users and nonusers and every os must incorporate measures to prevent it in the following we discuss important aspects of these fundamental goals efficient use an operating system must ensure efficient use of the fundamental computer system resources of memory cpu and io devices such as disks and printers poor efficiency can result if a program does not use a resource allocated to it eg chapter introduction if memory or io devices allocated to a program remain idle such a situation may have a snowballing effect since the resource is allocated to a program it is denied to other programs that need it these programs can not execute hence resources allocated to them also remain idle in addition the os itself consumes some cpu and memory resources during its own operation and this consumption of resources constitutes an overhead that also reduces the resources available to user programs to achieve good efficiency the os must minimize the waste of resources by programs and also minimize its own overhead efficient use of resources can be obtained by monitoring use of resources and performing corrective actions when necessary however monitoring use of resources increases the overhead which lowers efficiency of use in practice operating systems that emphasize efficient use limit their overhead by either restricting their focus to efficiency of a few important resources like the cpu and the memory or by not monitoring the use of resources at all and instead handling user programs and resources in a manner that guarantees high efficiency user convenience user convenience has many facets as table indicates in the early days of computing user convenience was synonymous with bare necessity the mere ability to execute a program written in a higher level language was considered adequate experience with early operating systems led to demands for better service which in those days meant only fast response to a user request other facets of user convenience evolved with the use of computers in new fields early operating systems had commandline interfaces which required a user to type in a command and specify values of its parameters users needed substantial training to learn use of the commands which was acceptable because most users were scientists or computer professionals however simpler interfaces were needed to facilitate use of computers by new classes of users hence graphical user interfaces guis were evolved these interfaces used icons on a screen to represent programs and files and interpreted mouse clicks on the icons and associated menus as commands concerning them in many ways this move can be compared to the spread of car driving skills in the first half of table facets of user convenience facet examples fulfillment of necessity ability to execute programs use the file system good service speedy response to computational requests user friendly interfaces easytouse commands graphical user interface gui new programming model concurrent programming weboriented features means to set up webenabled servers evolution add new features use new computer technologies summary the scheduler of an os decides which process longterm scheduler decides when a process should should be serviced next by the cpu and for how be admitted for servicing whereas the mediumlong it should be serviced its decisions influence term scheduler decides when a process should both user service and system performance in this be swapped out to a disk and when it should chapter we discussed three techniques of process be reloaded in memory the shortterm schedschedulers prioritybased scheduling reordering of uler selects one of the processes that is present in requests and variation of time slice and studied memory the multilevel adaptive scheduling policy how schedulers use them to provide a desired comassigns different values of time slice to processes bination of user service and system performance with different priorities and varies a processs priwe also studied realtime scheduling ority in accordance with its recent behavior to a nonpreemptive scheduling policy performs provide a combination of good response time and scheduling only when the process being serlow scheduling overhead the fair share schedulviced by the cpu completes the policy focuses ing policy ensures that processes of an application merely on reordering of requests to improve mean collectively do not exceed a specified share of the turnaround time of processes the shortest request cpu time next srn policy suffers from starvation as some realtime scheduling focuses on meeting the processes may be delayed indefinitely the highest time constraints of applications deadline schedulresponse ratio next hrn policy does not have this ing considers deadlines of processes while performproblem because the response ratio of a process ing scheduling decisions rate monotonic schedulkeeps increasing as it waits for the cpu ing assigns priorities to processes based on their preemptive scheduling policies preempt a properiods and performs prioritybased scheduling cess when it is considered desirable to make a fresh modern operating systems face diverse workscheduling decision the roundrobin rr policy loads so schedulers divide processes into different services all processes by turn limiting the amount classes such as realtime and nonrealtime and use of cpu time used by each process to the value of an appropriate scheduling policy for each class the time slice the least completed next lcn polperformance analysis is used to both study and icy selects the process that has received the least tune performance of scheduling policies without amount of service whereas the shortest time to go implementing them in an os it uses a mathematstg policy selects the process that is closest to ical characterization of the typical workload in a completing system to determine system throughput or values in practice an operating system uses an of key scheduler parameters such as the time slice arrangement involving three schedulers the and sizes of scheduling lists test your concepts classify each of the following statements as true srn scheduling policy and the system or false completes execution of these requests in a if the scheduling overhead is negligible the the sequence r r rn then weighted schedule length is identical in batch processturnaround of ri weighted turnaround of ing and multiprogramming systems rj if i j b if all requests arrive at the same time instant c the roundrobin scheduling policy with in a system using the shortest request next timeslicing provides approximately equal chapter scheduling response ratios to requests that arrive at the j if processes do not perform io the unix same time instant scheduling policy degenerates to the rr d if processes do not perform io the roundscheduling policy robin scheduling policy with timeslicing processes a b and c arrive at times and resembles the least completed next lcn respectively the processes do not perform scheduling policy io and require and second of cpu e when both cpubound and iobound time the processswitching time is negligible at requests are present the least completed what time does process b complete if the schednext lcn scheduling policy provides betuler uses the shortest time to go stg policy ter turnaround times for iobound requests a b c d than provided by the roundrobin scheduling which of the following scheduling policies will policy with timeslicing provide the least turnaround time for an iof the highest response ratio next hrn bound process both iobound and cpuscheduling policy avoids starvation bound requests are present in the system g if a feasible schedule exists for a realtime a rr application use of the earliest deadline first b lcn edf scheduling policy guarantees that no c multilevel adaptive scheduling deadline overruns will occur d none of these h an iobound process is executed twice once in a system using rr scheduling and again which of the following scheduling policies will in a system using multilevel adaptive schedulprovide the least turnaround time for a cpuing the number of times it is scheduled by the bound process both iobound and cpurr scheduler and by the multilevel scheduler bound requests are present in the system is identical a rr i a cpubound process can not starve when b lcn multilevel adaptive scheduling is employed c multilevel adaptive scheduling exercises give examples of conflicts between usercentric consumes ms ten independent executions and systemcentric views of scheduling of this program are started at the same time the study the performance of the nonpreemptive scheduling overhead of the kernel is ms comand preemptive scheduling policies on processes pute the response time of the first process in the described in table if their arrival times are first and subsequent iterations if and seconds respectively draw timing a the time slice is ms charts analogous to those in sections and b the time slice is ms to show operation of these policies the kernel of an os implements the hrn pol show that srn scheduling provides the minicy preemptively as follows every t seconds imum average turnaround time for a set response ratios of all processes are computed of requests that arrive at the same time and the process with the highest response ratio instant would it provide the minimum average is scheduled comment on this policy for large turnaround time if requests arrive at different and small values of t also compare it with the times following policies a program contains a single loop that executes a shortest time to go stg policy times the loop includes a computation that b least completed next lcn policy lasts ms followed by an io operation that c roundrobin policy with timeslicing rr part process management a process consists of two parts that are functionas cpubound or iobound based on its ally independent of one another it is proposed recent behavior visavis the time slice it was to separate the two parts and create two proconsidered to be a cpubound process if cesses to service them identify those scheduling it used up its entire timeslice when schedpolicies under which the user would receive betuled otherwise it was an iobound process ter user service through use of the two processes to obtain good throughput hasp required instead of the original single process that a fixed percentage of processes in the for each of the scheduling policies discussed scheduling queue must be iobound processes in sections and a group of requests periodically hasp adjusted the time slice to is serviced with negligible overheads and the satisfy this requirement the time slice was average turnaround time is determined the reduced if more processes were considered iorequests are now organized arbitrarily into two bound than desired and it was increased if groups of requests each these groups of lesser number of processes were iobound requests are now serviced one after another explain the purpose of adjusting the time through each of the scheduling policies used slice describe operation of hasp if most proearlier and the average turnaround time is comcesses in the system were a cpubound and puted compare the two average turnaround b iobound times for each scheduling policy and men comment on the similarities and differences tion conditions under which the two could be between different a lcn and unix scheduling a multilevel adaptive scheduler uses five priorb hasp and multilevel adaptive scheduling ity levels numbered from to level being the see exercise highest priority level the time slice for a prior determine the starting deadlines for the proity level is n where n is the level number cesses of example it puts every process in level initially a pro an os using a preemptive scheduling policy cess requiring seconds of cpu time is serviced assigns dynamically changing priorities the prithrough this scheduler compare the response ority of a process changes at different rates time of the process and the total scheduling overdepending on its state as follows head incurred if there are no other processes in the system if the process is serviced through a rate of change of priority when a roundrobin scheduler using a time slice of process is running cpu seconds what would be the response time rate of change of priority when a of the process and the total scheduling overhead process is ready incurred rate of change of priority when a a multilevel adaptive scheduling policy avoids process is performing io starvation by promoting a process to a higher note that the rate of change of priority can be priority level if it has spent seconds in its positive negative or zero a process has priorpresent priority level without getting scheduled ity when it is created a process with a larger comment on the advantages and disadvannumerical value of priority is considered to have tages of the following methods of implementing a higher priority for scheduling promotion comment on properties of the scheduling polia promote a process to the highest priority cies in each of the following cases level a b promote a process to the next higher priority b level c the houston automatic spooling system d hasp was a scheduling subsystem used in the ibm hasp assigned high priority will the behavior of the scheduling policies to iobound processes and low priority to change if the priority of a process is set to every cpubound processes a process was classified time it is scheduled chapter scheduling a background process should operate in such a of t for which the rate monotonic schedulmanner that it does not significantly degrade the ing policy will be able to meet deadlines of all service provided to other processes which of the processes following alternatives would you recommend for a system uses the fcfs scheduling policy idenimplementing it tical computational requests arrive in the system a assign the lowest priority to a background at the rate of requests per second it is desired process that the mean wait time in the system should b provide a smaller quantum to a backnot exceed seconds compute the size of each ground process than to other processes see request in cpu seconds section identical requests each requiring cpu sec prepare a schedule for operation of the periodic onds arrive in an os at the rate of requests per processes pp of section using edf second the kernel uses a fixedsize ready queue scheduling a new request is entered in the ready queue if if the response requirement of the application the queue is not already full else the request is of figure is seconds and service times discarded what should be the size of the ready of processes pp are as shown in figure queue if less than percent of requests should what is the largest service time of p for which be discarded a feasible schedule exists answer this question the mean arrival rate of requests in a system under two conditions using fcfs scheduling is requests per second a none of the processes perform any io operthe mean wait time for a request is seconds ations find the mean execution rate b process p performs io for seconds sec we define small request as a request whose onds of which can be overlapped with the service time is less than percent of comprocessing of process p pute the turnaround time for a small request in the service times of three processes p p and a system using the hrn scheduling policy when p are ms ms and ms respectively t and ms and t ms what is the smallest value bibliography corbato et al discusses use of multilevel feedbovet and cesati and love discuss back queues in the ctss operating system coffman scheduling in linux mauro and mcdougall and denning reports studies related to multilevel discusses scheduling in solaris while russinovich and scheduling a fair share scheduler is described in kay solomon discusses scheduling in windows and lauder and lottery scheduling is described trivedi is devoted to queuing theory hellerin waldspurger and weihl realtime scheduling man and conroy describes use of queuing theory is discussed in liu and layland zhao in performance evaluation khanna et al and liu power conservation is a crucial new element in scheduling power can be conserved by running the cpu at lower speeds zhu bach m j the design of the unix et al discusses speculative scheduling algorithms operating system prentice hall englewood that save power by varying the cpu speed and reducing cliffs nj the number of speed changes while ensuring that an bovet d p and m cesati understanding application meets its time constraints the linux kernel rd ed oreilly sebastopol bach mckusick et al and vahalia brinch hansen p operating system discuss scheduling in unix ogorman principles prentice hall englewood cliffs nj part process management coffman e g and r c wood mauro j and r mcdougall solaris interarrival statistics for time sharing systems internals nd ed prentice hall englewood communications of the acm cliffs nj mckusick m k k bostic m j karels and coffman e g and p j denning j s quarterman the design and operating systems theory prentice hall implementation of the bsd operating system englewood cliffs nj addisonwesley reading mass corbato f j m merwindaggett and ogorman j linux process manager r c daley an experimental the internals of scheduling interrupts and timesharing system proceedings of the signals john wiley new york afips fall joint computer conference russinovich m e and d a solomon microsoft windows internals th ed microsoft hellerman h and t f conroy press redmond wash computer system performance mcgrawhill trivedi k s probability and statistics kogakusha tokyo with reliability queuing and computer science kay j and p lauder a fair share applications prentice hall englewood scheduler communications of the acm cliffs nj vahalia u unix internals the new khanna s m sebree and j zolnowsky frontiers prentice hall englewood cliffs nj realtime scheduling in sunos proceedings waldspurger c a and w e weihl of the winter usenix conference lottery scheduling proceedings of the first san francisco january usenix symposium on operating system design love r linux kernel development and implementation osdi nd ed novell press zhao w special issue on realtime liu c l and j w layland scheduling operating systems operating system review algorithms for multiprogramming in a hard realtime environment journal of the acm zhu d d mosse and r melhem poweraware scheduling for andor graphs in liu j w s realtime systems pearson realtime systems ieee transactions on parallel education new york and distributed systems what is a deadlock a deadlock is a situation concerning a set of processes in which each process in the set waits for an event that must be caused by another process in the set each process is then waiting for an event that can not occur example illustrates how a deadlock could arise when two processes try to share resources part process management example twoprocess deadlock a system contains one tape drive and one printer two processes pi and pj make use of the tape drive and the printer through the following programs process pi process pj request tape drive request printer request printer request tape drive use tape drive and printer use tape drive and printer release printer release tape drive release tape drive release printer as the two processes execute resource requests take place in the following order process pi requests the tape drive process pj requests the printer process pi requests the printer process pj requests the tape drive the first two resource requests are granted right away because the system includes both a tape drive and a printer now pi holds the tape drive and pj holds the printer when pi asks for the printer it is blocked until pj releases the printer similarly pj is blocked until pi releases the tape drive both processes are blocked indefinitely because they wait for each other the deadlock illustrated in example is called a resource deadlock other kinds of deadlock can also arise in an os a synchronization deadlock occurs when the awaited events take the form of signals between processes for example if a process pi decides to perform an action ai only after process pj performs action aj and process pj decides to perform action aj only after pi performs ai both processes get blocked until the other process sends it a signal see section analogously a communication deadlock occurs for a set of processes if each process sends a message only after it receives a message from some other process in the set see chapter an os is primarily concerned with resource deadlocks because allocation of resources is an os responsibility the other two forms of deadlock are seldom handled by an os it expects user processes to handle such deadlocks themselves formally we say that a deadlock arises if the conditions in the following definition are satisfied deadlocks in resource allocation processes use hardware resources like memory and io devices and software resources such as files an os may contain several resources of a kind eg several disks tape drives or printers we use the term resource unit to refer to a resource of a specific kind and use the term resource class to refer to the collection of all resource units of a kind thus a resource class contains one or more resource units eg the printer class may contain two printers we use the notation ri for a resource class and rj for a resource unit in a resource class recall from section that the kernel maintains a resource table to keep track of the allocation state of a resource resource allocation in a system entails three kinds of events request for the resource actual allocation of the resource and release of the resource table describes these events a request event occurs when some process pi makes a request for a resource rl process pi will be blocked on an allocation event for rl if rl is currently allocated to some process pk in effect pi is waiting for pk to release rl a release event by pk frees resource rl and the kernel may decide to allocate resource rl to pi thus a release event by pk may cause the allocation event for which pi is waiting in which case pi will become the holder of the resource and enter the ready state however as we saw in example process pi will face an indefinite wait if pks release of rl is indefinitely delayed table events related to resource allocation event description request a process requests a resource through a system call if the resource is free the kernel allocates it to the process immediately otherwise it changes the state of the process to blocked allocation the process becomes the holder of the resource allocated to it the resource state information is updated and the state of the process is changed to ready release a process releases a resource through a system call if some processes are blocked on the allocation event for the resource the kernel uses some tiebreaking rule eg fcfs allocation to decide which process should be allocated the resource part process management conditions for a resource deadlock by slightly rewording parts and of definition we can obtain the conditions under which resource deadlocks occur each process pi in d is blocked for an allocation event to occur and the allocation event can be caused only by actions of some other process pj in d since pj is in d parts and of definition apply to pj as well in other words the resource requested by process pi is currently allocated to pj which itself waits for some other resource to be allocated to it this condition of each process taken by itself is called the holdandwait condition but parts and of definition also imply that processes in d must wait for each other this condition is called the circular wait condition a circular wait may be direct that is pi waits for pj and pj waits for pi or it may be through one or more other processes included in d for example pi waits for pj pj waits for pk and pk waits for pi two other conditions must hold for a resource deadlock to occur if process pi needs a resource that is currently allocated to pj pi must not be able to either share the resource with pj or preempt it from pj for its own use table summarizes the conditions that must be satisfied for a resource deadlock to exist all these conditions must hold simultaneously a circular wait is essential for a deadlock a holdandwait condition is essential for a circular wait and nonshareability and nonpreemptibility of resources are essential for a holdandwait condition besides the conditions listed in table another condition is also essential for deadlocks no withdrawal of resource requests a process blocked on a resource request can not withdraw its request this condition is essential because waits may not be indefinite if a blocked process is permitted to withdraw a resource request and continue its operation however it is not stated explicitly in the literature because many operating systems typically impose the nowithdrawal condition on resource requests table conditions for resource deadlock condition explanation nonshareable resources resources can not be shared a process needs exclusive access to a resource no preemption a resource can not be preempted from one process and allocated to another process holdandwait a process continues to hold the resources allocated to it while waiting for other resources circular waits a circular chain of holdandwait conditions exists in the system eg process pi waits for pj pj waits for pk and pk waits for pi chapter deadlocks modeling the resource allocation state example indicated that we must analyze information about resources allocated to processes and about pending resource requests to determine whether a set of processes is deadlocked all this information constitutes the resource allocation state of a system which we simply call the allocation state of a system two kinds of models are used to represent the allocation state of a system a graph model can depict the allocation state of a restricted class of systems in which a process can request and use exactly one resource unit of each resource class it permits use of a simple graph algorithm to determine whether the circular wait condition is satisfied by processes a matrix model has the advantage of generality it can model allocation state in systems that permit a process to request any number of units of a resource class graph models a resource request and allocation graph rrag contains two kinds of nodes process nodes and resource nodes a process node is depicted by a circle a resource node is depicted by a rectangle and represents one class of resources the number of bullet symbols in a resource node indicates how many units of that resource class exist in the system two kinds of edges can exist between a process node and a resource node of an rrag an allocation edge is directed from a resource node to a process node it indicates that one unit of the resource class is allocated to the process a request edge is directed from a process node to a resource node it indicates that the process is blocked on a request for one unit of the resource class an allocation edge rk pj is deleted when process pj releases a resource unit of resource class rk allocated to it when a pending request of process pi for a unit of resource class rk is granted the request edge pi rk is deleted and an allocation edge rk pi is added a waitfor graph wfg can represent the allocation state more concisely than an rrag when every resource class in the system contains only one resource unit the wfg contains nodes of only one kind namely process nodes an edge pi pj in the wfg represents the fact that process pi is blocked on a request for a resource that is currently allocated to process pj ie process pi is waiting for process pj to release a resource hence the name waitfor graph representing the same information in an rrag would have required two edges the next example illustrates and compares use of an rrag and a wfg rrag and wfg example figure a shows an rrag the printer class contains only one resource unit which is allocated to process p requests for a printer made by processes p and p are currently pending the tape class contains two tape drives which are allocated to processes p and p a request by process p for one tape drive is currently pending part process management p p p p printer tape p p r p p p p rrag rrag wfg figure a resource request and allocation graph rrag b equivalence of rrag and waitfor graph wfg when each resource class contains only one resource unit figure b shows both an rrag and a wfg for a system that has a resource class r that contains only one resource unit and three processes p p and p the edges p r and rp in the rrag together indicate that process p is waiting for the resource currently allocated to p hence we have an edge p p in the wfg edge p p similarly indicates that process p is waiting for the resource currently allocated to p paths in an rrag and a wfg we can deduce the presence of deadlocks from the nature of paths in an rrag and a wfg we define the following notation for this purpose blockedp set of blocked processes wfi the waitfor set of pi ie the set of processes that hold resources required by process pi with this notation parts and of definition can be restated as follows d blockedp for all pi d w f i d consider a system in which each resource class contains only one resource unit let the system contain a single path p r p r pn rn pn in its rrag thus process pn is not blocked on any resource and no resource is currently allocated to p the wfg of this system would contain the single path p p pn we can establish the absence of a deadlock in this system by showing that conditions and are not satisfied by any set of processes in the system blockedp is p pn first consider the set of processes p pn this set is not a subset of blockedp which violates condition and so this set of processes is not deadlocked now consider the set p pn here wf n pn violates condition any other subset of p pn can similarly be shown to violate condition for some process hence there is no deadlock in the system chapter deadlocks now if the unit of resource class rn were to be allocated to p instead of pn the path in the rrag would be p r p r pn rn p this is a cyclic path also called a cycle because it ends on the same node on which it begins ie node p the wfg also contains a cycle which is p p p blockedp is p pn same as before a deadlock exists because the set p pn satisfies both condition and since p pn blockedp for all pi p pn w f i contains a single process pl such that pl p pn from this analysis we can conclude that condition which implies existence of mutual waitfor relationships between processes of d can be satisfied only by cyclic paths hence a deadlock can not exist unless an rrag or a wfg contains a cycle rrag showing a deadlock example figure shows the rrag for example the rrag contains a cyclic path piprinterpj tapepi here wf i pj and wf j pi d p p satisfies both condition and hence processes pi and pj are deadlocked does presence of a cycle in an rrag or a wfg imply a deadlock in the system discussed so far each resource class contained a single resource unit so a cycle in the rrag or wfg implied a deadlock however it may not be so in all systems consider a path p r pi ri pi pn in a system in which a resource class ri contains many resource units a wfg can not be used to depict the allocation state of this system hence we will discuss the rrag for the system if some process pk not included in the path holds one unit of resource class ri that unit may be allocated to pi when pk released it the edge pi ri could thus vanish even without pi releasing the unit of ri held by it thus a cyclic path in an rrag may be broken when some process not included in the cycle releases a unit of the resource therefore the presence of a cycle in an rrag does not necessarily imply existence of a deadlock if a resource class contains more than one resource unit we draw on this knowledge pi printer tape pj figure rrag for the system of example part process management in section when we develop a formal characterization for deadlocks example illustrates such a situation example a cycle in rrag does not imply a deadlock a system has one printer and two tape drives and three processes pi pj and pk the nature of processes pi and pj is the same as depicted in example each of them requires a tape drive and a printer process pk requires only a tape drive for its operation let process pk request for a tape drive before requests are made as in example figure shows the rrag after all requests have been made the graph has a cycle involving pi and pj this cycle would be broken when process pk completes because the tape drive released by it would be allocated to pj hence there is no deadlock we come to the same conclusion when we analyze the set of processes pi pj according to definition because wf j pi pk and pk pi pj violates condition matrix model in the matrix model the allocation state of a system is primarily represented by two matrices the matrix allocatedresources indicates how many resource units of each resource class are allocated to each process in the system the matrix requestedresources represents pending requests it indicates how many resource units of each resource class have been requested by each process in the system if a system contains n processes and r resource classes each of these matrices is an n r matrix the allocation state with respect to a resource class rk indicates the number of units of rk allocated to each process and the number of units of rk requested by each process these are represented as ntuples allocatedresourcesk allocatedresourcesnk and requestedresourcesk requestedresourcesnk respectively some auxiliary matrices may be used to represent additional information required for a specific purpose two such auxiliary matrices are totalresources and freeresources which indicate the total number of resource units in each resource class and the number of resource units of each resource class that are free respectively each of these matrices is a column matrix that has r elements in it example is an example of a matrix model pi printer tape pj pk figure rrag after all requests of example are made handling deadlocks table describes the three fundamental approaches to deadlock handling each approach has different consequences in terms of possible delays in resource allocation the kind of resource requests that user processes are allowed to make and the os overhead under the deadlock detection and resolution approach the kernel aborts some processes when it detects a deadlock on analyzing the allocation state this action frees the resources held by the aborted process which are now allocated to other processes that had requested them the aborted processes have to be reexecuted thus the cost of this approach includes the cost of deadlock detection and the cost of reexecuting the aborted processes in the system of example the table deadlock handling approaches approach description deadlock detection and the kernel analyzes the resource state to check whether resolution a deadlock exists if so it aborts some processes and allocates the resources held by them to other processes so that the deadlock ceases to exist deadlock prevention the kernel uses a resource allocation policy that ensures that the four conditions for resource deadlocks mentioned in table do not arise simultaneously it makes deadlocks impossible deadlock avoidance the kernel analyzes the allocation state to determine whether granting a resource request can lead to a deadlock in the future only requests that can not lead to a deadlock are granted others are kept pending until they can be granted thus deadlocks do not arise deadlock detection and resolution consider a system that contains a process pi which holds a printer and a process pj that is blocked on its request for a printer if process pi is not in the blocked state there is a possibility that it might complete its operation without requesting any more resources on completion it would release the printer allocated to it which could then be allocated to process pj thus if pi is not in the blocked state pjs wait for the printer is not indefinite because of the following sequence of events process pi completesreleases printerprinter is allocated to pj if some other process pl waits for some other resource allocated to pj its wait is also not indefinite hence processes pi pj and pl are not involved in a deadlock at the current moment from this observation we can formulate the following rule for deadlock detection a process in the blocked state is not involved in a deadlock at the current moment if the request on which it is blocked can be satisfied through a sequence of process completion resource release and resource allocation events if each resource class in the system contains a single resource unit this check can be made by checking for the presence of a cycle in an rrag or wfg however more complex graphbased algorithms have to be used if resource classes may contain more than one resource unit see section so we instead discuss a deadlock detection approach using the matrix model we check for the presence of a deadlock in a system by actually trying to construct fictitious but feasible sequences of events whereby all blocked processes can get the resources they have requested success in constructing such a sequence implies the absence of a deadlock at the current moment and a failure to construct it implies presence of a deadlock when we apply this rule to examples and chapter deadlocks it correctly deduces that processes pi and pj of example are in a deadlock whereas a deadlock does not exist in example we perform the above check by simulating the operation of a system starting with its current state we refer to any process that is not blocked on a resource request as a running process ie we do not differentiate between the ready and running states of chapter in the simulation we consider only two events completion of a process that is not blocked on a resource request and allocation of resources to a process that is blocked on a resource request it is assumed that a running process would complete without making additional resource requests and that some of the resources freed on its completion would be allocated to a blocked process only if the allocation would put that process in the running state the simulation ends when all running processes complete the processes that are in the blocked state at the end of the simulation are those that could not obtain the requested resources when other processes completed hence these processes are deadlocked in the current state there is no deadlock in the current state if no blocked processes exist when the simulation ends example illustrates this approach deadlock detection example the allocation state of a system containing units of a resource class r and three processes pp is as follows r r total r p p resources p p free p p resources allocated requested resources resources process p is in the running state because it is not blocked on a resource request all processes in the system can complete as follows process p completes and releases units of the resource allocated to it these units can be allocated to p when it completes units of the resource can be allocated to p thus no blocked processes exist when the simulation ends so a deadlock does not exist in the system if the requests by processes p and p were for and units respectively none of them could complete even after process p released resource units these processes would be in the blocked state when the simulation ended and so they are deadlocked in the current state of the system in our simulation we assumed that a running process completes its execution without making further resource requests this assumption has two consequences first our conclusions regarding existence of a deadlock are not sensitive to the order in which blocked processes are assumed to become running or the order in which running processes are assumed to complete second even part process management if a system is deadlockfree at the current moment a deadlock could arise in the future in example this could happen if p makes a request for one more unit of r as a consequence deadlock detection has to be performed repeatedly during operation of the os it can be achieved by devoting a system process exclusively to deadlock detection and activating it at fixed intervals alternatively deadlock detection can be performed every time a process becomes blocked on a resource request the overhead of deadlock detection would depend on several factors like the number of processes and resource classes in the system and how often deadlock detection is performed a deadlock detection algorithm algorithm performs deadlock detection the inputs to the algorithm are two sets of processes blocked and running and a matrix model of the allocation state comprising the matrices allocatedresources requestedresources and freeresources the algorithm simulates completion of a running process pi by transferring it from the set running to the set finished steps a b resources allocated to pi are added to freeresources step c the algorithm now selects a blocked process whose resource request can be satisfied from the free resources step d and transfers it from the set blocked to the set running sometime later the algorithm simulates its completion and transfers it from running to finished the algorithm terminates when no processes are left in the running set processes remaining in the set blocked if any are deadlocked the complexity of the algorithm can be analyzed as follows the sets running and blocked can contain up to n processes where n is the total number of processes in the system the loop of step iterates n times and step d performs an order of n r work in each iteration hence the algorithm requires an order of n r work example illustrates the working of this algorithm algorithm deadlock detection inputs n number of processes r number of resource classes blocked set of processes running set of processes freeresources array r of integer allocatedresources array n r of integer requestedresources array n r of integer data structures finished set of processes repeat until set running is empty a select a process pi from set running b delete pi from set running and add it to set finished chapter deadlocks c for k r freeresourcesk freeresourcesk allocatedresourcesik d while set blocked contains a process pl such that for k r requestedresourceslk freeresourcesk i for k r freeresourcesk freeresourcesk requestedresourcesl k allocatedresourcesl k allocatedresourcesl k requestedresourcesl k ii delete pl from set blocked and add it to set running if set blocked is not empty then declare processes in set blocked to be deadlocked operation of a deadlock detection algorithm example a system has four processes pp and and units of resource classes r r and r respectively it is in the following state just before process p makes a request for unit of resource class r r r r r r r total r r r p p resources p p r r r p p free p p resources allocated requested resources resources one resource unit of resource class r is allocated to process p and algorithm is invoked to check whether the system is in a deadlock figure shows steps in operation of the algorithm inputs to it are the sets blocked and running initialized to p p p and p respectively and matrices allocatedresources requestedresources and freeresources as shown in figure a the algorithm transfers process p to the set finished and frees the resources allocated to it the number of free units of the resource classes is now and respectively the algorithm finds that process ps pending request can now be satisfied so it allocates the resources requested by p and transfers p to the set running see figure b since p is the only process in running it is transferred to the set finished after freeing ps resources the algorithm finds that ps resource request can be satisfied see figure c and after p completes ps resource request can be satisfied see figure d the set running is now empty so the algorithm completes a deadlock does not exist in the system because the set blocked is empty part process management initial state r r r r r r r r r p p free p p resources p p p p allocated requested resources resources after simulating allocation of resources to p when process p completes p p free p p resources p p p p allocated requested resources resources after simulating allocation of resources to p when process p completes p p free p p resources p p p p allocated requested resources resources after simulating allocation of resources to p when process p completes p p free p p resources p p p p allocated requested resources resources figure operation of algorithm the deadlock detection algorithm deadlock resolution given a set of deadlocked processes d deadlock resolution implies breaking the deadlock to ensure progress for some processes in d that is for processes in some set d d it can be achieved by aborting one or more processes in set d and allocating their resources to some processes in d each aborted process is called a victim of deadlock resolution thus deadlock resolution can be seen as the act of splitting a set of deadlocked processes d into two sets such that d d dv where each process in dv is a victim of deadlock resolution and the set of processes d is deadlockfree after the deadlock resolution actions are complete that is each process in d can complete its operation through a sequence of process completion resource release and resource allocation events operation of an os the primary concerns of an os during its operation are execution of programs use of resources and prevention of interference with programs and resources accordingly its three principal functions are program management the os initiates programs arranges their execution on the cpu and terminates them when they complete their execution since many programs exist in the system at any time the os performs a function called scheduling to select a program for execution chapter introduction resource management the os allocates resources like memory and io devices when a program needs them when the program terminates it deallocates these resources and allocates them to other programs that need them security and protection the os implements noninterference in users activities through joint actions of the security and protection functions as an example consider how the os prevents illegal accesses to a file the security function prevents nonusers from utilizing the services and resources in the computer system hence none of them can access the file the protection function prevents users other than the file owner or users authorized by him from accessing the file table describes the tasks commonly performed by an operating system when a computer system is switched on it automatically loads a program stored on a reserved part of an io device typically a disk and starts executing the program this program follows a software technique known as bootstrapping to load the software called the boot procedure in memory the program initially loaded in memory loads some other programs in memory which load other programs and so on until the complete boot procedure is loaded the boot procedure makes a list of all hardware resources in the system and hands over control of the computer system to the os a system administrator specifies which persons are registered as users of the system the os permits only these persons to log in to use its resources and services a user authorizes his collaborators to access some programs and data the os notes this information and uses it to implement protection the os also performs a set of functions to implement its notion of effective utilization these functions include scheduling of programs and keeping track of resource status and resource usage information table common tasks performed by operating systems task when performed construct a list of resources during booting maintain information for security while registering new users verify identity of a user at login time initiate execution of programs at user commands maintain authorization information when a user specifies which collaborators can acces what programs or data perform resource allocation when requested by users or programs maintain current status of resources during resource allocationdeallocation maintain current status of programs continually during os operation and perform scheduling part overview the following sections are a brief overview of os responsibilities in managing programs and resources and in implementing security and protection program management modern cpus have the capability to execute program instructions at a very high rate so it is possible for an os to interleave execution of several programs on a cpu and yet provide good user service the key function in achieving interleaved execution of programs is scheduling which decides which program should be given the cpu at any time figure shows an abstract view of scheduling the scheduler which is an os routine that performs scheduling maintains a list of programs waiting to execute on the cpu and selects one program for execution in operating systems that provide fair service to all programs the scheduler also specifies how long the program can be allowed to use the cpu the os takes away the cpu from a program after it has executed for the specified period of time and gives it to another program this action is called preemption a program that loses the cpu because of preemption is put back into the list of programs waiting to execute on the cpu the scheduling policy employed by an os can influence both efficient use of the cpu and user service if a program is preempted after it has executed for only a short period of time the overhead of scheduling actions would be high because of frequent preemption however each program would suffer only a short delay before it gets an opportunity to use the cpu which would result in good user service if preemption is performed after a program has executed for a longer period of time scheduling overhead would be lesser but programs would suffer longer delays so user service would be poorer resource management resource allocations and deallocations can be performed by using a resource table each entry in the table contains the name and address of a resource unit and its present status indicating whether it is free or allocated to some program table is such a table for management of io devices it is constructed by the boot procedure by sensing the presence of io devices in the system and updated by the operating system to reflect the allocations and deallocations made by it since any part of a disk can be accessed directly it is possible to treat different parts preempted program new scheduler cpu completed program programs waiting program for the cpu selected program figure a schematic of scheduling chapter introduction table resource table for io devices resource name class address allocation status printer printer allocated to p printer printer free printer printer free disk disk allocated to p disk disk allocated to p cdw cd writer free of a disk as independent devices thus the devices disk and disk in table could be two parts of the same disk two resource allocation strategies are popular in the resource partitioning approach the os decides a priori what resources should be allocated to each user program for example it may decide that a program should be allocated mb of memory disk blocks and a monitor it divides the resources in the system into many resource partitions or simply partitions each partition includes mb of memory disk blocks and a monitor it allocates one resource partition to each user program when its execution is to be initiated to facilitate resource allocation the resource table contains entries for resource partitions rather than for individual resources as in table resource partitioning is simple to implement hence it incurs less overhead however it lacks flexibility resources are wasted if a resource partition contains more resources than what a program needs also the os can not execute a program if its requirements exceed the resources available in a resource partition this is true even if free resources exist in another partition in the poolbased approach to resource management the os allocates resources from a common pool of resources it consults the resource table when a program makes a request for a resource and allocates the resource if it is free it incurs the overhead of allocating and deallocating resources when requested however it avoids both problems faced by the resource partitioning approach an allocated resource is not wasted and a resource requirement can be met if a free resource exists virtual resources a virtual resource is a fictitious resource it is an illusion supported by an os through use of a real resource an os may use the same real resource to support several virtual resources this way it can give the impression of having a larger number of resources than it actually does each use of a virtual resource results in the use of an appropriate real resource in that sense a virtual resource is an abstract view of a resource taken by a program use of virtual resources started with the use of virtual devices to prevent mutual interference between programs it was a good idea to allocate a device exclusively for use by one program however a computer system did not possess many real devices so virtual devices were used an os would create a virtual device when a user needed an io device eg the disks called disk and disk in part overview table could be two virtual disks based on the real disk which are allocated to programs p and p respectively virtual devices are used in contemporary operating systems as well a print server is a common example of a virtual device when a program wishes to print a file the print server simply copies the file into the print queue the program requesting the print goes on with its operation as if the printing had been performed the print server continuously examines the print queue and prints the files it finds in the queue most operating systems provide a virtual resource called virtual memory which is an illusion of a memory that is larger in size than the real memory of a computer its use enables a programmer to execute a program whose size may exceed the size of real memory some operating systems create virtual machines vms so that each machine can be allocated to a user the advantage of this approach is twofold allocation of a virtual machine to each user eliminates mutual interference between users it also allows each user to select an os of his choice to operate his virtual machine in effect this arrangement permits users to use different operating systems on the same computer system simultaneously see section security and protection as mentioned in section an os must ensure that no person can illegally use programs and resources in the system or interfere with them in any manner the security function counters threats of illegal use or interference that are posed by persons or programs outside the control of an operating system whereas the protection function counters similar threats posed by its users figure illustrates how security and protection threats arise in an os in a classical standalone environment a computer system functions in complete isolation in such a system the security and protection issues can be handled easily recall that an os maintains information that helps in implementing the security and protection functions see table the identity of a person wishing to use a computer system is verified through a password when the person logs in this action which is called authentication ensures that no person other computer system security resources intruder threats protection threats internet programs authentication users figure overview of security and protection threats deadlock prevention the four conditions described in table must hold simultaneously for a resource deadlock to arise in a system to prevent deadlocks the kernel must use a resource allocation policy that ensures that one of these conditions can not arise in this section we first discuss different approaches to deadlock prevention and then present some resource allocation policies that employ these approaches nonshareable resources waitfor relations will not exist in the system if all resources could be made shareable this way paths in an rrag would contain only allocation edges so circular waits could not arise figure a illustrates the effect of employing this approach the request edge pi rl would be replaced by an allocation edge rl pi because the resource unit of class rl is shareable part process management approach illustration without this approach in this approach make resources shareable rl rl no waits process pi does not get blocked on resource rl pi pj pi pj make resources preemptible rk rl rk rl no circular paths resource rl is preempted and allocated to pi pi pi rk rl rk rl prevent holdandwait no paths in rrag with process process pi is either pi pi not permitted to block on resource rl or not rk rl allowed to hold rk while requesting rl pi prevent circular waits pj pj process pj is not permitted to rk rl rk rl request resource rl pi pi figure approaches to deadlock prevention however some resources such as printers are inherently nonshareable so how can they be made shareable oss use some innovative techniques to solve this problem an example is found in the the multiprogramming system of the s it contained only one printer so it buffered the output produced by different processes formatted it to produce page images and used the printer to print one page image at a time this arrangement mixed up the printed pages produced by different processes and so the output of different processes had to be separated manually interestingly the reason the the system performed page formatting was not to prevent deadlocks but to improve printer utilization in fact the the system made no provisions for handling resource deadlocks the nonshareability of a device can also be circumvented by creating virtual devices see section eg virtual printers can be created and allocated to processes however this approach can not work for software resources like chapter deadlocks shared files which should be modified in a mutually exclusive manner to avoid race conditions preemption of resources if resources are made preemptible the kernel can ensure that some processes have all the resources they need which would prevent circular paths in rrag for example in figure b resource rl can be preempted from its current holder and allocated to process pi however nonpreemptibility of resources can be circumvented only selectively the page formatting approach of the the system can be used to make printers preemptible but in general sequential io devices can not be preempted holdandwait to prevent the holdandwait condition either a process that holds resources should not be permitted to make resource requests or a process that gets blocked on a resource request should not be permitted to hold any resources thus in figure c either edge pi rl would not arise or edge rk pl would not exist if pi rl arises in either case rrag paths involving more than one process could not arise and so circular paths could not exist a simple policy for implementing this approach is to allow a process to make only one resource request in its lifetime in which it asks for all the resources it needs we discuss this policy in section circular wait a circular wait can result from the holdandwait condition which is a consequence of the nonshareability and nonpreemptibility conditions so it does not arise if either of these conditions does not arise circular waits can be separately prevented by not allowing some processes to wait for some resources eg process pj in figure d may not be allowed to wait for resource rl it can be achieved by applying a validity constraint to each resource request the validity constraint is a boolean function of the allocation state it takes the value false if the request may lead to a circular wait in the system so such a request is rejected right away if the validity constraint has the value true the resource is allocated if it is available otherwise the process is blocked for the resource in section we discuss a deadlock prevention policy taking this approach all resources together this is the simplest of all deadlock prevention policies a process must ask for all resources it needs in a single request the kernel allocates all of them together this way a blocked process does not hold any resources so the holdandwait condition is never satisfied consequently circular waits and deadlocks can not arise under this policy both processes of example must request a tape drive and a printer together now a process will either hold both resources or hold none of them and the holdandwait condition will not be satisfied part process management simplicity of implementation makes all resources together an attractive policy for small operating systems however it has one practical drawback it adversely influences resource efficiency for example if a process pi requires a tape drive at the start of its execution and a printer only toward the end of its execution it will be forced to request both a tape drive and a printer at the start the printer will remain idle until the latter part of pis execution and any process requiring a printer will be delayed until pi completes its execution this situation also reduces the effective degree of multiprogramming and therefore reduces cpu efficiency resource ranking under this deadlock prevention policy a unique number called a resource rank is associated with each resource class when a process pi makes a request for a resource the kernel applies a validity constraint to decide whether the request should be considered the validity constraint takes the value true only if the rank of the requested resource is larger than the rank of the highest ranked resource currently allocated to pi in this case the resource is allocated to pi if it is available otherwise pi is blocked for the resource if the validity constraint is false the request is rejected and process pi which made the request would be aborted absence of circular waitfor relationships in a system using resource ranking can be explained as follows let rankk denote the rank assigned to resource class rk and let process pi hold some units of resource class rk pi can get blocked on a request for a unit of some resource class rl only if rankk rankl now consider a process pj that holds some units of resource class rl process pj can not request a unit of resource class rk since rankk rankl thus if pi can wait for pj pj can not wait for pi hence two processes can not get into a circular wait condition an analogous argument holds for the absence of a circular wait involving a larger number of processes example illustrates operation of the resource ranking policy example resource ranking in example let rankprinter ranktape request leads to allocation of the tape drive to pi and request leads to allocation of the printer to pj request which is pis request for the printer satisfies the validity constraint because rankprinter ranktape but it remains pending because the printer is not available request will be rejected since it violates the validity constraint and process pj will be aborted this action will release the printer which will then be allocated to pi the resource ranking policy works best when all processes require their resources in the order of increasing resource rank however difficulties arise deadlock avoidance a deadlock avoidance policy grants a resource request only if it can establish that granting the request can not lead to a deadlock either immediately or in the future but it raises an obvious question algorithm described in section can be used to check whether granting a resource request results in a deadlock immediately but how would the kernel know whether a deadlock can arise in the future the kernel lacks detailed knowledge about future behavior of processes so it can not accurately predict deadlocks to facilitate deadlock avoidance under these conditions it uses the following conservative approach each process declares the maximum number of resource units of each class that it may require the kernel permits a process to request these resource units in stages that is a few resource units at a time subject to the maximum number declared by it and uses a worstcase analysis technique to check for the possibility of future deadlocks a request is granted only if there is no possibility of deadlocks otherwise it remains pending until it can be granted this approach is conservative because a process may complete its operation without requiring the maximum number of units declared by it thus the kernel may defer granting of some resource requests that it would have granted immediately had it known about future behavior of processes this effect and the overhead of making this check at every resource request constitute the cost of deadlock avoidance we discuss a wellknown algorithm called the bankers algorithm that uses this approach table describes notation of the bankers algorithm maxneedjk indicates the maximum number of resource units of resource class rk that a process pj may require the kernel admits process pj only if maxneedjk totalresourcesk for all k the kernel may admit any number of processes satisfying this admission criterion thus j maxneedjk may exceed totalresourcesk allocatedresourcesjk indicates the actual number of resource units of resource class rk that are allocated to pj and totalallock indicates how many units of resource class rk are allocated to processes at present the bankers algorithm avoids deadlocks by part process management table notation used in the bankers algorithm notation explanation requestedresourcesjk number of units of resource class rk currently requested by process pj maxneed j k maximum number of units of resource class rk that may be needed by process pj allocatedresourcesjk number of units of resource class rk allocated to process pj totalallock total number of allocated units of resource class rk ie j allocatedresourcesjk totalresourcesk total number of units of resource class rk existing in the system ensuring that at every moment the system is in such an allocation state that all processes can complete their operation without the possibility of deadlocks it is called the bankers algorithm because bankers need a similar algorithm they admit loans that collectively exceed the banks funds and then release each borrowers loan in installments the bankers algorithm uses the notion of a safe allocation state to ensure that granting of a resource request can not lead to a deadlock either immediately or in future definition safe allocation state an allocation state in which it is possible to construct a sequence of process completion resource release and resource allocation events through which each process pj in the system can obtain maxneedjk resources for each resource class rk and complete its operation deadlock avoidance is implemented by taking the system from one safe allocation state to another safe allocation state as follows when a process makes a request compute the new allocation state the system would be in if the request is granted we will call this state the projected allocation state if the projected allocation state is a safe allocation state grant the request by updating the arrays allocatedresources and totalalloc otherwise keep the request pending when a process releases any resources or completes its operation examine all pending requests and allocate those that would put the system in a new safe allocation state the bankers algorithm determines the safety of a resource allocation state by trying to construct a sequence of process completion resource release and resource allocation events through which all processes can complete it can be performed through simulation as in section except for one change to complete a process pl whether in the running or blocked state may require maxneedlk allocatedresourceslk more resource units of each resource class rk so the chapter deadlocks algorithm checks whether for all rk t otalresourcesk t otalallock m axneedlk allocatedresourceslk when this condition is satisfied it simulates completion of process pl and release of all resources allocated to it by updating totalallock for each rk it then checks whether any other process can satisfy eq and so on the next example illustrates this method in a system having a single class of resources note that as in deadlock detection the determination of safety of an allocation state is not sensitive to the order in which processes are assumed to complete their operation bankers algorithm for a single resource class example a system contains units of resource class rk the maximum resource requirements of three processes p p and p are and resource units respectively and their current allocations are and resource units respectively figure depicts the current allocation state of the system process p now makes a request for one resource unit in the projected allocation state totalalloc and so there will be two free units of resource class rk in the system the safety of the projected state is determined as follows p satisfies condition since it is exactly two units short of its maximum requirements hence the two available resource units can be allocated to p if it requests them in the future and it can complete that will make five resource units available for allocation so ps balance requirement of four resource units can be allocated to it and it can complete now all resource units in the system are available to p so it too can complete thus the projected allocation state is safe hence the algorithm will grant the request by p the new allocation for the processes is and resource units and totalallock now consider the following requests p makes a request for resource units p makes a request for resource units p makes a request for resource units the requests by p and p do not put the system in safe allocation states because condition is not satisfied by any process so these requests will not be granted however the request by p will be granted p p p total p p p alloc p p p total resources max allocated requested need resources resources figure an allocation state in the bankers algorithm for a single resource class part process management algorithm is the bankers algorithm when a new request is made by a process its request is entered in the matrix requestedresources which stores pending requests of all processes and the algorithm is invoked with the id of the requesting process when a process releases some resources allocated to it or completes its operation the algorithm is invoked once for each process whose request is pending the algorithm can be outlined as follows after some initializations in step the algorithm simulates granting of the request in step by computing the projected allocation state step checks whether the projected allocation state is feasible ie whether sufficient free resources exist to permit granting of the request step is the core of the algorithm it is executed only if the projected allocation state is feasible to check whether the projected allocation state is a safe allocation state it checks whether the maximum need of any active process ie any process in the sets running or blocked can be satisfied by allocating some of the free resources if such a process exists this step simulates its completion by deleting it from the set active and releasing the resources allocated to it this action is performed repeatedly until no more processes can be deleted from the set active if the set active is empty at the end of this step the projected state is a safe allocation state so step deletes the request from the list of pending requests and allocates the requested resources this action is not performed if the projected allocation state is either not feasible or not safe so the request remains pending note the similarity of step to the deadlock detection algorithm algorithm accordingly the algorithm requires an order of n r work algorithm bankers algorithm inputs n number of processes r number of resource classes blocked set of processes running set of processes prequestingprocess process making the new resource request maxneed array n r of integer allocatedresources array n r of integer requestedresources array n r of integer totalalloc array r of integer totalresources array r of integer data structures active set of processes feasible boolean newrequest array r of integer simulatedallocation array n r of integer simulatedtotalalloc array r of integer active running blocked for k r newrequestk requestedresourcesrequesting process k chapter deadlocks simulatedallocation allocatedresources for k r compute projected allocation state simulatedallocationrequesting process k simulatedallocationrequesting process k newrequestk simulatedtotalallock totalallock newrequestk feasible true for k r check whether projected allocation state is feasible if totalresourcesk simulatedtotalallock then feasible false if feasible true then check whether projected allocation state is a safe allocation state while set active contains a process pl such that for all k totalresourcesk simulatedtotalallock maxneedl k simulatedallocationl k delete pl from active for k r simulatedtotalallock simulatedtotalallock simulatedallocationl k if set active is empty then projected allocation state is a safe allocation state for k r delete the request from pending requests requestedresourcesrequesting process k for k r grant the request allocatedresourcesrequesting process k allocatedresourcesrequesting process k newrequestk totalallock totalallock newrequestk bankers algorithm for multiple resource classes example figure illustrates operation of the bankers algorithm in a system containing four processes p p four resource classes contain and resource units of which and resource units are currently allocated process p has made a request which is about to be processed the algorithm simulates the granting of this request in step and checks the safety of the projected allocation state in step figure b shows the data structures of the bankers algorithm at the start of this check in this state and resource units are available so only process p can complete hence the algorithm simulates its completion figure c shows the data structures after p has completed resources allocated to p have been freed so they are deducted from simulatedalloc and p is deleted from set active process p needs and resource units to fulfill its maximum resource need so it can be allocated these resources now and it can complete the remaining processes can complete in the order p p hence the request made by process p is granted part process management a state after step r r r r r r r r r r r r p p p r r r r p p p total p p p alloc p p p total exist max allocated requested active p p p p need resources resources b state before while loop of step p p p simulated p p p totalalloc p p p p p p total exist max simulated requested active p p p p need allocation resources c state after simulating completion of process p p p p simulated p p p totalalloc p p p p p p total exist max simulated requested active p p p need allocation resources d state after simulating completion of process p p p p simulated p p p totalalloc p p p p p p total exist max simulated requested active p p need allocation resources e state after simulating completion of process p p p p simulated p p p totalalloc p p p p p p total exist max simulated requested active p need allocation resources figure operation of the bankers algorithm for example characterization of resource deadlocks by graph models a deadlock characterization is a statement of the essential features of a deadlock in section we presented a deadlock detection algorithm using the matrix model of the allocation state of a system following that algorithm we can characterize a deadlock as a situation in which we can not construct a sequence of process completion resource release and resource allocation events whereby all processes in the system can complete in this section we discuss characterization of deadlocks using graph models of allocation state and elements of graph theory as we saw in section a circular waitfor relationship among processes is a necessary condition for a deadlock it is manifest in a cycle in an rrag or wfg a cycle is a sufficient condition for a deadlock in some systems see example but not in others see example this difference is caused by the nature of resource classes and resource requests in the system hence we first classify systems according to the resource classes and resource requests used in them and develop separate deadlock characterizations for different classes of systems later we point at a deadlock characterization that is applicable to all systems we use an rrag to depict the allocation state of a system all through this discussion resource class and resource request models a resource class ri may contain a single instance of its resource or it may contain many instances we refer to the two kinds of classes as single instance si resource classes and multiple instance mi resource classes respectively we define two kinds of resource requests in a single request sr a process is permitted to request one unit of only one resource class in a multiple request mr a process is permitted to request one unit each of several resource classes the kernel never partially allocates a multiple request ie it either allocates all resources requested in a multiple request or does not allocate any of them in the latter case the process making the request is blocked until all resources can be allocated to it using the resource class and resource request models we can define four kinds of systems as shown in figure we name these systems by combining the name of the resource class model and the resource request model used by them accordingly the sisr system is one that contains si resource classes and sr requests singleinstance singlerequest sisr systems in an sisr system each resource class contains a single instance of the resource and each request is a single request as discussed in section existence of a cycle in an rrag implies a mutual waitfor relationship for a set of processes since each resource class contains a single resource unit each blocked process pi in the cycle waits for exactly one other process say pk to release the required resource hence a cycle that involves process pi also involves process pk this fact part process management resource request models single request sr multiple request mr model model multiple multipleinstance multipleinstance instance mi singlerequest multiplerequest resource model misr mimr instance models single singleinstance singleinstance instance si singlerequest multiplerequest model sisr simr figure classification of systems according to resource class and resource request models satisfies condition for all processes in the cycle a cycle is thus a necessary as well as a sufficient condition to conclude that a deadlock exists in the system multipleinstance singlerequest misr systems a cycle is not a sufficient condition for a deadlock in misr systems because resource classes may contain several resource units the system of example in section illustrated this property so we analyze it to understand what conditions should hold for a deadlock to exist in an misr system the rrag of the system contained a cycle involving processes pi and pj with pj requiring a tape drive and pi holding a tape drive see figure however process pk which did not belong to the cycle also held a unit of tape drive so the mutual waitfor relation between pi and pj ceased to exist when pk released a tape drive process pi would have been in deadlock only if processes pj and pk had both faced indefinite waits thus for a process to be in deadlock it is essential that all processes that hold units of a resource required by it are also in deadlock we use concepts from graph theory to incorporate this aspect in a characterization of deadlocks in misr systems a graph g is an ordered pair g n e where n is a set of nodes and e is a set of edges a graph g n e is a subgraph of a graph g n e if n n and e e ie if all nodes and edges contained in g are also contained in g g is a nontrivial subgraph of g if e ie if it contains at least one edge we now define a knot to characterize a deadlock in misr systems definition knot a nontrivial subgraph g n e of an rrag in which every node ni n satisfies the following conditions for every edge of the form ni nj in e ni nj is included in e and nj is included in n if a path ni nj exists in g a path nj ni also exists in g part of definition ensures that if a node is included in a knot all its outedges ie all edges emanating from it are also included in the knot chapter deadlocks part of definition ensures that each outedge of each node is included in a cycle this fact ensures that each process in the knot is necessarily in the blocked state parts and together imply that all processes that can release a resource needed by some process in the knot are themselves included in the knot which satisfies condition thus one can conclude that the presence of a knot in an rrag is a necessary and sufficient condition for the existence of a deadlock in an misr system deadlock in an misr system example the rrag of figure depicts the allocation state in example after requests are made it does not contain a knot since the path pi pk exists in it but a path pk pi does not exist in it now consider the situation after the following request is made pk requests a printer process pk now blocks on the sixth request the resulting rrag is shown in figure the complete rrag is a knot because part of definition is trivially satisfied and every outedge of every node is involved in a cycle which satisfies part of definition it is easy to verify that processes pi pj pk are in a deadlock since blockedp pi pj pk wf i pj wf j pi pk and wf k pj satisfies conditions and singleinstance multiplerequest simr systems each resource class in the simr system contains only one resource unit and so it has exactly one outedge in an rrag a process may make a multiple request in which case it has more than one outedge such a process remains blocked if even one of the resources requested by it is unavailable this condition is satisfied when the process is involved in a cycle so a cycle is a necessary and sufficient condition for a deadlock in an simr system this property is illustrated by the system of figure the process node pi has an outedge pi r that is a part of a cycle and an outedge pi r that is not a part of any cycle process pi printer tape pj pk figure a knot in the rrag of an misr system implies a deadlock part process management pi r r r pj pk figure a cycle is a necessary and a sufficient condition for a deadlock in an simr system pi remains blocked until a resource unit of r can be allocated to it since the outedge pi r is involved in a cycle pi faces an indefinite wait pj also faces an indefinite wait hence pi pj are involved in a deadlock multipleinstance multiplerequest mimr systems in the mimr model resource classes contain several resource units and processes may make multiple requests hence both process and resource nodes of an rrag can have multiple outedges if none of the resource nodes involved in a cycle in the rrag has multiple outedges the cycle is similar to a cycle in the rrag of an simr system and so it is a sufficient condition for the existence of a deadlock however if a resource node in a cycle has multiple outedges a cycle is a necessary condition but not a sufficient condition for a deadlock in such cases every outedge of the resource node must be involved in a cycle this requirement is similar to that in the misr systems example illustrates this aspect example deadlock in an mimr system the rrag of figure contains the cycle r pi r pj r resource node r contains an outedge r pk that is not included in a cycle hence process pk may obtain resource r and eventually release an instance of resource class r which could be allocated to process pj it will break the cycle in the rrag hence there is no deadlock in the system if the allocation edge of r were to be r pi both outedges of r would be involved in cycles process pjs request for r would now face an indefinite wait and so we would have a deadlock situation note that outedge pi r of pi is not involved in a cycle however a deadlock exists because pi has made a multiple request and its request for resource class r causes an indefinite wait for it from the above discussion and example it is clear that we must differentiate between process and resource nodes in the rrag of an mimr system all outedges of a resource node must be involved in cycles for a deadlock to arise whereas a process node needs to have only one outedge involved in a cycle chapter deadlocks pi r r r r pk pj pl figure rrag for an mimr system we define a resource knot to incorporate this requirement where a resource knot differs from a knot only in that part of definition applies only to resource nodes definition resource knot a nontrivial subgraph g n e of an rrag in which every node ni n satisfies the following conditions if ni is a resource node for every edge of the form ni nj in e ni nj is included in e and nj is included in n if a path ni nj exists in g a path nj ni also exists in g resource knot example nodes pi pj pk r r and r of figure would be involved in a resource knot if the allocation edge of resource class r is r pi note that outedge pi r of process pi is not included in the resource knot clearly a resource knot is a necessary and sufficient condition for the existence of a deadlock in an mimr system in fact we state here without proof that a resource knot is a necessary and sufficient condition for deadlock in all classes of systems discussed in this section see exercise processes in deadlock d the set of processes in deadlock contains processes represented by process nodes in resource knots it also contains some other processes that face indefinite waits we use the following notation to identify all processes in d rri the set of resource classes requested by process pi hsk the holder set of resource class rk ie set of processes to which units of resource class rk are allocated ks the set of process nodes in resource knots we call it the knotset of rrag as an auxiliary set of process nodes in rrag that face indefinite waits these nodes are not included in a resource knot deadlock handling in practice an operating system manages numerous and diverse resources hardware resources such as memory and io devices software resources such as files containing programs or data and interprocess messages and kernel resources such as data structures and control blocks used by the kernel the overhead of deadlock detectionandresolution and deadlock avoidance make them unattractive deadlock handling policies in practice hence an os either chapter deadlocks uses the deadlock prevention approach creates a situation in which explicit deadlock handling actions are unnecessary or simply does not care about possibility of deadlocks further since deadlock prevention constrains the order in which processes request their resources operating systems tend to handle deadlock issues separately for each kind of resources like memory io devices files and kernel resources we discuss these approaches in the following memory memory is a preemptible resource so its use by processes can not cause a deadlock explicit deadlock handling is therefore unnecessary the memory allocated to a process is freed by swapping out the process whenever the memory is needed for another process io devices among deadlock prevention policies the all resources together policy requires processes to make one multiple request for all their resource requirements this policy incurs the least cpu overhead but it has the drawback mentioned in section it leads to underutilization of io devices that are allocated much before a process actually needs them resource ranking on the other hand is not a feasible policy to control use of io devices because any assignment of resource ranks causes inconvenience to some group of users this difficulty is compounded by the fact that io devices are generally nonpreemptible operating systems overcome this difficulty by creating virtual devices for example the system creates a virtual printer by using some disk area to store a file that is to be printed actual printing takes place when a printer becomes available since virtual devices are created whenever needed it is not necessary to preallocate them as in the all resources together policy unless the system faces a shortage of disk space files and interprocess messages a file is a usercreated resource an os contains a large number of files deadlock prevention policies such as resource ranking could cause high overhead and inconvenience to users hence operating systems do not extend deadlock handling actions to files processes accessing a common set of files are expected to make their own arrangements to avoid deadlocks for similar reasons operating systems do not handle deadlocks caused by interprocess messages control blocks the kernel allocates control blocks such as process control blocks pcbs and event control blocks ecbs to processes in a specific order a pcb is allocated when a process is created and an ecb is allocated when the process becomes blocked on an event hence resource ranking can be a solution here if a simpler policy is desired all control blocks for a job or process can be allocated together at its initiation deadlock handling in unix most operating systems simply ignore the possibility of deadlocks involving user processes and unix is no exception however unix addresses deadlocks due to sharing of kernel data structures by user processes recall from section that a part process management unix process that was running on the cpu executes kernel code when an interrupt or system call occurs hence user processes could concurrently execute kernel code the kernel employs the resource ranking approach to deadlock prevention see section by requiring processes to set locks on kernel data structures in a standard order however there are exceptions to this rule and so deadlocks could arise we present simplified views of two arrangements used to prevent deadlocks the unix kernel uses a buffer cache see section to speed up accesses to frequently used disk blocks it consists of a pool of buffers in memory and a hashed data structure to check whether a specific disk block is present in a buffer to facilitate reuse of buffers a list of buffers is maintained in least recently used lru order the first buffer in the list is the least recently used buffer and the last buffer is the most recently used buffer the normal order of accessing a disk block is to use the hashed data structure to locate a disk block if it exists in a buffer put a lock on the buffer containing the disk block and then put a lock on the list of buffers to update the lru status of the buffer however if a process merely wants to obtain a buffer for loading a new disk block it directly accesses the list of buffers and takes off the first buffer that is not in use at the moment to perform this action the process puts a lock on the list then it tries to set the lock on the first buffer in the list deadlocks are possible because this order of locking the list and a buffer is different from the standard order of setting these locks unix uses an innovative approach to avoid such deadlocks it provides a special operation that tries to set a lock but returns with a failure condition code if the lock is already set the process looking for a free buffer uses this operation to check whether a buffer is free if a failure condition code is returned it simply tries to set the lock on the next buffer and so on until it finds a buffer that it can use this approach avoids deadlocks by avoiding circular waits another situation in which locks can not be set in a standard order is in the file system function that establishes a link see section a link command provides path names for a file and a directory that is to contain the link to the file this command can be implemented by locking the directories containing the file and the link however a standard order can not be defined for locking these directories consequently two processes concurrently trying to lock the same directories may become deadlocked to avoid such deadlocks the file system function does not try to acquire both locks at the same time it first locks one directory updates it in the desired manner and releases the lock it then locks the other directory and updates it thus it requires only one lock at any time this approach prevents deadlocks because the holdandwait condition is not satisfied by these processes deadlock handling in windows windows vista provides a feature called wait chain traversal wct which assists applications and debuggers in detecting deadlocks a wait chain starts summary a deadlock is a situation in which a set of proprocess currently blocked on a resource request cesses wait indefinitely for events because each of can be allocated the required resource through a the events can be caused only by other processes sequence of process completion resource release in the set a deadlock adversely affects user serand resource allocation events deadlock detecvice throughput and resource efficiency in this tion incurs a high overhead as a result of this chapter we discussed os techniques for handling check so approaches that ensure the absence of deadlocks deadlocks have been studied in the deadlock prea resource deadlock arises when four convention approach the resource allocation policy ditions hold simultaneously resources are nonimposes some constraints on resource requests so shareable and nonpreemptible a process holds that the four conditions for deadlock would not some resources while it waits for resources that be satisfied simultaneously in the deadlock avoidare in use by other processes which is called the ance approach the resource allocator knows a holdandwait condition and circular waits exist processs maximum need for resources at every among processes an os can discover a deadresource request it checks whether a sequence of lock by analyzing the allocation state of a system process completion resource release and resource which consists of information concerning allocated allocation events can be found through which all resources and resource requests on which processes processes could satisfy their maximum need and are blocked a graph model of allocation state can complete their operation it grants the resource be used in systems where a process can not request request only if this check is satisfied more than one resource unit of a resource class when a graph model of allocation state is used a resource request and allocation graph rrag deadlocks can be characterized in terms of paths in depicts resource allocation and pending resource the graph however the characterization becomes requests in the os whereas a waitfor graph wfg complex when a resource class can contain many depicts waitfor relationships between processes in resource units both models a circular wait condition is reflected for reasons of convenience and efficiency an in a circular path in the graph a matrix model os may use different deadlock handling policies represents the allocation state in a set of matrices for different kinds of resources typically an os when a process completes its operation it uses deadlock prevention approaches for kernel releases its resources and the kernel can allocate resources and creates virtual resources to avoid them to other processes that had requested them deadlocks over io devices however it does not when a matrix model of allocation state is used a handle deadlocks involving user resources like files deadlock can be detected by finding whether every and interprocess messages part process management test your concepts classify each of the following statements as true resources the system contains at least or false resource units a a cycle in the resource request and allocation i an os employing a multipleresource graph rrag is a necessary and sufficient bankers algorithm has been in operation for condition for a deadlock if each resource class some time with four processes a new procontains only one resource unit cess arrives in the system it is initially not b deadlock resolution guarantees that deadallocated any resources is the new allocation locks will not occur in future state of the system safe c the all resources together policy of deadj if every resource class in a system has a single lock prevention ensures that the circuresource unit every cycle in the rrag of the lar wait condition will never hold in the system is also a resource knot system an os contains n resource units of a resource d the resource ranking policy of deadlock class three processes use this resource class and prevention ensures that the holdandwait each of them has a maximum need of resource condition will never hold in the system units the manner and the order in which the e if a set of processes d is deadlocked the set processes request units of the resource class are blocked of algorithm will contain some not known what is the smallest value of n for of these processes when execution of the algooperation of the system to be free of deadlocks rithm completes however blocked may not a b c d contain all of them an os employs the bankers algorithm to conf if a process pi requests r units of a resource trol allocation of tape drives maximum need class rj and r units of rj are free then of three processes p p and p are and the bankers algorithm will definitely allocate drives respectively how many drives can the r units to pi os allocate safely to process p if the current g the bankers algorithm does not guarantee allocation state is as follows that deadlocks will not occur in future a and tape drives are allocated to proh an os has a single resource class that is concesses p p and p respectively trolled by the bankers algorithm units of i ii iii iv the resource have been currently allocated to b and tape drives are allocated to proprocesses of which process pi has been allocesses p p and p respectively cated resources if pi has a max need of i ii iii iv exercises clearly justify why deadlocks can not arise in a a resource requested by some process pi is bounded buffer producersconsumers system unavailable when resource ranking is used as a deadlock prea the resource is preempted from one of its vention policy a process is permitted to request a holder processes pj if pj is younger than pi unit of resource class rk only if rankk ranki for the resource is now allocated to pi it is every resource class ri whose resources are alloallocated back to pj when pi completes a cated to it explain whether deadlocks can arise process is considered to be younger if it was if the condition is changed to rankk ranki initiated later a system containing preemptible resources uses b if condition a is not satisfied pi is blocked the following resource allocation policy when for the resource chapter deadlocks a released resource is always allocated to its oldb would the following requests be granted in est requester show that deadlocks can not arise the current state by the bankers algorithm in this system also show that starvation does i process p requests not occur ii process p requests develop a matrix model for the allocation state iii process p requests of the system of figure apply algo three processes p p and p use a resource rithm to find the processes involved in controlled through the bankers algorithm two deadlock unallocated resource units exist in the current the system of figure is changed such that allocation state when p and p request for process p has made a multiple request for one resource unit each they become blocked on resources r and r what are the processes their requests however when p requests for involved in a deadlock process p is aborted two resource units its request is granted right and process p makes a request for resource r away explain why it may be so is the system in a deadlock now a system using the bankers algorithm for a system uses a deadlock detectionandresource allocation contains n and n resource resolution policy the cost of aborting one prounits of resource classes r and r and three cess is considered to be one unit discuss how processes p p and p the unallocated to identify victim processes so as to minimize resources with the system are the folthe cost of deadlock resolution in each of the following observations are made regarding the lowing systems a sisr systems b simr sysoperation of the system tems c misr systems and d mimr systems a if process p makes a request followed is the allocation state in which and by a request the request will be resource units are allocated to processes p p granted but the request will not be and p in the system of example safe granted would the allocation state in which and b if instead of making the resource requests in resource units are allocated be safe part a process p makes a request it would the following requests be granted in the will be granted current state by the bankers algorithm find a possible set of values for the current rr rr rr allocations and maximum resource requirep p total ments of the processes such that decisions using p p alloc the bankers algorithm will match the above total observations max allocated exist show that when the bankers algorithm is need resources applied to a finite set of processes each having a a process p requests finite execution time each resource request will b process p requests be granted eventually c process p requests processes in a particular os make multiple d process p requests requests this os uses a bankers algorithm e process p requests designed for a single resource class to implement in the following system deadlock avoidance as follows when a process requests resource units of some n resource rrr rrr rrr classes the request is viewed as a set of n single p p total requests eg a multiple request would p p alloc be viewed as three single requests p p total and the multiple request is exist granted only if each single request would have max allocated been granted in the current allocation state of need resources the system is this a sound approach to deadlock a is the current allocation state safe avoidance justify your answer either by giving part process management an argument about its correctness or by giving b can the system make a transition to a safe an example where it will fail allocation state if so give an example show a singleresource system contains total ing such a transition resourcess units of resource class rs if the sys show that a resource knot in an rrag is tem contains n processes show that a deadlock a necessary and sufficient condition for deadcannot arise if any one of the following conlocks in sisr misr simr and mimr ditions is satisfied see the notation used in systems algorithm a wfg is used to represent the allocation a for all i maxneedis totalresourcessn state of a system in which resource classes b i maxneedis totalresourcess may contain multiple units and processes can c i maxneed is totalresourcess n make multiple resource requests an mimr sys and for all i m axneedis tem develop a deadlock characterization using t otalresourcess the wfg hint a node in the wfg would in a singleresource system containing total have more than one outedge under two conresourcess units of resource class rs set pa is ditions when a process requests a resource defined as follows unit of a multipleinstance resource class and when a process makes a multiple request these pa pi pi has been allocated some are called or outedges and and outedges resources but all its resource respectively to differentiate between the two requirements have not been met kinds of outedges the and outedges of a process are joined by a straight line as shown in which of the following statements are true see figure b figure a shows the outthe notation used in algorithm justify your edges for the rrag of figure whereas answer figure b shows the outedges for the a processes in pa will definitely become rrag of figure these outedges have deadlocked if i maxneed is totaldifferent implications for deadlock detection resourcess an os uses a simple policy to deal with deadb processes in pa may be deadlocked only lock situations when it finds that a set of if there exists some integer k such that processes is deadlocked it aborts all of them num prock totalresourcess k where and restarts them immediately what are the num prock is the number of processes in pa conditions under which the deadlock will not whose maximum requirement for the units of recur resource class rs exceeds k an os has a single disk which it uses a the new allocation state of a system after grantto create user files and b to create a virtual ing of a resource request is not a safe allocation printer for every process space is allocated for state according to the bankers algorithm both uses on a demand basis and a process a does it imply that a deadlock will definitely is blocked if its disk space requirement can not arise in future be granted print requests directed at a virtual pi pi pj pj pk pk a b figure wfgs with multiple outedges a or edges b and edges chapter deadlocks printer are sent to a real printer when a process it is proposed to use a deadlock prevention finishes is there a possibility of deadlocks in this approach for the dining philosophers problem system if so under what conditions suggest a see section as follows seats at the dinner solution to the deadlock problem table are numbered from to n and forks are also a phantom deadlock is a situation wherein a numbered from to n such that the left fork for deadlock handling algorithm declares a deadseat i has the fork number i philosophers are lock but a deadlock does not actually exist required to obey the following rule a philosoif processes are permitted to withdraw their pher must first pick up the lowernumbered fork resource requests show that algorithm may then pick up the highernumbered fork show detect phantom deadlocks can detection of that deadlocks can not arise in this system phantom deadlocks be prevented a set of processes d is in deadlock it is observed a road crosses a set of railway tracks at two that points gates are constructed on the road at each a if a process pj d is aborted a set of crossing to stop road traffic when a train is about processes d d is still in deadlock to pass train traffic is stopped if a car blocks a b if a process pi d is aborted no deadlock track two way traffic of cars is permitted on exists in the system the road and twoway train traffic is permitted state some possible reasons for this difference on the railway tracks and explain with the help of an example hint a discuss whether deadlocks can arise in the refer to eqs and roadandtrain traffic would there be no after algorithm has determined that a set of deadlocks if both road and train traffic are processes d is in deadlock one of the processes only oneway in d is aborted what is the most efficient way to b design a set of simple rules to avoid deaddetermine whether a deadlock exists in the new locks in the roadandtrain traffic state bibliography dijkstra havender and habermann coffman e g m s elphick and a shoshani are early works on deadlock handling dijkstra system deadlocks computing surveys and habermann discuss the bankers algo rithm coffman et al discusses the deadlock dijkstra e w cooperating sequential detection algorithm for a system containing multipleprocesses technical report ewd instance resources holt provided a graph theotechnlogical university eindhoven retic characterization for deadlocks isloor and mars habermann a n prevention of system land is a good survey paper on this topic zobel deadlocks communications of the acm is an extensive bibliography howard discusses the practical deadlock handling approach habermann a n a new approach to described in section tay and loke and levine avoidance of system deadlocks in lecture notes discuss characterization of deadlocks in computer science vol springerverlag bach describes deadlock handling in unix havender j w avoiding deadlock in multitasking systems ibm systems journal bach m j the design of the unix holt r c some deadlock properties of operating system prentice hall englewood computer systems computing surveys cliffs n j part process management howard j h mixed solutions to the resources ieee transactions on software deadlock problem communications of the acm engineering tay y c and w t loke on deadlocks isloor s s and t a marsland the of exclusive andrequests for resources deadlock problem an overview computer distributed computing springer verlag levine g defining deadlock zobel d the deadlock problem a operating systems review classifying bibliography operating systems rypka d j and a p lucido deadlock review detection and avoidance for shared logical overview of message passing in section we summarized four ways in which processes interact with one another data sharing message passing synchronization and signals see table of these we discussed data sharing and synchronization in chapter and signals in chapter data sharing provides means to access values of shared data in a mutually exclusive manner process synchronization is performed by blocking a process until other processes have performed certain specific actions capabilities of message passing overlap those of data sharing and synchronization however each form of process interaction has its own niche application area we discuss this aspect after taking an overview of message passing figure shows an example of message passing process pi sends a message to process pj by executing the statement send pj message the compiled code of the send statement invokes the library module send send makes a part process management process pi process pj send pj message receive pi msgarea figure message passing system call send with pj and the message as parameters execution of the statement receive pi msgarea where msgarea is an area in pjs address space results in a system call receive the semantics of message passing are as follows at a send call by pi the kernel checks whether process pj is blocked on a receive call for receiving a message from process pi if so it copies the message into msgarea and activates pj if process pj has not already made a receive call the kernel arranges to deliver the message to it when pj eventually makes a receive call when process pj receives the message it interprets the message and takes an appropriate action messages may be passed between processes that exist in the same computer or in different computers connected to a network also the processes participating in message passing may decide on what a specific message means and what actions the receiver process should perform on receiving it because of this flexibility message passing is used in the following applications message passing is employed in the clientserver paradigm which is used to communicate between components of a microkernelbased operating system and user processes to provide services such as the print service to processes within an os or to provide webbased services to client processes located in other computers message passing is used as the backbone of higherlevel protocols employed for communicating between computers or for providing the electronic mail facility message passing is used to implement communication between tasks in a parallel or distributed program in principle message passing can be performed by using shared variables for example msgarea in figure could be a shared variable pi could deposit a value or a message in it and pj could collect it from there however this approach is cumbersome because the processes would have to create a shared variable with the correct size and share its name they would also have to use synchronization analogous to the producersconsumers problem see section to ensure that a receiver process accessed a message in a shared variable only after a sender process had deposited it there message passing is far simpler in this situation it is also more general because it can be used in a distributed system environment where the shared variable approach is not feasible the producersconsumers problem with a single buffer a single producer process and a single consumer process can be implemented by message passing as shown in figure the solution does not use any shared variables instead process pi which is the producer process has a variable called buffer and process chapter message passing begin parbegin var buffer var messagearea repeat repeat produce in buffer receive pi messagearea send pj buffer consume from messagearea remainder of the cycle remainder of the cycle forever forever parend end process pi process pj figure producersconsumers solution using message passing pj which is the consumer process has a variable called messagearea the producer process produces in buffer and sends the contents of buffer in a message to the consumer the consumer receives the message in messagearea and consumes it from there the send system call blocks the producer process until the message is delivered to the consumer and the receive system call blocks the consumer until a message is sent to it the producersconsumers solution of figure is much simpler than the solutions discussed in chapter however it is restrictive because it permits a single producer and a single consumer process in the general case it is effective to use the process synchronization means discussed in chapter to implement a system containing producers and consumers issues in message passing two important issues in message passing are naming of processes whether names of sender and receiver processes are explicitly indicated in send and receive statements or whether their identities are deduced by the kernel in some other manner delivery of messages whether a sender process is blocked until the message sent by it is delivered what the order is in which messages are delivered to the receiver process and how exceptional conditions are handled these issues dictate implementation arrangements and also influence the generality of message passing for example if a sender process is required to know the identity of a receiver process the scope of message passing would be limited to processes in the same application relaxing this requirement would extend message passing to processes in different applications and processes operating in different computer systems similarly providing fcfs message delivery may be rather restrictive processes may wish to receive messages in some other order direct and indirect naming in direct naming sender and receiver processes mention each others name for example the send and receive statements might have the following syntax part process management send destination process messagelength messageaddress receive source process messagearea where destination process and source process are process names typically they are process ids assigned by the kernel messageaddress is the address of the memory area in the sender processs address space that contains the textual form of the message to be sent and messagearea is a memory area in the receivers address space where the message is to be delivered the processes of figure used direct naming direct naming can be used in two ways in symmetric naming both sender and receiver processes specify each others name thus a process can decide which process to receive a message from however it has to know the name of every process that wishes to send it a message which is difficult when processes of different applications wish to communicate or when a server wishes to receive a request from any one of a set of clients in asymmetric naming the receiver does not name the process from which it wishes to receive a message the kernel gives it a message sent to it by some process in indirect naming processes do not mention each others name in send and receive statements we discuss indirect naming in section blocking and nonblocking sends a blocking send blocks a sender process until the message to be sent is delivered to the destination process this method of message passing is called synchronous message passing a nonblocking send call permits a sender to continue its operation after making a send call irrespective of whether the message is delivered immediately such message passing is called asynchronous message passing in both cases the receive primitive is typically blocking synchronous message passing provides some nice properties for user processes and simplifies actions of the kernel a sender process has a guarantee that the message sent by it is delivered before it continues its operation this feature simplifies the design of concurrent processes the kernel delivers the message immediately if the destination process has already made a receive call for receiving a message otherwise it blocks the sender process until the destination process makes a receive call the kernel can simply let the message remain in the senders memory area until it is delivered however use of blocking sends has one drawback it may unnecessarily delay a sender process in some situations for example while communicating with a heavily loaded print server asynchronous message passing enhances concurrency between the sender and receiver processes by letting the sender process continue its operation however it also causes a synchronization problem because the sender should not alter contents of the memory area which contains text of the message until the message is delivered to overcome this problem the kernel performs message buffering when a process makes a send call the kernel allocates a buffer in the system area and copies the message into the buffer this way the sender chapter message passing process is free to access the memory area that contained text of the message however this arrangement involves substantial memory commitment for buffers when many messages are awaiting delivery it also consumes cpu time as a message has to be copied twice once into a system buffer when a send call is made and later into the message area of the receiver at the time of message delivery exceptional conditions in message passing to facilitate handling of exceptional conditions the send and receive calls take two additional parameters the first parameter is a set of flags indicating how the process wants exceptional conditions to be handled we will call this parameter flags the second parameter is the address of a memory area in which the kernel provides a condition code describing the outcome of the send or receive call we will call this area statusarea when a process makes a send or receive call the kernel deposits a condition code in statusarea it then checks flags to decide whether it should handle any exceptional conditions and performs the necessary actions it then returns control to the process the process checks the condition code provided by the kernel and handles any exceptional conditions it wished to handle itself some exceptional conditions and their handling actions are as follows the destination process mentioned in a send call does not exist in symmetric naming the source process mentioned in a receive call does not exist a send call can not be processed because the kernel has run out of buffer memory no message exists for a process when it makes a receive call a set of processes becomes deadlocked when a process is blocked on a receive call in cases and the kernel may abort the process that made the send or receive call and set its termination code to describe the exceptional condition in case the sender process may be blocked until some buffer space becomes available case is really not an exception if receives are blocking they generally are but it may be treated as an exception so that the receiving process has an opportunity to handle the condition if it so desires a process may prefer the standard action which is that the kernel should block the process until a message arrives for it or it may prefer an action of its own choice like waiting for a specified amount of time before giving up more severe exceptions belong to the realm of os policies the deadlock situation of case is an example most operating systems do not handle this particular exception because it incurs the overhead of deadlock detection difficulttohandle situations such as a process waiting a long time on a receive call also belong to the realm of os policies implementing message passing buffering of interprocess messages when a process pi sends a message to some process pj by using a nonblocking send the kernel builds an interprocess message control block imcb to store all information needed to deliver the message see figure the control block contains names of the sender and destination processes the length of the message and the text of the message the control block is allocated a buffer in the kernel area when process pj makes a receive call the kernel copies the message from the appropriate imcb into the message area provided by pj the pointer fields of imcbs are used to form imcb lists to simplify message delivery figure shows the organization of imcb lists when blocking sends and fcfs message delivery are used in symmetric naming a separate list is used for every pair of communicating processes when a process pi performs a receive call to receive a message from process pj the imcb list for the pair pipj is used to deliver the message in asymmetric naming a single imcb list can be maintained per recipient process when a process performs a receive the first imcb in its list is processed to deliver a message if blocking sends are used at most one message sent by a process can be undelivered at any point in time the process is blocked until the message is delivered hence it is not necessary to copy the message into an imcb the sender process destination process message length message text or address imcb pointer figure interprocess message control block imcb list headers imcb lists list headers imcb lists for process pairs for processes pipj pi pipk pj pr pipl a b figure lists of imcbs for blocking sends in a symmetric naming b asymmetric naming chapter message passing kernel can simply note the address of the message text in the senders memory area and use this information while delivering the message this arrangement saves one copy operation on the message however it faces difficulties if the sender is swapped out before the message is delivered so it may be preferable to use an imcb fewer imcbs would be needed than when sends are nonblocking because at most one message sent by each process can be in an imcb at any time the kernel may have to reserve a considerable amount of memory for interprocess messages particularly if nonblocking sends are used in such cases it may save message texts on the disk an imcb would then contain the address of the disk block where the message is stored rather than the message text itself delivery of interprocess messages when a process pi sends a message to process pj the kernel delivers the message to pj immediately if pj is currently blocked on a receive call for a message from pi or from any process after delivering the message the kernel must also change the state of pj to ready if process pj has not already performed a receive call the kernel must arrange to deliver the message when pj performs a receive call later thus message delivery actions occur at both send and receive calls recall from section that the kernel uses an event control block ecb to note actions that should be performed when an anticipated event occurs the ecb contains three fields description of the anticipated event id of the process that awaits the event an ecb pointer for forming ecb lists figure shows use of ecbs to implement message passing with symmetric naming and blocking sends when pi makes a send call the kernel checks whether an ecb exists for the send call by pi ie whether pj had made a receive call and was waiting for pi to send a message if it is not the case the kernel knows that the receive call would occur sometime in future so it creates an ecb for the event receive from pi by pj and specifies pi as the process that will be affected by the event process pi is put into the blocked state and the address of the ecb is put in the event info field of its pcb see figure a figure b illustrates pi receive from pi pj send to pj blocked by pj blocked by pi event info pi event info pj pcb of ecb of pcb of ecb of sender process pi receiver process pj receiver process pj sender process pi a b figure ecbs to implement symmetric naming and blocking sends a at send b at receive part process management the case when process pj makes a receive call before pi makes a send call an ecb for a send to pj by pi event is now created the id of pj is put in the ecb to indicate that the state of pj will be affected when the send event occurs figure shows complete details of the kernel actions for implementing message passing by using symmetric naming and blocking sends for reasons mentioned earlier the kernel creates an imcb even though a sender process is blocked until message delivery when process pi sends a message to process pj the kernel first checks whether the send was anticipated ie whether an ecb was created for the send event it will have happened if process pj has already made a receive call for a message from pi if this is the case action s immediately delivers the message to pj and changes its state from blocked to ready the ecb and the imcb are now destroyed if an ecb for send does not exist step s creates an ecb for a receive call by process pj which is now anticipated blocks the sender process and enters the imcb in the imcb list of process pj converse actions are performed at a receive call if a matching send has already occurred a message is delivered to process pj and pi is activated otherwise an ecb is created for a send call and pj is blocked at send to pj by pi step description s create an imcb and initialize its fields s if an ecb for a send to pj by pi event exists s then a deliver the message to pj b activate pj c destroy the ecb and the imcb d return to pi s else a create an ecb for a receive from pi by pj event and put id of pi as the process awaiting the event b change the state of pi to blocked and put the ecb address in pis pcb c add the imcb to pj s imcb list at receive from pi by pj step description r if a matching ecb for a receive from pi by pj event exists r then a deliver the message from appropriate imcb in pj s list b activate pi c destroy the ecb and the imcb d return to pj r else a create an ecb for a send to pj by pi event and put id of pj as the process awaiting the event b change the state of pj to blocked and put the ecb address in pj s pcb figure kernel actions in message passing using symmetric naming and blocking sends mailboxes a mailbox is a repository for interprocess messages it has a unique name the owner of a mailbox is typically the process that created it only the owner process can receive messages from a mailbox any process that knows the name of a mailbox can send messages to it thus sender and receiver processes use the name of a mailbox rather than each others names in send and receive statements it is an instance of indirect naming see section figure illustrates message passing using a mailbox named sample process pi creates the mailbox using the statement createmailbox process pj sends a message to the mailbox using the mailbox name in its send statement if pi has not already executed a receive statement the kernel would store the message in a buffer the kernel may associate a fixed set of buffers with each mailbox or it may allocate buffers from a common pool of buffers when a message is sent both createmailbox and send statements return with condition codes the kernel may provide a fixed set of mailbox names or it may permit user processes to assign mailbox names of their choice in the former case confidentiality of communication between a pair of processes can not be guaranteed because any process can use a mailbox confidentiality greatly improves when processes can assign mailbox names of their own choice to exercise control over creation and destruction of mailboxes the kernel may require a process to explicitly connect to a mailbox before starting to use it and to disconnect when it finishes using it this way it can destroy a mailbox process pi process pj createmailbox sample send sample receive sample pj sample owner of pi pk users of sample sample buffers pl figure creation and use of mailbox sample part process management if no process is connected to it alternatively it may permit the owner of a mailbox to destroy it in that case it has the responsibility of informing all processes that have connected to the mailbox the kernel may permit the owner of a mailbox to transfer the ownership to another process use of a mailbox has following advantages anonymity of receiver a process sending a message to request a service may have no interest in the identity of the receiver process as long as the receiver process can perform the needed function a mailbox relieves the sender process of the need to know the identity of the receiver additionally if the os permits the ownership of a mailbox to be changed dynamically one process can readily take over the service of another classification of messages a process may create several mailboxes and use each mailbox to receive messages of a specific kind this arrangement permits easy classification of messages see example below anonymity of a receiver process as we just saw can offer the opportunity to transfer a function from one process to another consider an os whose kernel is structured in the form of multiple processes communicating through messages interrupts relevant to the process scheduling function can be modeled as messages sent to a mailbox named scheduling if the os wishes to use different process scheduling criteria during different periods of the day it may implement several schedulers as processes and pass ownership of the scheduling mailbox among these processes this way the process scheduler that currently owns scheduling can receive all schedulingrelated messages functionalities of os servers can be similarly transferred for example all print requests can be directed to a laser printer instead of a dot matrix printer by simply changing the ownership of a print mailbox although a process can also remain anonymous when sending a message to a mailbox the identity of the sender often has to be known for example a server may be programmed to return status information for each request it can be achieved by passing the senders id along with the text of the message the sender of the message on the other hand might not know the identity of the server then it would have to receive the servers reply through an asymmetric receive as an alternative the compiler can implement the send call as a blocking call requiring a reply containing the status information so return of status information would be a kernel responsibility example use of mailboxes an airline reservation system consists of a centralized data base and a set of booking processes each process represents one booking agent figure shows a pseudocode for the reservation server it uses three mailboxes named enquire book and cancel and expects a booking process to send enquiry booking and cancellation messages to these mailboxes respectively values higherlevel protocols using message passing in this section we discuss three protocols that use the message passing paradigm to provide diverse services the simple mail transfer protocol smtp delivers electronic mail the remote procedure call rpc is a programming language facility for distributed computing it is used to invoke a part of a program that is located in a different computer parallel virtual machine pvm and message passing interface mpi are message passing standards for parallel programming the simple mail transfer protocol smtp smtp is used to deliver electronic mail to one or more users reliably and efficiently it uses asymmetric naming see section a mail would be delivered to a users terminal if the user is currently active otherwise it would be deposited in the users mailbox the smtp protocol can deliver mail across a number of interprocess communication environments ipces where an ipce may cover a part of a network a complete network or several networks smtp is an applications layer protocol it uses the tcp as a transport protocol and ip as a routing protocol details of these networking layers and details of reliable delivery are however beyond the scope of this chapter they are discussed later in chapter smtp consists of several simple commands the relevant ones for our purposes are as follows the mail command indicates who is sending a mail it contains a reverse path in the network which is an optional list of hosts and the name of the sender mailbox the rcpt command indicates who is to receive the mail it contains a forward path that is an optional list of hosts and a destination mailbox one or more rcpt commands can follow a mail command the data command contains the actual data to be sent to its destinations after processing the data command the sender host starts processing of the mail part process management command to send the data to the destinations when a host accepts the data for relaying or for delivery to the destination mailbox the protocol generates a timestamp that indicates when the data was delivered to the host and inserts it at the start of the data when the data reaches the host containing the destination mailbox a line containing the reverse path mentioned in the mail command is inserted at the start of the data the protocol provides other commands to deliver a mail to the users terminal to both the users terminal and the users mailbox and either to the users terminal or the users mailbox smtp does not provide a mailbox facility in the receiver hence it is typically used with either the internet message access protocol imap or the post office protocol pop these protocols allow users to save messages in mailboxes remote procedure calls parts of a distributed program are executed in different computers the remote procedure call rpc is a programming language feature that is used to invoke such parts its semantics resemble those of a conventional procedure call its typical syntax is call procid message where procid is the id of a remote procedure and message is a list of parameters the call results in sending message to remote procedure procid the result of the call is modeled as the reply returned by procedure procid rpc is implemented by using a blocking protocol we can view the callercallee relationship as a clientserver relationship thus the remote procedure is the server and a process calling it is a client we will call the computers where the client and the server processes operate as the client node and server node respectively parameters may be passed by value or by reference if the architecture of the server node is different from that of the client node the rpc mechanism performs appropriate conversion of value parameters for reference parameters the caller must construct systemwide capabilities for the parameters see chapter these capabilities would be transmitted to the remote procedure in the message type checks on parameters can be performed at compilation time if the caller and the callee are integrated during compilation otherwise type checks have to be performed dynamically when a remote procedure call is made the schematic diagram of figure depicts the arrangement used to implement a remote procedure call the server procedure is the remote procedure that is to be invoked the client process calls the client stub procedure which exists in the same node the client stub marshals the parameters collects the parameters converts them into a machineindependent format and prepares a message containing this representation of parameters it now calls the server stub which exists in the node that contains the remote procedure the server stub converts the parameters into a machinespecific form and invokes the remote procedure results of the procedure call are passed back to the client process through the server stub and the client stub details concerning naming of the remote procedure and reliability of the remote procedure call are discussed later in chapter chapter message passing client client server server process stub stub procedure client server node node figure overview of a remote procedure call rpc two standards for remote procedure calls sunrpc and osfdce have emerged and are in use widely their use simplifies making of rpcs and makes programs using rpcs portable across computers and their operating systems these standards specify an external representation of data for passing parameters and results between the client and the server and an interface compiler that handles the drudgery of marshaling of parameters the remote method invocation rmi feature of java is an implementation of the remote procedure call that is integrated with the java language the remote method to be invoked is a method of some object parameters that are local objects are passed by value while nonlocal objects are passed by reference integration with the java language simplifies naming of the remote method and reliably passing parameters and results between the client and the server message passing standards for parallel programming a parallel program consists of a set of tasks that can be performed in parallel such programs can be executed on a heterogeneous collection of computers or on a massively parallel processor mpp parallel programs use message passing libraries that enable parallel activities to communicate through messages parallel virtual machine pvm and message passing interface mpi are the two standards that are used in coding message passing libraries both standards provide the following facilities pointtopoint communication between two processes using both symmetric and asymmetric naming and collective communication among processes which includes an ability to broadcast a message to a collection of processes barrier synchronization between a collection of processes wherein a process invoking the barrier synchronization function is blocked until all processes in that collection of processes have invoked the barrier synchronization function global operations for scattering disjoint portions of data in a message to different processes gathering data from different processes and performing global reduction operations on the received data case studies in message passing message passing in unix unix supports three interprocess communication facilities called pipes message queues and sockets a pipe is a data transfer facility while message queues and sockets are used for message passing these facilities have one common feature processes can communicate without knowing each others identities the three facilities are different in scope unnamed pipes can be used only by processes that belong to the same process tree while named pipes can be used by other processes as well message queues can be used only by processes existing within the unix system domain which is the domain of unix operating on one computer system sockets can be used by processes within the unix system domain and within certain internet domains figure illustrates the concepts of pipes message queues and sockets pipes a pipe is a firstin firstout fifo mechanism for data transfer between processes called reader processes and writer processes a pipe is implemented in the file system in many versions of unix however it differs from a file in one important respect the data put into a pipe can be read only once it is removed from the pipe when it is read by a process unix provides two kinds of pipes called named and unnamed pipes both kinds of pipes are created through the system call pipe their semantics are identical except for the following differences a named pipe has an entry in a directory and can thus be used by any process subject to file permissions through the system call open it is retained in the system until it is removed by an unlink system call an unnamed pipe does not have an entry in a directory it can be used only by its creator and its descendants chapter message passing writer processes sender processes write client server offset message messages pipe queue read socket socket data offset message reader processes receiver processes a b c figure interprocess communication in unix a pipe b message queue c socket in the process tree the kernel deletes an unnamed pipe when readers or writers no longer exist for it a pipe is implemented like a file except for two differences see section for a discussion of file implementation in unix the size of a pipe is limited so that data in a pipe is located in the direct blocks of the inode the kernel treats a pipe as a queue by maintaining two offsets one offset is used for writing data into the pipe and the other for reading data from the pipe see figure a the read and write offsets are maintained in the inode instead of in the file structure this arrangement forbids a process from changing the offset of a pipe through any means other than reading or writing of data when data is written it is entered into the pipe by using the write offset and the write offset is incremented by the number of bytes written data written by multiple writers gets mixed up if their writes are interleaved if a pipe is full a process wishing to write data into it would be put to sleep a read operation is performed by using the read offset and the read offset is incremented by the number of bytes read a process reading data from a pipe would be put to sleep if the pipe is empty message queues a message queue in unix is analogous to a mailbox it is created and owned by one process other processes can send or receive messages to or from a queue in accordance with access permissions specified by the creator of the message queue see figure b these permissions are specified by using the same conventions as file permissions in unix see section the size of a message queue in terms of the number of bytes that it can buffer is specified at the time of its creation a message queue is created by a system call msgget key flag where key specifies the name of the message queue and flag indicates some options the kernel maintains an array of message queues and their keys the position of a message queue in this array is used as the message queue id it is returned by the msgget call and the process issuing the call uses it for sending or receiving messages the naming issue is tackled as follows if a process makes a msgget call with a key that matches the name of an existing message queue the kernel simply returns its message queue id this way a message queue can be used by any process in the system if the key in a msgget call does not match the name of part process management an existing message queue the kernel creates a new message queue sets the key as its name and returns its message queue id the process making the call becomes the owner of the message queue each message consists of a message type in the form of an integer and a message text the kernel copies each message into a buffer and builds a message header for it indicating the size of the message its type and a pointer to the memory area where the message text is stored it also maintains a list of message headers for each message queue to represent messages that were sent to the message queue but have not yet been received messages are sent and received by using following system calls msgsnd msgqid msgstruct ptr count flag msgrcv msgqid msgstruct ptr maxcount type flag the count and flag parameters of a msgsnd call specify the number of bytes in a message and the actions to be taken if sufficient space is not available in the message queue eg whether to block the sender or return with an error code msgstruct ptr is the address of a structure that contains the type of a message which is an integer and the text of the message maxcount is the maximum length of the message and type indicates the type of the message to be received when a process makes a msgrcv call the type parameter which is an integer indicates the type of message it wishes to receive when the type parameter has a positive value the call returns the first message in the queue with a matching type if the type value is negative it returns the lowest numbered message whose type is smaller than the absolute value of the type if the type value is zero it returns with the first message in the message queue irrespective of its type the process becomes blocked if the message queue does not contain any message that can be delivered to it when a process makes a msgsnd call it becomes blocked if the message queue does not contain sufficient free space to accommodate the message the kernel activates it when some process receives a message from the message queue and the process repeats the check to find whether its message can be accommodated in the message queue if the check fails the process becomes blocked once again when it eventually inserts its message into the message queue the kernel activates all processes blocked on a receive on the message queue when scheduled each of these processes checks whether a message of the type desired by it is available in the message queue if the check fails it becomes blocked once again example shows how these features can be used to code the reservation server of example example unix message queues figure shows the reservation server coded using the system calls of unix the cancellation booking and enquiry messages are assigned the types and respectively the msgrcv call with type and flag no wait returns a cancellation message if one is present if no cancellation chapter message passing reservationserver msgqid msgget reservationdata flags repeat msgrcv msgqid msgstruct no wait if a message exists then process it whiletrue figure a reservation server in unix messages are present it returns a bookings message if present or an enquiry message this arrangement results in processing of cancellations before bookings and bookings before enquiries as desired it also obviates the need for the three mailboxes used in figure sockets a socket is simply one end of a communication path sockets can be used for interprocess communication within the unix system domain and in the internet domain we limit this discussion to the unix system domain a communication path between a client and the server is set up as follows the client and server processes create a socket each these two sockets are then connected together to set up a communication path for sending and receiving messages see figure c the server can set up communication paths with many clients simultaneously the naming issue is tackled as follows the server binds its socket to an address that is valid in the domain in which the socket will be used the address is now widely advertised in the domain a client process uses the address to perform a connect between its socket and that of the server this method avoids the use of process ids in communication it is an instance of indirect naming see section a server creates a socket s using the system call s socket domain type protocol where type and protocol are irrelevant in the unix system domain the socket call returns a socket identifier to the process the server process now makes a call bind s addr where s is the socket identifier returned by the socket call and addr is the address for the socket this call binds the socket to the address addr addr now becomes the name of the socket which is widely advertised in the domain for use by clients the server performs the system call listen s to indicate that it is interested in considering some connect calls to its socket s a client creates a socket by means of a socket call eg cs socket and attempts to connect it to a servers socket using the system call connect cs serversocketaddr serversocketaddrlen part process management the server is activated when a client tries to connect to its socket it now makes the call newsoc accept s clientaddr clientaddrlen the kernel creates a new socket connects it to the socket mentioned in a clients connect call and returns the id of this new socket the server uses this socket to implement the clientserver communication the socket mentioned by the server in its listen call is used merely to set up connections typically after the connect call the server forks a new process to handle the new connection this method leaves the original socket created by the server process free to accept more connections through listen and connect calls communication between a client and a server is implemented through read and write or send and receive calls a send call has the format count send s message messagelength flags it returns the count of bytes actually sent a socket connection is closed by using the call close s or shutdown s mode message passing in windows windows provides several facilities for secure message passing within a host and within a windows domain which consists of a group of hosts a named pipe is used for reliable bidirectional byte or message mode communication between a server and its clients it is implemented through the file system interface and supports both synchronous and asynchronous message passing the name of a pipe follows the windows universal naming convention unc which ensures unique names within a windows network the first createnamedpipe call for a named pipe is given by a server which specifies its name a security descriptor and the number of simultaneous connections it is to support the kernel notes this information and creates one connection to the pipe the server now makes a connectnamedpipe call which blocks it until a client connects to the pipe a client connects to a pipe through a createfile or callnamedpipe function with the name of the pipe as a parameter the call succeeds if the kind of access requested by it matches with the security descriptor of the pipe now the client can use readfile and writefile functions to access the pipe the server can give additional createnamedpipe calls to create additional connections to the pipe windows provides a mailslot for unreliable unidirectional communication it can be used for both pointtopoint message passing and broadcasting of a short message across a windows domain local procedure call lpc the lpc facility performs message passing between processes located within the same host it is used by components of the windows os for purposes such as invocation of the security authentication server and by processes in user computations to communicate with environment subsystem processes it is also invoked by the remote procedure call facility when the sender and receiver processes are located within the same host lpc provides a choice of three methods of message passing that suit passing of small and large messages and special messages for use by win gui the chapter message passing first two types of lpc use port objects to implement message passing each port object is like a mailbox it contains a set of messages in a data structure called a message queue to set up communication with clients a server creates a port publishes its name within the host and awaits connection requests from clients it is activated when a client sends a connection request to the port and gives a port handle to the client the client uses this handle to send a message the server can communicate with many clients over the same port for small messages the message queue contains the text of the message as discussed in section such messages are copied twice during message passing when a process sends a message it is copied into the message queue of the port from there it is copied into the address space of the receiver to control the overhead of message passing the length of a message is limited to bytes the second method of message passing is used for large messages the client and server processes map a section object into their address spaces when the client wishes to send a message it writes the text of the message in the section object and sends a short message containing its address and size to the port on receiving this message the server views the message text in the section object this way the message is copied only once the third method of lpc is called quick lpc it uses a section object to pass messages and an event pair object to perform synchronization between client and server processes the server creates an event pair object for each client which consists of two event objects it also creates a thread for every client which is devoted exclusively for handling requests made by the client message passing takes place as follows the client process deposits a message in the section object signals the event object on which the server thread is waiting and itself waits on the other event object of the pair the server thread processes the message signals the event object on which the client is waiting and itself waits on the other event object to facilitate message passing the kernel provides a function that atomically signals one event object of the pair and issues a wait on the other event object sockets and remote procedure calls windows socket winsock was originally modeled on the unix bsd socket but later included several extensions its features and implementation are analogous to those of unix sockets described in section winsock is integrated with windows message passing hence a program can perform an asynchronous socket operation and receive a notification of completion of the operation through a windows callback message the remote procedure call rpc facility of windows is compatible with the osfdce standard it is implemented by using the lpc if the procedure being invoked exists on the same host as its client otherwise it is implemented along the lines discussed in section an asynchronous rpc is also supported where the remote procedure operates concurrently with its client and at its completion the client is notified in the manner specified in the call through an event synchronization object through an asynchronous procedure call through an io port or through status information which the client can poll summary the message passing paradigm realizes exchange receive call the kernel considers messages sent by of information among processes without using all processes to it for delivery in indirect naming shared memory this feature makes it useful sender and receiver processes mention the name in diverse situations such as in communication of a mailbox rather than names of receiver and between os functionalities in a microkernelbased sender processes respectively it permits the same os in clientserver computing in higherlevel prosender and destination processes to engage in multocols for communication and in communication tiple independent conversations through different between tasks in a parallel or distributed promailboxes a mailbox contains a set of buffers gram in this chapter we studied message passing in which messages can be stored pending their facilities in programming languages and operating delivery when mailboxes are not used the kersystems nel employs its own buffers to store undelivered the key issues in message passing are nammessages ing of the sender and receiver processes in the send message passing is employed in higherlevel and receive calls and delivery of messages in symprotocols such as the simple mail transfer protocol metric naming the sender and receiver processes smtp the remote procedure call rpc and the name each other in send and receive calls it permits parallel virtual machine pvm and message passa process to engage in multiple independent coning interface mpi standards for parallel programversations simultaneously in asymmetric naming ming operating systems provide many message the receiver process does not name a sender in its passing facilities for use in diverse situations test your concepts classify each of the following statements as true of the process to which the message will be or false delivered a when a process sends a message by using a select the appropriate alternative in each of the blocking send call the kernel has to copy the following questions message into a buffer area a if an os has n processes and uses blocking b when a nonblocking send call is used a send calls and asymmetric receive calls message has to be copied two times before i the os may require up to n buffers for the receiver process can be allowed to each of the n processes at any time examine it ii the os may require upto n n buffers at c in symmetric naming a process that has any time become blocked on a receive call will be iii the os may require upto n buffers at any activated whenever any process sends it a time message iv none of iiii d when indirect naming is used a process sendb answer question a if processes use blocking a message need not know the identity ing send calls and symmetric receive calls exercises in figure a process may be blocked because an ecb explain how these conditions should of lack of memory needed to create an imcb or be handled chapter message passing modify the scheme of figure to implement it is proposed to introduce a timeout facility in message passing with asymmetric naming and message passing whereby a process performing blocking sends a receive specifies the amount of time it is pre the reservation system of example uses flags pared to wait for a message if this period elapses in a receive call to check for presence of penda timeout occurs and the process is activated ing messages a hypothetical mailbox facility give a design to implement this facility using does not support flags hence a process uses the event handling mechanism the following approach to obtain an equivalent processes in an os use asymmetric and asyneffect when a process wishes to check whether chronous message passing the kernel reserves messages exist in a mailbox it sends a special a limited amount of memory for use as mesmessage with the text testing for messages to sage buffers and does not use disk space for this the mailbox and then performs a receive from purpose analyze this system for deadlocks see the mailbox if its own special message is delivchapter how should the kernel detect such ered to it it concludes that there are no other deadlocks messages in the mailbox rewrite the reserva give a design to implement the asynchronous tion system using this approach hint beware send of the message passing interface mpi of outdated special messages standard described in section modify the scheme of figure to implement unix message queues bibliography interprocess communication in the rc system brinch hansen p the nucleus of a is described in brinch hansen accetta et al multiprogramming system communications of discusses the scheme used in mach bach the acm mckusick et al vahalia and stevens geist g j a kohl and p m papadopoulos and rago discusses message passing in unix pvm and mpi a comparison of bovet and cesati discusses message passing in features calculateurs paralleles linux while russinovich and solomon discusses mckusick m k k bostic m j karels and message passing in windows j s quarterman the design and geist et al describes and compares the implementation of the bsd operating system pvm and mpi message passing standards for parallel addison wesley reading mass programming russinovich m e and d a solomon accetta m r baron w bolosky d b golub microsoft windows internals th ed microsoft r rashid a tevanian and m young press redmond wash mach a new kernel foundation for unix stevens w r and s a rago advanced development proceedings of the summer programming in the unix environment nd ed usenix conference june addison wesley professional reading mass bach m j the design of the unix tanenbaum a s modern operating operating system prentice hall englewood systems nd ed prentice hall englewood cliffs n j cliffs n j bovet d p and m cesati understanding vahalia u unix internals the new the linux kernel rd ed oreilly sebastopol frontiers prentice hall englewood cliffs n j calif architecture of multiprocessor systems performance of a uniprocessor system depends on the performance of the cpu and memory which can be enhanced through faster chips and several levels of chapter synchronization and scheduling in multiprocessor operating systems table benefits of multiprocessors benefit description high throughput several processes can be serviced by the cpus at the same time hence more work is accomplished computation speedup several processes of an application may be serviced at the same time leading to a reduction in the duration ie running time of an application it provides better response times graceful degradation failure of a cpu does not halt operation of the system the system can continue to operate with somewhat reduced capabilities caches however chip speeds can not be increased beyond technological limits further improvements in system performance can be obtained only by using multiple cpus as a result of the presence of multiple cpus multiprocessor architectures possess the potential to provide the three benefits summarized in table high throughput is possible because the os can schedule several processes in parallel and so several applications can make progress at the same time the actual increase in throughput compared with a uniprocessor system may be limited by memory contention that occurs when several cpus try to make memory accesses at the same time which increases the effective memory access time experienced by processes computation speedup is obtained when processes of an application are scheduled in parallel the extent of the speedup may be limited by the amount of parallelism within an application that is whether processes of the application can operate without requiring synchronization frequently graceful degradation provides continuity of operation despite cpu failures this feature is vital for supporting missioncritical applications like online services and realtime applications a system model figure shows a model of a multiprocessor system the cpus the memory and the io subsystem are connected to the interconnection network each cpu chip may contain level and level caches ie l and l caches that hold blocks of instructions and data recently accessed by the cpu however for simplicity we assume that the cpu contains only an l cache the memory comprises several memory units we assume that an l cache is associated with each memory unit and holds blocks of instructions and data accessed recently from it every time a cpu or an io device wishes to make a memory access the interconnection network establishes a path between it and the memory unit containing the required byte and the access takes place over this path ignoring delays in the interconnection network effective memory access time depends on hit ratios in the l l and l caches and on the memory access time see section part process management cpu cpu l cache l cache interconnection io network l cache l cache memory memory figure model of multiprocessor system cache and tlb coherence when processes use shared data several copies of a data item d may be present in the system at the same time one of these copies would be in a memory unit and one may exist in the l cache associated with the memory unit while the rest would exist in the l caches of cpus where the processes were scheduled when a process operating on one cpu updates a copy of d the other copies of d become stale their use by processes would cause correctness and data consistency problems so the system uses a cache coherence protocol to ensure that a stale copy is never used in a computation cache coherence protocols are based on two fundamental approaches several variants of which are applied in practice the snoopingbased approach can be used if the interconnection network is a bus a cpu snoops on the bus to detect messages that concern caching and eliminates stale copies from its l cache in the writeinvalidate variant of this approach any process updating a copy of a shared data item d is required to update the copy of d existing in memory hence the memory never holds a stale copy a cpu that updates d sends a cache invalidate message for d on the bus on seeing this message every snooping cpu discards the copy of d if present from its l cache the next time such a cpu accesses d the value is copied afresh into the cpus l cache a directorybased cache coherence approach requires maintaining a directory of information about cached copies of data items in the system the directory could indicate which cpus contain cached copies of each data item while updating a data item d a cpu would send pointtopoint cache invalidation signals to these cpus alternatively the dictionary could indicate the location of the most recently updated copy of each shared data item when a cpu c wishes to access a data item d it would send a read d request to the directory the directory would send the request to the memory unit or the cpu that has the most recent copy of d in its cache which would forward the value of d to c after the update the directory entry of d would be set to point to c tlb coherence is an analogous problem whereby information in some entries in a cpus tlb becomes stale when other cpus perform page replacements or change access privileges of processes to shared pages a shared page pi of a process has entries in the tlbs of many cpus if a page fault arises in a process operating on one of the cpus say cpu c and page pi is replaced by a new page the tlb chapter synchronization and scheduling in multiprocessor operating systems entry of pi in c would be erased see section the tlb entries of pi in other cpus are now stale so they need to be erased too it is achieved through a tlb shootdown action in which cpu c sends interprocessor interrupts to other cpus with details of pis id and the other cpus invalidate pis entries in their tlbs similar actions are performed when access privileges of shared pages are changed the overhead of a tlb shootdown is reduced in two ways the page table entry of pi indicates which cpus have tlb entries for pi and c sends the interrupts to only these cpus a cpu receiving the intimation for shootdown could implement it in a lazy ie needbased manner if the shootdown concerns the currently operating process it erases the tlb entry immediately otherwise it queues the intimation and handles it when the process that it concerns is next scheduled classification of multiprocessor systems multiprocessor systems are classified into three kinds of systems according to the manner in which cpus and memory units are associated with one another uniform memory access architecture uma architecture all cpus in the system can access the entire memory in an identical manner ie with the same access speed some examples of uma architecture are the balance system by sequent and vax by digital the uma architecture is called the tightly coupled multiprocessor architecture in older literature it is also called symmetrical multiprocessor smp architecture nonuniform memory access architecture numa architecture the system consists of a number of nodes where each node consists of one or more cpus a memory unit and an io subsystem the memory unit of a node is said to be local to the cpus in that node other memory units are said to be nonlocal all memory units together constitute a single address space each cpu can access the entire address space however it can access the local memory unit faster than it can access nonlocal memory units some examples of the numa architecture are the hp alphaserver and the ibm numaq noremotememoryaccess architecture norma architecture each cpu has its local memory cpus can access remote memory units but this access is over the network and so it is very slow compared with access to local memory the hypercube system by intel is an example of a norma architecture a norma system is a distributed system according to definition therefore we shall not discuss architecture of norma systems in this chapter interconnection networks cpus in a multiprocessor system access memory units through an interconnection network two important attributes of an interconnection network are cost and effective access speed table lists the characteristics and relative advantages of three popular interconnection networks figure contains schematic diagrams of these networks a bus in a multiprocessor system is simply an extension of a bus in a uniprocessor system all memory units and all cpus are connected to the bus thus the bus supports data traffic between any cpu and any memory unit however only one cpumemory conversation can be in progress at any time the bus is simple part process management table features of interconnection networks interconnection network features bus low cost reasonable access speed at low traffic density only one cpumemory conversation can be in progress at any time crossbar switch high cost low expandability cpus and memory units are connected to the switch a cpumemory conversation is implemented by selecting a path between a cpu and a memory unit permits many cpumemory conversations in parallel multistage intera compromise between a bus and a crossbar switch it connection network consists of many stages of crossbar switches a min cpumemory conversation is set up by selecting a path through each stage permits some parallel conversations m m m m c c c m c c m c c bus crossbar switch bits in address of a memory unit first bit second bit third bit c s s s m c m cc s s s mm cc s s s m m cc s s s m m first second third stage stage stage multistage interconnection network min figure bus crossbar switch and multistage interconnection network min chapter synchronization and scheduling in multiprocessor operating systems and inexpensive but it is slow because of bus contention at medium or high traffic densities because more than one cpu might wish to access memory at the same time the bus may become a bottleneck when the number of cpus is increased a crossbar switch reduces the contention problem by providing many paths for cpumemory conversations it uses a matrix organization wherein cpus are arranged along one dimension and memory units along the other dimension see figure every cpu and every memory unit has its own independent bus when a cpu say cpu c wishes to access a byte located in a memory unit say memory unit m the switch connects the bus of c with the bus of m and the cpumemory conversation takes place over this path this conversation does not suffer contention due to conversations between other cpus and other memory units because such conversations would use different paths through the switch thus the switch can provide a large effective memory bandwidth contention would arise only if two or more cpus wish to converse with the same memory unit which has a low probability of happening at low overall traffic densities between cpus and memory units however a crossbar switch is expensive it also suffers from poor expandability a multistage interconnection network min is a compromise between a bus and a crossbar switch in terms of cost and parallelism it has been used in the bbn butterfly which has a numa architecture figure shows an omega interconnection network which permits cpus to access memory units whose binary addresses range from to it contains three stages because memory units have three bits in their binary addresses each column contains crossbar switches of one stage in the interconnection network for each switch a row represents a cpu and a column represents the value of one bit in the binary address of the memory unit to be accessed if an address bit is the upper output of the crossbar switch is selected if the bit is the lower output of the switch is selected these outputs lead to switches in the next stage when cpu c wishes to access memory unit m the interconnection takes place as follows the address of memory unit m is because the first bit is the lower output of switch s is selected this leads to s whose upper output is selected because the next address bit is this leads to s whose upper output is selected it leads to m as desired switches s s and s would be selected if cpu c wishes to access memory unit the interconnection network uses twelve switches the cost of these switches is much lower than that of an crossbar switch in general an nn multistage network uses logn stages and each stage contains n switches other interconnection networks use combinations of these three fundamental interconnection networks for example the ieee scalable coherent interface sci uses a ringbased network that provides buslike services but uses fast pointtopoint unidirectional links to provide high throughput a crossbar switch is used to select the correct unidirectional link connected to a cpu smp architecture smp architectures popularly use a bus or a crossbar switch as the interconnection network as discussed earlier only one conversation can be in progress over part process management the bus at any time other conversations are delayed hence cpus face unpredictable delays while accessing memory the bus may become a bottleneck and limit the performance of the system when a crossbar switch is used the cpus and the io subsystem face smaller delays in accessing memory so system performance would be better than when a bus is used switch delays are also more predictable than bus delays cache coherence protocols add to the delays in memory access in both of these variations of the smp architecture hence smp systems do not scale well beyond a small number of cpus numa architecture figure illustrates the architecture of a numa system each dashed box encloses a node of the system a node could consist of a singlecpu system however it is common to use smp systems as nodes hence a node consists of cpus local memory units and an io subsystem connected by a local interconnection network each local interconnection network also has a global port and the global ports of all nodes are connected to a highspeed global interconnection network capable of providing transfer rates upward of gbs ie bytes per second they are used for the traffic between cpus and nonlocal memory units a global port of a node may also contain a cache to hold instructions and data from nonlocal memories that were accessed by cpus of the node the global interconnection network shown in figure resembles the ieee scalable coherent interface sci it uses a ringbased network that provides fast pointtopoint unidirectional links between nodes as in an smp system the hardware of a numa system must ensure coherence between caches in cpus of a node it must also ensure coherence between nonlocal caches this requirement can slow down memory accesses and consume part of the bandwidth of interconnection networks ignoring delays in the local cpu cpu cpu cpu global l cache l cache port l cache l cache local remote remote local io interconnection cache cache interconnection io network network high l cache l cache speed l cache l cache global memory memory interconmemory memory nection network remote remote cache cache figure numa architecture preview of the book a computer system the services it provides to its users and their programs and its interfaces with other systems all make up the computing environment operating systems are designed to provide effective utilization of a computer system in its computing environment which is the appropriate combination of efficient use of resources and good user service in the computing environment and to ensure noninterference in the activities of its users parts of this book primarily discuss operating systems for conventional computing environments characterized by use of a single computer system having a single cpu only chapter discusses operating systems for the multiprocessor computing environment operating systems for the distributed computing environment are discussed in the chapters of part all through this book we will use abstract views to present the design and implementation of operating systems because as discussed in section abstract views help in managing complexity and presenting generic concepts or ideas part overview introduction to operating systems part of the book consists of chapters of which the present chapter is chapter we begin the study of operating systems in chapter with a discussion of how an operating system interacts with the computer and with user programs events and interrupts an os interleaves execution of several user programs on the cpu while a user program is in execution some situations concerning its own activity or concerning activities in other programs may require attention of the os hence occurrence of an event which is any situation that requires attention of the os causes control of the cpu to be passed to the operating system the operating system uses the cpu to execute instructions that analyze the event and perform appropriate actions when an event has been attended to the os schedules a user program for execution on the cpu hence operation of the os is said to be event driven for example if an io operation ends the os informs the program that had requested the io operation and starts another io operation on the device if one is pending if a program requests a resource the os allocates the resource if it is available in either case it performs scheduling to select the program to be executed next figure is an abstract view also called a logical view of the functioning of an operating system the end of an io operation or the making of a resource request by a program actually causes an interrupt in the computer system the cpu is designed to recognize an interrupt and divert itself to the os this physical view which is the foundation for a study of operating systems is developed in chapter effective utilization of a computer system computing environments evolved in response to advances in computer architecture and new requirements of computer users each computing environment had a different notion of effective utilization so its os used a different set of techniques to realize it a modern computing environment contains features of several classical computing environments such as noninteractive timesharing and distributed computing environments so techniques employed in these environments are used in modern oss as well chapter discusses these techniques to form the background for a detailed study of operating systems operating system event event computer user system programs computing environment figure an operating system in its computing environment chapter introduction portability and extensibility of operating systems early operating systems were developed for specific computer systems so they were tightly integrated with architectures of specific computer systems modern operating systems such as unix and windows pose two new requirements the operating system has to be portable that is it should be possible to implement it on many computer architectures and it should be extensible so that it can meet new requirements arising from changes in the nature of its computing environment chapter discusses the operating system design techniques for portability and extensibility managing user computations chapters which constitute part of the book discuss various facets of the program management function chapter lays the foundation of this study by discussing how the operating system handles execution of programs processes and threads a process is an execution of a program an os uses a process as a unit of computational work it allocates resources to a process and schedules it for servicing by the cpu it performs process switching when it decides to preempt a process and schedule another one for servicing by the cpu see figure process switching involves saving information concerning the preempted process and accessing information concerning the newly scheduled process it consumes some cpu time and constitutes overhead of the operating system the notion of a thread is introduced to reduce the os overhead switching between threads requires much less information to be stored and accessed compared with switching between processes however processes and threads are similar in other respects so we use the term process as a generic term for both a process and a thread except while discussing the implementation of threads process synchronization processes that have a common goal must coordinate their activities so that they can perform their actions in a desired order this requirement is called process synchronization figure illustrates two kinds of process synchronization figure a shows processes named credit and debit that access the balance in a bank account their results may be incorrect if both processes update the balance at the same time so they must perform their updates strictly one after another figure b shows a process named generate that credit debit generate analyze balance sample a b figure two kinds of process synchronization part overview produces some data and puts it into a variable named sample and the process named analyze that performs analysis on the data contained in variable sample here process analyze should not perform analysis until process generate has deposited the next lot of data in sample and process generate should not produce the next lot of data until process analyze has analyzed the previous data programming languages and operating systems provide several facilities that processes may use for performing synchronization chapter describes these facilities their use by processess and their implementation in an os message passing processes may also interact through message passing when a process sends some information in a message to another process the operating system stores the message in its own data structures until the destination process makes a request to receive a message unlike the situation in figure b synchronization of sender and destination processes is performed by the operating system it makes the destination process wait if no message has been sent to it by the time it makes a request to receive a message details of message passing are described in chapter scheduling the nature of a computing environment decides whether effective utilization of a computer system implies efficient use of its resources high user convenience or a suitable combination of both an os realizes effective utilization through a scheduling policy that shares the cpu among several processes this way many processes make progress at the same time which contributes to quick service for all users and hence to high user convenience the manner in which the cpu is shared among processes governs the use of resources allocated to processes so it governs efficient use of the computer system in chapter we discuss the classical scheduling policies which aimed either at efficient use of a computer system or at high user convenience and scheduling policies used in modern operating systems which aim at suitable combinations of efficient use and user convenience deadlocks user processes share a computer systems resources if a resource requested by some process pi is currently allocated to process pj pi has to wait until pj releases the resource such waits sometimes cause a deadlock which is a situation in which processes wait for other processes actions indefinitely figure illustrates such a situation the arrow drawn from process pi to pj indicates that process pi is waiting because it requested a resource that is currently allocated to process pj processes pj and pk similarly wait for resources that are currently allocated to processes pk and pi respectively hence the three processes are in a deadlock a deadlock adversely affects performance of a system because processes involved in the deadlock can not make any progress and resources allocated to them are wasted we discuss deadlock handling techniques used in operating systems in chapter multiprocessor operating systems a multiprocessor computer system can provide high performance because its cpus can service several processes simultaneously it can also speed up operation of a computer application if its processes are scheduled simultaneously on several cpus to realize these advantages the chapter introduction pk pk requires a resource pj requires a resource allocated to pi allocated to pk pi pj pi requires a resource allocated to pj figure a deadlock involving three processes operating system has to use special scheduling and synchronization techniques to ensure that processes can operate efficiently and harmoniously on the cpus we discuss these techniques in chapter management of memory memory management involves efficient allocation release and reuse of memory to meet requests of processes in the classical model of memory allocation a single contiguous area of memory is allocated to a process this model does not support reuse of a memory area that is not large enough to accommodate a new process so the kernel has to use the technique of compaction to combine several free areas of memory into one large free area of memory it incurs substantial overhead the noncontiguous memory allocation model allows many disjoint areas of memory to be allocated to a process which enables direct reuse of several small areas of memory we describe memory reuse techniques and the model of noncontiguous memory allocation in chapter the kernel uses special techniques to meet its own memory requirements efficiently these techniques are also discussed in this chapter virtual memory modern operating systems provide virtual memory which is a storage capability that is larger than the actual memory of a computer system the os achieves it by storing the code and data of a process on a disk and loading only some portions of the code and data in memory this way a process can operate even if its size exceeds the size of memory the operating system employs the noncontiguous memory allocation model to implement virtual memory it maintains a table of memory allocation information to indicate which portions of the code and data of a process are present in memory and what their memory addresses are during operation of the process the cpu passes each instruction address or data address used by it to a special hardware unit called the memory management unit mmu which consults the memory allocation information for the process and computes the address in memory where the instruction or data actually resides if the required instruction or data does not exist in memory the mmu causes a missing from memory interrupt the operating system now loads the portion that contains the required instruction or data in memory for which it might have to remove some other part overview memory memory allocation operating information of pi system operand address missing in instruction from memory being executed interrupt memory management loadingremoval code and data unit of portions of of processes code and data memory address of operand memory areas allocated to process pi figure a schematic of virtual memory operation user process file system input output control system iocs computer hardware figure an overview of file system and input output control system iocs portion from memory and resumes operation of the process figure is a schematic diagram of virtual memory when a process pi is in operation a missing from memory interrupt slows down progress of a process so the operating system has to make two key decisions to ensure a low rate of these interrupts how many and which portions of the code and data of a process should it keep in memory the techniques used in making these decisions are described in chapter management of files and io devices a file system has to meet several expectations of its users provide fast access to a file protect the file against access by unauthorized persons and provide reliable operation in the presence of faults such as faulty io media or power outages and also ensure efficient use of io devices a file system uses a layered organization to separate the various issues involved in fulfilling these expectations figure shows an abstract view the upper layer which is the file system itself permits a user to share his files with some other users implements file protection and provides reliability to implement an operation on a file the file chapter introduction system invokes the lower layer which contains the input output control system iocs this layer ensures fast access to files by a process and efficient use of io devices file system the file system provides each user with a logical view in which the user has a home directory at an appropriate place in the directory structure of the file system the user can create directories or folders as they are called in the windows operating system in his home directory and other directories or folders in these directories and so on a user can authorize some collaborators to access a file by informing the file system of the names of collaborators and the name of the file the file system uses this information to implement file protection to ensure reliability the file system prevents damage to the data in a file and to its own data such as a directory which is called the metadata due to faults like faulty io media or power outages all these features of file systems are discussed in chapter input output control system iocs the iocs implements a file operation by transferring data between a process and a file that is recorded on an io device it ensures efficient implementation of file operations through three means by reducing the time required to implement a data transfer between a process and an io device by reducing the number of times data has to be transferred between a process and an io device and by maximizing the number of io operations that an io device can complete in a given period of time its techniques are discussed in chapter security and protection security and protection threats and the arrangement used to implement security and protection were described earlier in section the os encrypts the password data through an encryption function known only to itself encryption strengthens the security arrangement because an intruder can not obtain passwords of users except through an exhaustive search which would involve trying out every possible string as a password various security and protection threats the technique of encryption and various methods used to implement protection are described in chapter distributed operating systems a distributed computer system consists of several computer systems each with its own memory connected through networking hardware and software each computer system in it is called a node use of a distributed computer system provides three key advantages speeding up of a computer application by scheduling its processes in different nodes of the system simultaneously high reliability through redundancy of computer systems and their resources and resource sharing across node boundaries to realize these advantages a distributed os must tackle the following fundamental issues networking causes delays in the transfer of data between nodes of a distributed system such delays may lead to an inconsistent view of data located in different nodes and make it difficult to know the chronological order in which events occurred in the system issues in multiprocessor operating systems to realize the benefits of high throughput and computation speedup offered by a multiprocessor system the cpus must be used effectively and processes of an application should be able to interact harmoniously these two considerations will of course influence process scheduling and process synchronization they also affect the operating systems own methods of functioning in response to interrupts and system calls table highlights the three fundamental issues raised by these considerations early multiprocessor operating systems functioned in the masterslave mode in this mode one cpu is designated as the master and all other cpus operate as its slaves only the master cpu executes the kernel code it handles interrupts and system calls and performs scheduling it communicates its scheduling decisions to other cpus through interprocessor interrupts ipis the primary advantage of the masterslave kernel structure is its simplicity when a process makes a system call the cpu on which it operated is idle until either the process resumes its operation or the master cpu assigns new work to the cpu none of these can table issues in synchronization and scheduling in a multiprocessor os issue description kernel structure many cpus should be able to execute kernel code in parallel so that execution of kernel functions does not become a bottleneck process synchronization presence of multiple cpus should be exploited to reduce the overhead of switching between processes and synchronization delays process scheduling the scheduling policy should exploit presence of multiple cpus to provide computation speedup for applications part process management happen until the master cpu handles the system call and performs scheduling hence execution of kernel functions by the master is a bottleneck that affects system performance this problem can be solved by structuring the kernel so that many cpus can execute its code in parallel presence of multiple cpus can be exploited to reduce synchronization delays in a uniprocessor system letting a process loop until a synchronization condition is met denies the cpu to other processes and may lead to priority inversion see section hence synchronization is performed through blocking of a process until its synchronization condition is met however in a multiprocessor system synchronization through looping does not lead to priority inversion because the process holding the lock can execute on another cpu in parallel with the looping process it would be preferable to let a process loop rather than block it if the amount of time for which it would loop is less than the total cpu overhead of blocking it and scheduling another process and activating and rescheduling it sometime in future this condition would be met if a process looping for entry to a critical section and the holder of the critical section are scheduled in parallel multiprocessor operating systems provide special synchronization techniques for exploiting this feature scheduling of processes is influenced by two factors cache performance during operation of a process and synchronization requirements of processes of an application scheduling a process on the same cpu every time may lead to a high cache hit ratio which would improve performance of the process and also contribute to better system performance if the processes of an application interact frequently scheduling them at the same time on different cpus would provide them an opportunity to interact in real time which would lead to a speedup of the application for example a producer and a consumer in a singlebuffer producersconsumers system may be able to perform several cycles of producing and consuming of records in a time slice if they are scheduled to run in parallel thus kernel structure and the algorithms it uses for scheduling and synchronization together determine whether a multiprocessor os will achieve high throughput however computer systems grow in size with advances in technology or requirements of their users so another aspect of performance called scalability is equally important scalability of a system indicates how well the system will perform when its size grows the size of a multiprocessor os may grow through addition of more cpus memory units and other resources to the system or through creation of more processes in applications two kinds of performance expectations arise when a system grows in size the throughput of the system should increase linearly with the number of cpus and delays faced by individual processes due to either synchronization or scheduling should not increase as the number of processes in the system increases scalability is important in the design of both hardware and software interconnection technologies that work well when the system contains a small number of cpus and memory units may not work as well when their number grows to be scalable the effective bandwidth of an interconnection network should increase linearly as the number of cpus is increased as we discussed in section kernel structure the kernel of a multiprocessor operating system for an smp architecture is called an smp kernel it is structured so that any cpu can execute code in the kernel and many cpus could do so in parallel this capability is based on two fundamental provisions the code of the smp kernel is reentrant see section for a discussion of reentrant code and the cpus executing it in parallel coordinate their activities through synchronization and interprocessor interrupts synchronization the kernel uses binary semaphores to ensure mutual exclusion over kernel data structures see section we will refer to them as mutex locks locking is said to be coarsegrained if a mutex lock controls accesses to a group of data structures and it is said to be finegrained if a mutex lock controls accesses to a single data item or a single data structure coarsegrained locking provides simplicity however two or more of the data structures controlled by a lock can not be accessed in parallel so execution of kernel functionalities may become a bottleneck finegrained locking permits cpus to access different data structures in parallel however finegrained locking may increase the locking overhead because a cpu executing the kernel code would have to set and release a larger number of locks it may also cause deadlocks if all cpus do not set the locks in the same order hence deadlock prevention policies such as the resource ranking policy see section would have to be used numerical ranks could be associated with locks and a cpu could set locks in the order of increasing ranks good performance of smp kernels is obtained by ensuring parallelism without incurring substantial locking overhead it is achieved through two means use of separate locks for kernel functionalities cpus can perform different kernel functionalities in parallel without incurring high locking overhead partitioning of the data structures of a kernel functionality cpus can perform the same kernel functionality in parallel by locking different partitions of the data structures locking can be dispensed with altogether by permanently associating a different partition with each cpu heap management parallelism in heap management can be provided by maintaining several free lists ie lists of free memory areas in the heap see section locking is unnecessary if each cpu has its own free list however this arrangement would degrade performance because the allocation decisions would not be optimal forming separate free lists to hold free memory areas of different sizes and letting a cpu lock an appropriate free list would provide parallelism between cpus that seek memory areas of different sizes it would part process management cpu assigned lawt id work lrq c pi pi highestpriority queue c pj pj lowerthanhighest priority queue assigned workload table lowestpriority queue awt figure scheduling data structures in an smp kernel also avoid suboptimal performance caused by associating a free list permanently with a cpu scheduling figure illustrates simple scheduling data structures used by an smp kernel cpus c and c are engaged in executing processes pi and pj respectively the ready queues of processes are organized as discussed in section each ready queue contains pcbs of ready processes having a specific priority the kernel maintains an additional data structure named assigned workload table awt in which it records the workload assigned to various cpus mutex locks called lrq and lawt guard the ready queues data structure and the awt respectively let us assume that cpus set these locks in the order lrq followed by lawt however use of the scheduling data structures shown in figure suffers from heavy contention for mutex locks lrq and lawt because every cpu needs to set and release these locks while scheduling to reduce this overhead some operating systems partition the set of processes into several subsets of processes and entrust each subset to a different cpu for scheduling in this arrangement the ready queues and the assigned workload table get partitioned on a percpu basis now each cpu would access the ready queues data structure that has only the ready processes in its charge in a preemptible kernel mutex locks would still be needed to avoid race conditions on each of the percpu data structures because the cpu may be diverted due to interrupts however these locks would rarely face contention so the synchronization overhead would be low the price for this reduction in the synchronization overhead is either poor system performance because some cpus may be idle while others are heavily loaded or the overhead of balancing the load across the cpus by periodically transferring some processes from heavily loaded cpus to lightly loaded cpus an smp kernel provides graceful degradation because it continues to operate despite failures even though its efficiency may be affected for example failure of a cpu when it is not executing kernel code does not interfere with operation of other cpus in the system hence they would continue to execute normally nonavailability of the failed cpu would affect the process whose code it was executing when the failure occurred it would also affect throughput and response times in the system to some extent as fewer processes can be scheduled in parallel process synchronization process synchronization involves use of critical sections or indivisible signaling operations as discussed in section each of these is implemented by using a lock variable that has only two possible values open and closed a process can not begin execution of a critical section or an indivisible operation if the lock variable associated with the critical section or indivisible operation has the value closed if it finds the value of the lock variable to be open it changes the value to closed executes the critical section or indivisible signaling operation and changes the value back to open a process that finds the value of a lock variable to be closed must wait until the value is changed to open we refer to this arrangement involving use of a lock variable as a synchronization lock or simply a lock and refer to the actions of closing and opening the lock as setting and resetting it two qualities of synchronization locks are important for performance of a multiprocessor system the first quality is scalability of a synchronization lock which indicates the degree to which the performance of an application using the lock is independent of the number of processes in the application and the number part process management table kinds of synchronization locks lock description queued lock a process waiting for a queued lock becomes blocked and its id is entered into a queue of processes waiting for the lock the process is activated when the lock is reset and it is the first process in the queue spin lock if a spin lock is already set when a process tries to set it the process enters into a busy wait for the lock the cpu on which the process is operating can handle interrupts during the busy wait sleep lock when a process waits for a sleep lock the cpu on which it is running is put into a special sleep state in which it does not execute instructions or process interrupts the cpu is activated when the cpu that resets the lock sends it an interprocessor interrupt pi pk pi pi c c c c ipi l l l l pi a b c d figure synchronization locks in multiprocessor operating systems a general schematic diagram of a lock guarding a mutual exclusion region b queued lock c spin lock d sleep lock of cpus in the system the second quality concerns ability of a cpu to handle interrupts while the process operating on the cpu is engaged in trying to set the synchronization lock this ability helps the kernel in providing a quick response to events in the system table summarizes the features of three kinds of synchronization locks the queued spin and sleep locks processes waiting for a queued lock become blocked they are activated in fcfs order when the lock is opened the spin lock is the synchronization lock we illustrated in figures and it leads to a busy wait because a process that is trying to set it is not blocked interestingly we had discarded the spin lock because of a busy wait but it is useful in a multiprocessor system the sleep lock is a new kind of lock we discuss characteristics of all three kinds of locks in the following figure illustrates use of the three kinds of synchronization locks figure a shows a process pi executing on cpu c and a lock l that is chapter synchronization and scheduling in multiprocessor operating systems used to guard a mutual exclusion region the mark inside the box representing the lock indicates that the lock is set a similar mark inside a circle representing a process indicates that the process is in the blocked state we discuss features of these synchronization locks in the following queued lock a queued lock is a conventional lock used for process synchronization the kernel performs the following actions when process pi executing on cpu c requests a lock l lock l is tested if it is not already set the kernel sets the lock on behalf of pi and resumes its execution if the lock is already set by another process pi is blocked and its request for the lock is recorded in a queue figure b illustrates the situation after blocking of pi the id of pi is entered in the queue of lock l and cpu c has switched to execution of some other process pk when the process that had set lock l completes its use of the critical section the process at the head of ls queue is activated and the lock is awarded to it a process that can not set a queued lock relinquishes the cpu on which it is executing such a process will not be using a cpu and will not be accessing memory while it waits to set the lock the average length of the queue for a lock determines whether the solution is scalable if processes do not require lock l frequently the queue length is bounded by some constant c that is it is never larger than c hence increasing the number of cpus or processes in the system does not increase the average delay in acquiring the lock the solution is scalable under these conditions if processes require lock l frequently the length of the queue may be proportional to the number of processes in this case the solution is not scalable spin lock a spin lock differs from a queued lock in that a process that makes an unsuccessful attempt to set a lock does not relinquish the cpu instead it enters into a loop in which it makes repeated attempts to set the lock until it succeeds see figure c hence the name spin lock we depict the situation in which cpu c spins on lock l by drawing an arrow from c to l cpu c repeatedly accesses the value of the lock and tests it using an indivisible instruction like a testandset instruction see section this action creates traffic on the memory bus or across the network use of spin locks may degrade system performance on two counts first the cpu remains with the process looping on the spin lock and so other processes are denied use of the cpu second memory traffic is generated as the cpu spins on the lock the latter drawback may not be significant if the memory bus or the network is lightly loaded but it causes performance degradation in other situations however use of spin locks can be justified in two situations when the number of processes does not exceed the number of cpus in the system because there is no advantage in preempting a process and when a lock is used to control a critical section and the cpu time needed to execute the critical section is smaller than the total cpu time needed to block a process and schedule another one and activate and reschedule the original process in the first case blocking is unnecessary in the second case it is counterproductive part process management a spin lock has an interesting advantage over a queued lock a cpu spinning on a lock can handle interrupts and the process operating on it can handle signals this feature is particularly important in a realtime application as delays in servicing interrupts and signals can degrade response times nevertheless spin locks are not scalable because of the memory or network traffic that they generate in a numa system a process using spin locks may face a situation called lock starvation in which it might be denied the lock for long periods of time possibly indefinitely consider a process pi that is trying to set a spin lock that is in its nonlocal memory let processes pj and pk which exist in the same node as the lock try to set it since access to local memory is much faster than access to nonlocal memory processes pj and pk are able to spin much faster on the lock than process pi hence they are likely to get an opportunity to set the lock before pi if they repeatedly set and use the lock pi may not be able to set the lock for a long time a scheme that we will see in section avoids lock starvation sleep lock when a process makes an unsuccessful attempt to set a sleep lock the cpu on which it is operating is put into a special state called a sleep state in this state it does not execute instructions and does not respond to any interrupts except interprocessor interrupts in figure d we depict this situation by putting a mark against all interrupts except ipi the cpu waiting for the lock does not spin on it and so it does not cause memory or network traffic the cpu that releases the lock has the responsibility to send interprocessor interrupts to those cpus that are sleeping on the lock this feature leads to the overhead of generating and servicing interprocessor interrupts both of which involve a context switch and execution of kernel code the sleep lock will scale poorly if heavy contention exists for a lock however it will perform well if this is not the case use of sleep locks in a realtime application can also affect response times of the application nevertheless sleep locks may be preferred to spin locks if the memory or network traffic densities are high scheduling aware synchronization as discussed earlier some kinds of synchronization are effective only when processes involved in the synchronization are scheduled to run at the same time the solaris os for sun systems provides a synchronization lock called an adaptive lock a process waiting for this lock spins on it if the holder of the lock is scheduled to run in parallel otherwise the process is preempted and queued as in a queued lock thus implementation of a synchronization lock depends on scheduling decisions in the system special hardware for process synchronization some systems use special hardware to avoid the performance problems caused by queued spin and sleep locks the sequent balance system uses a special bus called the system link and interface controller slic for synchronization slic consists of a special bit register in each cpu in the system the registers of different cpus are connected over the slic bus see figure each bit represents a spin lock thus slic can support spin locks when a cpu c wishes to set a lock lk it tries to set the corresponding bit say bk in its special chapter synchronization and scheduling in multiprocessor operating systems slic bus c c slic register memory bus figure slic bus register if the bit is not already set an attempt to set it results in communication over the slic bus if no other cpu is simultaneously trying to set the same bit the lock is awarded to c and bit bk is set in the special registers of all cpus c can now proceed with its execution when it releases the lock bit bk is reset in special registers of all cpus if two or more cpus simultaneously try to set the same lock the hardware arbiter awards the lock to one cpu the attempt to set lock lk fails if bit bk is already set on behalf of some other cpu in this case the cpu keeps spinning on this lock ie on bit bk of its special register the advantage of the slic approach is that a cpu spins on a lock located within the cpu therefore spinning does not generate memory or network traffic use of spinning rather than sleeping also avoids use of interprocessor interrupts for synchronization use of a special synchronization bus relieves pressure on the memory bus this is a significant advantage when memory traffic density is high a scalable software scheme for process synchronization we describe a scheme for process synchronization in numa and norma architectures that achieves scalable performance by minimizing the synchronization traffic to nonlocal memory units in a numa architecture and over the network in a norma architecture it does not require any special hardware and provides an effect that is analogous to the slic chip it also avoids the lock starvation problem of spin locks the scheme uses two types of locks a primary lock is like a conventional lock used for synchronization when a process is unable to set a primary lock it creates a shadow lock in the local memory of the node where it resides associates the shadow lock with the primary lock and spins on the shadow lock this way spinning does not generate nonlocal memory traffic or network traffic when a process wishes to reset a primary lock that it has set it checks whether any shadow locks are associated with the primary lock if so it resets one of the shadow locks which enables one of the processes waiting for the primary lock to proceed otherwise it resets the primary lock figure illustrates an implementation of this scheme using the same notation as in figure a queue of shadow locks is maintained for each primary lock each entry in the queue contains the address of a shadow lock and a pointer to the next shadow lock in the queue if a process fails to set the process scheduling a process can be scheduled on any cpu in a multiprocessor system however its performance can be improved by making an intelligent choice of the cpu ie by deciding where to schedule it performance of a group of processes that synchronize and communicate with one another can be improved by deciding how and when to schedule them this section discusses issues involved in making these decisions choice of the cpu when a process pi operates on a cpu say cpu c some parts of its address space are loaded into the l cache of the cpu when the cpu is switched to another process some of these parts are overwritten by parts of the address space of the new process however some other parts of pis address space may survive in cs cache memory for some time these parts are called the residual address space of a process a process is said to have an affinity for a cpu if it has a residual address space in its cache the process would have a higher cache hit ratio on this cpu than on a cpu for which it does not have affinity affinity scheduling schedules a process on a cpu for which it has an affinity this technique provides a good cache hit ratio thereby speeding up operation of the process and reducing the memory bus traffic another way to exploit the affinity is to schedule the threads of a process on the same cpu in close succession however affinity scheduling interferes with load balancing across cpus since processes and threads become tied to specific cpus section describes how it also leads to scheduling anomalies in the windows system chapter synchronization and scheduling in multiprocessor operating systems c c c c pi pj pk pi pj pk pi pj pk pi pj pk blocked running running running running ready a b figure process pj is shuffled from cpu c to cpu c when process pi becomes ready in section we discussed how the smp kernel permits each cpu to perform its own scheduling this arrangement prevents the kernel from becoming a performance bottleneck however it leads to scheduling anomalies in which a higherpriority process is in the ready state even though a lowpriority process has been scheduled correcting this anomaly requires shuffling of processes between cpus as indicated in the next example process shuffling in an smp kernel example an smp system contains two cpus c and c and three processes pi pj and pk with priorities and respectively figure a shows the situation in which process pi is in the blocked state due to an io operation see contents of its pcb fields and processes pj and pk are executing using cpus c and c respectively when the io operation of pi completes the io interrupt is processed by cpu c which changes pis state to ready and switches itself to service process pi so process pj which is the process with the next higher priority is in the ready state and pk whose priority is the lowest is in operation to correct this situation process pk should be preempted and process pj should be scheduled on cpu c figure b shows the situation after these actions are performed process shuffling can be implemented by using the assigned workload table awt discussed in section and the interprocessor interrupt ipi however process shuffling leads to high scheduling overhead this effect is more pronounced in a system containing a large number of cpus hence some operating systems do not correct scheduling anomalies through process shuffling synchronizationconscious scheduling parts of a computation may be executed on different cpus to achieve computation speedup however synchronization and communication among processes of an application influence the nature of parallelism between its processes so a scheduling policy should take these into account as well as commented earlier in section processes of an application should be scheduled on different cpus at the same time if they use spin locks case studies mach the mach operating system developed at carnegie mellon university is an os for multiprocessor and distributed systems the multiprocessor mach uses an smp kernel structure figure shows an overview of the scheduling arrangement used in mach the processors of the multiprocessor system are divided into processor sets each processor set is assigned a subset of threads for execution threads can have priorities between and where is the highest priority each processor set has ready queues to hold information about threads at each of the priority levels these queues are common to all processors in the processor set in addition every processor has a local queue of threads these are the threads that must be executed only on this processor these threads have a higher priority than all threads in the thread queues this feature provides for affinity scheduling a thread is preempted at the end of a time slice only if some other ready thread exists in the thread queues otherwise the thread is given another time slice the time slice is varied according to the number of ready threads a smaller time slice if many ready threads exist and a larger time slice if few ready threads exist an interesting feature in the mach operating system is the technique of scheduling hints a thread issues a hint to influence processor scheduling subset of threads p p p local queues of threads p p processor set processor set figure scheduling in mach chapter synchronization and scheduling in multiprocessor operating systems decisions it is presumed that a hint is based on the threads knowledge of some execution characteristic of an application a thread may issue a hint to ensure better scheduling when threads of an application require synchronization or communication a discouragement hint reduces the priority of a thread this type of hint can be issued by a thread that has to spin on a lock that has been set by some other process a handsoff hint is given by a thread to indicate that it wishes to relinquish the processor to another thread the thread can also indicate the identity of the thread to which it wishes to hand over the processor on receiving such a hint the scheduler switches the processor to execution of the named thread irrespective of its priority this feature can be used effectively when a thread spins on a lock while the holder of the lock is preempted the spinning thread can handoff its processor to the preempted thread this action will lead to an early release of the lock it can also be used to implement the priority inheritance protocol discussed in chapter linux multiprocessing support in linux was introduced in the linux kernel coarsegrained locking was employed to prevent race conditions over kernel data structures granularity of locks was made finer in later releases however the kernel was still nonpreemptible with linux kernel the linux kernel became preemptible see section the linux kernel also employs very finegrained locking the linux kernel provides spin locks for locking of data structures it also provides a special readerwriter spin lock which permits any number of reader processes that is processes that do not modify any kernel data to access protected data at the same time however it permits only one writer process to update the data at any time the linux kernel uses another lock called the sequence lock that incurs low overhead and is scalable the sequence lock is actually an integer that is used as a sequence counter through an atomic ie indivisible increment instruction whenever a process wishes to use a kernel data structure it simply increments the integer in the sequence lock associated with the data structure notes its new value and performs the operation after completing the operation it checks whether the value in the sequence lock has changed after it had executed its increment instruction if the value has changed the operation is deemed to have failed so it annuls the operation it had just performed and attempts it all over again and so on until the operation succeeds linux uses percpu data structures to reduce contention for locks on kernel data structures as mentioned in section a percpu data structure of a cpu is accessed only when the kernel code is executed by that cpu however even this data structure needs to be locked because concurrent accesses may be made to it when an interrupt occurs while kernel code is being executed to service a system call and an interrupt servicing routine in the kernel is activated linux eliminates this lock by disabling preemption of this cpu due to interrupts while executing kernel code the code executed by the cpu makes a system call to part process management disable preemption when it is about to access the percpu data structures and makes another system call to enable preemption when it finishes accessing the percpu data structures as described earlier in section linux scheduling uses the ready queues data structure of figure scheduling for a multiprocessor incorporates considerations of affinity a user can specify a hard affinity for a process by indicating a set of cpus on which it must run and a process has a soft affinity for the last cpu on which it was run since scheduling is performed on a percpu basis the kernel performs load balancing to ensure that computational loads directed at different cpus are comparable this task is performed by a cpu that finds that its ready queues are empty it is also performed periodically by the kernel every ms if the system is idle and every ms otherwise the function loadbalance is invoked to perform load balancing with the id of an underloaded cpu loadbalance finds a busy cpu that has at least percent more processes in its ready queues than the ready queues of the underloaded cpu it now locates some processes in its ready queues that do not have a hard affinity to the busy cpu and moves them to the ready queues of the underloaded cpu it proceeds as follows it first moves the highestpriority processes in the exhausted list of the busy cpu because these processes are less likely to have a residual address space in the cache of the busy cpu than those in the active list if more processes are needed to be moved it moves the highestpriority processes in the active list of the busy cpu which would improve their response times smp support in windows the windows kernel provides a comprehensive support for multiprocessor and numa systems and for cpus that provide hyperthreading a hyperthreaded cpu is considered to be a single physical processor that has several logical processors spin locks are used to implement mutual exclusion over kernel data structures to guarantee that threads do not incur long waits for kernel data structures the windows kernel never preempts a thread holding a spin lock if some other thread is trying to acquire the same lock the windows server and windows vista use several free lists of memory areas as described in section which permits cpus to perform memory allocation in parallel these kernels also use perprocessor scheduling data structures as described in section however cpus may have to modify each others data structures during scheduling to reduce the synchronization overhead in this operation the kernel provides a queued spinlock that follows the schematic of section a processor spins over a lock in its local memory which avoids traffic over the network in numa systems and makes the lock scalable the windows process and thread objects have several schedulingrelated attributes the default processor affinity of a process and thread processor affinity of a thread together define an affinity set for a thread which is a set of processors in a system with a numa architecture a process can be confined to a single node chapter synchronization and scheduling in multiprocessor operating systems in the system by letting its affinity set be a subset of processors in the node the kernel assigns an ideal processor for each thread such that different threads of a process have different ideal processors this way many threads of a process could operate in parallel which provides the benefits of coscheduling the affinity set and the ideal processor together define a hard affinity for a thread a processor is assumed to contain a part of the address space of a thread for milliseconds after the thread ceases to operate on it the thread has a soft affinity for the processor during this interval so its identity is stored in the last processor attribute of the thread when scheduling is to be performed for say cpu c the kernel examines ready threads in the order of diminishing priority and selects the first ready thread that satisfies one of the following conditions the thread has c as its last processor the thread has c as its ideal processor the thread has c in its affinity set and has been ready for three clock ticks the first criterion realizes soft affinity scheduling while the other two criteria realize hard affinity scheduling if the kernel can not find a thread that satisfies one of these criteria it simply schedules the first ready thread it can find if no such thread exists it schedules the idle thread see section when a thread becomes ready because of an interrupt the cpu handling the interrupt chooses a cpu to execute this newly readied thread as follows it checks whether there are idle cpus in the system and whether the ideal processor or the last processor of the newly readied thread is one of them if so it schedules the newly readied thread on this cpu by entering the threads id in the scheduling data structure of the selected cpu the selected idle cpu would be executing the idle thread which would pick up the identity of the scheduled thread in the next iteration of its idle loop and switch to it if the ideal processor or the last processor of the newly readied thread is not idle the cpu handling the interrupt is itself idle and it is included in the affinity set of the newly readied thread it itself takes up the thread for execution if this check fails and some cpus in the affinity set of the thread are idle it schedules the thread on the lowest numbered such cpu otherwise it schedules the thread on the lowest numbered idle cpu that is not included in the affinity set of the thread if no cpu is idle the cpu handling the interrupt compares the priorities of the newly readied thread and the thread running on the ideal processor of the newly readied thread if the newly readied thread has a higher priority an interprocessor interrupt is sent to its ideal processor with a request to switch to the newly readied thread if this is not the case a similar check is made on the last processor of the newly readied thread if that check also fails the cpu handling the interrupt simply enters the newly readied thread in the ready queue structure it would be scheduled sometime in future by an idle cpu in this case an anomalous situation may exist in the system because the priority of the newly readied thread may exceed the priority of some thread that is executing on some other cpu however correcting this anomaly may cause too much shuffling of threads between cpus so it is not attempted by the scheduling policy summary a multiprocessor os exploits the presence of be able to execute the kernels code in parallel multiple cpus in the computer to provide high so that the kernel can respond to events readthroughput of the system computation speedup of ily and it does not become a performance botan application and graceful degradation of the os tleneck synchronization and scheduling of user capabilities when faults occur in the system in this processes should be performed in such a manner chapter we studied the architecture of multiprothat processes do not incur large delays the os cessor systems and os issues involved in ensuring has to also ensure that its algorithms are scalgood performance able that is they perform well even when the size multiprocessor systems are classified into of the system increases because of an increase three kinds based on the manner in which memin the number of cpus memory units or user ory can be accessed by different cpus in the processes uniform memory architecture uma the memory multiprocessor oss employ special kinds of is shared between all cpus this architecture is locks called spin locks and sleep locks to control also called the symmetrical multiprocessor smp the overhead of process synchronization affinity architecture in the nonuniform memory architecscheduling is employed to schedule a process on the ture numa each cpu has some local memory same cpu so that it would obtain high cache hit that can be accessed faster than the rest of the memratios during its operation and coscheduling is used ory which is accessible over an interconnection to schedule processes of an application on different network cpus at the same time so that they can coma multiprocessor os should exploit presence municate efficiently among themselves operating of multiple cpus to schedule user processes in systems employ process shuffling to ensure that the parallel and also to ensure efficiency of its own highestpriority ready processes are always in operfunctioning two issues are important in this conation on its cpus we discussed features of linux text kernel structure and delays caused by synmach and windows operating systems in this chronization and scheduling many cpus should context test your concepts classify each of the following statements as true what would be the consequence of not impleor false menting cache coherence in a multiprocessor a scheduling performed by one cpu in a symsystem metric multiprocessor system may result in a results produced by a process that does not shuffling of processes operating on many interact with any other process might be cpus in the system wrong b the interprocessor interrupt ipi is not used b results produced by a group of interacting in process synchronization in a symmetric processes that use the same cpu might be multiprocessor system wrong c when a process spins on a lock it affects perc results produced by a group of interacting formance of processes being serviced by other processes that do not use the same cpu cpus might be wrong d when affinity scheduling is used a process d none of ac may require less cpu time to complete its operation chapter synchronization and scheduling in multiprocessor operating systems exercises describe two situations in which an smp kerfor process synchronization discussed in section nel requires use of the interprocessor interrupt ipi can priority inversion occur when spin or sleep an os assigns the same priority to all processes locks are used see section for a definition or threads of an application but uses different of priority inversion priorities for different applications discuss suitability of various kinds of locks for a in a uniprocessor system does this assignsynchronization of parallel activities within an ment of priorities provide an advantage smp kernel that is similar to that provided by affinity processes of an application interact among scheduling themselves very frequently among queued b in a multiprocessor system does this assignspin and sleep locks which would you consider ment of priorities provide an advantage that suitable for implementing this application on a is similar to that provided by coscheduling multiprocessor system and why can the handsoff feature of mach be used to advantage in implementing the software scheme bibliography most books on computer architecture discuss architecsunos kernel proceedings of the summer ture of multiprocessors and interconnection networks usenix conference eg hennessy and patterson hamacher et al hamacher c z vranesic and s zaky and stallings computer organization th ed mcgrawhill mellorcrummey and scott menasse et al new york and wisniewski et al discuss synchroniza hennessy j and d patterson computer tion of processes in a multiprocessor environment the architecture a quantitative approach rd ed efficient software solution for process synchronization morgan kaufmann san mateo calif described in fig is adapted from mellorcrummey mellorcrummey and m l scott and scott ousterhout tucker and gupta algorithms for scalable synchronization on and squillante discuss scheduling issues shared memory multiprocessor acm in multiprocessor operating systems transactions on computer systems eykholt et al discusses multithreading of karlin a r k li m s menasse and the sunos kernel to enhance effectiveness of its smp s owicki empirical studies of structure accetta et al describes the mach competitive spinning for shared memory multiprocessor operating system love discusses multiprocessor proceedings of th acm synchronization and scheduling in linux while symposium on operating system principles russinovich and solomon describes synchroni zation and scheduling in windows kontothanassis l i r w wisniewski and accetta m r baron w bolosky d b golub m l scott scheduler conscious r rashid a tevanian and m young synchronization acm transactions on mach a new kernel foundation for unix computer systems development proceedings of the summer love r linux kernel development nd usenix conference june ed novell press eykholt j r s r kleiman s barton ousterhout j k scheduling techniques s faulkner a shivalingiah m smith d stein for concurrent systems proceedings of the rd j voll m weeks and d william international conference on distributed beyond multiprocessing multithreading the computing systems part process management russinovich m e and d a solomon tanenbaum a s modern operating microsoft windows internals th ed microsoft systems nd ed prentice hall englewood press redmond wash cliffs nj squillante m issues in sharedmemory tucker a and a gupta process control multiprocessor scheduling a performance and scheduling issues for multiprogrammed evaluation phd dissertation dept of shared memory multiprocessors proceedings of computer science engineering university th acm symposium on operating system of washington principles stallings w computer organization and architecture th ed prentice hall upper saddle river nj part managing the memory hierarchy as discussed earlier in chapter a memory hierarchy comprises cache memories like the l and l caches the memory management unit mmu memory and a disk its purpose is to create an illusion of a fast and large memory at a low cost the upper half of figure illustrates the memory hierarchy the part memory management cpu l cache mmu l cache memory virtual memory disk levels how managed performance issues caches allocation and use is managed by ensuring high hit ratios hardware memory allocation is managed by the kernel accommodating more process and use of allocated memory is managed in memory ensuring high hit ratios by runtime libraries disk allocation and use is managed by quick loading and storing of parts of the kernel process address spaces figure managing the memory hierarchy cpu refers to the fastest memory the cache when it needs to access an instruction or data if the required instruction or data is not available in the cache it is fetched from the next lower level in the memory hierarchy which could be a slower cache or the random access memory ram simply called memory in this book if the required instruction or data is also not available in the next lower level memory it is fetched there from a still lower level and so on performance of a process depends on the hit ratios in various levels of the memory hierarchy where the hit ratio in a level indicates what fraction of instructions or data bytes that were looked for in that level were actually present in it eq of chapter indicates how the effective memory access time depends on a hit ratio the caches are managed entirely in the hardware the kernel employs special techniques to provide high cache hit ratios for a process for example the kernel switches between threads of the same process whenever possible to benefit from presence of parts of the process address space in the cache and it employs affinity static and dynamic memory allocation memory allocation is an aspect of a more general action in software operation known as binding two other actions related to a program its linking and loading are also aspects of binding any entity in a program eg a function or a variable has a set of attributes and each attribute has a value binding is the act of specifying the value of an attribute for example a variable in a program has attributes such as name type dimensionality scope and memory address a name binding specifies the variables name and a type binding specifies its type memory binding is the act of specifying the variables memory address it constitutes memory allocation for part memory management the variable memory allocation to a process is the act of specifying memory addresses of its instructions and data a binding for an attribute of an entity such as a function or a variable can be performed any time before the attribute is used different binding methods perform the binding at different times the exact time at which binding is performed may determine the efficiency and flexibility with which the entity can be used broadly speaking we can differentiate between early binding and late binding late binding is useful in cases where the os or runtime library may have more information about an entity at a later time using which it may be able to perform a better quality binding for example it may be able to achieve more efficient use of resources such as memory early and late binding are represented by the two fundamental binding methods of static and dynamic binding respectively definition static binding a binding performed before the execution of a program or operation of a software system is set in motion definition dynamic binding a binding performed during the execution of a program or operation of a software system static memory allocation can be performed by a compiler linker or loader while a program is being readied for execution dynamic memory allocation is performed in a lazy manner during the execution of a program memory is allocated to a function or a variable just before it is used for the first time static memory allocation to a process is possible only if sizes of its data structures are known before its execution begins if sizes are not known they have to be guessed wrong estimates can lead to wastage of memory and lack of flexibility for example consider an array whose size is not known during compilation memory is wasted if we overestimate the arrays size whereas the process may not be able to operate correctly if we underestimate its size dynamic memory allocation can avoid both these problems by allocating a memory area whose size matches the actual size of the array which would be known by the time the allocation is performed it can even permit the array size to vary during operation of the process however dynamic memory allocation incurs the overhead of memory allocation actions performed during operation of a process operating systems choose static and dynamic memory allocation under different circumstances to obtain the best combination of execution efficiency and memory efficiency when sufficient information about memory requirements is available a priori the kernel or the runtime library makes memory allocation decisions statically which provides execution efficiency when little information is available a priori the memory allocation decisions are made dynamically which incurs higher overhead but ensures efficient use of memory in other situations the available information is used to make some decisions concerning memory allocation statically so that the overhead of dynamic memory allocation can be execution of programs a program p written in a language l has to be transformed before it can be executed several of these transformations perform memory binding each one binds the instructions and data of the program to a new set of addresses figure is a schematic diagram of three transformations performed on program p before it can be loaded in memory for execution compilation or assembly a compiler or an assembler is generically called a translator it translates program p into an equivalent program in the object module form this program contains instructions in the machine language of the computer while invoking the translator the user specifies the origin of the program which is the address of its first instruction or byte otherwise the translator assumes a default address typically the translator accordingly assigns addresses to other instructions and data in the program and uses these addresses as operand addresses in its instructions the execution start address or simply the start address of a program is the address of the instruction with which its execution is to begin it can be the same as the origin of the program or it can be different the addresses assigned by the translator are called translated addresses thus the translator binds instructions and data in program p to translated addresses an object module indicates the translated origin of the program its translated start address and size linking program p may call other programs during its execution eg functions from mathematical libraries these functions should be included in the program and their start addresses should be used in the function call instructions in p this procedure is called linking it is achieved by selecting object modules for the called functions from one or more libraries and merging them with program p library data source compiler binary program or linker loader program results p assembler object binary data flow modules programs control flow figure schematic diagram of transformation and execution of a program part memory management relocation some object modules merged with program p may have conflicting translated time addresses this conflict is resolved by changing the memory binding of the object modules this action is called relocation of object modules it involves changing addresses of operands used in their instructions the relocation and linking functions are performed by a program called a linker the addresses assigned by it are called linked addresses the user may specify the linked origin for the program otherwise the linker assumes the linked origin to be the same as the translated origin in accordance with the linked origin and the relocation necessary to avoid address conflicts the linker binds instructions and data of the program to a set of linked addresses the resulting program which is in a readytoexecute program form called a binary program is stored in a library the directory of the library stores its name linked origin size and the linked start address a binary program has to be loaded in memory for execution this function is performed by the loader if the start address of the memory area where a program is to be loaded which is called its load origin differs from the linked origin of program the loader has to change its memory binding yet again a loader possessing this capability is called a relocating loader whereas a loader without this capability is called an absolute loader note that translators linkers and loaders are not parts of the os in this section we discuss different forms of programs and their properties concerning memory bindings processing by the linker and memory requirements during execution we use programs written in a simple hypothetical assembly language to illustrate the relocation and linking actions performed by the linker a simple assembly language an assembly language statement has the following format label opcode operand spec operand spec the first operand is always a generalpurposeregister gpr areg breg creg or dreg the second operand is either a gpr or a symbolic name that corresponds to a memory byte selfexplanatory opcodes like add and mult are used to designate arithmetic operations the mover instruction moves a value from its memory operand to its register operand whereas the movem instruction does the opposite all arithmetic is performed in a register and sets a condition code the condition code can be tested by a branchoncondition bc instruction the assembly statement corresponding to it has the format bc condition code spec instruction address where condition code spec is a selfexplanatory character string describing a condition eg gt for and eq for the bc instruction transfers control to the instruction with the address instruction address if the current value of condition code matches condition code spec for simplicity we assume that all addresses and constants are in decimal and all instructions occupy bytes the sign is not a part of an instruction the opcode and operands of an instruction chapter memory management assembly statement generated code address code start entry total extrn max alpha read a loop mover areg alpha bc any max bc lt loop stop a ds total ds end figure assembly program p and its generated code occupy and digits respectively and the gprs areg breg creg and dreg are represented by and respectively in an instruction relocation figure shows program p an assembly program and its generated code the entry and extrn statements have significance for linking they are discussed later in section a ds statement merely reserves the number of bytes mentioned as its operand the statement start indicates that the translated origin of the program should be the translated address of loop is therefore the address of a is the instructions in bytes with addresses and use these addresses to refer to loop and a respectively these addresses depend on the origin of the program in an obvious way instructions using such addresses are called addresssensitive instructions a program containing addresssensitive instructions can execute correctly only if it is loaded in the memory area whose start address coincides with the origin of the program if it is to execute in some other memory area addresses in addresssensitive instructions have to be suitably modified this action is called relocation it requires knowledge of translated and linked origins and information about addresssensitive instructions the next example illustrates relocation of p relocation of a program example the translated origin of program p in figure is the translated address of the symbol a is the instruction corresponding to the statement read a is an addresssensitive instruction if the linked origin of p is the linked address of a would be it can be obtained by adding the difference between the translated and linked origins ie to its part memory management translated address thus relocation can be performed by adding to the address used in each addresssensitive instruction thus the address in the read instruction would be changed to similarly the instruction in translated memory byte uses the address which is the address of loop this address would be changed to note that operand addresses in the instructions with addresses and also need to be corrected however it is an instance of linking which is discussed in the next section static and dynamic relocation of programs when a program is to be executed the kernel allocates it a memory area that is large enough to accommodate it and invokes the loader with the name of the program and the load origin as parameters the loader loads the program in the memory allocated to it relocates it using the scheme illustrated in example if the linked origin is different from the load origin and passes it control for execution this relocation is static relocation as it is performed before execution of the program begins some time after the programs execution has begun the kernel may wish to change the memory area allocated to it so that other programs can be accommodated in memory this time the relocation has to be performed during execution of the program hence it constitutes dynamic relocation dynamic relocation can be performed by suspending a programs execution carrying out the relocation procedure described earlier and then resuming its execution however it would require information concerning the translated origin and addresssensitive instructions to be available during the programs execution it would also incur the memory and processing costs described earlier some computer architectures provide a relocation register to simplify dynamic relocation the relocation register is a special register in the cpu whose contents are added to every memory address used during execution of a program the result is another memory address which is actually used to make a memory reference thus effective memory address memory address used in the current instruction contents of relocation register the following example illustrates how dynamic relocation of a program is achieved by using the relocation register example dynamic relocation through relocation register a program has the linked origin of and it has also been loaded in the memory area that has the start address of during its execution it is to be shifted to the memory area having the start address of so it has to be relocated to execute in this memory area this relocation is achieved simply chapter memory management program cpu memory add psw relocation register add a b figure program relocation using a relocation register a program b its execution by loading an appropriate value in the relocation register which is computed as follows value to be loaded in relocation register start address of allocated memory area linked origin of program consider execution of the add instruction in the program shown in figure a this instruction has the linked address in the program and uses an operand whose linked address is as a result of relocation the program exists in the memory area starting with the address figure b shows the load addresses of its instructions and data the corresponding linked addresses are shown in parenthesis for easy reference the add instruction exists in the location with address the address of its operand is and the relocation register contains so during execution of the instruction the effective address of its operand is hence the actual memory access is performed at the address linking an entry statement in an assembly program indicates symbols that are defined in the assembly program and may be referenced in some other assembly programs such symbols are called entry points an extrn statement in an assembly program indicates symbols that are used in the assembly program but are defined in some other assembly program these symbols are called external symbols and uses of these symbols in the assembly program are called external references the assembler puts information about the entry and extrn statements in an object module for use by the linker linking is the process of binding an external reference to the correct linked address the linker first scans all object modules being linked together to collect the names of all entry points and their linked addresses it stores this information in a table for its own use it then considers each external reference obtains the part memory management linked address of the external symbol being referenced from its table and puts this address in the instruction containing the external reference this action is called resolution of an external reference the next example illustrates the steps in linking example linking the statement entry total in program p of figure indicates that total is an entry point in the program note that loop and a are not entry points even though they are defined in the program the statement extrn max alpha indicates that the program contains external references to max and alpha the assembler does not know the addresses of max and alpha while processing program p so it puts zeroes in the operand address fields of instructions containing references to these symbols see figure consider program q shown below assembly statement generated code address code start entry alpha alpha dc end the dc statement declares a constant symbol alpha is an entry point in q it has the translated address let the linked origin of program p of figure be the size of p is bytes so the linker assigns the address to the linked origin of q therefore the linked address of alpha is the linker resolves the external reference to alpha in program p by putting the address in the operand address field of the instruction that uses alpha ie in the instruction with the translated address in p this instruction has the linked address static and dynamic linkingloading the distinction between the terms linking and loading has become blurred in modern operating systems however we use the terms as follows a linker links modules together to form an executable program a loader loads a program or a part of a program in memory for execution in static linking the linker links all modules of a program before its execution begins it produces a binary program that does not contain any unresolved external references if several programs use the same module from a library each program will get a private copy of the module several copies of the module might be present in memory at the same time if programs using the module are executed simultaneously dynamic linking is performed during execution of a binary program the linker is invoked when an unresolved external reference is encountered during chapter memory management its execution the linker resolves the external reference and resumes execution of the program this arrangement has several benefits concerning use sharing and updating of library modules modules that are not invoked during execution of a program need not be linked to it at all if the module referenced by a program has already been linked to another program that is in execution the same copy of the module could be linked to this program as well thus saving memory dynamic linking also provides an interesting benefit when a library of modules is updated a program that invokes a module of the library automatically starts using the new version of the module dynamically linked libraries dlls use some of these features to advantage to facilitate dynamic linking each program is first processed by the static linker the static linker links each external reference in the program to a dummy module whose sole function is to call the dynamic linker and pass the name of the external symbol to it this way the dynamic linker is activated when such an external reference is encountered during execution of the program it maintains a table of entry points and their load addresses if the external symbol is present in the table it uses the load address of the symbol to resolve the external reference otherwise it searches the library of object modules to locate a module that contains the required symbol as an entry point this object module is linked to the binary program through the scheme illustrated in example and information about its entry points is added to the linkers table program forms employed in operating systems two features of a program influence its servicing by an os can the program execute in any area of memory or does it have to be executed in a specific memory area can the code of the program be shared by several users concurrently if the load origin of the program does not coincide with the start address of the memory area the program has to be relocated before it can execute this is expensive a program that can execute in any area of memory is at an advantage in this context shareability of a program is important if the program may have to be used by several users at the same time if a program is not shareable each user has to have a copy of the program and so several copies of the program will have to reside in memory at the same time table summarizes important programs employed in operating systems an object module is a program form that can be relocated by a linker whereas a binary program can not be relocated by a linker the dynamically linked program form conserves memory by linking only those object modules that are referenced during its execution we discussed these three program forms in previous sections a selfrelocating program can be executed in any part of memory this program form is not important when a computer provides either a relocation register or virtual memory the reentrant program form avoids the need to have multiple copies of a program in memory these two program forms are discussed in the following sections part memory management table program forms employed in operating systems program form features object module contains instructions and data of a program and information required for its relocation and linking binary program readytoexecute form of a program dynamically linked linking is performed in a lazy manner ie an object program module defining a symbol is linked to a program only when that symbol is referenced during the programs execution selfrelocating program the program can relocate itself to execute in any area of memory reentrant program the program can be executed on several sets of data concurrently selfrelocating programs recall from section that relocation of a program involves modification of its addresssensitive instructions so that the program can execute correctly from a desired area of memory relocation of a program by a linker requires its object module form to be available it also incurs considerable overhead the selfrelocating program form was developed to eliminate these drawbacks it performs its own relocation to suit the area of memory allocated to it a selfrelocating program knows its own translated origin and translated addresses of its addresssensitive instructions it also contains a relocating logic ie code that performs its own relocation the start address of the relocating logic is specified as the execution start address of the program so the relocating logic gains control when the program is loaded for execution it starts off by calling a dummy function the return address formed by this function call is the address of its next instruction using this address it obtains address of the memory area where it is loaded for execution ie its load origin it now has all the information needed to implement the relocation scheme of section after performing its own relocation it passes control to its first instruction to begin its own execution reentrant programs programs can be shared in both static and dynamic manner consider two programs a and b that use a program c we designate a and b as sharing programs and c as the shared program static sharing of c is performed by using static linking hence the code and data of c are included in both a and b the identity of c is lost in the binary programs produced by the linker if programs a and b are executed simultaneously two copies of c will exist in memory see figure a thus static sharing of a program is simple to implement but may waste memory when dynamic sharing is used a single copy of a shared programs code is loaded in memory and used by all sharing programs in execution dynamic chapter memory management c program program a a program c program b b program c a b figure sharing of program c by programs a and b a static sharing b dynamic sharing areg datacb areg data areg dataca dataca c c c a b c figure a structure of a reentrant program bc concurrent invocations of the program sharing is implemented by using dynamic linking the kernel keeps track of shared programs in memory when a program wishes to use one of the shared programs the kernel dynamically links the program to the copy of the shared program in memory figure b illustrates dynamic sharing when program a needs to use program c in a shared mode the kernel finds that c does not exist in memory hence it loads a copy of c in memory and dynamically links it to a in figure b this linking is depicted by the arrow from a to c when program b needs to use program c the kernel finds that a copy of c already exists in memory so it merely links this copy to b this arrangement avoids the need to have multiple copies of a program in memory but we need to ensure that concurrent executions of a program do not interfere with one another a reentrant program is one that can be executed concurrently by many users without mutual interference when invoked the reentrant program allocates a new copy of its data structures and loads the memory address of this copy in a generalpurpose register gpr its code accesses its data structures through the gpr this way if the reentrant program is invoked concurrently by many programs the concurrent invocations would use different copies of the data structure figure illustrates execution of program c coded as a reentrant program program c is coded so that it assumes areg to point to the start of its data area see figure a data items in this area are accessed by using different offsets from the address contained in areg when program a calls c c allocates a data area for use during this invocation it is depicted as dataca in figure b when execution of a is preempted the contents of areg are stored in as pcb they would be loaded back in areg when a is scheduled again when c is called by b a data area datacb is similarly allocated and areg is set to point to the memory allocation to a process stacks and heaps the compiler of a programming language generates code for a program and allocates its static data it creates an object module for the program see section the linker links the program with library functions and the runtime support of the programming language prepares a readytoexecute form of the program and stores it in a file the program size information is recorded in the directory entry of the file the runtime support allocates two kinds of data during execution of the program the first kind of data includes variables whose scope is associated with functions procedures or blocks in a program and parameters of function or procedure calls this data is allocated when a function procedure or block is entered and is deallocated when it is exited because of the lastin firstout nature of the allocationdeallocation the data is allocated on the stack the second kind of data is dynamically created by a program through language features like the new statement of pascal c or java or the malloc calloc statements of c we refer to such data as programcontrolled dynamic data pcd data the pcd data is allocated by using a data structure called a heap stack in a stack allocations and deallocations are performed in a lastin firstout lifo manner in response to push and pop operations respectively we assume each entry in the stack to be of some standard size say l bytes only the last entry of the stack is accessible at any time a contiguous area of memory is reserved for the stack a pointer called the stack base sb points to the first entry of the stack while a pointer called the top of stack tos points to the last entry allocated in the stack we will use the convention that a stack grows toward the lower end of memory we depict it as upward growth in the figures during execution of a program a stack is used to support function calls the group of stack entries that pertain to one function call is called a stack frame it is also called an activation record in compiler terminology a stack frame is pushed on the stack when a function is called to start with the stack frame contains either addresses or values of the functions parameters and the return address ie the address of the instruction to which control should be returned after completing the functions execution during execution of the function the runtime support of the programming language in which the program is coded creates local data of the function within the stack frame at the end of the functions execution the entire stack frame is popped off the stack and the return address contained in it is used to pass control back to the calling program two provisions are made to facilitate use of stack frames the first entry in a stack frame is a pointer to the previous stack frame on the stack this entry facilitates popping off of a stack frame a pointer called the frame base fb is chapter memory management top of stack local data stack tos of calc frame sum for b call on a calc frame retad sample top of base previous fb stack local data fb local data tos of sample stack of sample stack i frame i frame y for y for x call on x call on frame retad main sample retad main sample base previous fb previous fb fb a b figure stack after a main calls sample b sample calls calc used to point to the start of the topmost stack frame in the stack it helps in accessing various stack entries in the stack frame example illustrates how the stack is used to implement function calls use of a stack example figure shows the stack during execution of a program containing nested function calls figure a shows the stack after main the primary function of the program has made a function call samplexyi a stack frame was pushed on the stack when the call was made the first entry in the stack frame contains the previous value of the frame base ie a pointer to the previous stack frame in the stack the second entry is retadmain which is the return address into function main the next three entries pertain to the parameters x y and i while the entries following them pertain to the local data of function sample the frame base fb points to the first entry in this stack frame the tos pointer points to the last local data in the stack frame the code for function sample accesses the return address information about the parameters and its local data using displacements from the frame base fb assuming each stack entry to be bytes the return address is at a displacement of from the address in the frame base the first parameter is at a displacement of etc figure b shows the stack after function sample has made a function call calca b sum a new stack frame has been pushed on the stack the value of the fb has been saved in the first entry of this stack frame the fb has been set to point at the start of the new stack frame and the top of stack pointer now points at the last entry in the new stack frame at the completion of the function the tos pointer would be set to point at the stack entry preceding the entry pointed to by fb and fb would be loaded with the address contained part memory management in the stack entry to which it was pointing these actions would effectively pop off the stack frame of calc and set fb to point at the start of the stack frame for sample the resulting stack would be identical to the stack before function sample called calc heap a heap permits allocation and deallocation of memory in a random order an allocation request by a process returns with a pointer to the allocated memory area in the heap and the process accesses the allocated memory area through this pointer a deallocation request must present a pointer to the memory area to be deallocated the next example illustrates use of a heap to manage the pcd data of a process as illustrated there holes develop in the memory allocation as data structures are created and freed the heap allocator has to reuse such free memory areas while meeting future demands for memory example use of a heap figure shows the status of a heap after executing the following c program float floatptr floatptr int intptr floatptr float calloc sizeof float floatptr float calloc sizeof float intptr int calloc sizeof int free floatptr the calloc routine is used to make a request for memory the first call requests sufficient memory to accommodate floating point numbers the heap allocator allocates a memory area and returns a pointer to it this pointer is stored in floatptr the first few bytes of each allocated memory area are assumed to contain a length field this field is used during deallocation when the routine free is called with a pointer to an allocated memory area figure a shows the heap after all calloc calls have been processed figure b shows the heap after the free call free has freed the memory area pointed to by floatptr this action has created a hole in the allocation the memory allocation model the kernel creates a new process when a user issues a command to execute a program at this time it has to decide how much memory it should allocate to the following components code and static data of the program stack programcontrolled dynamic data pcd data chapter memory management floatptr floatptr floatptr floatptr free intptr intptr area length field free free area area a b figure a a heap b a hole in the allocation when memory is deallocated low end of allocated memory code static data pcd data direction of growth free area stack direction high end of of growth allocated memory figure memory allocation model for a process the size of the program can be obtained from its directory entry sizes of the stack and the pcd data vary during execution of a program so the kernel does not know how much memory to allocate to these components it can guess the maximum sizes the stack and the heap would grow to and allocate them accordingly however this amounts to static allocation which lacks flexibility as discussed in section the allocated memory may be wasted or a process may run out of memory during its operation to avoid facing these problems individually for these two components operating systems use the memory allocation model shown in figure the code and static data components in the program are allocated memory areas that exactly match their sizes the pcd data and the stack share a single large area of memory but grow in opposite directions when memory is allocated to new data the pcd data is allocated by starting at the low end of this area while the stack is allocated by starting at the high end of the area the memory between these two components is free it can be used to create new data in either component in this model the stack and pcd data components do not have individual size restrictions a program creates or destroys pcd data by calling appropriate routines of the runtime library of the programming language in which it is coded the library routines perform allocationsdeallocations in the pcd data area allocated to the process thus the kernel is not involved in this kind of memory management in fact it is oblivious to it heap management reuse of memory the speed of memory allocation and efficient use of memory are the two fundamental concerns in the design of a memory allocator stackbased allocation addresses both these concerns effectively since memory allocation and deallocation is very fast the allocator modifies only the sb fb and tos pointers to manage the free and allocated memory see section and released memory is reused automatically when fresh allocations are made however stackbased allocation can not be used for data that are allocated and released in an unordered manner hence heap allocators are used by runtime support of programming languages to manage pcd data and by the kernel to manage its own memory requirements in a heap reuse of memory is not automatic the heap allocator must try to reuse a free memory area while making fresh allocations however the size of a memory request rarely matches the size of a previously used memory area so some memory area is left over when a fresh allocation is made this memory area will be wasted if it is too small to satisfy a memory request so the allocator must carefully select the memory area that is to be allocated to the request this requirement slows down the allocator because of the combined effect of unusably small memory areas and memory used by the allocator for its own data chapter memory management table kernel functions for reuse of memory function description maintain a free list the free list contains information about each free memory area when a process frees some memory information about the freed memory is entered in the free list when a process terminates each memory area allocated to it is freed and information about it is entered in the free list select a memory area for when a new memory request is made the kernel selects allocation the most suitable memory area from which memory should be allocated to satisfy the request merge free memory areas two or more adjoining free areas of memory can be merged to form a single larger free area the areas being merged are removed from the free list and the newly formed larger free area is entered in it a free list header a x b y c d z e b free list header a x b y c d z e figure free area management a singly linked free list b doubly linked free list structures a heap allocator may not be able to ensure a high efficiency of memory utilization the kernel uses the three functions described in table to ensure efficient reuse of memory the kernel maintains a free list to keep information about free memory areas in the system a memory request is satisfied by using the free memory area that is considered most suitable for the request and the memory left over from this memory area is entered in the free list the allocation policy prevents free memory areas from becoming unusably small the kernel tries to merge free areas of memory into larger free areas so that larger memory requests can be granted maintaining a free list the kernel needs to maintain two items of control information for each memory area in the free list the size of the memory area and pointers used for forming the list to avoid incurring a memory overhead for this control information the kernel stores it in the first few bytes of a free memory area itself figure a shows a singly linked free list in a heap that contains five areas marked ae in active use and three free areas xz each memory area in the free list contains its size and a pointer to the next memory area in the list this organization is part memory management simple however it requires a lot of work when a memory area is to be inserted into the list or deleted from it for example deletion of a memory area from the list requires a change in the pointer stored in the previous memory area in the list insertion of a memory area at a specific place in the list also involves a similar operation therefore insertion and deletion operations on a singly linked list are performed by processing the list from its start it requires an order of m work where m is the number of memory areas in the free list a doubly linked free list is used to facilitate faster insertion and deletion operations on memory areas each entry in this list contains two pointers one points to the next memory area in the list while the other points to the previous memory area see figure b if a memory area with a specific address is to be deleted from the list the kernel can simply take the pointers to the previous and following memory areas in the list and manipulate the pointers in these areas to perform the deletion analogous operations would suffice to add a new memory area at a specific place in the list thus the amount of work required to insert or delete a memory area is a constant irrespective of the number of memory areas in the free list performing fresh allocations by using a free list three techniques can be used to perform memory allocation by using a free list firstfit technique bestfit technique nextfit technique to service a request for n bytes of memory the firstfit technique uses the first free memory area it can find whose size is n bytes it splits this memory area in two parts n bytes are allocated to the request and the remaining part of the memory area if any is put back into the free list this technique may split memory areas at the start of the free list repeatedly so free memory areas become smaller with time consequently the allocator may not have any large free memory areas left to satisfy large memory requests also several free memory areas may become unusably small the bestfit technique uses the smallest free memory area with size n thus it avoids needless splitting of large memory areas however it tends to generate a small free memory area at every split hence in the long run it too may suffer from the problem of numerous small free memory areas the bestfit technique also incurs higher allocation overhead because it either has to process the entire free list at every allocation or maintain the free list in ascending order by size of free memory areas the nextfit technique remembers which entry in the free list was used to make the last allocation to make a new allocation it searches the free list starting from the next entry and performs allocation using the first free memory area of size n bytes that it can find this way it avoids splitting the same free area repeatedly as in the firstfit technique and also avoids the allocation overhead of the bestfit technique chapter memory management a free list header b firstfit c bestfit d nextfit figure a free list bd allocation using firstfit bestfit and nextfit first best and nextfit allocation example the free list in figure a contains three free memory areas of size and bytes respectively processes make allocation requests for and bytes the firstfit technique will allocate and bytes from the first free memory area thus leaving a free memory area of bytes and allocates bytes from the third free memory area the bestfit technique will allocate and bytes from the second free memory area leaving a free memory area of bytes the nextfit technique allocates and bytes from the three free memory areas knuth presents experimental data on memory reuse and concludes that both firstfit and nextfit perform better than bestfit however nextfit tends to split all free areas if the system has been in operation long enough whereas firstfit may not split the last few free areas this property of firstfit facilitates allocation of large memory areas memory fragmentation definition memory fragmentation the existence of unusable areas in the memory of a computer system table describes two forms of memory fragmentation external fragmentation occurs when a memory area remains unused because it is too small to be allocated internal fragmentation occurs when some of the memory allocated to a process remains unused which happens if a process is allocated more memory than it needs in figure c bestfit allocation creates a free memory area of bytes which is too small to be allocated it is an example of external fragmentation we would have internal fragmentation if an allocator were to allocate say bytes of memory when a process requests bytes this would happen if an part memory management table forms of memory fragmentation form of fragmentation description external fragmentation some area of memory is too small to be allocated internal fragmentation more memory is allocated than requested by a process hence some of the allocated memory remains unused allocator dealt exclusively with memory blocks of a few standard sizes to limit its overhead memory fragmentation results in poor utilization of memory in this section and in the remainder of this chapter we discuss several techniques to avoid or minimize memory fragmentation merging of free memory areas external fragmentation can be countered by merging free areas of memory to form larger free memory areas merging can be attempted every time a new memory area is added to the free list a simple method would be to search the free list to check whether any adjoining area is already in the free list if so it can be removed from the free list and merged with the new area to form a larger free memory area this action can be repeated until no more merging is possible and the free memory area at hand can be added to the free list however this method is expensive because it involves searching of the free list every time a new memory area is freed we now describe two generic techniques that perform merging more efficiently in section we describe a special merging technique used in the buddy system allocator boundary tags a tag is a status descriptor for a memory area it consists of an ordered pair giving allocation status of the area whether it is free or allocated represented by f or a respectively and its size boundary tags are identical tags stored at the start and end of a memory area ie in the first and last few bytes of the area if a memory area is free the free list pointer can be put following the tag at its starting boundary figure shows this arrangement when an area of memory becomes free the kernel checks the boundary tags of its neighboring areas these tags are easy to find because they immediately precede and follow boundaries of the newly freed area if any of the neighbors are free it is merged with the newly freed area figure shows actions to be performed when memory areas x y and z are freed while a system using boundary tags is in the situation depicted in figure a in figure b memory area x is freed only its left neighbor is free and so x is merged with it boundary tags are now set for the merged area the left neighbor already existed in the free list so it is enough to simply change its size field only the right neighbor of y is free hence when y is freed it is merged with its right neighbor and boundary tags are set for the merged area now the free list has to be modified to remove the entry for the right neighbor and add an entry for the merged area see figure c both neighbors of memory area z are free hence when z chapter memory management boundary tag boundary tag of left neighbor of right neighbor allocatedfree area free list pointer allocation allocation status size status size boundary tag boundary tag figure boundary tags and the free list pointer a free list f f a x a a y a f f a z a f f header b f f a y a f f a z a f f c f f a x a f f a z a f f d f f a x a a y a f f status flag values a allocated f free figure merging using boundary tags a free list bd freeing of areas x y and z respectively is freed it is merged with both of them to form a single free area the size field of the left neighbors entry in the free list is modified to reflect the merging since the right neighbor also had an entry in the free list the free list is modified to remove this entry see figure d whenever merging occurs with the right neighbor management of the free list requires an order of m work where m is the number of entries in the free list as mentioned earlier in section maintaining the free list as a doubly linked list would enable this operation to be performed efficiently a relation called the percent rule holds when we use this method of merging when an area of memory is freed the total number of free areas in the system increases by decreases by or remains the same depending on whether the area being freed has zero two or one free areas as neighbors these areas of memory are shown as areas of type c a and b respectively in the following a b c b a b b a when an allocation is made the number of free areas of memory reduces by if the requested size matches the size of some free area otherwise it remains unchanged since the remaining free area would be returned to the free list part memory management a free list a b c d e header b a b c d e figure memory compaction assuming a large memory so that the situation at both ends of memory can be ignored and assuming that each area of memory is equally likely to be released we have number of allocated areas n a b c number of free areas m a b where a is the number of free areas of type a etc in the steady state a c hence m n that is the number of free areas is half the number of allocated areas this relation is called the percent rule the percent rule helps in estimating the size of the free list and hence the effort involved in an allocation method like the bestfit method that requires the entire free list to be analyzed it also gives us a method of estimating the free area in memory at any time if sf is the average size of free areas of memory the total free memory is sf n memory compaction in this approach memory bindings are changed in such a manner that all free memory areas can be merged to form a single free memory area as the name suggests it is achieved by packing all allocated areas toward one end of the memory figure illustrates compaction to merge free areas compaction is not as simple as suggested by this discussion because it involves movement of code and data in memory if area b in figure contains a process it needs to be relocated to execute correctly from the new memory area allocated to it relocation involves modification of all addresses used by a process including addresses of heapallocated data and addresses contained in generalpurpose registers it is feasible only if the computer system provides a relocation register see section relocation can be achieved by simply changing the address in the relocation register buddy system and powerof allocators the buddy system and powerof allocators perform allocation of memory in blocks of a few standard sizes this feature leads to internal fragmentation because some memory in each allocated memory block may be wasted however it enables the allocator to maintain separate free lists for blocks of different sizes this arrangement avoids expensive searches in a free list and leads to fast allocation and deallocation buddy system allocator a buddy system splits and recombines memory blocks in a predetermined manner during allocation and deallocation blocks created by splitting a block are called buddy blocks free buddy blocks are merged to form the block that was split to create them this operation is called coalescing under chapter memory management this system adjoining free blocks that are not buddies are not coalesced the binary buddy system which we describe here splits a block into two equalsize buddies thus each block b has a single buddy block that either precedes b in memory or follows b in memory memory block sizes are n for different values of n t where t is some threshold value this restriction ensures that memory blocks are not meaninglessly small in size the buddy system allocator associates a bit tag with each block to indicate whether the block is allocated or free the tag of a block may be located in the block itself or it may be stored separately the allocator maintains many lists of free blocks each free list is maintained as a doubly linked list and consists of free blocks of identical size ie blocks of size k for some k t operation of the allocator starts with a single free memory block of size z for some z t it is entered in the free list for blocks of size z the following actions are performed when a process requests a memory block of size m the system finds the smallest power of that is m let this be i if the list of blocks with size i is not empty it allocates the first block from the list to the process and changes the tag of the block from free to allocated if the list is empty it checks the list for blocks of size i it takes one block off this list and splits it into two halves of size i these blocks become buddies it puts one of these blocks into the free list for blocks of size i and uses the other block to satisfy the request if a block of size i is not available it looks into the list for blocks of size i splits one of them to obtain blocks of size i splits one of these blocks further to obtain blocks of size i and allocates one of them and so on thus many splits may have to be performed before a request can be satisfied when a process frees a memory block of size i the buddy system changes the tag of the block to free and checks the tag of its buddy block to see whether the buddy block is also free if so it merges these two blocks into a single block of size i it now repeats the coalescing check transitively ie it checks whether the buddy of this new block of size i is free and so on it enters a block in a free list only when it finds that its buddy block is not free operation of a buddy system example figure illustrates operation of a binary buddy system parts a and b of the figure show the status of the system before and after the block marked with the symbol is released by a process in each part we show two views of the system the upper half shows the free lists while the lower half shows the layout of memory and the buddy blocks for ease of reference corresponding blocks in the two halves carry identical numbers the block being released has a size of bytes its buddy is the free block numbered in figure a and so the buddy system allocator merges these two blocks to form a new block of bytes the buddy of this new block is block which is also free so block is removed from the free list of byte blocks and merged with the new block to form a free block of size bytes this free block is numbered in figure b it is now entered in the appropriate free list part memory management block free list free block free list free size header memory blocks size header memory blocks memory layout buddy blocks layout a b figure buddy system operation when a block is released the check for a buddys tag can be performed efficiently because block sizes are powers of let the block being freed have a size of bytes since is its address is of the form y where four s follow y and y is or its buddy block has the address z where z y this address can be obtained simply by performing an exclusive or operation with a number ie with for example if the address of a block is its buddys address is in general address of the buddy of a block of size n bytes can be found by performing exclusive or with n this advantage is applicable even if the tags are stored separately in a bitmap see exercise powerof allocator as in the binary buddy system the sizes of memory blocks are powers of and separate free lists are maintained for blocks of different sizes similarity with the buddy system ends here however each block contains a header element that contains the address of the free list to which it should be added when it becomes free when a request is made for m bytes the allocator first checks the free list containing blocks whose size is i for the smallest value of i such that i m if this free list is empty it checks the list containing blocks that are the next higher power of in size and so on an entire block is allocated to a request ie no splitting of blocks takes place also no effort is made to coalesce adjoining blocks to form larger blocks when released a block is simply returned to its free list chapter memory management system operation starts by forming blocks of desired size and entering them into the appropriate free lists new blocks can be created dynamically either when the allocator runs out of blocks of a given size or when a request can not be fulfilled comparing memory allocators memory allocators can be compared on the basis of speed of allocation and efficient use of memory the buddy and powerof allocators are faster than the firstfit bestfit and nextfit allocators because they avoid searches in free lists the powerof allocator is faster than the buddy allocator because it does not need to perform splitting and merging to compare memory usage efficiency in different memory allocators we define a memory utilization factor as follows memory utilization factor memory in use total memory committed where memory in use is the amount of memory being used by requesting processes and total memory committed includes memory allocated to processes free memory existing with the memory allocator and memory occupied by the allocators own data structures memory in use may be smaller than memory allocated to processes because of internal fragmentation and smaller than total memory committed because of external fragmentation the largest value of the memory utilization factor represents the bestcase performance of an allocator and the smallest value at which the allocator fails to grant a memory request represents its worstcase performance allocators using the firstfit bestfit or nextfit techniques do not incur internal fragmentation however external fragmentation limits their worstcase performance because free blocks may be too small to satisfy a request see exercise the buddy and powerof allocators allocate blocks whose sizes are powers of so internal fragmentation exists unless memory requests match block sizes these allocators also use up additional memory to store the free list headers and tags or header elements for blocks in a powerof allocator the header element in a block can not be used by a process thus the useful portion of a block is somewhat smaller than a power of if a memory request is for an area that is exactly a power of in size this method uses up twice that amount of memory a powerof allocator fails to satisfy a request if a sufficiently large free block does not exist since it does not merge free blocks into larger blocks this situation can arise even when the total free memory available in smallersize blocks exceeds the size of the request in a buddy system this situation can arise only if adjoining free blocks are not buddies this is rare in practice in fact knuth reports that in simulation studies the bestcase performance of a buddy allocator was percent contiguous memory allocation contiguous memory allocation is the classical memory allocation model in which each process is allocated a single contiguous area in memory thus the kernel allocates a large enough memory area to accommodate the code data stack and pcd data of a process as shown in figure contiguous memory allocation faces the problem of memory fragmentation in this section we focus on techniques to address this problem relocation of a program in contiguous memory allocation and memory protection were discussed earlier in sections and handling memory fragmentation we discussed the causes of internal and external fragmentation earlier in section internal fragmentation has no cure in contiguous memory allocation because the kernel has no means of estimating the memory requirement of a process accurately the techniques of memory chapter memory management kernel kernel kernel a a a b c c c d d d e a b c figure memory compaction compaction and reuse of memory discussed earlier in section can be applied to overcome the problem of external fragmentation example illustrates use of memory compaction contiguous memory allocation example processes a b c and d are in memory in figure a two free areas of memory exist after b terminates however neither of them is large enough to accommodate another process see figure b the kernel performs compaction to create a single free memory area and initiates process e in this area see figure c it involves moving processes c and d in memory during their execution memory compaction involves dynamic relocation which is not feasible without a relocation register see section in computers not having a relocation register the kernel must resort to reuse of free memory areas however this approach incurs delays in initiation of processes when large free memory areas do not exist eg initiation of process e would be delayed in example even though the total free memory in the system exceeds the size of e swapping the basic mechanism of swapping and the rationale behind it was described in section the kernel swaps out a process that is not in the running state by writing out its code and data space to a swapping area on the disk the swapped out process is brought back into memory before it is due for another burst of cpu time a basic issue in swapping is whether a swappedin process should be loaded back into the same memory area that it occupied before it was swapped out if so its swapping in depends on swapping out of some other process that may have been allocated that memory area in the meanwhile it would be useful to be able to place the swappedin process elsewhere in memory however it would amount noncontiguous memory allocation modern computer architectures provide the noncontiguous memory allocation model in which a process can operate correctly even when portions of its address space are distributed among many areas of memory this model of memory allocation permits the kernel to reuse free memory areas that are smaller than the size of a process so it can reduce external fragmentation as we shall see later in this section noncontiguous memory allocation using paging can even eliminate external fragmentation completely example illustrates noncontiguous memory allocation we use the term component for that portion of the process address space that is loaded in a single memory area example noncontiguous memory allocation in figure a four free memory areas starting at addresses k k k and k where k with sizes of kb kb kb and kb respectively are present in memory process p which has a size of kb is to be initiated see figure b if process p consists of three components called p p and p with sizes of kb kb and kb respectively these components can be loaded into three of the free memory areas as follows see figure c process component size memory start address p kb k p kb k p kb k memory memory k kernel k kernel kb p f f k kb k p k c process p k c p kb xyz kb d d k kb k k kb a b c figure noncontiguous memory allocation to process p chapter memory management logical addresses physical addresses and address translation in section we mentioned that the abstract view of a system is called its logical view and the arrangement and relationship among its components is called the logical organization on the other hand the real view of the system is called its physical view and the arrangement depicted in it is called the physical organization accordingly the views of process p shown in figures b and figures c constitute the logical and physical views of process p of example respectively a logical address is the address of an instruction or data byte as used in a process it may be obtained using index base or segment registers the logical addresses in a process constitute the logical address space of the process a physical address is the address in memory where an instruction or data byte exists the set of physical addresses in the system constitutes the physical address space of the system logical and physical address spaces example in example the logical address space of p extends from to k while the physical address space extends from to k data area xyz in the program of process p has the address see figure b this is the logical address of xyz the process component p in figure has a size of kb ie bytes so xyz is situated in component p and has the byte number since p is loaded in the memory area with the start address kb ie bytes the physical address of xyz is see figure c the schematic diagram of figure shows how the cpu obtains the physical address that corresponds to a logical address the kernel stores information about the memory areas allocated to process p in a table and makes it available to the memory management unit mmu in example this memory memory allocation kernel information area of p operand address in current instruction memory management unit memory areas allocated to process p memory address where operand exists figure a schematic of address translation in noncontiguous memory allocation part memory management information would consist of the sizes and memory start addresses of p p and p the cpu sends the logical address of each data or instruction used in the process to the mmu and the mmu uses the memory allocation information stored in the table to compute the corresponding physical address this address is called the effective memory address of the data or instruction the procedure of computing the effective memory address from a logical address is called address translation a logical address used in an instruction consists of two parts the id of the process component containing the address and the id of the byte within the component we represent each logical address by a pair of the form compi bytei the memory management unit computes its effective memory address through the formula effective memory address of compi bytei start address of memory area allocated to compi byte number of bytei within compi in examples and instructions of p would refer to the data area xyz through the logical address p the mmu computes its effective memory address as approaches to noncontiguous memory allocation there are two fundamental approaches to implementing noncontiguous memory allocation paging segmentation in paging each process consists of fixedsize components called pages the size of a page is defined by the hardware of a computer and demarcation of pages is implicit in it the memory can accommodate an integral number of pages it is partitioned into memory areas that have the same size as a page and each of these memory areas is considered separately for allocation to a page this way any free memory area is exactly the same size as a page so external fragmentation does not arise in the system internal fragmentation can arise because the last page of a process is allocated a pagesize memory area even if it is smaller than a page in size in segmentation a programmer identifies components called segments in a process a segment is a logical entity in a program eg a set of functions data structures or objects segmentation facilitates sharing of code data and program modules between processes however segments have different sizes so the kernel has to use memory reuse techniques such as firstfit or bestfit allocation consequently external fragmentation can arise a hybrid approach called segmentation with paging combines the features of both segmentation and paging it facilitates sharing of code data and program chapter memory management table comparison of contiguous and noncontiguous memory allocation function contiguous allocation noncontiguous allocation memory the kernel allocates a single the kernel allocates allocation memory area to a process several memory areas to a process each memory area holds one component of the process address address translation is not address translation is translation required performed by the mmu during program execution memory external fragmentation in paging external fragmentation arises if firstfit bestfit or fragmentation does not nextfit allocation is used occur but internal internal fragmentation fragmentation can occur arises if memory allocation in segmentation external is performed in blocks of a fragmentation occurs but few standard sizes internal fragmentation does not occur swapping unless the computer system components of a provides a relocation swappedin process can be register a swappedin placed anywhere in process must be placed in its memory originally allocated area modules between processes without incurring external fragmentation however internal fragmentation occurs as in paging we discuss features of these three approaches in later sections table summarizes the advantages of noncontiguous memory allocation over contiguous memory allocation swapping is more effective in noncontiguous memory allocation because address translation enables the kernel to load components of a swappedin process in any parts of memory memory protection each memory area allocated to a program has to be protected against interference from other programs the mmu implements this function through a bounds check while performing address translation for a logical address compi bytei the mmu checks whether compi actually exists in the program and whether bytei exists in compi a protection violation interrupt is raised if either of these checks fails the bounds check can be simplified in paging it is not necessary to check whether bytei exists in compi because as we shall see in the next section a logical address does not have enough bits in it to specify a value of bytei that exceeds the page size paging in the logical view the address space of a process consists of a linear arrangement of pages each page has s bytes in it where s is a power of the value of s is specified in the architecture of the computer system processes use numeric logical addresses the mmu decomposes a logical address into the pair pi bi where pi is the page number and bi is the byte number within page pi pages in a program and bytes in a page are numbered from so in a logical address pi bi pi and bi s in the physical view pages of a process exist in nonadjacent areas of memory consider two processes p and r in a system using a page size of kb the bytes in a page are numbered from to process p has the start address and a size of bytes hence it has pages numbered from to the last page contains only bytes if a data item sample had the address which is the mmu would view its address as the pair process r has a size of bytes hence it has pages numbered from to figure shows the logical view of processes p and r the hardware partitions memory into areas called page frames page frames in memory are numbered from each page frame is the same size as a page at any moment some page frames are allocated to pages of processes while others are free the kernel maintains a list called the free frames list to note the frame numbers of free page frames while loading a process for execution the kernel consults the free frames list and allocates a free page frame to each page of the process to facilitate address translation the kernel constructs a page table pt for each process the page table has an entry for each page of the process which indicates the page frame allocated to the page while performing address translation for a logical address pi bi the mmu uses the page number pi to index the page table of the process obtains the frame number of the page frame allocated to pi and computes the effective memory address according to eq figure shows the physical view of execution of processes p and r each page frame is kb in size the computer has a memory of kb so page frames are numbered from to six page frames are occupied by process p and three page frames are occupied by process r the pages contained in the page frames are shown as p p and r r page frame is free hence the free frames list contains only one entry the page table of p indicates the page frame allocated to each page of p as mentioned earlier the variable sample of sample process p process r figure logical view of processes in paging chapter memory management page id of frame page r p r page frame p r sample page frame p p p p free frames list memory page table of p page table of r figure physical organization in paging process p has the logical address when process p uses this logical address during its execution it will be translated into the effective memory address by using eq as follows effective memory address of start address of page frame we use the following notation to describe how address translation is actually performed s size of a page ll length of a logical address ie number of bits in it lp length of a physical address nb number of bits used to represent the byte number in a logical address np number of bits used to represent the page number in a logical address nf number of bits used to represent the frame number in a physical address the size of a page s is a power of nb is chosen such that s nb hence the least significant nb bits in a logical address give us bi the byte number within a page the remaining bits in a logical address form pi the page number the mmu obtains the values of pi and bi simply by grouping the bits of a logical address as follows np nb pi bi ll where np ll nb use of a power of as the page size similarly simplifies construction of the effective memory address let page pi be allocated page frame qi since pages and page frames have identical sizes nb bits are needed to address the bytes in a page frame the physical address of byte of page frame qi is therefore nf nb qi lp segmentation a segment is a logical entity in a program eg a function a data structure or an object hence it is meaningful to manage it as a unit load it into memory for execution or share it with other programs in the logical view a process consists of a collection of segments in the physical view segments of a process exist in nonadjacent areas of memory a process q consists of five logical entities with the symbolic names main database search update and stack while coding the program the programmer declares these five as segments in q this information is used by the compiler or assembler to generate logical addresses while translating the program each logical address used in q has the form si bi where si and bi are the ids of a segment and a byte within a segment for example the instruction corresponding to a statement call getsample where getsample is a procedure in segmentation with paging in this approach each segment in a program is paged separately accordingly an integral number of pages is allocated to each segment this approach simplifies memory allocation and speeds it up and also avoids external fragmentation a page table is constructed for each segment and the address of the page table is kept in the segments entry in the segment table address translation for a logical address si bi is now done in two stages in the first stage the entry of si is located in the segment table and the address of its page table is obtained the byte number bi is now split into a pair psi bpi where psi is the page number in kernel memory allocation the kernel creates and destroys data structures at a high rate during its operation these are mostly control blocks that control the allocation and use of resources in the system some familiar control blocks are the process control block pcb created for every process and the event control block ecb created whenever the occurrence of an event is anticipated in chapters and we will introduce two other frequently used control blocks the io control block iocb created for an io operation and the file control block fcb created for every open file the sizes of control blocks are known in the design stage of an os this prior knowledge helps make kernel memory allocation simple and efficient memory that is released when one control block is destroyed can be reused when a similar control block is created to realize this benefit a separate free list can be maintained for each type of control block kernels of modern operating systems use noncontiguous memory allocation with paging to satisfy their own memory requirements and make special efforts to use each page effectively three of the leading memory allocators are mckusickkarels allocator lazy buddy allocator slab allocator the mckusickkarels and lazy buddy allocators allocate memory areas that are powers of in size within a page since the start address of each page in memory is a larger power of the start address of each allocated memory chapter memory management area of size n is a multiple of n this characteristic which is called boundary alignment on a power of leads to a cache performance problem as follows some parts of an object are accessed more frequently than others because of boundary alignment on a power of the frequently accessed parts of objects may be mapped into the same areas of a cache by the setassociative technique of cache lookup hence some parts of the cache face a lot of contention leading to poor cache performance of the kernel code the slab allocator uses an interesting technique to avoid this cache performance problem descriptions of these three allocators follow in interest of consistency we use the same terminology we used in previous sections it differs from the terminology used in the literature on these allocators the bibliography at the end of the chapter indicates which modern operating systems use these allocators mckusick karels allocator this is a modified powerof allocator it is used in unix bsd the allocator has an integral number of pages at its disposal at any time and asks the paging system for more pages when it runs out of memory to allocate the basic operating principle of the allocator is to divide each page into blocks of equal size and record two items of information the block size and a free list pointer under the logical address of the page this way the address of the page in which a block is located will be sufficient for finding the size of the block and the free list to which the block should be added when it is freed hence it is not necessary to have a header containing this information in each allocated block as in a conventional powerof allocator with the elimination of the header element the entire memory in a block can be used for the intended purpose consequently the mckusickkarels allocator is superior to the powerof allocator when a memory request is for an area whose size is an exact power of a block of identical size can be allocated to satisfy the request whereas the conventional powerof allocator would have allocated a block whose size is the next higher power of the allocator seeks a free page among those in its possession when it does not find a block of the size it is looking for it then divides this page into blocks of the desired size it allocates one of these blocks to satisfy the current request and enters the remaining blocks in the appropriate free list if no free page is held by the allocator it asks the paging system for a new page to be allocated to it to ensure that it does not consume a larger number of pages than necessary the allocator marks any page in its possession as free when all blocks in it become free however it lacks a feature to return free pages to the paging system thus the total number of pages allocated to the allocator at any given moment is the largest number of pages it has held at any time this burden may reduce the memory utilization factor lazy buddy allocator the buddy system in its basic form may perform one or more splits at every allocation and one or more coalescing actions at every release some of these actions are wasteful because a coalesced block may need to be split again later the basic design principle of the lazy buddy allocator is to delay coalescing actions if a data structure requiring the same amount of memory as part memory management a released block is likely to be created under the correct set of conditions this principle avoids the overhead of both coalescing and splitting the lazy buddy allocator used in unix works as follows blocks with the same size are considered to constitute a class of blocks coalescing decisions for a class are made on the basis of the rates at which data structures of the class are created and destroyed accordingly the allocator characterizes the behavior of the os with respect to a class of blocks into three states called lazy reclaiming and accelerated for simplicity we refer to these as states of a class of blocks in the lazy state allocations and releases of blocks of a class occur at matching rates consequently there is a steady and potentially wasteful cycle of splitting and coalescing as a remedy excessive coalescing and splitting can both be avoided by delaying coalescing in the reclaiming state releases occur at a faster rate than allocations so it is a good idea to coalesce at every release in the accelerated state releases occur much faster than allocations and so it is desirable to coalesce at an even faster rate the allocator should attempt to coalesce a block being released and additionally it should also try to coalesce some other blocks that were released but not coalesced in the past the lazy buddy allocator maintains the free list as a doubly linked list this way both the start and end of the list can be accessed equally easily a bit map is maintained to indicate the allocation status of blocks in the lazy state a block being released is simply added to the head of the free list no effort is made to coalesce it with its buddy it is also not marked free in the bit map this way the block will not be coalesced even if its buddy is released in future such a block is said to be locally free being at the head of the list this block will be allocated before any other block in the list its allocation is efficient and fast because the bit map does not need to be updated it still says that the block is allocated in the reclaiming and accelerated states a block is both added to the free list and marked free in the bit map such a block is said to be globally free globally free blocks are added to the end of the free list in the reclaiming state the allocator tries to coalesce a new globally free block transitively with its buddy eventually a block is added to some free list either to a free list to which the block being released would have belonged or to a free list containing largersize blocks note that the block being added to a free list could be a locally free block or a globally free block according to the state of that class of blocks in the accelerated state the allocator tries to coalesce the block being released just as in the reclaiming state and additionally tries to coalesce one other locally free block the block found at the start of the free list with its buddy the state of a class of blocks is characterized as follows let a l and g be the number of allocated locally free and globally free blocks of a class respectively the total number of blocks of a class is given by n a l g a parameter called slack is computed as follows slack n l g a class is said to be in the lazy reclaiming or accelerated state if the value of slack is or respectively the allocator ensures that slack is never chapter memory management the coalescing overhead is different in these three states there is no overhead in the lazy state hence release and allocation of blocks is fast in the reclaiming state the overhead would be comparable with that in the buddy system whereas in the accelerated state the overhead would be heavier than in the buddy system it has been shown that the average delays with the lazy buddy allocator are to percent lower than average delays in the case of a buddy allocator the implementation of the lazy buddy allocator in unix uses two kinds of blocks small blocks vary in size between and bytes large blocks vary in size between and kb the allocator obtains memory from the paging system in kb areas in each area it creates a pool of blocks and a bit map to keep track of the allocation status of the blocks when all blocks in the pool are free it returns the area to the paging system this action overcomes the problem of nonreturnable blocks seen in the mckusickkarels allocator slab allocator the slab allocator was first used in the solaris operating system it has been used in linux since version a slab consists of many slots where each slot can hold an active object that is a kernel data structure or it may be empty the allocator obtains standardsize memory areas from the paging system and organizes a slab in each memory area it obtains an additional memory area from the paging system and constructs a slab in it when it runs out of memory to allocate and it returns a memory area to the paging system when all slots in its slab are unused all kernel objects of the same class form a pool for small objects a pool consists of many slabs and each slab contains many slots large objects are not discussed here the slabs of a pool are entered in a doubly linked list to facilitate addition and deletion of slabs a slab may be full partially empty or empty depending on the number of active objects existing in it to facilitate searches for an empty slab the doubly linked list containing the slabs of a pool is sorted according to the slabs status all full slabs are at the start of the list partially empty slabs are in the middle and empty slabs are at the end of the list each slab contains a free list from which free slots can be allocated each pool contains a pointer to the first slab that contains a free slot this arrangement makes allocation very efficient figure shows the format of a slab when the allocator obtains a memory area from the paging system it formats the memory area into a slab by creating an integral number of slots a free list containing all slots and a descriptor field coloring unused area area slot descriptor free active free free aobcjteivcet slot object slot slot free list pointers figure format of a slab using idle ram effectively a workstation or laptop has a large memory because it is needed for running specific applications however memory remains idle when the applications are not active operating system designers have long pondered the issue of how idle memory can be exploited for the benefit of the user a typical solution is to run utilities such as antivirus software during idle periods of a computer so that their execution does not tie up memory and consume cpu time when the computer is being used for productive purposes however even such operation of utilities can have a negative impact on performance because the utilities might displace important applications from memory so they have to be loaded back into memory before they can be used summary in this chapter we discussed techniques of effective execution of a program commences however it management of memory which involves performrequires knowledge of the exact amount of meming fast allocation and deallocation of memory to ory required failing which it may overallocate processes and ensuring efficient use of memory so and waste memory dynamic memory allocation is that many processes can be accommodated in it performed during execution of a program which simultaneously incurs a memory management overhead during when a program is coded or compiled it is execution but makes efficient use of memory by not known which area of the memory would be allocating only the required amount of memory allocated for its execution however instructions the kernel uses a model of memory allocation for used in it need to use memory addresses for its a process that contains a statically allocated comoperands this dilemma is resolved as follows a ponent for the code and data of the program and compiler assumes a specific memory area to be dynamically allocated components for the stack available to a program and generates a program and for the heap in which a program can dynamiform called object module the linker which is a cally allocate memory through statements such as system program uses the procedure called relonew or alloc cation which changes the operand addresses in when a process completes its execution or a programs instructions such that the program releases the memory allocated to it the kernel can execute correctly in the allocated memory reuses the memory to satisfy the requirements of area the linker also connects the program with other processes when static memory allocation is library functions required by it to prepare a readyused some of the memory allocated to a process toexecute program selfrelocating programs can may remain unused which is called internal fragperform their own relocation computer hardware mentation when dynamic memory allocation is assists in dynamic relocation of programs through used unless new requests exactly match the sizes of a special register in the cpu called the relocareleased memory some memory is left over when a tion register it permits the kernel to change the new request is satisfied it remains unused if it is too memory area allocated to a program during the small to satisfy a request which is called external programs execution fragmentation memory allocation can be performed in two two approaches can be used to tackle the ways static memory allocation is performed before fragmentation problem in the first approach part memory management the kernel minimizes fragmentation while reusing approaches noncontiguous memory allocation memory various techniques called firstfit allorequires use of a memory management unit in the cation bestfit allocation etc are used to minhardware imize external fragmentation while techniques the kernel creates and destroys control blocks called buddy systems allocation and powerof such as the pcb at a very fast rate since the sizes allocation are used to eliminate external fragof control blocks are known to the kernel it minimentation in the other approach noncontiguous mizes the memory management overhead and the memory allocation is used whereby a process fragmentation problem by having many memory can be executed even when it is allocated many blocks of required size and allocating one of them small memory areas that add up to its total size when a new control block is to be created the lazy requirement this way external fragmentation is buddy allocator and the slab allocator are some of eliminated paging and segmentation are two such the techniques used by the kernel test your concepts classify each of the following statements as true select the correct alternative in each of the or false following questions a when a stack is used reuse of a released a a worstfit allocator always splits the largest memory area is automatic free memory area while making an allocation b pcd data can be allocated on a stack a free list contains three memory areas of c the relocation register helps the kernel persizes kb kb and kb the next four form compaction of programs to avoid extermemory requests are for kb kb kb nal fragmentation and kb of memory the only placement d memory allocation performed by using a strategy that would be able to accommodate buddy system allocator does not suffer from all four processes is internal fragmentation i firstfit e when a memory area is released in a sysii bestfit tem employing a buddy system allocator the iii worstfit number of free memory areas increases by iv nextfit decreases by or remains unchanged b three processes requiring kb kb f external fragmentation can occur when and kb of memory are in operation in either a buddy system allocator or a poweran os employing a paging system with a page of allocator is used size of kb the maximum internal memory g when dynamic linking and loading is fragmentation due to memory allocation to employed a routine that is not used in an exethe three processes is cution of a program is not loaded in memory i approximately kb h in a paging system it is not possible to swap ii approximately kb in a process into a set of noncontiguous memiii kb ory areas that is different from the set of iv none of iiii noncontiguous memory areas from which it c a reentrant program is one that was swapped out i calls itself recursively i in a paging system a programmer has to ii can have several copies in memory that demarcate the pages in the code and data of can be used by different users a program iii can have a single copy in memory j there would be no need for linkers if all prothat is executed by many users grams were coded as selfrelocating proconcurrently grams chapter memory management exercises a hypothetical programming language permits each to meet a request for kb it merges two one of the following three attributes to be assoadjoining free areas of kb each if present and ciated with a variable in a program allocates the resulting contiguous area when an a static variables with this attribute are alloarea of kb is released it treats the freed area cated memory at compilation time as two free areas of kb each show that if the b automatic when execution of a program allocator has kb available for allocation it is initiated or a functionsubroutine is may not be able to honor requests for a total of invoked variables with the automatic kb attribute declared in the program function a buddy system allocator is allocated an area of or subroutine are allocated memory memory kb blocks of size kb kb bytes is deallocated when the program completes and kb are allocated in that order or the invocation of the functionsubroutine a show the allocation status and free lists is exited of the allocator how many splits were c controlled a variable x with the controlled performed attribute is allocated memory when the prob show the allocation status and free lists of gram executes the statement new x memory the allocator after the block of bytes is is deallocated when the program executes the freed how many coalesce operations were statement release x performed discuss the method used to allocate memory a powerof allocator uses a minimum block to variables with each of these attributes comsize of bytes and a maximum block size ment on i memory utilization efficiency and ii of kb it starts its operation with one free execution efficiency of these methods block each of sizes bytes kb kb and a memory allocator using the bestfit allocation kb calculate the internal fragmentation if policy organizes its free list in ascending order the allocator processes the same requests as in by sizes of free areas this organization avoids exercise having to scan the entire free list for making an when a memory block is freed a memory alloallocation however while handling a request cator makes an effort to merge it with one or for n bytes the allocator has to skip over the both of its neighbors do you agree with the entries for memory areas that are n bytes in following statement if sizes of neighboring size propose a method of organizing the free list blocks are known it is adequate to have a tag that would eliminate the overhead of examining at only one boundary of each block however and skipping over these entries if sizes of neighboring blocks are not known it the kernel of an os uses a separate memory is essential to have tags at both boundaries of allocator for handling its own memory requireeach block ments it is found that this memory allocator a buddy system organizes tags of the blocks in receives requests to grant and release memory a bitmap which is a onedimensional array of areas of only two sizes namely bytes and tags comment on how best the bitmap can be bytes at a high rate comment on memory organized and used hint note that blocks may utilization efficiency and speed of allocation if be split and coalesced during operation of the the memory allocator is buddy system a a firstfit allocator if a binary buddy system starts its operation with b a bestfit allocator a single free block of size z bytes c a slab allocator a justify the statement when a block is a memory allocator uses the following policy released the number of free blocks in the to allocate a single contiguous area for requests system may increase by may remain of kb and kb it sets apart a contiguous unchanged or may decrease by a nummemory area of n kb for handling such requests ber between and n both inclusive where and splits this memory area into n areas of kb n z part memory management b determine the value of n if the minimum does the percent rule apply to the following block size in the buddy system is bytes allocators a fibonacci buddy system uses blocks whose a buddy system sizes are multiples of the terms of the fibonacci b powerof allocator series for example hence c slab allocator the size of a block is the sum of the sizes of the an os receives requests for memory allocation two immediately smaller blocks this formula at a high rate it is found that a large fracgoverns the splitting and merging of blocks tion of the requests are for memory areas of compare the execution efficiency and memory size and bytes let us call these efficiency of the fibonacci buddy system with standard sizes other requests are for areas of the binary buddy system various other sizes design a memory allocation a memory allocator works as follows small scheme in which no fragmentation arises while memory areas are allocated by using a buddy allocating areas of standard sizes and no intersystem large memory areas are allocated by nal fragmentation arises while allocating areas using a free list and a firstfit allocator comof other sizes ment on the efficiency and memory utilization compute the slack for each class of buffers if a achieved by this allocator lazy buddy allocator were to be used instead of an os has mb available for user processes the buddy allocator in exercise the maximum memory requirement of a pro if the os of exercise employed paging with cess for its own code and data is mb while a page size of kb is it possible to compute the the average memory requirement of a process average internal fragmentation in the system is mb if the os uses contiguous memory allocation and does not know sizes of individual processes what is the average internal and external fragmentation bibliography linkers and loaders are described in dhamdhere and bovet and cesati describe its implementa tion in linux the windows kernel uses several memory knuth is the classical starting point for a allocation policies for its own memory requirements it study of contiguous memory management he describes implements buddysystemlike allocation for mediumvarious techniques of memory allocation and efficient size blocks and heapbased allocation for small block data structures to keep track of free memory hoare sizes russinovich and solomon describes heap and mckeag surveys various memory manageallocation and kernel memory allocation in windows ment techniques randell is an early paper on the motivation for virtual memory systems denning beck m h bohme m dziadzka u kunitz describes the fundamentals of virtual memory systems r magnus c schroter and d verworner vahalia describes the various kernel memory linux kernel programming rd ed allocators used in unix systems mckusick and karels pearson education new york describes the mckusickkarels memory alloca bonwick j the slab allocator an tor lee and barkley describes the lazy buddy alloobjectcaching kernel memory allocator cator both these allocators are used in unix bonwick proceedings of the summer usenix technical and bonwick and adams describe the slab conference allocator mauro and mcdougall describes use bonwick j and j adams extending the of the slab allocator in solaris while beck et al slab allocator to many cpus and arbitrary chapter memory management resources proceedings of the usenix memory allocation proceedings of the summer annual technical conference usenix technical conference bovet d p and m cesati understanding mauro j and r mcdougall solaris the linux kernel rd ed oreilly sebastopol internals nd ed prentice hall englewood calif cliffs n j denning p j virtual memory mckusick m k and m j karels computing surveys design of a generalpurpose memory allocator dhamdhere d m systems programming for the bsd unix kernel proceedings of the and operating systems nd revised ed summer usenix technical conference tata mcgrawhill new delhi hoare c a r and r m mckeag peterson j l and t a norman buddy a survey of store management techniques in systemscommunications of the acm operating systems techniques by car hoare and rh perrott eds academic press london randell b a note on storage knuth d e the art of computer fragmentation and program segmentation programming nd ed vol i fundamental communications of the acm algorithms addisonwesley reading mass kuck d j and d h lowrie the use russinovich m e and d a solomon and performance of memory hierarchies in microsoft windows internals th ed microsoft software engineering jt tou ed academic press redmond wash press new york vahalia u unix internals the new lee t p and r e barkley frontiers prentice hall englewood a watermarkbased lazy buddy system for kernel cliffs n j summary a computer users requirements are determined by user convenience provide convenient methods a computers role in fulfilling his need for some of using a computer system users computing is merely a means to fulfilling a noninterference prevent interference in the need like internet browsing or sending of emails activities of its users whereas for some others it directly satisfies their needs like running programs to perform data proan operating system meets these requirements cessing or scientific computations an operating by performing three primary functions during its system has to meet the needs of all its users so it operation management of programs managehas diverse functionalities ment of resources and security and protection an a modern computer has an abundance of os is a complex software system that may conresources like memory and disk space and it also tain millions of lines of code so we use abstraction has a powerful cpu to ensure that computer to master the complexity of studying its design users benefit from this abundance the operating abstraction helps us to focus on a specific aspect system services many programs simultaneously by of a system whether a hardware system like a comdistributing its resources among them and interputer a software system like an operating system leaving their execution on the cpu the os has to or a reallife system like the urban transportation satisfy three requirements to ensure effectiveness network and ignore details that are not relevant of computing to this aspect we will use abstraction throughout efficient use ensure efficient use of a comthe book to study different aspects of design and puters resources operation of operating systems chapter introduction the plan of the book is as follows we begin and use of files by programs and ensures secuby discussing how an operating system interacts rity and protection this is followed by the study with a computer system to control its operation of distributed operating systems which control we then study how the operating system manoperation of several computer systems that are ages execution of programs allocation of memory networked test your concepts classify each of the following statements as true and ii efficient use of a computer or false system a the boot procedure is used to initiate a user a virtual memory program b file protection b the technique of preemption is employed to c noncontiguous memory allocation share the cpu among user programs classify the following into security lapses and c resources may be wasted if an os employs protection lapses poolbased resource allocation a scribbling your password on a piece of paper d assignment of virtual resources to processes b authorizing everybody to perform read and prevents mutual interference between them write operations on your file e threats posed by an authenticated user are c leaving your monitor unattended in the midsecurity threats dle of a session indicate whether each of the following technid downloading a program that is known to quesarrangements provides i user convenience contain a virus exercises a computer can operate under two operating its memory at any time and consequently the systems os and os a program p always execpu is often idle because of lack of work cutes successfully under os when executed swapping is a technique of removing an inactive under os it is sometimes aborted with the program from memory and loading a program error insufficient resources to continue executhat requires use of the cpu in its place so that tion but executes successfully at other times the cpu can service it does swapping improve what is the reason for this behavior of proa user service and b efficiency of use what gram p can it be cured if so explain how is its effect on os overhead and describe its consequences hint think of comment on validity of the following stateresource management policies ment partitioned resource allocation provides a timesharing operating system uses the folmore user convenience but may provide poor lowing scheduling policy a program is given efficiency a limited amount of cpu time called the time a program is in a dormant state if it is not slice each time it is selected for execution it is engaged in any activity eg it may be waiting for preempted at the end of the time slice and it an action by a user what resources does a doris considered for execution only after all other mant program consume how can this resource programs that wish to use the cpu have been consumption be reduced given an opportunity to use the cpu comment an os creates virtual devices when it is on a user service and b efficiency of use in a short of real devices does creation of virtual timesharing system devices improve a user service b efficiency if a computer has a very fast cpu but a small of use memory few computer programs can fit into part overview can deadlocks arise in the following situa a user wishes to let his collaborators access some tions of his files but expects the os to prevent his cola a system performs partitioned allocation of laborators from accessing his other files and also resources to programs prevent noncollaborators from accessing any of b a set of programs communicate through his files explain how it is achieved jointly by the message passing during their execution user and the os bibliography the view of an os as the software that manages a com garfinkel s g spafford and a schwartz puter system is usually propounded in most operating practical unix and internet security systems texts tanenbaum nutt silberrd ed oreilly sebastopol calif schatz et al and stallings are some of the kilburn t d j howarth r b payne and recent texts on operating systems f h sumner the manchester berzins et al discusses how the complexuniversity atlas operating system part i ity of designing a software system can be reduced by internal organization computer journal constructing a set of abstractions that hide the inter nal working of a subsystem most books on software ludwig m a the giant black book of engineering discuss the role of abstraction in software computer viruses nd ed american eagle design the paper by parnas and siewiorek on show low the concept of transparency in software design is con ludwig m a the little black book of sidered a classic of software engineering the book by email viruses american eagle show low booch discusses abstractions in object oriented nutt g operating systems a modern software development perspective rd ed addisonwesley reading the concept of virtual devices was first used in the mass spooling system of the atlas computer system developed parnas d l and d p siewiorek use of at manchester university it is described in kilburn et al the concept of transparency in the design of hierarchically structured systems ludwig and ludwig describe differcommunications of the acm ent kinds of viruses while berghel describes the pfleeger c p and s pfleeger security in code red worm that caused havoc in pfleeger computing prentice hall englewood cliffs nj and pfleeger is a text on computer security russinovich m e and d a solomon garfinkel et al discusses security in solaris microsoft windows internals th ed microsoft mac os linux and freebsd operating systems russipress redmond wash novich and solomon discusses security features silberschatz a p b galvin and g gagne in windows operating system principles th ed berghel h the code red worm john wiley new york communications of the acm stallings w operating systems berzins v m gray and d naumann internals and design principles th ed pearson abstractionbased software development education new york communications of the acm tanenbaum a s modern operating booch g objectoriented analysis and systems nd ed prentice hall englewood design benjamincummings santa clara cliffs nj virtual memory basics users always want more from a computer system more resources and more services the need for more resources is satisfied either by obtaining more efficient use of existing resources or by creating an illusion that more resources exist in the system a virtual memory is what its name indicates it is an illusion of chapter virtual memory a memory that is larger than the real memory ie ram of the computer system as we pointed out in section this illusion is a part of a users abstract view of memory a user or his application program sees only the virtual memory the kernel implements the illusion through a combination of hardware and software means we refer to real memory simply as memory we refer to the software component of virtual memory as a virtual memory manager the illusion of memory larger than the systems memory crops up any time a process whose size exceeds the size of memory is initiated the process is able to operate because it is kept in its entirety on a disk and only its required portions are loaded in memory at any time the basis of virtual memory is the noncontiguous memory allocation model described earlier in section the address space of each process is assumed to consist of portions called components the portions can be loaded into nonadjacent areas of memory the address of each operand or instruction in the code of a process is a logical address of the form compi bytei the memory management unit mmu translates it into the address in memory where the operand or instruction actually resides use of the noncontiguous memory allocation model reduces memory fragmentation since a free area of memory can be reused even if it is not large enough to hold the entire address space of a process more user processes can be accommodated in memory this way which benefits both users and the os the kernel carries this idea further even processes that can fit in memory are not loaded fully into memory this strategy reduces the amount of memory that is allocated to each process thus further increasing the number of processes that can be in operation at the same time figure shows a schematic diagram of a virtual memory the logical address space of the process shown consists of five components three of these components are presently in memory information about the memory areas where these components exist is maintained in a data structure of the virtual memory manager this information is used by the mmu during address translation when an instruction in the process refers to a data item or instruction that is not in memory the component containing it is loaded from the disk occasionally the virtual memory manager removes some components from memory to make room for other components memory process loading disk removal of components logical memory address allocation space information physical address space figure overview of virtual memory part memory management the arrangement shown in figure is a memory hierarchy as discussed in section and illustrated in figure the hierarchy consists of the systems memory and a disk memory is fast but small in size the disk is slow but has a much larger capacity the mmu and the virtual memory manager together manage the memory hierarchy so that the current instruction in a process finds its operands in memory we are now ready to define virtual memory definition virtual memory a memory hierarchy consisting of a computer systems memory and a disk that enables a process to operate with only some portions of its address space in memory demand loading of process components the virtual memory manager loads only one component of a process address space in memory to begin with the component that contains the start address of the process that is address of the instruction with which its execution begins it loads other components of the process only when they are needed this technique is called demand loading to keep the memory commitment to a process low the virtual memory manager removes components of the process from memory from time to time these components would be loaded back in memory when needed again performance of a process in virtual memory depends on the rate at which its components have to be loaded into memory the virtual memory manager exploits the law of locality of reference to achieve a low rate of loading of process components we discuss this law in section table comparison of paging and segmentation issue comparison concept a page is a fixedsize portion of a process address space that is identified by the virtual memory hardware a segment is a logical entity in a program eg a function a data structure or an object segments are identified by the programmer size of components all pages are of the same size segments may be of different sizes external fragmentation not found in paging because memory is divided into page frames whose size equals the size of pages it occurs in segmentation because a free area of memory may be too small to accommodate a segment internal fragmentation occurs in the last page of a process in paging does not occur in segmentation because a segment is allocated a memory area whose size equals the size of the segment sharing sharing of pages is feasible subject to the constraints on sharing of code pages described later in section sharing of segments is freely possible demand paging as discussed earlier in section a process is considered to consist of pages numbered from onward each page is of size s bytes where s is a power of the memory of the computer system is considered to consist of page frames where a page frame is a memory area that has the same size as a page page frames are numbered from to frames where frames is the number of page frames of memory accordingly the physical address space consists of addresses from to frames s at any moment a page frame may be free or it may contain a page of some process each logical address used in a process is considered to be a pair pi bi where pi is a page number and bi is the byte number in pi bi s the effective memory address of a logical address pi bi is computed as follows effective memory address of logical address pi bi start address of the page frame containing page pi bi the size of a page is a power of and so calculation of the effective address is performed through bit concatenation which is much faster than addition see section for details figure is a schematic diagram of a virtual memory using paging in which page size is assumed to be kb where kb bytes three processes p p and p have some of their pages in memory the memory contains page frames numbered from to memory allocation information for a process is stored in a page table each entry in the page table contains memory allocation information for one page of a process it contains the page frame number where a page resides process p has its pages and in memory they occupy page frames and respectively process p has its pages and in page frames and while process p has its pages and in page frames and respectively the free frames list contains a list of free page frames currently only page frame is free part memory management page table of p memory mmu page pi bi frame page table of p add qi bi page table of p free frames list figure address translation in virtual memory using paging process p is currently executing the instruction add so the mmu uses ps page table for address translation the mmu views the operand address as the pair because it now accesses the entry for page in ps page table this entry contains frame number so the mmu forms the effective address according to eq and uses it to make a memory access in effect byte in page frame is accessed demand paging preliminaries if an instruction of p in figure refers to a byte in page the virtual memory manager will load page in memory and put its frame number in entry of ps page table these actions constitute demand loading of pages or simply demand paging to implement demand paging a copy of the entire logical address space of a process is maintained on a disk the disk area used to store this copy is called the swap space of a process while initiating a process the virtual memory manager allocates the swap space for the process and copies its code and data into the swap space during operation of the process the virtual memory manager is alerted when the process wishes to use some data item or instruction that is located in a page that is not present in memory it now loads the page from the swap space into memory this operation is called a pagein operation when the virtual memory manager decides to remove a page from memory the page is copied back into the swap space of the process to which it belongs if the page was modified since the last time it was loaded in memory this operation is called a pageout operation this way the swap space of a process contains an uptodate copy of every page of the process that is not present in memory a page replacement operation is one that loads a page into a page frame that previously contained another page it may involve a pageout operation if the previous page was modified while it occupied the page frame and involves a pagein operation to load the new page chapter virtual memory in this section we describe the data structures used by the virtual memory manager and the manner in which the virtual memory manager performs the pagein pageout and page replacement operations we then discuss how the effective memory access time for a process depends on the overhead of the virtual memory manager and the time consumed by the pagein pageout and page replacement operations page table the page table for a process facilitates implementation of address translation demand loading and page replacement operations figure shows the format of a page table entry the valid bit field contains a boolean value to indicate whether the page exists in memory we use the convention that indicates resident in memory and indicates not resident in memory the page frame field which was described earlier facilitates address translation the misc info field is divided into four subfields information in the prot info field is used for protecting contents of the page against interference it indicates whether the process can read or write data in the page or execute instructions in it ref info contains information concerning references made to the page while it is in memory as discussed later this information is used for page replacement decisions the modified bit indicates whether the page has been modified ie whether it is dirty it is used to decide whether a pageout operation is needed while replacing the page the other info field contains information such as the address of the disk block in the swap space where a copy of the page is maintained page faults and demand loading of pages table summarizes steps in address translation by the mmu while performing address translation for a logical address pi bi the mmu checks the valid bit of the page table entry of pi misc info valid page prot ref modiother bit frame info info fied info field description valid bit indicates whether the page described by the entry currently exists in memory this bit is also called the presence bit page frame indicates which page frame of memory is occupied by the page prot info indicates how the process may use contents of the page whether read write or execute ref info information concerning references made to the page while it is in memory modified indicates whether the page has been modified while in memory ie whether it is dirty this field is a single bit called the dirty bit other info other useful information concerning the page eg its position in the swap space figure fields in a page table entry part memory management table steps in address translation by the mmu step description obtain page number a logical address is viewed as a pair pi bi where bi and byte number in consists of the lower order nb bits of the address and pi page consists of the higher order np bits see section look up page table pi is used to index the page table a page fault is raised if the valid bit of the page table entry contains a ie if the page in not present in memory form effective memory the page frame field of the page table entry contains address a frame number represented as an nf bit number it is concatenated with bi to obtain the effective memory address of the byte swap space of p vm manager pagein page table operation page fault updating memory page table pi bi of p sub qi bi valid page misc mmu bit frame info free frames list figure demand loading of a page see step in table if the bit indicates that pi is not present in memory the mmu raises an interrupt called a missing page interrupt or a page fault which is a program interrupt see section the interrupt servicing routine for program interrupts finds that the interrupt was caused by a page fault so it invokes the virtual memory manager with the page number that caused the page fault ie pi as a parameter the virtual memory manager now loads page pi in memory and updates its page table entry thus the mmu and the virtual memory manager interact to decide when a page of a process should be loaded in memory figure is an overview of the virtual memory managers actions in demand loading of a page the broken arrows indicate actions of the mmu whereas chapter virtual memory firm arrows indicate accesses to the data structures memory and the disk by the virtual memory manager when a page fault occurs the numbers in circles indicate the steps in address translation raising and handling of the page fault steps were described earlier in table process p of figure is in operation while translating the logical address the mmu raises a page fault because the valid bit of page s entry is when the virtual memory manager gains control it knows that a reference to page caused the page fault the misc info field of the page table entry of page contains the address of the disk block in ps swap space that contains page the virtual memory manager obtains this address it now consults the free frames list and finds that page frame is currently free so it allocates this page frame to page and starts an io operation to load page in page frame when the io operation completes the virtual memory manager updates page s entry in the page table by setting the valid bit to and putting in the page frame field execution of the instruction sub which had caused the page fault is now resumed the logical address is translated to the effective address of byte number in page frame ie pagein pageout and page replacement operations figure showed how a pagein operation is performed for a required page when a page fault occurs in a process and a free page frame is available in memory if no page frame is free the virtual memory manager performs a page replacement operation to replace one of the pages existing in memory with the page whose reference caused the page fault it is performed as follows the virtual memory manager uses a page replacement algorithm to select one of the pages currently in memory for replacement accesses the page table entry of the selected page to mark it as not present in memory and initiates a pageout operation for it if the modified bit of its page table entry indicates that it is a dirty page in the next step the virtual memory manager initiates a pagein operation to load the required page into the page frame that was occupied by the selected page after the pagein operation completes it updates the page table entry of the page to record the frame number of the page frame marks the page as present and makes provision to resume operation of the process the process now reexecutes its current instruction this time the address translation for the logical address in the current instruction completes without a page fault the pagein and pageout operations required to implement demand paging constitute page io we use the term page traffic to describe movement of pages in and out of memory note that page io is distinct from io operations performed by processes which we will call program io the state of a process that encounters a page fault is changed to blocked until the required page is loaded in memory and so its performance suffers because of a page fault the kernel can switch the cpu to another process to safeguard system performance effective memory access time the effective memory access time for a process in demand paging is the average memory access time experienced by the process it depends on two factors time consumed by the mmu in performing address translation and the average time consumed by the virtual memory manager in part memory management handling a page fault we use the following notation to compute the effective memory access time pr probability that a page exists in memory tmem memory access time tpfh time overhead of page fault handling pr is called the memory hit ratio tpfh is a few orders of magnitude larger than tmem because it involves disk io one disk io operation is required if only a pagein operation is sufficient and two disk io operations are required if a page replacement is necessary a processs page table exists in memory when the process is in operation hence accessing an operand with the logical address pi bi consumes two memory cycles if page pi exists in memory one to access the page table entry of pi for address translation and the other to access the operand in memory using the effective memory address of pi bi if the page is not present in memory a page fault is raised after referencing the page table entry of pi ie after one memory cycle now the required page is loaded in memory and its page table entry is updated to record the frame number where it is loaded when operation of the process is resumed it requires two more memory references one to access the page table and the other to actually access the operand accordingly the effective memory access time is as follows effective memory access time pr tmem pr tmem tpfh tmem the effective memory access time can be improved by reducing the number of page faults one way of achieving it is to load pages before they are needed by a process the windows operating system performs such loading speculatively when a page fault occurs it loads the required page and also a few adjoining pages of the process this action improves the average memory access time if a preloaded page is referenced by the process the linux operating system permits a process to specify which pages should be preloaded a programmer may use this facility to improve the effective memory access time page replacement page replacement becomes necessary when a page fault occurs and there are no free page frames in memory however another page fault would arise if the replaced page is referenced again hence it is important to replace a page that is not likely to be referenced in the immediate future but how does the virtual memory manager know which page is not likely to be referenced in the immediate future the empirical law of locality of reference states that logical addresses used by a process in any short interval of time during its operation tend to be bunched together in certain portions of its logical address space processes exhibit this behavior for two reasons execution of instructions in a process is mostly sequential in nature because only percent of instructions executed by a process chapter virtual memory are branch instructions processes also tend to perform similar operations on several elements of nonscalar data such as arrays due to the combined effect of these two reasons instruction and data references made by a process tend to be in close proximity to previous instruction and data references made by it we define the current locality of a process as the set of pages referenced in its previous few instructions thus the law of locality indicates that the logical address used in an instruction is likely to refer to a page that is in the current locality of the process as mentioned in section the computer exploits the law of locality to ensure high hit ratios in the cache the virtual memory manager can exploit the law of locality to achieve an analogous effect fewer page faults would arise if it ensures that pages that are in the current locality of a process are present in memory note that locality of reference does not imply an absence of page faults let the proximity region of a logical address ai contain all logical addresses that are in close proximity to ai page faults can occur for two reasons first the proximity region of a logical address may not fit into a page in this case the next address may lie in an adjoining page that is not included in the current locality of the process second an instruction or data referenced by a process may not be in the proximity of previous references we call this situation a shift in locality of a process it typically occurs when a process makes a transition from one action in its logic to another the next example illustrates the locality of a process current locality of a process example in figure bullets indicate the last few logical addresses used during operation of a process pi dashed boxes show the proximity regions of these logical addresses note that the proximity region of a logical address may extend beyond a page boundary proximity regions of logical addresses may also overlap we show the cumulative proximity regions in figure eg the proximity regions of logical addresses referenced in page cumulatively cover the entire page and parts of pages and thus proximity regions are located in pages and however the current locality of pi is the set of pages whose numbers are marked with the marks in figure ie the set of pages the law of locality helps to decide which page should be replaced when a page fault occurs let us assume that the number of page frames allocated to a process pi is a constant hence whenever a page fault occurs during operation of pi one of pis own pages existing in memory must be replaced let t and t be the periods of time for which pages p and p have not been referenced during the operation of pi let t t implying that some byte of page p has been referenced or executed as an instruction more recently than any byte of page p hence page p is more likely to be a part of the current locality of the process than page p that is a byte of page p is more likely to be referenced or executed than a byte of page p we use this argument to choose page p for replacement part memory management page logical address no space of process pi logical address a page accessed in current locality proximity region of logical address figure proximity regions of previous references and current locality of a process when a page fault occurs if many pages of pi exist in memory we can rank them according to the times of their last references and replace the page that has been least recently referenced this page replacement policy is called lru page replacement memory allocation to a process figure shows how the page fault rate of a process should vary with the amount of memory allocated to it the page fault rate is large when a small amount of memory is allocated to the process however it drops when more memory is allocated to the process this page fault characteristic of a process is desired because it enables the virtual memory manager to take corrective action when it finds that a process has a high page fault rate it can bring about a reduction in the page fault rate by increasing the memory allocated to the process as we shall discuss in section the lru page replacement policy possesses a page fault characteristic that is similar to the curve of figure because it replaces a page that is less likely to be in the current locality of the process than other pages of the process that are in memory how much memory should the virtual memory manager allocate to a process two opposite factors influence this decision from figure we see that an overcommitment of memory to a process implies a low page fault rate for the process hence it ensures good process performance however a smaller number of processes would fit in memory which could cause cpu idling and poor system performance an undercommitment of memory to a process causes a high page fault rate which would lead to poor performance of the process the desirable operating zone marked in figure avoids the regions of overcommitment and undercommitment of memory the main problem in deciding how much memory to allocate to a process is that the page fault characteristic ie the slope of the curve and the page chapter virtual memory region of low region of moderate region of high memory allocation memory allocation memory allocation and high and moderate and low page fault rate page fault rate page fault rate page desirable fault operating rate zone no of page frames allocated to a process figure desirable variation of page fault rate with memory allocation fault rate in figure varies among processes even for the same process the page fault characteristic may be different when it operates with different data consequently the amount of memory to be allocated to a process has to be determined dynamically by considering the actual page fault characteristic of the process this issue is discussed in section thrashing consider a process that is operating in the region of low memory allocation and high page fault rate in figure due to the high page fault rate this process spends a lot of its time in the blocked state such a process is not in a position to use the cpu effectively it also causes high overhead due to high page fault rate and process switching caused by page faults if all processes in the system operate in the region of high page fault rates the cpu would be engaged in performing page traffic and process switching most of the time cpu efficiency would be low and system performance measured either in terms of average response time or throughput would be poor this situation is called thrashing definition thrashing a condition in which high page traffic and low cpu efficiency coincide note that low cpu efficiency can occur because of other causes as well eg if too few processes exist in memory or all processes in memory perform io operations frequently the thrashing situation is different in that all processes make poor progress because of high page fault rates from figure we can infer that the cause of thrashing is an undercommitment of memory to each process the cure is to increase the memory allocation for each process this may have to be achieved by removing some processes from memory that is by reducing the degree of multiprogramming a process may individually experience a high page fault rate without the system thrashing the same analysis now applies to the process it must suffer from an part memory management undercommitment of memory so the cure is to increase the amount of memory allocated to it optimal page size the size of a page is defined by computer hardware it determines the number of bits required to represent the byte number in a page page size also determines memory wastage due to internal fragmentation size of the page table for a process page fault rates when a fixed amount of memory is allocated to a process consider a process pi of size z bytes a page size of s bytes implies that the process has n pages where n zs is the value of zs rounded upward average internal fragmentation is s bytes because the last page would be half empty on the average the number of entries in the page table is n thus internal fragmentation varies directly with the page size while page table size varies inversely with it interestingly page fault rate also varies with page size if a fixed amount of memory is allocated to pi this can be explained as follows the number of pages of pi in memory varies inversely with the page size hence twice as many pages of pi would exist in memory if the page size were made s now let the proximity region of an instruction or data byte as defined in section be small compared with s so that it can be assumed to fit within the page that contains the byte when the page size is s memory contains twice as many proximity regions of recent logical addresses as when the page size is s bytes from the page fault characteristic of figure page fault rates would be smaller for smaller page sizes we can compute the page size that minimizes the total of memory penalty due to internal fragmentation and memory commitment to page tables if s zand each page table entry occupies byte of memory the optimal value of s is z thus the optimal page size is only bytes for a process size of kb and it is bytes for a process of kb however computers tend to use larger page sizes eg pentium and mips use page sizes of kb or more sun ultrasparc uses page sizes of kb or more and the powerpc uses a page size of kb for the following reasons page table entries tend to occupy more than byte hardware costs are high for smaller page sizes for example the cost of address translation increases if a larger number of bits is used to represent a page number disks which are used as paging devices tend to operate less efficiently for smaller disk block sizes the decision to use larger page sizes than the optimal value implies somewhat higher page fault rates for a process this fact represents a tradeoff between the hardware cost and efficient operation of a process chapter virtual memory paging hardware figure illustrates address translation in a multiprogrammed system page tables for many processes are present in memory the mmu contains a special register called the pagetable address register ptar to point to the start of a page table for a logical address pi bi the mmu computes ptar pi lptentry to obtain the address of the page table entry of page pi where lptentry is the length of a page table entry and ptar denotes the contents of the ptar the ptar has to be loaded with the correct address when a process is scheduled to facilitate this the kernel can store the address of the page table of a process in its process control block pcb table summarizes the functions performed by the paging hardware we describe the techniques used in implementing these functions and name a few modern computer systems that use them memory protection a memory protection violation interrupt should be raised if a process tries to access a nonexistent page or exceeds its access privileges while accessing a page the mmu provides a special register called the pagetable size register ptsr to detect violations of the first kind the kernel records the number of pages page table memory mmu of p ptar add pi bi fi page table of p ptsr fi bi page table of p memory protection exception figure address translation in a multiprogrammed system table functions of the paging hardware function description memory protection ensure that a process can access only those memory areas that are allocated to it efficient address provide an arrangement to perform address translation translation efficiently page replacement support collect information concerning references made to pages the virtual memory manager uses this information to decide which page to replace when a page fault occurs part memory management contained in a process in its process control block pcb and loads this number from the pcb in the ptsr when the process is scheduled a memory protection violation is raised if the page number in a logical address is not smaller than contents of ptsr this check is analogous to the one using the size register in the memory protection scheme of chapter the access privileges of a process to a page are stored in the prot info field of the pages entry in the page table during address translation the mmu checks the kind of access being made to the page against this information and raises a memory protection violation if the two are not compatible the information in the prot info field can be bitencoded for efficient access each bit in the field corresponds to one kind of access to the page eg read write etc it is set on only if the process possesses the corresponding access privilege to the page address translation and page fault generation the mmu follows the steps of table to perform address translation for a logical address pi bi it accesses the page table entry of pi by using pi lptentry as an offset into the page table where lptentry is the length of a page table entry lptentry is typically a power of so pi lptentry can be computed efficiently by shifting the value of pi by a few bits address translation buffers a reference to the page table during address translation consumes one memory cycle because the page table is stored in memory the translation lookaside buffer tlb is a small and fast associative memory that is used to eliminate the reference to the page table thus speeding up address translation the tlb contains entries of the form page page frame protection info for a few recently accessed pages of a program that are in memory during address translation of a logical address pi bi the tlb hardware searches for an entry of page pi if an entry is found the page frame from the entry is used to complete address translation for the logical address pi bi figure illustrates page page prot frame info translation lookaside buffer tlb memory mmu tlb pi bi hit page table of p add qi bi valid page prot bit frame info figure address translation using the translation lookaside buffer and the page table chapter virtual memory address pi bi y pis entry page fault in tlb n pageout n pi in needed load n memory y page pi raise remove page pi y page fault from memory update pt enter pi fi entry of pi in tlb update pt entry erase tlb entry invoke scheduler form physical of pj address using fi mmu actions virtual memory manager actions figure summary of address translation of pi bi note pt page table operation of the tlb the arrows marked and indicate tlb lookup the tlb contains entries for pages and of process p if pi is either or the tlb lookup scores a hit so the mmu takes the page frame number from the tlb and completes address translation a tlb miss occurs if pi is some other page hence the mmu accesses the page table and completes the address translation if page pi is present in memory otherwise it generates a page fault which activates the virtual memory manager to load pi in memory figure summarizes the mmu and software actions in address translation and page fault handling for a logical address pi bi mmu actions concerning use of the tlb and the page table are as described earlier the virtual memory manager is activated by a page fault if an empty page frame is not available to load page pi it initiates a pageout operation for some page pj to free the page frame say page frame fj occupied by it pjs page table entry is updated to indicate that it is no longer present in memory if pj has an entry in the tlb the virtual memory manager erases it by executing an erase tlb entry instruction this action is essential for preventing incorrect address translation at pjs next reference a pagein operation is now performed to load pi in page frame fj and pis page table entry is updated when the pagein operation is completed execution of the instruction that caused the page fault is repeated when the process is scheduled again this time pi does not have an entry in the tlb but it exists in memory and so the mmu uses information in the page table to complete the address translation an entry for pi has to be made in the tlb at this time new entries in the tlb can be made either by the hardware or by the virtual memory manager hardware handling of the tlb is more efficient the hardware can make a new entry in the tlb whenever it has to complete address part memory management translation through a reference to the page table when the tlb is managed by the virtual memory manager the mmu raises a missing tlb entry interrupt whenever it can not find an entry for the required page in the tlb and the virtual memory manager executes several instructions to make the tlb entry in this approach the mmu performs address translation exclusively through the tlb and the page table is used only by the virtual memory manager this arrangement provides flexibility because the virtual memory manager can use different organizations of the page table to conserve memory see section the powerpc and intel x architectures use hardwaremanaged tlbs while the mips sparc alpha and parisc architectures use softwaremanaged tlbs a few features are common to both the approaches a replacement algorithm is used to decide which tlb entry should be overwritten when a new entry is to be made use of the tlb can undermine protection if the mmu performs address translation through tlb entries that were made while some other process was in operation this issue is analogous to the protection issue in a cache discussed earlier in section hence the solutions are also analogous each tlb entry can contain the id of the process that was in operation when the entry was made that is each tlb entry can have the form process id page page frame protection info so that the mmu can avoid using it when some other process is in operation alternatively the kernel must flush the tlb while performing process switching we use the following notation to compute the effective memory access time when a tlb is used pr probability that a page exists in memory pr probability that a page entry exists in tlb tmem memory access time ttlb access time of tlb tpfh time overhead of page fault handling as mentioned earlier in section pr is the memory hit ratio and tmem is a few orders of magnitude smaller than tpfh typically ttlb is at least an order of magnitude smaller than tmem pr is called the tlb hit ratio when the tlb is not used the effective memory access time is as given by eq the page table is accessed only if the page being referenced does not have an entry in the tlb accordingly a page reference consumes ttlb tmem time if the page has an entry in the tlb and ttlb tmem time if it does not have a tlb entry but exists in memory the probability of the latter situation is prpr when the tlb is used pr is the probability that an entry for the required page exists in the tlb the probability that a page table reference is both necessary and sufficient for address translation is prpr the time consumed by each such reference is ttlb tmem since an unsuccessful tlb search would precede the page table lookup the probability of a page fault is pr it occurs after the tlb and the page table have been looked up and it requires tpfh ttlb tmem time if we assume that the tlb entry is made for the chapter virtual memory page while the effective memory address is being calculated hence the effective memory access time is effective memory access time pr ttlb tmem pr pr ttlb tmem pr ttlb tmem tpfh ttlb tmem to provide efficient memory access during operation of the kernel most computers provide wired tlb entries for kernel pages these entries are never touched by replacement algorithms superpages sizes of computer memories and processes have grown rapidly since the s tlb sizes have not kept pace with this increase because tlbs are expensive as a result of their associative nature their sizes have grown from about eight in the s to only about a thousand in hence tlb reach which is the product of the number of entries in a tlb and the page size has increased marginally but its ratio to memory size has shrunk by a factor of over consequently tlb hit ratios are poor and average memory access times are high see eq processor caches have also become larger than the tlb reach which affects performance of a cache that is searched by physical addresses because access to contents of the cache may be slowed down by tlb misses and lookups through the page table a generic way of countering these problems is to use a larger page size so that the tlb reach becomes larger however it leads to larger internal fragmentation and more page io in the absence of a generic solution techniques were developed to address specific problems created by the low tlb reach searching the cache by logical addresses took the tlb out of the path from the cpu to the cache which avoided a slowdown of cache lookup due to limited tlb reach however poor tlb hit ratios continued to degrade virtual memory performance superpages were evolved as a generic solution to the problems caused by low tlb reach a superpage is like a page of a process except that its size is a powerof multiple of the size of a page and its start address in both the logical and physical address spaces is aligned on a multiple of its own size this feature increases the tlb reach without increasing the size of the tlb and helps to obtain a larger tlb hit ratio most modern architectures permit a few standard superpage sizes and provide an additional field in a tlb entry to indicate the size of superpage that can be accessed through the entry the virtual memory manager exploits the superpages technique by adapting the size and number of superpages in a process to its execution characteristics it may combine some pages of a process into a superpage of an appropriate size if the pages are accessed frequently and satisfy the requirement of contiguity and address alignment in the logical address space this action is called a promotion the virtual memory manager may have to move the individual pages in memory during promotion to ensure contiguity and address alignment in memory a promotion increases the tlb reach and releases some of the tlb entries that were assigned to individual pages of the new superpage part memory management if the virtual memory manager finds that some pages in a superpage are not accessed frequently it may decide to disband the superpage into individual pages this action called demotion frees some memory that can be used to load other pages thus it has the potential to reduce page fault frequency support for page replacement the virtual memory manager needs two kinds of information for minimizing page faults and the number of pagein and pageout operations during page replacement the time when a page was last used whether a page is dirty ie whether a write operation has been performed on any byte in the page a page is clean if it is not dirty the time of last use indicates how recently a page was used by a process it is useful in selecting a candidate for page replacement however it is expensive to provide a sufficient number of bits in a page table entry for this purpose so most computers provide a single bit called the reference bit the modified bit in a page table entry is used to indicate whether a page is clean or dirty if a page is clean its copy in the swap space of the process is still current so no pageout operation is needed the page being loaded can simply overwrite such a page in memory for a dirty page a pageout operation must be performed because its copy in the swap space is stale a pagein operation for the new page to be loaded can be started only after the pageout operation is completed practical page table organizations a process with a large address space requires a large page table hence the virtual memory manager has to commit a large amount of memory for each page table for example in a computer system using bit logical addresses and a page size of kb a process can have million pages if the size of a page table entry is bytes the page table has a size of mb thus the virtual memory manager might tie up a few hundred megabytes of memory for storing page tables of processes the memory requirements would be even larger when bit logical addresses are used two approaches are followed to reduce the size of memory committed to page tables inverted page table the inverted page table ipt has one entry for each page frame in memory that indicates which page if any occupies the page frame the table got this name because the information in it is the inverse of the information in a page table the size of an inverted page table is governed by the size of memory so it is independent of the number and sizes of processes however information about a page can not be accessed directly as in a page table it has to be searched for in the ipt multilevel page table the page table of a process is itself paged the entire page table therefore does not need to exist in memory at any time a higherlevel page table is used to access pages of the page table if the higherlevel page table is large it could itself be paged and so on in this organization chapter virtual memory the page table entry of a page has to be accessed through relevant entries of the higherlevel page tables in both approaches the tlb is used to reduce the number of memory references needed to perform address translation inverted page tables figure a illustrates address translation using an inverted page table ipt each entry of the inverted page table is an ordered pair consisting of a process id and a page number thus a pair r pi in the fith entry indicates that page frame fi is occupied by page pi of a process r while scheduling a process the scheduler copies the id of the process from its pcb into a register of the mmu let this id be p the mmu performs address translation for a logical address pi bi in process p using the following steps separate the components pi and bi of the logical address using the process id p form the pair p pi search for the pair p pi in the ipt raise a page fault if the pair does not exist in the ipt a mmu page fault memory p process id page ref pi bi id info p pi fi p pi fi bi add inverted page table b pi page ref p id info pointer h fk q pk v fk fi p pi fl r pl hash table inverted page table figure inverted page table a concept b implementation using a hash table part memory management if the pair p pi exists in entry fi of the ipt copy the page frame number fi for use in address translation calculate the effective memory address using fi and bi these steps are shown as the circled numbers to in figure a the search for p pi in step should be conducted efficiently otherwise it would slow down address translation accordingly a hash table is used to speed up the search in the inverted page table figure b shows an arrangement called hashwithchaining which operates as follows each entry of the inverted page table contains an additional field pointer which points to another entry in the same table to hash a pair p pi we first concatenate the bit strings representing p and pi to obtain a larger bit string we now interpret this bit string as an integer number x and apply the following hash function h to it hx remainder x a where a is the size of the hash table which is typically some prime number hx which is in the range a is an entry number in the hash table let v designate its value hashing of many process idpage id pairs may produce the same value v because the total number of pages of all processes in memory is much larger than the size of the hash table entries of all these pairs in the inverted page table are chained together by the pointer field the inverted page table is constructed and maintained by the virtual memory manager as follows when page pi of process p is loaded in page frame fi in memory the virtual memory manager stores the pair p pi in the fith entry of the inverted page table it now hashes this pair to obtain an entry number say v and adds the fith entry of the inverted page table in the chain starting on the vth entry of the hash table as follows it copies the value found in the vth entry of the hash table into the pointer field of the fith entry of the inverted page table and enters fi into the vth entry of the hash table when this page is removed from memory the virtual memory manager deletes its entry from the chain starting on the vth entry of the hash table in figure b the pages r pl p pi and q pk were loaded into page frames fl fi and fk respectively and they all happened to hash into the vth entry of the hash table example describes how the mmu uses the inverted page table during address translation example search in the inverted page table the logical address pi bi is to be translated by using the inverted page table of figure b the pair p pi is hashed to obtain an entry number v in the hash table the chain starting on this entry is searched the pair p pi does not match with the pair q pk found in the page id field of the first entry of the chain therefore the mmu uses the pointer field of this entry to locate the next entry in the chain the pair in this entry matches p pi so the mmu uses the entry number of this entry ie fi as the frame number to form the physical address fi bi chapter virtual memory the average number of comparisons required to locate the entry of a pair p pi in the inverted page table depends on the average length of the chain starting on an entry of the hash table increasing the size of the hash table a reduces the average length of the chain a value of a frames ensures that the average number of entries in a linked list is less than the inverted page table contains exactly frames entries in it note that the inverted page table does not contain any information about pages that are not present in memory a conventional page table would have to be maintained on disk to contain their information inverted page tables have been used in the ibm rs and as systems and in the powerpc and parisc architectures they have also been used in solaris oss for sparc architectures multilevel page tables the memory requirement of the page table of a process is reduced by paging the page table itself and loading its pages on demand just like pages of processes this approach requires a twotiered addressing arrangement in which a higherlevel page table contains entries that hold information about pages of the page table and the page table contains information concerning pages of the process the information in each of these tables is similar to the information contained in a conventional page table figure illustrates the concept of a twolevel page table memory now contains two kinds of pages pages of processes and pages of page tables of processes which we shall call pt pages only three pt pages of a process p are currently in memory for address translation of a logical address pi bi in process p page pi of process p should exist in memory and the pt page that contains the entry for page pi should also exist in memory as mentioned in section the page number and byte number in a logical address pi bi are represented in np and nb bits the size of each page table entry is a power of so the number of page table entries that fit in one pt page is also a power of if the size of a table entry is e bytes the number of page table entries in one pt page is nb e ie nbe therefore the page number pi in the logical address itself consists of two parts id of the pt page that contains the page table entry of pi and an entry number within the pt page as shown in figure we call these two parts pi and pi respectively from the preceding discussion pi is contained in the lower order nb e bits of pi since the binary representation of pi contains np bits pi is contained in np nb e higherorder bits figure illustrates address translation for a logical address pi bi it consists of the following steps the address pi bi is regrouped into three fields np nb enb e nb pi pi bi the contents of these fields are pi pi and bi respectively part memory management pages of pages of the process p page table of p ie pt pages of p higherlevel page table pi x x x pi page pi bi byte with address pi bi pi pi bi pi figure twolevel page table organization the pt page with the number pi contains the page table entry for pi the mmu checks whether this page exists in memory and raises a page fault if it does not the page fault is serviced by the virtual memory manager to load the pt page in memory pi is the entry number for pi in the pt page the mmu uses information in this entry to check whether page pi exists in memory and raises a page fault if it does not the virtual memory manager services the page fault and loads page pi in memory the contents of pis page table entry are used to perform address translation thus address translation requires two memory accesses one to access the higherlevel page table and another to access the page table of process p it can be speeded up through the tlb by making two kinds of entries entries of the form p pi frame number protection info help to eliminate accesses to the higherlevel page table of process p and entries of the form p pi pi frame number protection info help to eliminate accesses to the page table of p when the size of the higherlevel page table in a twolevel page table organization is very large the higherlevel page table can itself be paged this arrangement chapter virtual memory results in a threelevel page table structure address translation using threelevel page tables is performed by an obvious extension of address translation in twolevel page tables a logical address pi bi is split into four components pi pi pi and bi and the first three components are used to address the three levels of the page table thus address translation requires up to three memory accesses in computer systems using bit addresses even the highestlevel page table in a threelevel page table organization may become too large fourlevel page tables are used to overcome this problem the intel architecture used twolevel page tables three and fourlevel page tables have been used in the sun sparc and motorola architectures respectively io operations in a paged environment a process makes a system call for performing io operations two of its parameters are the number of bytes to be transferred and the logical address of the data area which is the area of memory that participates in the data transfer the call activates the io handler in the kernel the io subsystem does not contain an mmu it uses physical addresses to implement data transfer to and from the memory consequently the io handler has to perform a few preparatory actions before initiating the io operation the first of these is to replace the logical address of the data area with its physical address using information from the page table of the process it has to perform some more actions to address two more issues discussed in the following the data area in an io operation may span several pages of the process a page fault while accessing a page of the data area would disrupt the io operation so all these pages must remain in memory while io is being performed the io handler satisfies this requirement by loading all pages of the data area into memory and putting an io fix on each page to instruct the virtual memory manager that these pages should not be replaced until the io fix is removed at the end of the io operation it now starts the io operation a simple way to implement io fixing of pages is to add an io fix bit in the misc info field of each page table entry since the io subsystem operates without an mmu it expects the data area to occupy a contiguous area of memory however the process is paged hence pages of the data area may not have contiguous physical addresses this situation can be addressed in two ways most io subsystems provide a scattergather feature which can deposit parts of an io operations data in noncontiguous areas of memory for example the first few bytes from an io record can be read into a page frame located in one part of memory and the remaining bytes can be read into another page frame located in a different part of memory analogously a gather write can draw the data of the io operation from noncontiguous memory areas and write it into one record on an io device example illustrates how a scatterread operation is used to implement an io operation that spans two pages in a process if an io subsystem does not provide the scattergather feature the io handler can handle the situation in part memory management two ways it can either instruct the virtual memory manager to put pages containing the data area contiguously in memory or it can first read the data into a kernel area that has contiguous physical addresses and then copy it to the data area in the process analogous provisions can be made to support a write operation example io operations in virtual memory page i of a process pi contains a system call perfio alpha read i where alpha is a file is the count of data bytes to be read and i is the logical address of the start of the data area figure illustrates how the io operation is implemented the page size is kb and so the data area is situated in pages i and i of the process before initiating the io operation the io handler invokes the virtual memory manager to load pages i and i into memory they are loaded into page frames and of memory the io handler puts an io fix on these pages by setting the io fix bits in the misc info field of their page table entries these pages are not replaced until the io fix is removed at the end of the io operation the io handler now generates a scatterread operation to read the first bytes starting at byte number in page frame and the remaining bytes starting at byte number in page frame it removes the io fix on pages and when the io operation completes logical address space frame memory scatterread perfio read i i perfio read i i i valid page misc bit frame info i io fix i io fix i page table figure an io operation in virtual memory the virtual memory manager the virtual memory manager uses two data structures the page table whose entry format is shown in figure and the free frames list the ref info and modified fields in a page table entry are typically set by the paging hardware all other fields are set by the virtual memory manager itself table summarizes the functions of the virtual memory manager we discuss the first four functions in this section other functions page replacement allocation of memory to processes and implementation of page sharing are discussed in the next few sections management of the logical address space of a process the virtual memory manager manages the logical address space of a process through the following subfunctions organize a copy of the instructions and data of the process in its swap space maintain the page table perform pagein and pageout operations perform process initiation as mentioned earlier in section a copy of the entire logical address space of a process is maintained in the swap space of the process when a reference to a page leads to a page fault the page is loaded from the swap space by using a pagein operation when a dirty page is to be removed from memory a pageout operation is performed to copy it from memory into a disk block in the swap table functions of the virtual memory manager function description manage logical address set up the swap space of a process organize its logical space address space in memory through pagein and pageout operations and maintain its page table manage memory keep track of occupied and free page frames in memory implement memory maintain the information needed for memory protection protection collect page reference paging hardware provides information concerning page information references this information is maintained in appropriate data structures for use by the page replacement algorithm perform page replacement perform replacement of a page when a page fault arises and all page frames in memory or all page frames allocated to a process are occupied allocate physical memory decide how much memory should be allocated to a process and revise this decision from time to time to suit the needs of the process and the os implement page sharing arrange sharing of pages be processes part memory management space thus the copy of a page in the swap space is current if that page is not in memory or it is in memory but it has not been modified since it was last loaded for other pages the copy in the swap space is stale ie outdated whereas that in memory is current one issue in swap space management is size of the swap space of a process most virtual memory implementations permit the logical address space of a process to grow dynamically during its operation this can happen for a variety of reasons the size of stack or pcd data areas may grow see section or the process may dynamically link more modules or may perform memory mapping of files see section an obvious approach to handling dynamic growth of address spaces is to allocate swap space dynamically and noncontiguously however this approach faces the problem that the virtual memory manager may run out of swap space during operation of a process to initiate a process only the page containing its start address ie address of its first instruction need managers to be loaded in memory other pages are brought in on demand details of the page table and the pagein and pageout operations have been described earlier in section management of memory the free frames list is maintained at all times a page frame is taken off the free list to load a new page and a frame is added to it when a pageout operation is performed all page frames allocated to a process are added to the free list when the process terminates protection during process creation the virtual memory manager constructs its page table and puts information concerning the start address of the page table and its size in the pcb of the process the virtual memory manager records access privileges of the process for a page in the prot info field of its page table entry during dispatching of the process the kernel loads the pagetable start address of the process and its pagetable size into registers of the mmu during translation of a logical address pi bi the mmu ensures that the entry of page pi exists in the page table and contains appropriate access privileges in the prot info field collection of information for page replacement the ref info field of the page table entry of a page indicates when the page was last referenced and the modified field indicates whether it has been modified since it was last loaded in memory page reference information is useful only so long as a page remains in memory it is reinitialized the next time a pagein operation is performed for the page most computers provide a single bit in the ref info field to collect page reference information this information is not adequate to select the best candidate for page replacement hence the virtual memory manager may periodically reset the bit used to store this information we discuss this aspect in section example page replacement the memory of a computer consists of eight page frames a process p consists of five pages numbered to only pages and are in memory at the moment they occupy page frames and respectively remaining page chapter virtual memory memory memory valid page misc valid page misc bit frame info bit frame info t m t t t t t page table of p page table of p a b figure data structures of the virtual memory manager a before and b after a page replacement frames have been allocated to other processes and no free page frames are left in the system figure a illustrates the situation in the system at time instant t ie a little after t only the page table of p is shown in the figure since process p has been scheduled contents of the ref info and modified fields are shown in the misc info field pages and were last referenced at time instants t t and t respectively page was modified sometime after it was last loaded hence the misc info field of its page table entry contains the information t m at time instant t process p gives rise to a page fault for page since all page frames in memory are occupied the virtual memory manager decides to replace page of the process the mark m in the misc info field of page s page table entry indicates that it was modified since it was last loaded so a pageout operation is necessary the page frame field of the page table entry of page indicates that the page exists in page frame the virtual memory manager performs a pageout operation to write the contents of page frame into the swap area reserved for page of p and modifies the valid bit in the page table entry of page to indicate that it is not present in memory a pagein operation is now initiated for page of p at the end of the operation the page table entry of page is modified to indicate that it exists in memory in page frame execution of p is resumed it now makes a reference to page and so the page reference information of page indicates that it was last referenced at t figure b indicates the page table of p at time instant t overview of operation of the virtual memory manager the virtual memory manager makes two important decisions during its operation when a page fault occurs during operation of some process proci it decides which page should be replaced page replacement policies as discussed earlier in section a page replacement policy should replace a page that is not likely to be referenced in the immediate future we evaluate the following three page replacement policies to see how well they fulfill this requirement optimal page replacement policy firstin firstout fifo page replacement policy least recently used lru page replacement policy chapter virtual memory for our analysis of these page replacement policies we rely on the concept of page reference strings a page reference string of a process is a trace of the pages accessed by the process during its operation it can be constructed by monitoring the operation of a process and forming a sequence of page numbers that appear in logical addresses generated by it the page reference string of a process depends on the data input to it so use of different data would lead to a different page reference string for a process for convenience we associate a reference time string t t t with each page reference string this way the kth page reference in a page reference string is assumed to have occurred at time instant tk in effect we assume a logical clock that runs only when a process is in the running state and gets advanced only when the process refers to a logical address example illustrates the page reference string and the associated reference time string for a process page reference string example a computer supports instructions that are bytes in length and uses a page size of kb it executes the following nonsense program in which the symbols a and b are in pages and respectively start read b loop mover areg a sub areg b bc lt loop stop a ds b ds end the page reference string and the reference time string for the process are as follows page reference string reference time string t t t t t t t t the logical address of the first instruction is and so it lies in page the first page reference in the string is therefore it occurs at time instant t b the operand of the instruction is situated in page and so the second page reference in the string is at time t the next instruction is located in page and refers to a which is located in page and thus the next two page references are to pages and the next two instructions are located in page and the instruction with the label loop is located in page therefore if the value of b input to the read statement is greater than the value of a the next four page references would be to pages and respectively otherwise the next four page references would be to pages and respectively part memory management optimal page replacement optimal page replacement means making page replacement decisions in such a manner that the total number of page faults during operation of a process is the minimum possible ie no other sequence of page replacement decisions can lead to a smaller number of page faults to achieve optimal page replacement at each page fault the page replacement policy would have to consider all alternative page replacement decisions analyze their implications for future page faults and select the best alternative of course such a policy is infeasible in reality the virtual memory manager does not have knowledge of the future behavior of a process as an analytical tool however this policy provides a useful comparison in hindsight for the performance of other page replacement policies see example below and exercise although optimal page replacement might seem to require excessive analysis belady showed that it is equivalent to the following simple rule at a page fault replace the page whose next reference is farthest in the page reference string fifo page replacement at every page fault the fifo page replacement policy replaces the page that was loaded into memory earlier than any other page of the process to facilitate fifo page replacement the virtual memory manager records the time of loading of a page in the ref info field of its page table entry when a page fault occurs this information is used to determine pearliest the page that was loaded earlier than any other page of the process this is the page that will be replaced with the page whose reference led to the page fault lru page replacement the lru policy uses the law of locality of reference as the basis for its replacement decisions its operation can be described as follows at every page fault the least recently used lru page is replaced by the required page the page table entry of a page records the time when the page was last referenced this information is initialized when a page is loaded and it is updated every time the page is referenced when a page fault occurs this information is used to locate the page plru whose last reference is earlier than that of every other page this page is replaced with the page whose reference led to the page fault analysis of page replacement policies example illustrates operation of the optimal fifo and lru page replacement policies example operation of page replacement policies a page reference string and the reference time string for a process p are as follows page reference string reference time string t t t t t t t figure illustrates operation of the optimal fifo and lru page replacement policies for this page reference string with alloc for convenience we show only two fields of the page table valid bit and ref info in the interval t chapter virtual memory optimal fifo lru time page valid ref replacevalid ref replacevalid ref replaceinstant ref bit info ment bit info ment bit info ment t t t t t t t t t t t t t replace replace t replace t by t by by t t t replace t t by t t replace t replace t replace t by t by t by replace replace t t by t by t t figure comparison of page replacement policies with alloc to t inclusive only two distinct pages are referenced pages and they can both be accommodated in memory at the same time because alloc t is the first time instant when a page fault leads to page replacement the left column shows the results for optimal page replacement page reference information is not shown in the page table since information concerning past references is not needed for optimal page replacement when the page fault occurs at time instant t page is replaced because its next reference is farther in the page reference string than that of page at time t page replaces page because page s next reference is farther than that of page the middle column of figure shows the results for the fifo replacement policy when the page fault occurs at time t the ref info field shows that page was loaded earlier than page and so page is replaced by page the last column of figure shows the results for the lru replacement policy the ref info field of the page table indicates when a page was last referenced at time t page is replaced by page because the last reference of page is earlier than the last reference of page part memory management the total number of page faults occurring under the optimal fifo and lru policies are and respectively by definition no other policy has fewer page faults than the optimal page replacement policy when we analyze why the lru policy performed better than the fifo policy in example we find that the fifo policy removed page at time t but lru did not do so because it had been referenced later than page this decision is consistent with the law of locality of reference which indicates that because page was referenced more recently than page it has a higher probability of being referenced again than page the lru policy performed better because page was indeed referenced earlier after time t than page was however the lru page replacement policy may not perform better than the fifo policy if references in a page reference string do not follow the law of locality of reference for example for alloc the lru and fifo policies would perform identically for the page reference string while the lru policy would perform worse than the fifo policy for the string however such situations are not encountered frequently to achieve the desirable page fault characteristic of figure a page replacement policy must possess the stack property also called the inclusion property it is defined by using the following notation pi kn set of pages existing in memory at time instant tk if alloci n all through the operation of process proci tk implies a time after time instant tk but before tk definition stack property a page replacement policy possesses the stack property if pink pikm for all n m such that n m figure illustrates pink for different values of n for a page replacement policy we find that pikn pink for n hence the algorithm possesses the stack property to understand how the stack property ensures the desirable page fault characteristic of figure consider two runs of process proci one with alloci n all through the execution and another with alloci m such that n m if a n n n n n figure pikn for different n for a page replacement policy processing the stack property chapter virtual memory page replacement policy exhibits the stack property then at identical points during these operations of proci ie at identical time instants all pages that were in memory when alloci n would also be in memory when alloci m in addition memory also contains m n other pages of the process if any of these pages are referenced in the next few page references of proci page faults occur if alloci n but not if alloci m thus the page fault rate is higher if alloci n than if alloci m this satisfies the page fault characteristic of figure the page fault rates will be identical if these m n pages are not referenced in the next few page references however in no case will the page fault rate increase when the memory allocation for a process is increased if a page replacement policy does not exhibit the stack property then pimk may not contain some pages contained in pink references to these pages would result in page faults hence the page fault rate can increase when the memory allocation for a process is increased example illustrates that the fifo page replacement policy does not exhibit the stack property one can prove that the lru page replacement policy exhibits the stack property see exercise problems in fifo page replacement example consider the following page reference and reference time strings for a process page reference string reference time string t t t t t t t t t t t t t figure shows operation of the fifo and lru page replacement policies for this page reference string page references that cause page faults and result in page replacement are marked with a mark a column of boxes is associated with each time instant each box is a page frame the number contained in it indicates which page occupies it after execution of the memory reference marked under the column for fifo page replacement we have pi while pi thus fifo page replacement does not exhibit the stack property this leads to a page fault at t when alloci but not when alloci thus a total of page faults arise in time instants when alloci while page faults arise when alloci for lru we see that pi pi at all time instants figure illustrates the page fault characteristic of fifo and lru page replacement for page reference string for simplicity the vertical axis shows the total number of page faults rather than the page fault frequency figure a illustrates an anomaly in behavior of fifo page replacement the number of page faults increases when memory allocation for the process is increased this anomalous behavior was first reported by belady and is therefore known as beladys anomaly part memory management fifo alloci alloci time line t t t t t t t t t t t t t lru alloci alloci time line t t t t t t t t t t t t t figure performance of fifo and lru page replacement number number of page of page faults faults fifo alloc lru alloc figure a beladys anomaly in fifo page replacement b page fault characteristic for lru page replacement the virtual memory manager can not use fifo page replacement because increasing the allocation to a process may increase the page fault frequency of the process this feature would make it difficult to combat thrashing in the system however when lru page replacement is used the number of page faults is a nonincreasing function of alloc hence it is possible to combat thrashing by increasing the value of alloc for each process practical page replacement policies figure shows a schematic diagram of a practical virtual memory manager the virtual memory manager maintains a free frames list and tries to keep a few page frames in this list at all times the virtual memory manager consists chapter virtual memory virtual memory manager page free page fault frames io handler manager manager page tables free frames list swap disk figure page replacement in practice of two daemon threads the thread called free frames manager is activated by the virtual memory manager when the number of free page frames drops below a threshold defined by the virtual memory manager the free frames manager scans the pages present in memory to identify a few pages that can be freed and adds the page frames occupied by these pages to the free frames list if the page contained in a newly added page frame is dirty it marks the page frame as dirty in the free frames list it also resets the valid bit of this page in the page table of the process to which it belongs the free frames manager puts itself to sleep when the number of free page frames exceeds another threshold of the virtual memory manager the thread called page io manager performs pageout operations on dirty page frames in the free frames list it resets the dirty bit of a page frame when its pageout operation is completed the page fault handler runs as an event handler of the kernel it is activated when a page fault occurs it first checks whether the required page exists in any of the page frames in the free frames list if so it simply reclaims the page by removing its page frame from the free frames list setting the valid bit of the page in the page table of the process and copying the value of the dirty bit of the page frame into the modified bit of the page this operation makes the required page available without having to perform a pagein operation if the required page does not exist in any page frame it takes a clean page frame off the free frames list and starts the pagein operation on it effectively the page replacement policy is implemented in the free frames manager of the virtual memory manager however in the following we will discuss page replacement as if it were done directly by the virtual memory manager the lru page replacement policy should be the automatic choice for implementation in a virtual memory manager because it exhibits the stack property however lru page replacement is not feasible because computers do not provide sufficient bits in the ref info field to store the time of last reference most computers provide a single reference bit for collecting information about references to a page therefore page replacement policies have to be implemented part memory management using only the reference bit this requirement has led to a class of policies called not recently used nru policies where the reference bit is used to determine whether a page has been recently referenced and some page that has not been recently referenced is replaced a simple nru policy is as follows the reference bit of a page is initialized to when the page is loaded and it is set to when the page is referenced when page replacement becomes necessary if the virtual memory manager finds that the reference bits of all pages have become it resets the bits of all pages to and arbitrarily selects one of the pages for replacement otherwise it replaces a page whose reference bit is future page replacement would depend on which of the pages were referenced after the reference bits were reset page replacement algorithms called clock algorithms provide better discrimination between pages by resetting reference bits of pages periodically rather than only when all of them become so that it would be possible to know whether a page has been referenced in the immediate past say within the past instructions rather than since the time when all reference bits were reset to clock algorithms in clock algorithms pages of all processes in memory are entered in a circular list and pointers used by the algorithms move over the pages repeatedly the algorithms get their name from the fact that movement of the pointers is analogous to movement of the hands of a clock over the clock dial the page pointed to by a pointer is examined a suitable action is performed on it and the pointer is advanced to point to the next page the clock algorithms can also be applied at the level of a single process when the memory allocation for a process is to be decreased in this case the virtual memory manager would maintain a separate list of pages for each process and the clock algorithm would scan only the list of the process whose memory allocation is to be decreased in the onehanded clock algorithm a scan consists of two passes over all pages in the first pass the virtual memory manager simply resets the reference bit of the page pointed to by the pointer in the second pass it finds all pages whose reference bits are still off and adds them to the free list in the twohanded clock algorithm two pointers are maintained one pointer which we will call the resetting pointer rp is used for resetting the reference bits and the other pointer which we will call the examining pointer ep is used for checking the reference bits both pointers are incremented simultaneously the page frame to which the checking pointer points is added to the free frames list if its reference bit is off example describes operation of the twohanded clock algorithm example twohanded clock algorithm figure illustrates operation of the twohanded clock algorithm when used by the free frames manager of figure the ref mark against a page implies that the reference bit of the page is set to absence of this mark implies that the reference bit is when the free frames manager is activated it examines page which is pointed to by the examining pointer see figure a its reference bit is so both the resetting and examining fundamental principles of os operation before we discuss features of operating systems in chapter and their design in later chapters it is important to have a functional understanding of the operation of an os what features of a modern computer system are important from the os viewpoint how the os uses these features during its operation to control user programs and resources and implement security and protection and how user programs obtain services from the os as discussed in section the kernel of the operating system is the collection of routines that form the core of the operating system it controls operation of the computer by implementing the tasks discussed in section hence we controlling memory allocation to a process section described how an overcommitment of memory to processes affects system performance because of a low degree of multiprogramming whereas an undercommitment of memory to processes leads to thrashing which is characterized by high page io low cpu efficiency and poor performance of processes and the system keeping the memory allocation for a process within the desirable operating zone shown in figure avoids both overcommitment and undercommitment of memory to a process however it is not clear how the virtual memory manager should decide the correct number of page frames to be allocated to each process that is the correct value of alloc for each process two approaches have been used to control the memory allocation for a process fixed memory allocation the memory allocation for a process is fixed hence performance of a process is independent of other processes in the system when a page fault occurs in a process one of its own pages is replaced this approach is called local page replacement part memory management variable memory allocation the memory allocation for a process may be varied in two ways when a page fault occurs all pages of all processes that are present in memory may be considered for replacement this is called global page replacement alternatively the virtual memory manager may revise the memory allocation for a process periodically on the basis of its locality and page fault behavior but perform local page replacement when a page fault occurs in fixed memory allocation memory allocation decisions are performed statically the memory to be allocated to a process is determined according to some criterion when the process is initiated to name a simple example the memory allocated to a process could be a fixed fraction of its size page replacement is always performed locally the approach is simple to implement and the overhead of page replacement is moderate as only pages of the executing process are considered in a page replacement decision however the approach suffers from all problems connected with a static decision an undercommitment or overcommitment of memory to a process would affect the processs own performance and performance of the system also the system can encounter thrashing in variable memory allocation using global page replacement the allocation for the currently operating process may grow too large for example if an lru or nru page replacement policy is used the virtual memory manager will be replacing pages of other processes most of the time because their last references will precede references to pages of the currently operating process memory allocation to a blocked process would shrink and so the process would face high page fault rates when it is scheduled again in variable memory allocation using local page replacement the virtual memory manager determines the correct value of alloc for a process from time to time in the following we discuss how this can be done in practice working set model the concept of a working set provides a basis for deciding how many and which pages of a process should be in memory to obtain good performance of the process a virtual memory manager following the working set model is said to be using a working set memory allocator definition working set the set of pages of a process that have been referenced in the previous instructions of the process where is a parameter of the system the previous instructions are said to constitute the working set window we introduce the following notation for our discussion ws i t working set for process proci at time t for window size wss i t size of the working set wsit ie the number of pages in wsit note that wssit because a page may be referenced more than once in a working set window we omit t when t and are either unimportant or obvious from the context chapter virtual memory a working set memory allocator either holds the complete working set of a process in memory or suspends the process thus at any time instant t a process proci either has wsi in memory and alloci wssi or it has alloci this strategy helps in ensuring a good hit ratio in memory through the law of locality of reference it also prevents undercommitment of memory to a process thereby preventing thrashing the working set memory allocator must vary the degree of multiprogramming in accordance with changes in working set sizes of processes for example if prock is the set of processes in memory the degree of multiprogramming should be decreased if kwssk frames where frames is the total number of page frames in memory the working set memory allocator removes some processes from memory until kwssk frames the degree of multiprogramming should be increased if kwssk frames and there exists a process procg such that wssg frames k wss k procg should now be allocated wssg page frames and its operation should be resumed variations in the degree of multiprogramming are implemented as follows the virtual memory manager maintains two items of information for each process alloci and wss i when the degree of multiprogramming is to be reduced the virtual memory manager selects the process to be suspended say process proci it now performs a pageout operation for each modified page of proci and changes the status of all page frames allocated to it to free alloci is set to however the value of wssi is left unchanged when the degree of multiprogramming is to be increased and the virtual memory manager decides to resume proci it sets alloci wssi and allocates as many page frames as the value of alloci it now loads the page of proci that contains the next instruction to be executed other pages would be loaded when page faults occur alternatively the virtual memory manager loads all pages of wsi when execution of proci is resumed however this approach may lead to redundant loading of pages because some pages in wsi may not be referenced again performance of a working set memory allocator is sensitive to the value of if is too large memory will contain some pages that are not likely to be referenced again this is overcommitment of memory to processes too large a value of also forces the virtual memory manager to reduce the degree of multiprogramming thereby affecting system performance if is too small there is a danger of undercommitment of memory to processes leading to an increase in page fault frequency and the possibility of thrashing implementation of a working set memory allocator use of a working set memory allocator suffers from one practical difficulty it is expensive to determine wsi t and alloci at every time instant t to address this difficulty a virtual part memory management process t t t t wss alloc wss alloc wss alloc wss alloc p p p p figure operation of a working set memory allocator memory manager using a working set memory allocator can determine the working sets of processes periodically rather than at every time instant working sets determined at the end of an interval are used to decide values of alloc for use during the next interval the next example illustrates this approach example working set memory allocator a virtual memory manager has page frames available for allocation to user processes it recomputes the working sets of all processes at time instants tj j following the computation of working sets it handles each process pi as follows it sets alloci wssi if it can allocate wssi page frames to it else it sets alloci and removes all pages of pi from memory the value of alloc assigned at tj is held constant until tj figure illustrates operation of the working set memory allocator it shows values of alloc and wss for all processes at time instants t t t and t at t wss alloc and i wssi it implies that the working set size of p is page frames however its operation has been suspended because only ie page frames are free at t values of wssi i are recomputed the value of wss is carried over from t since p has not been executed in the interval tt alloci i are now assigned new values p still can not be swapped in for lack of memory since iwssi so only five page frames are free and wss at t p is swapped in however it is swapped out again at t note that during the interval t t the smallest allocation for p is page frames and the largest allocation is page frames this variation is performed to adjust the processs memory allocation to its recent behavior expansion and contraction of alloc is performed as follows at t the virtual memory manager decides to reduce alloc from page frames to page frames so it uses an nrulike policy to remove two pages of p at t it increases alloc to page frames so it allocates two more page frames to alloc these page frames will be used when page faults arise during operation of p the virtual memory manager can use the reference bits provided by the paging hardware to determine the working sets reference bits of all pages in memory can be turned off when working sets are determined these bits will be turned on the computer figure is a schematic of a computer showing the functional units that are relevant from the viewpoint of an operating system the cpu and memory are directly connected to the bus while the io devices are connected to the bus through device controllers and the dma if the cpu and io devices try to access the memory at the same time the bus permits only one of them to proceed the other accesses are delayed until this access completes we describe important details of the functional units in the next few sections in a later section we discuss how the os uses features of a computer to control the operation of chapter the os the computer and user programs interrupts protection io bus memory psw mmu l cache l cache cpu ram registers dma device controllers io devices data path control path interrupt path figure schematic of a computer the computer and execution of user programs on it discussions in this chapter are restricted to computers with a single cpu features of multiprocessor and distributed computer systems are described in later chapters the cpu generalpurpose registers gprs and the program status word psw two features of the cpu are visible to user programs or the operating system the first is those registers that are used to hold data addresses index values or the stack pointer during execution of a program these registers are variously called generalpurpose registers gprs or programaccessible registers we prefer to call them gprs the other feature is a set of control registers which contain information that controls or influences operation of the cpu for simplicity we will call the collection of control registers the program status word psw and refer to an individual control register as a field of the psw figure describes the fields of the psw two fields of the psw are commonly known to programmers the program counter pc contains the address of the next instruction to be executed by the cpu the condition code cc contains a code describing some characteristics of the last arithmetic or logical result computed by the cpu eg whether the result of an arithmetic operation is or the result of a comparison is not equal these characteristics are often stored in a set of discrete flags however we will view them collectively as the condition code field or a field called flags contents and uses of other control registers are described later in this section part overview program condition mode memory protection interrupt interrupt counter code m information mask code pc cc mpi im ic field description program counter contains address of the next instruction to be executed condition code indicates some characteristics of the result of the last arithmetic or flags logical instruction eg whether the result of an arithmetic instruction was or this code is used in execution of a conditional branch instruction mode indicates whether the cpu is executing in kernel mode or user mode we assume a singlebit field with the value to indicate that the cpu is in kernel mode and to indicate that it is in user mode memory protection memory protection information for the currently executing program information this field consists of subfields that contain the base register and size register interrupt mask indicates which interrupts are enabled that is which interrupts can occur at present and which ones are masked off interrupt code describes the condition or event that caused the last interrupt this code is used by an interrupt servicing routine figure important fields of the program status word psw kernel and user modes of cpu operation the cpu can operate in two modes called user mode and kernel mode the cpu can execute certain instructions only when it is in the kernel mode these instructions called privileged instructions implement special operations whose execution by user programs would interfere with the functioning of the os or activities of other user programs eg an instruction that changes contents of the memory protection information mpi field of the psw could be used to undermine memory protection in the system section contains an example the os puts the cpu in kernel mode when it is executing instructions in the kernel so that the kernel can execute special operations and puts it in user mode when a user program is in execution so that the user program can not interfere with the os or other user programs we assume the mode m field of the psw to be a singlebit field that contains a when the cpu is in kernel mode and a when it is in user mode state of the cpu the generalpurpose registers and the psw together contain all the information needed to know what the cpu is doing we say that this information constitutes the state of the cpu as discussed in section the kernel may preempt the program that is currently using the cpu see figure to ensure that the program can resume its execution correctly when scheduled in future the kernel saves the state of the cpu when it takes away the cpu from the program and simply reloads the saved cpu state into the gprs and the psw when execution of the program is to be resumed example illustrates how saving and restoring the state of the cpu suffices to correctly resume execution of a program chapter the os the computer and user programs state of the cpu example figure a shows an assembly language program for a hypothetical computer whose cpu has two data registers a and b an index register x and the stack pointer register sp each assembly language instruction in this program corresponds to either an instruction in the cpu or a directive to the assembler eg the last statement declares alpha to be a memory location that contains the value the first instruction moves the value of alpha into register a the second instruction compares the value in register a with the value this comparison sets an appropriate value in the condition code field also called the flags field the third instruction which has the operation code beq is a conditional branch instruction that transfers control to the instruction with label next if the result of the comparison is equal we assume that the result of the compare instruction was equal and that condition code corresponds to this result if the kernel decides to take away the cpu from the program after the program has executed the compare instruction it saves the state of the cpu which is shown in figure b the state consists of the contents of the psw and the registers a b x and sp the pc contains which is the address of the next instruction to be executed the condition code field contains to indicate that the values that were compared were equal the mpi field contains memory protection information for the program which we shall discuss in section if this cpu state is loaded back into the cpu the program will resume its execution at the beq instruction that exists in the memory location with the address since the condition code field contains implying equal the beq instruction will transfer control to the instruction labeled next thus the program would execute correctly when resumed pc cc m address instruction psw move a alpha mpi im ic compare a a beq next registers b next x alpha dclconst sp a b figure a listing of an assembly language program showing address assigned to each instruction or data b state of the cpu after executing the compare instruction part overview memory management unit mmu as mentioned in section virtual memory is an illusion of a memory that may be larger that the real memory of a computer as described in section an os implements virtual memory by using noncontiguous memory allocation and the mmu figure the os allocates a set of memory areas to a program and stores information concerning these areas in a table of memory allocation information during the execution of the program the cpu passes the address of a data or instruction used in the current instruction to the mmu this address is called a logical address the mmu uses the memory allocation information to find the address in memory where the required data or instruction actually resides this address is called the physical address and the process of obtaining it from a logical address is called address translation in the interest of simplicity we do not describe details of address translation here they are described in chapter memory hierarchy a computer system should ideally contain a large enough and fast enough memory so that memory accessing will not slow down the cpu however fast memory is expensive so something that can provide the same service as a large and fast memory but at a lower cost is desirable the solution is a memory hierarchy containing a number of memory units with differing speeds the fastest memory in the hierarchy is the smallest in size slower memories are larger in size the cpu accesses only the fastest memory if the data or instruction needed by it is present in the fastest memory it is used straightaway otherwise the required data is copied into the fastest memory from a slower memory and then used the data remains in the fastest memory until it is removed to make place for other data this arrangement helps to speed up accesses to repeatedly used data other levels in the memory hierarchy are used analogously if data is not present in a faster memory it is copied there from a slower memory and so on the effective memory access time depends on how frequently this situation arises in a faster memory figure shows a schematic of a simple memory hierarchy the hierarchy contains three memory units the cache memory is fast and small main memory which is also called random access memory ram is slow and large we will simply call it memory the disk is the slowest and largest unit in the hierarchy we discuss operation of this memory hierarchy before discussing memory hierarchies in modern computers cache memory the cache memory holds some instructions and data values that were recently accessed by the cpu to enhance cache performance the memory hardware does not transfer a single byte from memory into the cache it always loads a block of memory with a standard size into an area of the cache called a cache block or cache line this way access to a byte in close proximity of a recently accessed byte can be implemented without accessing memory when the cpu writes a new value into a byte the changed byte is written into the cache chapter the os the computer and user programs cpu requests contents of byte with logical address al mmu sends a physical address ap byte with fastest memory cache block address ap smallest size cache transfer of transfer of cache blocks bytes or cache blocks page byte with memory address ap memory hierarchy transfer of pages slowest memory disk block largest size disk figure operation of a memory hierarchy sooner or later it also has to be written into the memory different schemes have been used for writing a byte into memory a simple one is to write the byte into the cache and the memory at the same time it is called the writethrough scheme for every data or instruction required during execution of a program the cpu performs a cache lookup by comparing addresses of the required bytes with addresses of bytes in memory blocks that are present in the cache a hit is scored if the required bytes are present in memory in which case the bytes can be accessed straightaway otherwise a miss is scored and the bytes have to be loaded into the part overview cache from memory the hit ratio h of the cache is the fraction of bytes accessed by the cpu that score a hit in the cache high hit ratios are obtained in practice as a result of an empirical law called locality programs tend to access bytes located in close proximity of recently accessed bytes which is called spatial locality and access some data and instructions repeatedly which is called temporal locality effective memory access time of a memory hierarchy consisting of a cache and memory is given by the formula tema h tcache h ttra tcache tcache h ttra where tema effective memory access time tcache access time of cache and ttra time taken to transfer a cache block from memory to cache larger cache blocks are needed to ensure a high hit ratio through spatial locality however a large cache block would increase ttra hence advanced memory organizations are used to reduce ttra and the cache block size that provides the best combination of the hit ratio and ttra is chosen the intel pentium processor uses a cache block size of bytes and a memory organization that makes ttra only about times the memory access time if we consider tcache ns and a memory that is times slower than the cache we have ttra ns ns with a cache hit ratio of this organization provides tema ns which is percent of the access time of memory note that the hit ratio in a cache is poor at the start of execution of a program because few of its instructions or data have been transferred to the cache the hit ratio is higher when the program has been in execution for some time memory hierarchies in modern computers differ from that shown in figure in the number of cache memories and the placement of the mmu because of the large mismatch in the speeds of memory and the cache a hierarchy of cache memories is used to reduce the effective memory access time instead of the single cache shown in figure as shown in figure an l cache that is a level cache is incorporated into the cpu chip itself the cpu chip may also contain another cache called the level or l cache which is slower but larger than the l cache a much larger and slower l cache is typically external to the cpu we show it to be associated with memory as in figure all these cache levels help to improve the effective memory access time to determine how much just substitute the transfer time of a block from the lower cache level in place of ttra in eq and use the equation analogously to account for a cache miss in the lower cache level during the transfer see exercise another difference is that the mmu is replaced by a parallel configuration of the mmu and the l cache this way a logical address is sent to the l cache rather than a physical address it eliminates the need for address translation before looking up the l cache which speeds up access to the data if a hit is scored in the l cache it also permits address translation performed by the mmu to overlap with lookup in the l cache which saves time if a cache miss occurs in the l cache chapter the os the computer and user programs memory as a part of the memory hierarchy operation of memory is analogous to operation of a cache the similarities are in transferring a block of bytes typically called a page from the disk to memory when a program refers to some byte in the block and transferring it from memory to the disk to make place for other blocks that are needed in memory the difference lies in the fact that the management of memory and transfer of blocks between memory and the disk are performed by the software unlike in the cache where it is performed by the hardware the memory hierarchy comprising the memory management unit mmu memory and the disk is called the virtual memory virtual memory is discussed in chapter elsewhere in the book for simplicity we ignore the role of the mmu and disks memory protection many programs coexist in a computers memory so it is necessary to prevent one program from reading or destroying the contents of memory used by another program this requirement is called memory protection it is implemented by checking whether a memory address used by a program lies outside the memory area allocated to it two control registers are used to implement memory protection the base register contains the start address of the memory area allocated to a program while the size register also called the limit register contains the size of memory allocated to the program accordingly the last byte of memory allocated to a program has the address address of last byte base size where base and size indicate contents of the base register and size register respectively before making any memory access say access to a memory location with address aaa the memory protection hardware checks whether aaa lies outside the range of addresses defined by contents of the base and size registers if so the hardware generates an interrupt to signal a memory protection violation and abandons the memory access as described in a later section the kernel aborts the erring program in response to the interrupt the memory protection information mpi field of the psw see figure contains the base and size registers this way the memory protection information also becomes a part of the cpu state and gets saved or restored when the program is preempted or resumed respectively fundamentals of memory protection example program p is allocated the byte memory area to by the kernel figure illustrates memory protection for this program using the base and size registers the start address of the allocated area ie is loaded in the base register while the number is loaded in the size register a memory protection violation interrupt would be generated if the instruction being executed by the cpu uses an address that lies outside the range say the address part overview memory address cpu no kernel base base load p yes size address p base no size byte with p yes address normal protection memory violation access interrupt figure memory protection using the base and size registers a program could undermine the memory protection scheme by loading information of its choice in the base and size registers for example program p could load the address in the base register and the size of the computers memory in the size register and thereby get itself a capability of modifying contents of any part of memory which would enable it to interfere with the os or other user programs to prevent this instructions to load values into the base and size registers are made privileged instructions since the cpu is in the user mode while executing a user program this arrangement prevents a user program from undermining the memory protection scheme memory protection in a cache memory is more complex recall from the earlier discussion that the l cache is accessed by using logical addresses a program of size n bytes typically uses logical addresses n thus many programs may use the same logical addresses so a check based on a logical address can not be used to decide whether a program may access a value that exists in the cache memory a simple approach to memory protection would be to flush the cache ie to erase contents of the entire cache whenever execution of a program is initiated or resumed this way the cache would not hold contents of memory areas allocated to other programs however any parts of the program that were loaded in the cache during its execution in the past would also be erased hence execution performance of the program would suffer initially because of a poor cache hit ratio in an alternative scheme the id of the program whose instructions or data are loaded in a cache block is remembered and only that program is permitted to access contents of the cache block it is implemented as follows when a program generates a logical address that is covered by contents of a cache block a cache hit occurs only if the programs id matches the id of the program whose instructions or data are loaded in the cache block this scheme is preferred because it does not require flushing of the cache and does not affect execution performance of programs inputoutput an io operation requires participation of the cpu memory and an io device the manner in which the data transfer between memory and the io device chapter the os the computer and user programs table modes of performing io operations io mode description programmed io data transfer between the io device and memory takes place through the cpu the cpu can not execute any other instructions while an io operation is in progress interrupt io the cpu is free to execute other instructions after executing the io instruction however an interrupt is raised when a data byte is to be transferred between the io device and memory and the cpu executes an interrupt servicing routine which performs transfer of the byte this sequence of operations is repeated until all bytes get transferred direct memory access data transfer between the io device and memory takes dmabased io place directly over the bus the cpu is not involved in data transfer the dma controller raises an interrupt when transfer of all bytes is complete is implemented determines the data transfer rates and the extent of the cpus involvement in the io operation the io organization we find in modern computers has evolved through a sequence of steps directed at reducing the involvement of the cpu in an io operation apart from providing higher data transfer rates it also frees the cpu to perform other activities while an io operation is in progress we assume that operands of an io instruction indicate the address of an io device and details of io operations to be performed execution of the io instruction by the cpu initiates the io operation on the indicated device the io operation is performed in one of the three modes described in table in the programmed io mode data transfer is performed through the cpu hence data transfer is slow and the cpu is fully occupied with it consequently only one io operation can be performed at a time the interrupt mode is also slow as it performs a bytebybyte transfer of data with the cpus assistance however it frees the cpu between byte transfers the direct memory access dma mode can transfer a block of data between memory and an io device without involving the cpu hence it achieves high data transfer rates and supports concurrent operation of the cpu and io devices the interrupt and dma modes permit io operations on several devices to be performed simultaneously dma operations are actually performed by the dma controller which is a specialpurpose processor dedicated to performing io operations however for simplicity we will not maintain this distinction in this chapter and refer to both simply as dma in figure the io organization employs a dma several io devices of the same class are connected to a device controller a few device controllers are connected to the dma when an io instruction is executed say a read instruction on device d the cpu transfers details of the io operation to the dma the cpu is not involved in the io operation beyond this point it part overview is free to execute instructions while the io operation is in progress the dma passes on details of the io operation to the device controller which initiates the read operation on device d the device transfers the data to the device controller transfer of data between the device controller and memory is organized by the dma thus the cpu and the io subsystem can operate concurrently at the end of the data transfer the dma generates an io interrupt as described in the next section the cpu switches to execution of the kernel when it notices the interrupt the kernel analyzes the cause of the interrupt and realizes that the io operation is complete interrupts an event is any situation that requires the operating systems attention the computer designer associates an interrupt with each event whose sole purpose is to report the occurrence of the event to the operating system and enable it to perform appropriate event handling actions it is implemented using the following arrangement in the instruction execution cycle of the cpu it performs four steps repeatedly fetching the instruction whose address is contained in the program counter pc decoding it executing it and checking whether an interrupt has occurred during its execution if an interrupt has occurred the cpu performs an interrupt action that saves the cpu state that is contents of the psw and the gprs and loads new contents into the psw and the gprs so that the cpu starts executing instructions of an interrupt servicing routine often called isr in the kernel sometime in the future the kernel can resume execution of the interrupted program simply by loading back the saved cpu state into the psw and gprs see example the computer designer associates a numeric priority with each interrupt if several interrupts occur at the same time the cpu selects the highestpriority interrupt for servicing other interrupts remain pending until they are selected classes of interrupts table describes three classes of interrupts that are important during normal operation of an os an io interrupt indicates the end of an io operation or occurrence of exceptional conditions during the io operation a timer interrupt is provided to implement a timekeeping arrangement in an operating system it is used as follows a clock tick is defined as a specific fraction of a second now an interrupt can be raised either periodically ie after a predefined number of ticks or after a programmable interval of time ie after occurrence of the number of ticks specified in a special timer register which can be loaded through a privileged instruction a program interrupt also called a trap or an exception is provided for two purposes the computer hardware uses the program interrupt to indicate occurrence of an exceptional condition during the execution of an instruction eg an overflow during arithmetic or a memory protection violation see section user programs use the program interrupt to make requests to the kernel for resources or services that they are not allowed to provide for themselves they achieve it by using a special instruction provided in the computer whose sole chapter the os the computer and user programs table classes of interrupts class description io interrupt caused by conditions like io completion and malfunctioning of io devices timer interrupt raised at fixed intervals or when a specified interval of time elapses program caused by exceptional conditions that arise during the interrupt execution of an instruction eg arithmetic exceptions like overflow addressing exceptions and memory protection violations caused by execution of a special instruction called the software interrupt instruction whose sole purpose is to cause an interrupt purpose is to raise a program interrupt so that control gets transferred to the kernel the operation code of this instruction machinespecific eg it is called int in the intel pentium trap in motorola and syscall in mips r generically we assume that a computer provides an instruction called a software interrupt instruction with the operation code si and call the interrupt raised by it a software interrupt interrupt code when an interrupt of some class occurs the hardware sets an interrupt code in the interrupt code ic field of the psw to indicate which specific interrupt within that class of interrupts has occurred this information is useful for knowing the cause of the interrupt for example if a program interrupt occurs the interrupt code would help to decide whether it was caused by an overflow condition during arithmetic or by a memory protection violation interrupt codes are machinespecific for an io interrupt the interrupt code is typically the address of the io device that caused the interrupt for a program interrupt a computer assigns distinct codes for exceptional conditions such as overflow and memory protection violation and reserves a set of interrupt codes for software interrupts typically the software interrupt instruction si instruction has a small integer as an operand it is treated as the interrupt code when the interrupt occurs if a computer does not provide an operand in the si instruction an operating system has to evolve its own arrangement eg it may require a program to push a software interrupt number on the stack before executing the si instruction to cause a software interrupt interrupt masking the interrupt mask im field of the psw indicates which interrupts are permitted to occur at the present moment of time the im field may contain an integer m to indicate that only interrupts with priority m are permitted to occur alternatively it may contain a bitencoded value where each bit in the value indicates whether a specific kind of interrupt is permitted to occur interrupts that are permitted to occur are said to be enabled and others are said part overview interrupt cpu pc m vectors psw im area ddd ic ddd saved psw ic information area step description set interrupt code the interrupt hardware forms a code describing the cause of the interrupt this code is stored in the interrupt code ic field of the psw save the psw the psw is copied into the saved psw information area in some computers this action also saves the generalpurpose registers load interrupt vector the interrupt vector corresponding to the interrupt class is accessed information from the interrupt vector is loaded into the corresponding fields of the psw this action switches the cpu to the appropriate interrupt servicing routine of the kernel figure the interrupt action to be masked or masked off if an event corresponding to a masked interrupt occurs the interrupt caused by it is not lost it remains pending until it is enabled and can occur interrupt action after executing every instruction the cpu checks for occurrence of an interrupt if an interrupt has occurred the cpu performs the interrupt action which saves the state of the cpu in memory and switches the cpu to an interrupt servicing routine in the kernel as shown in the schematic of figure the interrupt action consists of three steps step sets the interrupt code in the interrupt code ic field of the psw according to the cause of the interrupt step of the interrupt action saves contents of the psw in memory so that the kernel can form the cpu state of the interrupted program see figure which it can use to resume execution of the program at a later time the saved psw information area where the psw of the interrupted program is stored is either a reserved area in memory or an area on the stack step of the interrupt action switches the cpu to execution of the appropriate interrupt servicing routine in the kernel as follows the interrupt vectors area contains several interrupt vectors each interrupt vector is used to control interrupt servicing for one class of interrupts depending on which class an interrupt belongs to the interrupt action chooses the correct interrupt vector shared pages sharing of programs was discussed in section static sharing results from static binding performed by a linker or loader before execution of a program begins see section figure a shows the logical address space of program c the add instruction in page has its operand in page with static binding if two processes a and b statically share program c then c is included in the code of both a and b let the th page of c become page i of process a see figure a the instruction add in page of program c would be relocated to use the address i if the th page of c becomes page j in process b the add instruction would be relocated to become add j thus each page of program c has two copies in the address spaces of a and b these copies may exist in memory at the same time if processes a and b are in operation simultaneously dynamic binding see section can be used to conserve memory by binding the same copy of a program or data to several processes in this case the program or data to be shared would retain its identity see figure c it is achieved as follows the virtual memory manager maintains a shared pages table to hold information about shared pages in memory process a makes a system call to bind program c as a shared program starting at a specific page say page i in its logical address space the kernel invokes the virtual memory manager which creates entries in the page table of a for pages of program c and sets an s flag in each of these entries to indicate that it pertains to a shared page it now checks whether the pages of program c have entries in the shared pages table if not it makes such entries now sets up the swap space for program c and invokes part memory management the dynamic linker which dynamically binds program c to the code of process a during this binding it relocates the addresssensitive instructions of c thus the add instruction in page of program c is modified to read add i see figure c when a reference to an address in program c page faults the virtual memory manager finds that it is a shared page so it checks the shared pages table to check whether the required page is already in memory which would happen if another process had used it recently if so it copies the page frame number of the page from the shared pages table into the entry of that page in as page table otherwise it loads the page in memory and updates its entry in as page table and in the shared pages table similar actions are performed when process b dynamically binds program c to the start address of page i and references to cs pages in process bs instructions cause page faults figure shows the resulting arrangement two conditions should be satisfied for dynamic binding of programs to work the program to be shared should be coded as a reentrant program so that it can be invoked by many processes at the same time see section the program should also be bound to identical logical addresses in every process that page process a page process a program i add i c i page program c page program c add page process b page process b add i program j add i c j a b c figure sharing of program c by processes a and b a program c b static binding of c to the codes of processes a and b and c dynamic binding of c page table swap space page process a of a memory of c pages of i s i page frame s i program c i s id c c page table page process b of b add i pages of i s i shared pages program c i s i table s figure dynamic sharing of program c by processes a and b chapter virtual memory shared it it would ensure that an instruction like add i in page i of figure will function correctly in each of the processes these conditions are unnecessary when data rather than a program is dynamically bound to several processes however processes sharing the data would have to synchronize their accesses to the shared data to prevent race conditions when sharing of pages is implemented by making the page table entries of sharing processes point at the same page frame page reference information for shared pages will be dispersed across many page tables the page replacement algorithm will have to gather this information together to get the correct picture about references to shared pages this is rather cumbersome a better method would be to maintain information concerning shared pages in the shared pages table and collect page reference information for shared pages in entries in this table this arrangement also permits a different page replacement criterion to be used for managing shared pages in section we describe a related technique used in windows operating systems copyonwrite the copyonwrite feature is used to conserve memory when data in shared pages could be modified but the modified values are to be private to a process when processes a and b dynamically bind such data the virtual memory manager sets up the arrangement shown in figure a which is analogous to the arrangement illustrated in figure except for a copyonwrite flag in each page table entry which indicates whether the copyonwrite feature is to be employed for that page the mark c in a page table entry in figure indicates that the copyonwrite flag is set for that page if process a tries to modify page k the mmu raises a page fault on seeing that page k is a copyonwrite page the virtual memory manager now makes a private copy of page k for process a accordingly changes the page frame number stored in page ks entry in the page table of a and also turns off the copyonwrite flag in this entry figure b page table page table of a memory of a memory kk c kk c c page table page table of b of b kk c kk c c c a b figure implementing copyonwrite a before and b after process a modifies page k memorymapped files memory mapping of a file by a process binds that file to a part of the logical address space of the process this binding is performed when the process makes a memory map system call it is analogous to dynamic binding of programs and data discussed earlier in section after memory mapping a file the process refers to data in the file as if it were data located in pages of its own address space and the virtual memory manager coordinates with the file system to load pagesize parts of the file into memory on demand when the process updates the data contained in such pages the modified bits of the pages are set on but the data is not immediately written out into the file dirty pages of data are written out to the file when the page frames containing them are to be freed when the process makes a memory unmap call the virtual memory manager writes out any dirty pages that still contain the files data and deletes the file from the logical address space of the process figure shows the arrangement used for memory mapping of file info by process a note that the pagein and pageout operations on those pages of process a that do not belong to file info involve the swap space of the process and are performed by the virtual memory manager reading and writing of data from file info are performed by the file system in conjunction with the virtual memory manager if several processes memory map the same file we have an arrangement analogous to that shown in figure these processes would effectively share the memorymapped file page page table process a of a memory memoryii ii mapped swap space file of process a info file info file system figure memory mapping of file info by process a chapter virtual memory table advantages of memorymapped files advantage description file data as pages access to file data is looked upon as access to pages which is inherently more efficient because of virtual memory hardware avoids file data is a part of the process space hence the memorytomemory process does not have to copy it into a variable for copying processing fewer readwrite file data is read in or written out one page at a time operations rather than at every file operation and so a single readwrite operation may suffice for several file operations prefetching of data for sequential reads data will already be in memory if the page that contains the data was read in during a previous file operation efficient data access file data can be accessed efficiently irrespective of file organization table summarizes the advantages of memory mapping of files memorymapping makes file records accessible through the virtual memory hardware this is inherently more efficient memorytomemory copy operations are avoided as follows when a process accesses some data in a nonmemorymapped input file the file system first copies the record into a memory area used as a file buffer or disk cache see chapter the process now copies the data from the buffer or the disk cache into its own address space ie into some variables for accessing it thus one disktomemory copy operation and one memorytomemory copy operation are performed when a file is memorymapped the memorytomemory copy operation from the buffer to the process address space is not necessary since the data is already a part of the process address space similarly fewer copy operations are performed when file data is modified data located in a page that was read in or written into during a previous file operation can be accessed without disk io so memory mapping reduces the number of io operations performed during file processing the last advantage efficient access to data in a file irrespective of its organization arises from the fact that data in a memorymapped file is accessed through the virtual memory hardware hence any part of the data can be accessed equally efficiently whereas as discussed in chapter efficiency of access to the same data through file operations would depend on the manner in which the data is organized in the file memory mapping of files poses some performance problems the open and close operations on a memorymapped file incur more overhead than the open and close operations on normal files it is caused by updating of page table and tlb entries while setting up and dismantling that part of the process address space where the file is mapped the virtual memory manager also has to differentiate between memorymapped pages and other pages in an address space dirty data case studies of virtual memory using paging unix virtual memory unix has been ported on computer systems with diverse hardware designs a variety of ingenuous schemes have been devised to exploit features of the paging hardware of different host machines this section describes some features common to all unix virtual memory implementations and some interesting techniques used in different versions of unix its purpose is to provide a view of the practical issues in virtual memory implementations rather than to study the virtual memory manager of any specific unix version in detail wherever possible we are replacing the unix terminology with terminology we used in previous sections of this chapter logical address space and swap space the page table of a process differentiates among three kinds of pages resident unaccessed and swappedout pages a resident page is currently in memory an unaccessed page is one that has not been accessed even once during operation of the process and therefore has never been loaded in memory it will be loaded when a reference to it causes a page fault as described later the page exists either in a file or in the swap space depending on whether it is a text page ie it contains instructions or it is a data page a swappedout page is a page that is currently in the swap space at a page fault it is loaded back in memory from its location in the swap space an unaccessed page may be a text page or a data page a text page is loaded from an executable file existing in the file system locating such a page in the file system may require reading of several disk blocks in the inode and the file allocation table see section to avoid this overhead the virtual memory manager maintains information about text pages in a separate table and refers to it when a page needs to be loaded as described later the bsd virtual memory manager maintains this information in the page table entry itself this information gets overwritten by the page frame number when the page is loaded in memory and so it is not available if the page gets removed from memory and has to be reloaded to overcome this difficulty the virtual memory manager writes out a text page into the swap space when it is removed from memory for the first time and thereafter loads it from the swap space on demand a data page is called a zerofill page it is filled with zeroes when its first use leads to a page fault thereafter it is either a resident page or a swappedout page a text page may remain in memory even if it is marked nonresident in its page table entry this situation arises if some other process is using the page or has used it in the past when a page fault occurs for a text page the virtual memory chapter virtual memory manager first checks whether the page already exists in memory if so it simply puts the page frame information in its page table entry and marks it as resident this action avoids a pagein operation and also conserves memory to conserve disk space an effort is made to allocate as little swap space as possible to start with sufficient swap space is allocated to accommodate the user stack and the data area thereafter swap space is allocated in large chunks whenever needed this approach suffers from the problem that swap space in the system may become exhausted when the data area of a process grows the process then has to be suspended or aborted copyonwrite the semantics of fork require that the child process should obtain a copy of the parents address space these semantics can be implemented by allocating distinct memory areas and a swap space for the child process however child processes frequently discard the copy of their parents address space by loading some other program for execution through the exec call in any case a child process may not wish to modify much of the parents data hence memory and swap space can be optimized through the copyonwrite feature see section copyonwrite is implemented as follows when a process is forked the reference count of all data pages in the parents address space is incremented by and all data pages are made readonly by manipulating bits in the access privileges field of their page table entries any attempt at modifying a data page raises a protection fault the virtual memory manager finds that the reference count of the page is so it realizes that this is not a protection fault but a reference to a copyonwrite page it now reduces the count makes a copy of the page for the child process and assigns the read and write privileges to this copy by setting appropriate bits in its page table entry if the new reference count is it also enables the read and write privileges in the page table entry that had led to the page fault because the entry no longer pertains to a shared page efficient use of page table and paging hardware if a page is not present in memory the valid bit of its page table entry is off under these circumstances bits in other fields of this entry like the ref info field or the page frame field do not contain any useful information hence these bits can be used for some other purposes unix bsd uses these bits to store the address of a disk block in the file system that contains a text page the vax architecture does not provide a reference bit to collect page reference information its absence is compensated by using the valid bit in a novel manner periodically the valid bit of a page is turned off even if the page is in memory the next reference to the page causes a page fault however the virtual memory manager knows that this is not a genuine page fault and so it sets the valid bit and resumes the process in effect the valid bit is used as the reference bit page replacement the system permits a process to fix a certain fraction of its pages in memory to reduce its own page fault rate and improve its own performance these pages can not be removed from memory until they are unfixed by part memory management the process interestingly there is no io fixing of pages in unix since io operations take place between a disk block and a block in the buffer cache rather than between a disk block and the address space of a process unix page replacement is analogous to the schematic of figure including the use of a clock algorithm to facilitate fast pagein operations unix virtual memory manager maintain a list of free page frames and try to keep at least percent of total page frames on this list at all times a daemon called the pageout daemon which is labeled process in the system is created for this purpose it is activated any time the total number of free page frames falls below percent it tries to add pages to the free list and puts itself to sleep when the free list contains more than percent free page frames some versions of unix use two thresholds a high threshold and a low threshold instead of a single threshold at percent the daemon goes to sleep when it finds that the number of pages in the free list exceeds the high threshold it is activated when this number falls below the low threshold this arrangement avoids frequent activation and deactivation of the daemon the virtual memory manager divides pages that are not fixed in memory into active pages ie pages that are actively in use by a process and inactive pages ie pages that have not been referenced in the recent past the virtual memory manager maintains two lists the active list and the inactive list both lists are treated as queues a page is added to the active list when it becomes active and to the inactive list when it is deemed to have become inactive thus the least recently activated page is at the head of the active list and the oldest inactive page is at the head of the inactive list a page is moved from the inactive list to the active list when it is referenced the pageout daemon tries to maintain a certain number of pages computed as a fraction of total resident pages in the inactive list if it reaches the end of the inactive list while adding page frames to the free list it checks whether the total number of pages in the inactive list is smaller than the expected number if so it transfers a sufficient number of pages from the active list to the inactive list the pageout daemon is activated when the number of free page frames falls below the low threshold while the system is handling a page fault it frees page frames in the following order page frames containing pages of inactive processes page frames containing inactive pages of active processes and page frames containing active pages of active processes the daemon finds inactive processes if any and activates the swapper to swap them out it goes back to sleep if the number of free page frames now exceeds the high threshold if the number of free page frames after swapping out inactive processes is still below the high threshold the pageout daemon scans the inactive list and decides whether and when to add page frames occupied by inactive pages to the free list a page frame containing an inactive page is added to the free list immediately if the page is unreferenced and not dirty if the page is dirty and not already being swapped out the pageout daemon starts a pageout operation on the page and proceeds to examine the next inactive page if a page is being swapped out the daemon merely skips it the modified bit of a page is reset when its pageout operation is completed the page frame containing this page would chapter virtual memory be added to the free list in a subsequent pass if it is still inactive and the daemon finds that its pageout operation is complete the daemon activates the swapper if it can not add a sufficient number of page frames to the free list the swapper swaps out one or more active processes to free a sufficient number of page frames to optimize page traffic the virtual memory manager writes out dirty pages to the swap space in clusters when the page daemon finds a dirty page during its scan it examines adjacent pages to check if they are also dirty if so a cluster of dirty pages is written out to the disk in a single io operation another optimization concerns redundant pagein operations when a page frame fi occupied by some clean page pi is added to the free list the valid bit of pis page table entry is set to however the page is not immediately overwritten by loading another page in the page frame this happens sometime later when the pages entry comes to the head of the free list and it is allocated to some process the next reference to pi would create a page fault since the valid bit in its page table entry has been set to if pi is still in fi ie if fi is still in the free list fi can be simply taken out of the free list and pi can be reconnected to the logical address space of the process this saves a pagein operation and consequent delays to the pagefaulting process swapping the unix virtual memory manager does not use a working set memory allocator because of the high overhead of such an allocator instead it focuses on maintaining needed pages in memory a process is swapped out if all its required pages can not be maintained in memory and conditions resembling thrashing exist in the system an inactive process ie a process that is blocked for a long time may also be swapped out in order to maintain a sufficient number of free page frames when this situation arises and a swapout becomes necessary the pageout daemon activates the swapper which is always process in the system the swapper finds and swaps out inactive processes if that does not free sufficient memory it is activated again by the pageout daemon this time it swaps out the process that has been resident the longest amount of time when swapped out processes exist in the system the swapper periodically checks whether sufficient free memory exists to swap some of them back in a swapin priority which is a function of when the process was swapped out when it was last active its size and its nice value is used for this purpose see section for details of the nice value this function ensures that no process remains swapped out indefinitely in unix bsd a process was swappedin only if it could be allocated as much memory as it held when it was swapped out in unix bsd this requirement was relaxed a process is brought in if enough memory to accommodate its user structure and kernel stack can be allocated to it linux virtual memory linux uses a page size of kb on bit architectures it uses a threelevel page table see section the three levels are the page global directory the page middle directory and the page table accordingly a logical address consists part memory management of four parts three of these are for the three levels and the fourth one is the byte number within a page linux uses an interesting arrangement to eliminate pagein operations for pages that were loaded previously in memory but were marked for removal this is achieved by using the following states for page frames a free page frame is one that has not been allocated to a process while an active page frame is one that is in use by a process to which it has been allocated an inactive dirty page frame was modified by the process to which it was allocated but it is not in use by the process any more an inactive laundered page is one what was inactive dirty and is therefore being written out to the disk an inactive laundered page becomes inactive clean when its contents are copied to the disk if a process page faults for a page that is in a page frame marked inactive clean the page frame is once again allocated to the process and the page is simply marked as present in memory if the page is in a page frame marked inactive laundered these actions are performed when its disk operation completes apart from saving on disk operations this arrangement also prevents access to a stale copy of a page an inactive clean page can also be allocated to another process straightaway page replacement in linux is based on a clock algorithm the kernel tries to maintain a sufficient number of free page frames at all times so that page faults can be quickly serviced by using one of the free page frames it uses two lists called active list and inactive list and maintains the size of the active list to twothirds the size of the inactive list when the number of free page frames falls below a lower threshold it executes a loop until a few page frames are freed in this loop it examines the page frame at the end of the inactive list if its reference bit is set it resets the bit and moves the page frame to the head of the list otherwise it frees the page frame when the balance between the active and inactive lists is to be maintained it processes a few page frames from the end of the active list in a similar manner and either moves them to the head of the active list or moves them to the head of the inactive list with their reference bits on a page frame is moved from the inactive list to the active list if it is referenced by a process linux uses a buddy system allocator for allocating page frames to processes see section this method facilitates performing of io operations through older dma buses that use physical addresses because such io operations require memory to be contiguously allocated see section the logical address space of a process can consist of several virtual memory regions each region can have different characteristics and is handled by using separate policies for loading and replacement of pages a page in a zerofilled memory region is filled with zeroes at its first use a filebacked region facilitates memory mapping of files the page table entries of its pages point at the disk buffers used by the file system this way any update in a page of such a region is immediately reflected in the file and is visible to concurrent users of the file a private memory region is handled in a different manner when a new process is forked the child process is given a copy of the parents page table at this time pages of a private memory region are given a copyonwrite status when a process modifies such a page a private copy of the page is made for it chapter virtual memory virtual memory in solaris solaris provides multiple page size support whereby it uses both normal pages and superpages superpages are used automatically for processes with large address spaces other processes can request use of superpages through the memcntl system call superpages are not used for memorymapped files because a small change in a superpage requires the complete superpage to be written to the file which poses a sizable performance penalty because dirty superpages of a memorymapped file are written to the disk frequently to ensure reliability of the file see section a component of the virtual memory manager called the page scanner tries to keep a sufficient number of page frames on the cyclic page cache which is like the inactive clean list of linux so that the virtual memory manager can allocate a page frame from the cyclic page cache straightaway when a page fault occurs it selects a page for removal from memory using a twohanded clock algorithm on a global basis writes it out to the disk if it is dirty and adds its page frame to the cyclic page cache the page scanner is implemented as two kernel threads analogous to those shown in figure one thread identifies page frames for addition to the cyclic page cache while the other thread writes out dirty pages from these page frames to the disk if the page for which a process page faulted exists in a page frame included in the cyclic page cache the virtual memory manager simply removes the page frame from the cyclic page cache and attaches it to the page table of the process this arrangement saves on a pagein operation to reduce page traffic the page scanner does not put shared pages on the cyclic page cache if a sufficiently large number of processes are sharing them lotsfree is a parameter of the page scanner that indicates how many page frames should be free at any time the page scanner starts scanning pages using the twohanded clock algorithm when the number of free page frames falls below lotsfree the scan rate which is the number of pages scanned per second is varied according to the number of page frames that are actually free it is smaller when this number is close to lotsfree and it is increased as the number falls below lotsfree the spread between the two hands of the clock algorithm is calculated at boot time on the basis of the amount of memory in the system this spread and the scan rate together determine the elapsed time between the resetting of a bit by one hand of the twohanded clock algorithm and its examination by the other hand of the algorithm a smaller elapsed time implies that only most recently accessed pages will survive in memory and a larger elapsed time means that only pages that have not been accessed for a long time will be removed from memory to safeguard system performance the virtual memory manager limits the amount of cpu overhead that the page scanner can cause if the page scanner is not able to keep pace with the demand for free pages using the clock algorithm the virtual memory manager swaps out inactive processes and frees all page frames occupied by them solaris virtual memory manager has evolved into its present form through several design updates prior to solaris the page scanner maintained a free part memory management list that contained clean page frames allocated to both user processes and files the file system took pages from the free list to accommodate data read from files during periods of heavy file activity the file system effectively stole pages from address spaces of user processes which affected their performance solaris introduced the feature called priority paging which ensured that only those page frames in the free list that were allocated to file pages would be considered for allocation to data read from files this way file processing activity did not affect operation of processes however page frames were still allocated from the free list which caused high scan rates and high overhead of the page scanner solaris introduced the cyclic page cache described earlier and made the file system steal pages from itself directly so that the file processing activity does not affect scan rates and overhead of the page scanner virtual memory in windows windows operates on several architectures hence it supports both bit and bit logical addresses the page size is kb the address space of a process is either gb or gb the remainder of the logical address space is reserved for os use the kernel is mapped into this part of every processs address space on different architectures windows uses two threeor fourlevel page tables and various page table entry formats the page table of a process is itself stored in the reserved part of the logical address space of the process on an intel x architecture windows uses a twolevel page table organization similar to the one shown in figure the higherlevel page table is called a page directory pd the pd contains entries of bytes each each entry in the pd points to a page table pt each page table contains page table entries of bytes each each bit logical address is split into three components as shown below bits bits bits pd index pt index byte index during address translation the pd index field is used to locate a page table the pt index field is used to select a bit page table entry pte in the page table which contains a bit address of the page frame that contains the page the byte index is concatenated with this address to obtain the effective physical address the virtual memory manager uses the remaining bits in a page table entry to indicate how the process may access the page whether readonly or readwrite and whether the page frame allocated to it is dirty ie modified or accessed ie read from or modified if the page is not in memory the address bits would specify the offset into the paging file ie the swap space if the page contains code a copy of it would exist in a code file hence bits in the page table entry would point to a system data structure that indicates its position in the code file such a page is directly loaded from the code file so it is not copied into a paging file chapter virtual memory a page frame can be in any one of eight states some of these states are valid the page is in active use free the page is not in active use zeroed the page is cleaned out and available for immediate use standby the page has been removed from the working set of the process to which it was allocated but it could be reconnected to the process if it were referenced again modified the page is dirty and yet to be written out bad the page can not be accessed because of a hardware problem a process can not use the virtual address space available to it straightaway it must first reserve it for use and then actually commit it for accommodating specific entities like files and objects thus only some portions of the logical address space of a process may have been reserved at any time and only a part of the reserved logical address space may be in actual use an access to a page that has not been reserved and committed leads to an access violation when a thread in the process makes a system call to commit virtual memory to a region the virtual memory manager constructs a virtual address descriptor vad describing the range of logical addresses committed to it to minimize the size of the page table of a process the virtual memory manager builds it incrementally the page table entry for a committed page is created only when an access to it leads to a page fault to facilitate this operation the vads for committed portions of the logical address space are stored in an avl tree which is a balanced binary tree a section object represents a section of memory that can be shared it can be connected to a file in which case it provides memorymapped files or to memory in which case it provides shared memory a process maps a view of a section into its own address space by making a system call with parameters that indicate an offset into the section object the number of bytes to be mapped and the logical address in its address space where the object is to be mapped when the process accesses a page in the view for the first time the virtual memory manager allocates a page frame and loads it unless it is already present in memory as a result of access by another process if the memory section has the attribute based the shared memory has the same virtual address in the logical address space of each sharing process it facilitates sharing of code among processes see section a copyonwrite feature is used for sharing the pages see section it is implemented by setting the protection field of a page to read only a protection exception is raised when a process tries to modify the page the virtual memory manager now makes a private copy of the page for use by the process loading accessing and removal of shared pages is performed as follows a prototype pte is created for each shared page in an area of memory reserved for prototype ptes each process that uses the shared page has a pte for the page in its page table when the shared page does not exist in memory that is it is either not yet loaded in memory or it has been removed from memory it is marked invalid in the prototype pte and in the ptes in page tables of all sharing part memory management processes in addition the ptes in the page tables of processes are set to point to the prototype pte when the shared page is referenced by one of the sharing processes it is loaded in memory and the page frame number where it is loaded is stored in both the prototype pte and the pte of the process when another process references this page its pte is updated by simply copying the page frame number information from the prototype pte translation lookaside buffers are employed to speed up address translation in bit architectures they are managed entirely by the mmu hardware while in bit architectures they are managed by the virtual memory manager when a memory access by a thread leads to a page fault the thread is blocked until the pagein operation for the page completes several threads may pagefault for a shared page at the same time these page faults are called collided page faults the virtual memory manager ensures that all threads whose page faults collided become unblocked when the pagein operation is completed to reduce the number of page faults through page reference locality the virtual memory manager always loads a few pages preceding and following a pagefaulted page into memory while booting the system or starting an application the logical prefetcher loads a few pages into memory and monitors page faults that arise so that it could load a more effective set of pages in memory the next time the system is booted or the application is started the windows kernel uses the notion of working sets to control the amount of memory allocated to a process it defines a minimum and maximum working set size for each process these sizes are determined by the memory configuration of the system rather than by the size or nature of a process for large memory configurations the minimum and maximum working set sizes are and pages respectively at a page fault the kernel considers the amount of free memory in the system the current working set size of the process and its minimum and maximum working set sizes it allocates an additional page frame to the process if its current allocation is smaller than the maximum working set size and free memory exists otherwise it replaces one of the pages of the process in memory through a clock algorithm implemented by using the accessed bits in the page table the working set manager is activated periodically and when working sets of processes need to be adjusted if the amount of free memory has fallen below a threshold due to allocation of page frames it examines working sets whose sizes exceed the minimum working set size and removes from memory those pages that have not been used for a long time this too is performed by using a clock algorithm the virtual memory manager maintains a number of page lists a free list a list of zeroinitialized pages a modified list and a standby list when a page is to be removed from memory or when its process has terminated it would be moved to the standby list if it were a clean page otherwise it would be moved to the modified list recall that a standby page could be simply reconnected to a process that wished to use it the page writer writes out modified pages and changes their status to standby it uses two thresholds an upper threshold on the number of modified pages in the system and a lower threshold on the number of available pages to decide when pages need to be written out virtual memory using segmentation a segment is a logical entity in a program such as a function a data structure or an object or it is a module that consists of some or all of these a program is composed of segments during a programs execution the kernel treats segments as the unit for memory allocation this results in noncontiguous memory allocation for processes which provides efficient use of memory by reducing memory fragmentation being a logical entity a segment is also a convenient unit for sharing and protection this feature is useful in constructing large software systems that comprise of a set of modules or objects a logical address in a segmented process is viewed as a pair si bi where si and bi are the segment and byte ids respectively there are variations in the way si and bi are indicated in a logical address one method is to represent each of them numerically the logical address thus consists of a segment number and a byte number we shall discuss the second method separately later in this section the logical address space of a process is twodimensional in nature one dimension is defined by the set of segments in the process the number of segments can vary subject to a maximum number that may be specified by the computer architecture or the virtual memory manager the other dimension is defined by the set of bytes in a segment the number of bytes in a segment can vary subject to the maximum imposed by the number of bits available to represent bi in a logical address the twodimensional nature of the address space implies that the last byte of a segment and the first byte of another segment are not logically adjoining bytes if we add to the address of the last byte in a segment it does not spill over into the next segment it is merely an invalid address these are significant differences from paging there are also significant similarities which we now discuss in the context of address translation figure shows how address translation is performed in virtual memory using segmentation some parallels with paging are the existence of a segment table st for a process and a special hardware register called the segment table address register star that points to the segment table of a process for a memory mmu valid misc star bit addr info add si bi si bi ai ai bi segment table figure virtual memory implementation using segmentation part memory management logical address si bi address translation is performed by using the memory address found in sis entry in the segment table and the byte number bi in the segment a missing segment fault is raised if segment si does not exist in memory a difference with paging is that segments do not have a standard length hence address translation involves adding the byte number bi to the start address of si it can not be performed by using bit concatenation as in paging address translation can be speeded up by using address translation buffers an entry in the address translation buffer would contain a segment id and its address in memory which is copied from its segment table entry in a logical address si bi si and bi could also be specified in a symbolic form ie as ids in this case a logical address is of the form alpha beta where alpha is the name of a segment and beta is an id associated with a byte contained in segment alpha address translation of such logical addresses is performed as follows while compiling a segment the compiler builds a table showing byte ids defined in the segment and the byte numbers of corresponding bytes in the segment this table is made available to the virtual memory manager for use during address translation we will call it the segment linking table slt and refer to the segment linking table for alpha as sltalpha during address translation the mmu obtains the start address of alpha from the segment table picks up the address of sltalpha from the misc info field of alphas entry and obtains the byte number of beta from sltalpha and adds the two to obtain the effective memory address example effective address calculation in segmentation figure illustrates effective address calculation for the logical address alpha beta part a of the figure shows segment alpha beta and gamma are two ids associated with specific instructions or data in alpha these ids are associated with the bytes numbered and in the segment respectively the segment linking table sltalpha contains entries for beta and gamma showing their byte numbers as and respectively the segment table entry of alpha indicates that it exists in the memory area with the start address the byte number associated with beta is hence the effective address of alpha beta would be computed as both numeric and symbolic ids have been used in segmented virtual memory multics is a wellknown system that used symbolic identifiers management of memory memory management in virtual memory using segmentation has some similarities to memory management in paging a segment fault indicates that a required segment is not present in memory a segmentin operation is performed to load the segment if there is insufficient free memory some segmentout operations may have to precede loading of the required segment the virtual memory chapter virtual memory a segment b valid misc alpha name bit addr size info name offset beta beta alpha gamma gamma segment table segment linking table slt alpha figure use of symbolic segment and word ids manager can use a working set of segments to control memory allocation for a process segments could be replaced on an nru basis by collecting segment reference information in each segment entry one difference from virtual memory using paging is that segments do not have a fixed size the memory freed by removing one segment from memory may not suffice for loading another segment hence many segments may have to be removed before a new segment can be loaded differences in segment sizes can lead to external fragmentation which can be tackled either through compaction or through memory reuse techniques such as firstfit or bestfit compaction is aided by presence of the mmu only the address field of the segment table entry needs to be modified when a segment is moved in memory however the virtual memory manager should ensure that segments being moved are not involved in io operations the twodimensional nature of the logical address space permits a segment to dynamically grow or shrink in size dynamic growth can be handled by allocating a larger memory area to a segment and releasing the memory area allocated to it earlier a segment can be permitted to grow in its present location in memory if an adjoining free area exists sharing and protection two important issues in sharing and protection of segments are static and dynamic sharing of segments detecting use of invalid addresses a segment is a convenient unit for sharing because it is a logical entity in a process it can be shared statically or dynamically by using the schemes described earlier in section if segment ids are numeric segments must occupy identical positions in logical address spaces of sharing processes this requirement is analogous to that concerning shared pages in virtual memory using paging see section and figure it does not apply if segment ids are symbolic processes sharing a segment may have different access privileges to programs and data in it the virtual memory manager puts the access privileges in the misc info field of a segment table entry while translating a logical address si bi the part memory management mmu makes two kinds of protection checks it checks whether the kind of access being made to the logical address is consistent with the access privileges of the process for the segment it also checks whether si bi is a valid address by checking whether bi size of si it raises a memory protection violation interrupt if any of these checks fails segmentation with paging external fragmentation exists in a virtual memory using segmentation because segment sizes are different this problem can be addressed by superimposing paging on a segmentoriented addressing scheme a system using this approach retains the fundamental advantage of segmentation the logical address space is twodimensional which permits dynamic changes in the size of a segment while avoiding external fragmentation each segment contains an integral number of pages and memory management is performed through demand paging this arrangement may achieve more effective utilization of memory since only required pages of a segment need to be present in memory at any time however paging introduces internal fragmentation in the last page of a segment a logical address in such a system has the form si pi bi since each segment consists of a number of pages a page table is built for each segment the segment table entry of a segment points to its page table figure illustrates this arrangement the name field of the segment table is needed only if symbolic segment ids are used address translation now involves an access to the segment table followed by an access to the page table of the segment it requires two memory references if the segment and page tables are held in memory to speed up address translation address translation buffers would have to be employed for both the segment and page table references a simple extension to the scheme described earlier in section can be used for this purpose alternatively a single address translation buffer may be employed each entry in the buffer containing a pair si pi and the corresponding page frame number valid page table misc name bit addr info segment table memory mmu si pi bi valid page misc bit frame info add si pi bi ai ai bi page table figure address translation in segmentation with paging summary virtual memory is a part of the memory hierarchy used in practice because they require less memory consisting of memory and a disk during operation than the conventional page table of a process some components of its address space the virtual memory manager has to make two exist in memory while others reside on a disk this key decisions that influence the performance of a arrangement permits the total memory requireprocess which page should it remove from memments of a process to exceed size of the systems ory to make space for a new page required by a memory it also permits a larger number of proprocess and how much memory should it allocate cesses to exist in memory simultaneously because to a process it uses a page replacement algorithm to each of them occupies less memory than its own decide which page should be removed from memsize the performance of a process depends on the ory the empirical principle of locality of reference rate at which its parts have to be loaded in memindicates that a recently accessed page is more ory from the disk in this chapter we studied the likely to be accessed in future than a page that techniques used by the kernel to ensure efficient has not been recently accessed accordingly the operation of a process and good performance of least recently used lru page replacement algothe system rithm removes the page that has been least recently two basic actions in the operation of virtual used it possesses the stack property which guaranmemory using paging are address translation and tees that the page fault rate would not increase if demand loading of pages the memory management the memory allocation to a process is increased unit mmu which is a hardware unit and the virhowever it is expensive to collect information tual memory manager which is a part of the kernel about when a page was last referenced hence jointly implement these two actions the memmmus typically provide a single bit for collecting ory is divided into parts called page frames whose information about page references and a class of size matches the size of pages the virtual memory page replacement algorithms called the not recently manager maintains a page table for each process used nru algorithms are used in practice clock to indicate which of its pages exist in which page algorithms are a widely used subclass of nru frames of memory when an operand in the current algorithms instruction in a process exists in one of the pages the working set of a process is the collection that is present in memory the mmu obtains the of distinct pages referenced by it recently its size page frame number where it exists from the page provides a useful pointer to how many pages of table and uses it to compute the effective memory the process should be in memory to ensure good address of the operand if the page is not in memperformance of the process the virtual memory ory the mmu raises an interrupt called a page manager can use the notion of working sets to fault and the virtual memory manager loads the avoid the situation called thrashing in which most page in memory a fast translation lookaside buffer processes in the system have insufficient amounts tlb is used to speed up address translation it of memory allocated to them so they produce page caches some entries of page tables of processes the faults at a high rate and little useful work gets done inverted page table and the multilevel page table are in the system part memory management an operating system uses special techniques memory while memory mapping of files enables a that exploit the virtual memory to speed up operfile to be treated as a part of the address space ation of processes the copyonwrite technique of a process thereby speeding up accesses to its avoids keeping identical copies of shared pages in data test your concepts classify each of the following statements as true the reference bit in the entry of page pi of or false process pk indicates a in a computer providing virtual memory the i whether page pi is likely to be referenced number of bits in a logical address can exceed in the future the number of bits in a physical address ii whether page pi will be the next page to b a pageout operation is always needed be referenced during operation of pk in a page replacement operation irrespeciii whether page pi has been referenced since tive of whether the page being replaced is it was last loaded in memory dirty iv whether page pi is the most recently c loss of protection can result if an entry in the referenced page of pk translation lookaside buffer tlb that was b during operation of a process pk the transmade during operation of one process is used lation lookaside buffer contains during operation of another process i some arbitrary entries from the page table d the inverted page table organization requires of pk more accesses to memory during address ii the most recently referenced entries of translation than the conventional organizathe page table of pk tion of page tables iii the last few entries of the page table of e the fifo page replacement policy guaranpk tees that allocating more page frames to a iv the least recently referenced entries of the program would reduce its page fault rate page table of pk f if the virtual memory hardware provides a c the stack property of a page replacement single reference bit and the reference bits in algorithm implies that if more memory would the page table entries of all memoryresident have been allocated to a process pages are set the lru page replacement i fewer page faults would have occurred algorithm degenerates to fifo replacement ii more page faults would have occurred g page faults would not occur during operation iii the number of page faults would have of a process if all pages included in the workbeen smaller or the same ing set of a process are in memory at every iv none of iiii instant d if pfri and pfrj are the page fault rates of h heavy page traffic implies that thrashing has processes pi and pj when process pi has occurred percent of its pages in memory process pj i if a single copy of a program c is shared by has percent of its pages in memory and the two processes a and b pages of c should page replacement policy possesses the stack occupy identical positions in the page tables property then of processes a and b i pfri pfrj select the most appropriate alternative in each ii pfri pfrj of the following questions iii pfri pfrj a if the virtual memory hardware provides a iv nothing can be said about the relative single reference bit in an entry of a page table magnitudes of pfri and pfrj chapter virtual memory e if pfri and pfri are the page fault rates of proiv nothing can be said about the relative cess pi when it is operated with percent magnitudes of pfri and pfri of its pages and percent of its pages in f thrashing can be overcome if memory respectively and the page replacei the degree of multiprogramming is ment policy possesses the stack property increased then ii the io speed is increased i pfri pfri iii memory allocation for a process is conii pfri pfri trolled by its working set size iii pfri pfri iv none of iiii exercises page tables are stored in a memory that has an page replacement policy is used with alloc access time of nanoseconds the translation than when the optimal page replacement policy lookaside buffer tlb can hold page table is used with alloc entries and has an access time of nanosec a process makes r page references during its onds during operation of a process it is found operation the page reference string of the prothat percent of the time a required page table cess contains d distinct page numbers in it the entry exists in the tlb and only percent of size of the process is p pages and it is allocated f the references lead to page faults the average page frames all through its operation time for page replacement is ms compute the a what is the least number of page faults that effective memory access time can occur during its operation using the access speeds and hit ratios menb what is the maximum number of page faults tioned in exercise compute the effective that can occur during its operation memory access time in twolevel threelevel and prove the validity of the following statement if fourlevel page table organizations the page replacement policy uses a fixed memory three approaches to paging of the kernel in allocation and local page replacement if a provirtual memory are cess does not modify any of its pages then it is a make the kernel permanently memoryoptimal to replace the page whose next reference resident is farthest in the page reference string show b page the kernel in a manner analogous to the that this policy may not lead to the minimum paging of user processes number of pagein and pageout operations if c make the kernel a compulsory part of the the process modifies some of its pages logical address space of every process in the what is beladys anomaly show that a page system and manage its pages as shared pages replacement algorithm that possesses the stack which approach would you recommend give property can not exhibit beladys anomaly reasons prove that the lru page replacement policy execution performance of a process in virtual possesses the stack property memory depends on locality of reference dis optimal page replacement can be implemented played during its operation develop a set of by replacing the page whose next reference is guidelines that a programmer can follow to farthest in the page reference string does this obtain good performance of a process describe policy possess the stack property does the clock the rationale behind each guideline hint algorithm possess the stack property consider array references occurring in nested for the page reference string loops a show the working set at each time instant if give a sample page reference string for a process the size of the working set window is i three that produces more page faults when the lru instructions ii four instructions part memory management b compare the operation and performance of most recently executed instruction is said to be the working set allocator with the fifo and instruction in the past of the process the lru allocators instruction before it is said to be instruc a working set allocator is used for a page refertions in the past etc a memory allocator ence string with two values of pfr refers to the page reference in the instruction and pfr are page fault rates when and that is i instructions in the past as the i page are used respectively is pfr pfr if working reference it uses a parameter w and the folsets are recomputed a after every instruction lowing rules for memory allocation and page and b after every n instructions for some n replacement describe the actions of a virtual memory mana do nothing if the next page reference matches ager using a working set memory allocator when the w page reference it decides to reduce the degree of multiprob else if the next page reference matches the gramming clearly indicate how it uses and i page reference for some i w do the manipulates its data structures for this purpose following if the w page reference does not explain with the help of examples why the match with the j page reference for some working set size of a process may increase or j w then reduce the memory allocation for decrease during its operation the process by one page frame and remove justify the following statement thrashing can the least recently used page otherwise do arise when a working set memory allocator is nothing used however it can not last for long c else if the next page reference causes a a virtual memory manager uses the following page fault and the w page reference does page replacement policy when a combination not match with the page reference in the of a high page fault rate in the system and low j instruction for some j w then percpu efficiency is noticed reduce the allocation form a page replacement using the lru page for each process and load one more process replacement policy comment on the effectiveness of this policy d else increase the memory allocation for the explain why the twohanded clock algorithm for process by one page frame and load the page page replacement is superior to the onehanded contained in the next page reference clock algorithm see section show that the actions of the memory alloca a virtual memory manager implements a worktor are equivalent to actions of the working set ing set memory allocator and uses dynamic sharmemory allocator with w ing of pages describe the housekeeping actions compare the following memory manageperformed by it in the following situations ment proposals in virtual memory using a when a page fault occurs segmentationwithpaging b when a shared page drops out of the working a use the lru policy within a process set of one of the sharing processes b use the lru policy within a segment the amount of memory allocated to a process in comment on the validity of the following statea system using virtual memory is held constant ment in virtual memory using segmentationand the page size is varied this action varies withpaging the role of segmentation is limited the number of pages of the process in memory to sharing it does not play any role in memory draw a graph of page size versus expected page management fault rate an io operation consists of the execution of a the degree of multiprogramming in a system sequence of io commands a selfdescribing io using virtual memory is varied by changing the operation is an io operation some of whose io memory allocation for processes draw a graph commands are read in by a previous io comof degree of multiprogramming versus cpu effimand of the same io operation for example ciency explain the nature of the graph in the consider the io operation region of high degree of multiprogramming we refer to instructions in the past dur read d aaa ing operation of a process as follows the read d count bbb chapter virtual memory where d is the id of the io device the first the swap space when it is used for the first time io command reads bytes into the memory discuss the advantages and drawbacks of the area with address aaa let this be the area where optimization the fields containing count bytes and bbb performance of a virtual memory is determined bytes of the second io command are stored by the interplay of three factors cpu speed thus the first io command modifies the secsize of memory and peak throughput of the ond io command let n and ccc be the values paging device possible causes of low or high read into fields count and bbb respectively by efficiency of the cpu and the paging disk can the first io command after io for the first be summarized as follows io command is completed the second io command reads n bytes into the memory area with high low address ccc the data for this io operation utilization utilization would be n ccc cpu processes only few of the are cpuprocesses are n bytes of data bound or cpubound cpu is slow or thrashing is can the methods of performing io in virpresent tual memory described in section handle paging thrashing is memory is selfdescribing io operations correctly clearly disk present or overcommited justify your answer in a simplified form of selfdisk is slow to each process describing io the first io command reads in only bytes and stores them in the count field can the methods described in section performance of virtual memory may improve handle such io operations correctly if one or several of the following changes are while initiating a process the virtual memory made the cpu is replaced by a faster cpu manager copies the code of the process which the paging disk is replaced by a faster disk the exists in a file into the swap space reserved for memory is increased or the degree of multithe process from the swap space code pages are programming is increased in each of the folloaded into memory when needed explain the lowing situations which of the above changes advantages of this arrangement why not load would you recommend for improving system code pages directly from the file when needed performance some code pages may not be used during a run a low cpu efficiency low disk efficiency hence it is redundant to copy them into the swap b low cpu efficiency high disk efficiency space to avoid redundant copying some virc high cpu efficiency low disk efficiency tual memory managers copy a code page into d high cpu efficiency high disk efficiency class project simulation of virtual memory manager a virtual memory manager uses the twothread arrangeillustrated in figure it performs page replacement ment shown in figure where the thread called on a global basis free frames manager tries to maintain a sufficient numthe working of this virtual memory manager is to ber of free page frames at all times and the thread called be simulated the simulation is controlled by commands page io manager performs pageout operations on dirty in an input file where each command has the format page frames the virtual memory manager uses the twoaction parameters details of the actions are as handed clock algorithm discussed in example and follows part memory management action name parameters and explanation memorysize number of page frames integer lowerthreshold minimum number of free page frames integer upperthreshold maximum number of free page frames integer distance distance between clock hands in terms of number of page frames integer processes number of processes integer process ids are p p processsize process id number of pages both are integers read process id page number the indicated process reads the indicated page both are integers modify process id page number the indicated process modifies the indicated page both are integers pagetable no parameters simulator displays the page tables of processes iolist no parameters simulator displays the list of page frames on which pageout operations need to be performed hitratio simulator displays hit ratios for processes resetcounters simulator resets counters used for calculation of hit ratios develop a simulator of the virtual memory manager numbers in this list the page io manager performs the simulator must maintain page tables and swap pageout operations on the page frames in a suitable spaces of the processes it must also maintain a list of order it informs the free frames manager when the page frames on which pageout operations should be pageout operation of a page frame has been completed performed the free frames manager puts page frame bibliography randell is an early paper on the motivation for vir compares virtual memory features in mips pentual memory systems ghanem discusses memory tium and powerpc architectures swanson et al partitioning in virtual memory systems for multiproand navarro et al describe superpages gramming denning is a survey article on virtual car and hennessy discusses the clock algomemory hatfield discusses aspects of program rithm bach and vahalia describe unix performance in a virtual memory system virtual memory beck et al gorman belady discusses the anomaly that carries his bovet and cesati and love discuss linux name mattson et al discusses stack property of virtual memory mauro and mcdougall dispage replacement algorithms denning a b cusses virtual memory in solaris while russinovich and discusses thrashing and the fundamental working set solomon discusses windows virtual memory model denning is a comprehensive discussion on organick describes virtual memory in working sets smith is a bibliography on paging multics and related topics wilson et al discusses memory allocation in virtual memory environments johnstone aho a v p j denning and j d ullman and wilson discusses the memory fragmentation principles of optimal page replacement problem journal of acm chang and mergen describes the inverted bach m j the design of the unix page table while tanenbaum discusses the twooperating system prentice hall englewood level page tables used in intel jacob and mudge cliffs nj chapter virtual memory beck m h bohme m dziadzka u kunitz jacob b and t mudge virtual memory r magnus c schroter and d verworner in contemporary microprocessors ieee micro linux kernel programming rd ed magazine pearson education new york johnstone m s and p r wilson belady l a a study of replacement the memory fragmentation problem solved algorithms for virtual storage computers ibm proceedings of the first international symposium systems journal on memory management bensoussen a c t clingen and r c daley love r linux kernel development nd the multics virtual ed novell press memory concepts and design communications mauro j and r mcdougall solaris of the acm internals nd ed prenticehall englewood bryant p predicting working set sizes cliffs nj ibm journal of r and d mattson r l j gecsei d r slutz and bovet d p and m cesati understanding i l traiger evaluation techniques for the linux kernel rd ed oreilly sebastopol storage hierarchies ibm systems journal calif carr w r and j l hennessy navarro j s iyer p druschel and a cox wsclock a simple and effective algorithm for practical transparent operating system virtual memory management proceedings of the support for superpages acm sigops acm symposium on operating systems operating systems review issue si principles organick e i the multics system chang a and m mergen storage mit press cambridge mass architecture and programming acm randell b a note on storage transactions on computer systems fragmentation and program segmentation daley r c and j b dennis virtual communications of the acm memory processes and sharing in multics rosell j r and j p dupuy the design communications of the acm implementation and evaluation of a working set denning p j a the working set model dispatcher communications of the acm for program behavior communications of the acm russinovich m e and d a solomon denning p j b thrashing its causes microsoft windows internals th ed microsoft and prevention proceedings of afips fjcc press redmond wash smith a j bibliography on paging and denning p j virtual memory related topics operating systems review computing surveys denning p j working sets past and swanson m l stoller and j carter present ieee transactions on software increasing tlb reach using superpages backed engineering by shadow memory proceedings of the th ghanem m z study of memory international symposium on computer partitioning for multiprogramming systems with architecture virtual memory ibm journal of r and d tanenbaum a s modern operating systems nd ed prentice hall englewood gorman m understanding the linux cliffs nj virtual memory manager prentice hall vahalia u unix internals the new englewood cliffs nj frontiers prentice hall englewood cliffs nj guertin rl programming in a paging wilson p r m s johnstone m neely and environment datamation d boles dynamic storage allocation a hatfield d j and j gerald program survey and critical review proceedings of the restructuring for virtual memory ibm systems international workshop on memory management journal part overview of file processing we use the term file processing to describe the general sequence of operations of opening a file reading data from the file or writing data into it and closing the file figure shows the arrangement through which an os implements file processing activities of processes each directory contains entries describing some files the directory entry of a file indicates the name of its owner its location on a disk the way its data is organized and which users may access it in what manner the code of a process pi is shown in the left part of figure when it opens a file for processing the file system locates the file through the directory part file systems and io management directory structure directory file system process file pi beta beta phi open beta logical read beta view of file data close beta file data in memory iocs file data on disk figure file system and the iocs structure which is an arrangement of many directories in figure there are two files named beta located in different directories when process pi opens beta the manner in which it names beta the directory structure and identities of the user who initiated process pi will together determine which of the two files will be accessed a file system provides several file types see section each file type provides its own abstract view of data in a file we call it a logical view of data figure shows that file beta opened by process pi has a recordoriented logical view while file phi has a byte streamoriented logical view in which distinct records do not exist the iocs organizes a files data on an io device in accordance with its file type it is the physical view of the files data the mapping between the logical view of the files data and its physical view is performed by the iocs the iocs also provides an arrangement that speeds up a file processing activity it holds some data from a file in memory areas organized as buffers a file cache or a disk cache when a process performs a read operation to get some data from a file the iocs takes the data from a buffer or a cache if it is present there this way the process does not have to wait until the data is read off the io device that holds the file analogously when a process performs a write operation on a file the iocs copies the data to be written in a buffer or in a cache the actual io chapter file systems operations to read data from an io device into a buffer or a cache or to write it from there onto an io device are performed by the iocs in the background file system and the iocs a file system views a file as a collection of data that is owned by a user can be shared by a set of authorized users and has to be reliably stored over an extended period of time a file system gives users freedom in naming their files as an aspect of ownership so that a user can give a desired name to a file without worrying whether it conflicts with names of other users files and it provides privacy by protecting against interference by other users the iocs on the other hand views a file as a repository of data that need to be accessed speedily and are stored on an io device that needs to be used efficiently table summarizes the facilities provided by the file system and the iocs the file system provides directory structures that enable users to organize their data into logical groups of files eg one group of files for each professional activity the file system provides protection against illegal file accesses and ensures correct operation when processes access and update a file concurrently it also ensures that data is reliably stored ie data is not lost when system crashes occur facilities of the iocs are as described earlier the file system and the iocs form a hierarchy each of them has policies and provides mechanisms to implement the policies in the language of section the iocs and the file system provide different abstractions that lead to the following division of functions the file system provides an interface through which a process can perform open readwrite and close operations on files its policy modules handle protection and sharing of files during open and readwrite operations its mechanism modules assist in the implementation of open and close operations by accessing directories they also pass on readwrite requests for file data to the iocs the iocs policy modules ensure efficient operation of io devices and efficient file processing in each process through the iocs mechanism modules the mechanism modules in the iocs in turn invoke the kernel through system calls to initiate io operations table facilities provided by the file system and the inputoutput control system file system directory structures for convenient grouping of files protection of files against illegal accesses file sharing semantics for concurrent accesses to a file reliable storage of files inputoutput control system iocs efficient operation of io devices efficient access to data in a file part file systems and io management data and metadata a file system houses two kinds of data data contained within files and data used to access files we call the data within files file data or simply data the data used to access files is called control data or metadata in the logical view shown in figure data contained in the directory structure is metadata as discussed later in this chapter and in chapter other metadata play a role in implementing file operations file processing in a program at the programming language level a file is an object that possesses attributes describing the organization of its data and the method of accessing the data a program contains a declaration statement for a file which specifies values of its attributes and statements that open it perform readwrite operations on it and close it we call them file processing statements during execution of the program file processing is actually implemented by library modules of the file system and the iocs figure illustrates how file processing is actually implemented the program of figure a declares alpha as a sequentialaccess file that contains records with a size of bytes see section for a discussion of records in a file it also contains statements to open alpha and read a record from it the compiler of the programming language processes the file declaration statement in the program and determines attributes of the file it now replaces open close read and write statements with calls on file system library modules open close read and write and passes the file attributes as parameters to the open call see figure b the file system modules invoke modules of the iocs to actually perform io operations the linker links the file system library a b c file alpha sequential record open alpha call openalpha call read read call read alpha call readalpha xyz xyz file system modules open close iocs module seqread figure implementing a file processing activity a program containing file declaration statements b compiled program showing calls on file system modules c process invoking file system and iocs modules during operation files and file operations file types a file system houses and organizes different types of files eg data files executable programs object modules textual information documents spreadsheets photos and video clips each of these file types has its own format for recording the data these file types can be grouped into two classes structured files byte stream files a structured file is a collection of records where a record is a meaningful unit for processing of data a record is a collection of fields and a field contains a single data item each record in a file is assumed to contain a key field the value in the key field of a record is unique in a file ie no two records contain an identical key many file types mentioned earlier are structured files file types used by standard system software like compilers and linkers have a structure determined by the os designer while file types of user files depend on the applications or programs that create them a byte stream file is flat there are no records and fields in it it is looked upon as a sequence of bytes by the processes that use it the next example illustrates structured and byte stream files structured and byte stream files example figure a shows a structured file named employeeinfo each record in the file contains information about one employee a record contains four fields employee id name designation and age the field containing the employee id is the key field figure b shows a byte stream file report file attributes a file attribute is a characteristic of a file that is important either to its users or to the file system or both commonly used attributes of a file are type organization size location on disk access control information which indicates the manner in which different users can access the file owner name time of creation and time of last use the file system stores the attributes of a file in its directory entry during a file processing activity the file system uses the attributes of a file to locate it and to ensure that each operation being performed on it is consistent with its attributes at the end of the file processing activity the file system stores changed values of the files attributes if any in the files directory entry fundamental file organizations and access methods we use the term record access pattern to describe the order in which records in a file are accessed by a process the two fundamental record access patterns are sequential access in which records are accessed in the order in which they chapter file systems fall in a file or in the reverse of that order and random access in which records may be accessed in any order the file processing actions of a process will execute efficiently only if the processs record access pattern can be implemented efficiently in the file system the characteristics of an io device make it suitable for a specific record access pattern for example a tape drive can access only the record that is placed immediately before or after the current position of its readwrite head hence it is suitable for sequential access to records a disk drive can directly access any record given its address hence it can efficiently implement both the sequential and random record access patterns a file organization is a combination of two features a method of arranging records in a file and a procedure for accessing them a file organization is designed to exploit the characteristics of an io device for providing efficient record access for a specific record access pattern a file system supports several file organizations so that a process can employ the one that best suits its file processing requirements and the io device in use this section describes three fundamental file organizations sequential file organization direct file organization and index sequential file organization other file organizations used in practice are either variants of these fundamental ones or are specialpurpose organizations that exploit less commonly used io devices accesses to files governed by a specific file organization are implemented by an iocs module called an access method an access method is a policy module of the iocs while compiling a program the compiler infers the file organization governing a file from the files declaration statement or from the rules for default if the program does not contain a file declaration statement and identifies the correct access method to invoke for operations on the file we describe the functions of access methods after discussing the fundamental file organizations sequential file organization in sequential file organization records are stored in an ascending or descending sequence according to the key field the record access pattern of an application is expected to follow suit hence sequential file organization supports two kinds of operations read the next or previous record and skip the next or previous record a sequentialaccess file is used in an application if its data can be conveniently presorted into an ascending or descending order the sequential file organization is also used for byte stream files direct file organization the direct file organization provides convenience and efficiency of file processing when records are accessed in a random order to access a record a readwrite command needs to mention the value in its key field we refer to such files as directaccess files a directaccess file is implemented as follows when a process provides the key value of a record to be accessed the access method module for the direct file organization applies a transformation to the key value that generates the address of the record in the storage medium if the file is organized on a disk part file systems and io management the transformation generates a trackno recordno address the disk heads are now positioned on the track trackno before a read or write command is issued on the record recordno consider a file of employee information organized as a directaccess file let p records be written on one track of the disk assuming the employee numbers and the track and record numbers of the file to start from the address of the record for employee number n is track number tn record number rn where tn n p rn n tn p and indicates a roundedup integer value direct file organization provides access efficiency when records are processed randomly however it has three drawbacks compared to sequential file organization record address calculation consumes cpu time disks can store much more data along the outermost track than along the innermost track however the direct file organization stores an equal amount of data along each track hence some recording capacity is wasted the address calculation formulas and work correctly only if a record exists for every possible value of the key so dummy records have to exist for keys that are not in use this requirement leads to poor utilization of the io medium hence sequential processing of records in a directaccess file is less efficient than processing of records in a sequentialaccess file another practical problem is that characteristics of an io device are explicitly assumed and used by the address calculation formulas and which makes the file organization devicedependent rewriting the file on another device with different characteristics eg different track capacity will imply modifying the address calculation formulas this requirement affects the portability of programs example sequential and directaccess files figure shows the arrangement of employee records in sequential and direct file organizations employees with the employee numbers and have left the organization however the directaccess file needs to contain a record for each of these employees to satisfy the address calculation formulas and this fact leads to the need for dummy records in the directaccess file index sequential file organization an index helps to determine the location of a record from its key value in a pure indexed file organization the index of a file contains an index entry with chapter file systems employee dummy records a b figure records in a sequential file b directaccess file the format key value disk address for each key value existing in the file to access a record with key k the index entry containing k is found by searching the index and the disk address mentioned in the entry is used to access the record if an index is smaller than a file this arrangement provides high access efficiency because a search in the index is more efficient than a search in the file the index sequential file organization is a hybrid organization that combines elements of the indexed and the sequential file organizations to locate a desired record the access method module for this organization searches an index to identify a section of the disk that may contain the record and searches the records in this section of the disk sequentially to find the record the search succeeds if the record is present in the file otherwise it results in a failure this arrangement requires a much smaller index than does a pure indexed file because the index contains entries for only some of the key values it also provides better access efficiency than the sequential file organization while ensuring comparably efficient use of io media for a large file the index would still contain a large number of entries and so the time required to search through the index would be large a higherlevel index can be used to reduce the search time an entry in the higherlevel index points to a section of the index this section of the index is searched to find the section of the disk that may contain a desired record and this section of the disk is searched sequentially for the desired record the next example illustrates this arrangement index sequential file organization example figure illustrates a file of employee information organized as an index sequential file records are stored in ascending order by the key field two indexes are built to facilitate speedy search the track index indicates the smallest and largest key value located on each track see the fields named low and high in figure the higherlevel index contains entries for groups of tracks containing tracks each to locate the record with a key k first the higherlevel index is searched to locate the group of tracks that may contain the desired record the track index for the tracks of the group is now searched to locate the track that may contain the desired record and the selected track is searched sequentially for the record with key k the search ends unsuccessfully if it fails to find the record on the track directories a directory contains information about a group of files each entry in a directory contains the attributes of one file such as its type organization size location and the manner in which it may be accessed by various users in the system figure chapter file systems file type and location protection open misc name size info info count lock flags info field description file name name of the file if this field has a fixed size long file names beyond a certain length will be truncated type and size the files type and size in many file systems the type of file is implicit in its extension eg a file with extension c is a byte stream file containing a c program and a file with extension obj is an object program file which is often a structured file location info information about the files location on a disk this information is typically in the form of a table or a linked list containing addresses of disk blocks allocated to a file protection info information about which users are permitted to access this file and in what manner open count number of processes currently accessing the file lock indicates whether a process is currently accessing the file in an exclusive manner flags information about the nature of the file whether the file is a directory a link or a mounted file system misc info miscellaneous information like id of owner date and time of creation last use and last modification figure fields in a typical directory entry shows the fields of a typical directory entry the open count and lock fields are used when several processes open a file concurrently the open count indicates the number of such processes as long as this count is nonzero the file system keeps some of the metadata concerning the file in memory to speed up accesses to the data in the file the lock field is used when a process desires exclusive access to a file the flags field is used to differentiate between different kinds of directory entries we put the value d in this field to indicate that a file is a directory l to indicate that it is a link and m to indicate that it is a mounted file system later sections in this chapter will describe these uses the misc info field contains information such as the files owner its time of creation and last modification a file system houses files owned by several users therefore it needs to grant users two important prerogatives file naming freedom a users ability to give any desired name to a file without being constrained by file names chosen by other users file sharing a users ability to access files created by other users and ability to permit other users to access his files part file systems and io management master directory user directories a b c uds beta alpha gamma beta calendar figure a directory structure composed of master and user directories the file system creates several directories and uses a directory structure to organize them for providing file naming freedom and file sharing we include schematic diagrams to illustrate directory structures using the convention that a directory is represented by a rectangle while a file is represented by a circle figure shows a simple directory structure containing two kinds of directories a user directory ud contains entries describing the files owned by one user the master directory contains information about the uds of all registered users of the system each entry in the master directory is an ordered pair consisting of a user id and a pointer to a ud in the file system shown users a and b have each created their own file named beta these files have entries in the users respective uds we describe the directory structure shown in figure as a twolevel directory structure use of separate uds is what provides naming freedom when a process created by user a executes the statement open beta the file system searches the master directory to locate as ud and searches for beta in it if the call openbeta had instead been executed by some process created by b the file system would have searched bs ud for beta this arrangement ensures that the correct file is accessed even if many files with identical names exist in the system use of uds has one drawback however it inhibits users from sharing their files with other users a special syntax may have to be provided to enable a user to refer to another users file for example a process created by user c may execute the statement open abeta to open as file beta the file system can implement this simply by using as ud rather than cs ud to search and locate file beta to implement file protection the file system must determine whether user c is permitted to open as file beta it checks the protection info field of betas directory entry for this purpose details of file protection are discussed in section directory trees the multics file system of the s contained features that allowed the user to create a new directory give it a name of his choice and create files and other directories in it up to any desired level the resulting directory structure is a tree chapter file systems root x a b admin projects alpha realtime beta mainpgm figure directory trees of the file system and of user a we call it the directory tree after multics most file systems have provided directory trees a user can create a file to hold data or to act as a directory when a distinction between the two is important we will call these files respectively data files and directory files or simply directories the file system provides a directory called root that contains the home directory for each user which is a directory file that typically has the same name as the users name a user structures his information by creating directory files and data files in his home directory creating files and other directories in a directory file and so on we will assume that the file system puts a d in the flags field of a files entry if the file is a directory file figure shows the directory tree of the file system the root of this tree is the directory root which contains a home directory for each user that bears the users name user a has created a file called alpha and directories called admin and projects the projects directory contains a directory realtime which contains a file mainpgm thus user a has a directory tree of his own its root is his home directory at any time a user is said to be in some specific directory which is called his current directory when the user wishes to open a file the file name is searched for in this directory whenever the user logs in the os puts him in his home directory the home directory is then the users current directory a user can change his current directory at any time through a change directory command a files name may not be unique in the file system so a user or a process uses a path name to identify it in an unambiguous manner a path name is a sequence of one or more path components separated by a slash where each path component is a reference through a directory and the last path component is the name of the file path names for locating a file from the current directory are called relative path names relative path names are often short and convenient to use however they can be confusing because a file may have different relative path names when accessed from different current directories for example in figure the part file systems and io management file alpha has the simple relative path name alpha when accessed from current directory a whereas it has relative path names of the form alpha and alpha when accessed from the directories projects and realtime respectively to facilitate use of relative path names each directory stores information about its own parent directory in the directory structure the absolute path name of a file starts on the root directory of the file systems directory tree identically named files created in different directories differ in their absolute path names we will use the convention that the first path component in an absolute path is a null symbol and the home directory of a user a is specified asa thus in figure the absolute path name of file alpha is aalpha an alternative path name for it is aalpha directory graphs in a directory tree each file except the root directory has exactly one parent directory this directory structure provides total separation of different users files and complete file naming freedom however it makes file sharing rather cumbersome a user wishing to access another users files has to use a path name that involves two or more directories for example in figure user b can access file beta using the path name aprojectsbeta or aprojectsbeta use of the tree structure leads to a fundamental asymmetry in the way different users can access a shared file the file will be located in some directory belonging to one of the users who can access it with a shorter path name than can other users this problem can be solved by organizing the directories in an acyclic graph structure in this structure a file can have many parent directories and so a shared file can be pointed to by directories of all users who have access to it acyclic graph structures are implemented through links links a link is a directed connection between two existing files in the directory structure it can be written as a triple from filename to filename linkname where from filename is a directory and to filename can be a directory or a file once a link is established to filename can be accessed as if it were a file named linkname in the directory from filename the fact that linkname is a link in the directory from filename is indicated by putting the value l in its flags field example illustrates how a link is set up example link in a directory structure figure shows the directory structure after user c creates a link using the command c csoftwarewebserver quest the name of the link is quest the link is made in the directory c and it points to the file csoftwarewebserver this link permits csoftwarewebserver to be accessed by the name cquest chapter file systems c quest personal job software web server figure a link in the directory structure an unlink command nullifies a link implementation of the link and unlink commands involves manipulation of directories that contain the files from filename and to filename deadlocks may arise while link and unlink commands are implemented if several processes issue these commands simultaneously the file system can use some simple policy to ensure absence of deadlocks see section operations on directories a search is the most frequent operation on directories other operations on directories are maintenance operations like creating or deleting files updating file entries when a process performs a close operation listing a directory and deleting a directory the deletion operation specifies a path name for the file to be deleted it becomes complicated when the directory structure is a graph because a file may have multiple parents a file is deleted only if it has a single parent otherwise it is simply made inaccessible from its parent directory in the path name specified in the delete command to simplify the delete operation the file system maintains a link count with each file the count is set to when the file is created incremented by whenever a link is set to point to it and decremented by at a delete command the file can be deleted only when its link count becomes this simple strategy is not adequate if the directory structure contains cycles a cycle develops when a link is set from a directory to one of its ancestor directories eg if a link is set up from the directory realtime in figure to the directory projects now the link count of projects is so its deletion by using the path name aprojects would lead only to deletion of the entry of projects in a however there is no reason to retain directory projects and files reachable from it since projects would not be accessible from the home directory of any user this problem can be solved either by using a technique to detect cycles that are not reachable from any home directories which can be expensive or by preventing cycles from arising in the directory structure which is equally expensive organization of directories a directory could be a flat file that is searched linearly to find the required file entry however this organization is inefficient if the directory contains a large part file systems and io management number of entries hash tables and b trees are used to provide greater search efficiency hash table directory a hash table using the hash with chaining organization was discussed in section in connection with inverted page tables a directory can be maintained by using a simpler hash table organization called hash with open addressing that requires a single table when a new file is to be created in a directory a hashing function h is applied to a bit string obtained from the files name which yields an entry number e if the eth entry in the directory is already occupied by another file the entry given by e modn where n is the size of the hash table is checked and so on until an unused entry is found and the new files details are entered in it when a file is to be opened a similar search is carried out to locate its entry in the directory hash table organizations that do not require more than two comparisons to locate a required file name are practical so a hash table directory can be searched efficiently however use of a hash table directory organization has a few drawbacks it is cumbersome to change the size of a directory or to delete an entry from it b tree directory a b tree is an mway search tree where m d d being an integer called the order of the tree the b tree is a balanced tree ie the length of the path from the root to any leaf node is the same this property has a useful implication for directory search it takes approximately the same amount of time to find the information concerning any file name existing in the directory a b tree directory is organized as follows information about files is recorded only in leaf nodes of the tree nonleaf nodes are used merely to direct search to appropriate parts of the tree the nonleaf nodes of the tree contain index entries where each index entry is an ordered pair consisting of a pointer to another node in the tree and a file name the last index entry in a node does not contain a file name it contains only a pointer to another node in the tree the leaf nodes of the tree contain only information entries for files each entry is an ordered pair consisting of a pointer to information associated with a file name and the file name itself the root node contains between and d entries both inclusive where d is the order of the tree a nonroot node contains between d and d entries both inclusive to facilitate search for a file name the entries in a node whether index entries or information entries are lexicographically ordered on file names thus a file name in an entry is larger than the file name in the preceding entry in the node and smaller than the file name in the following entry in the node a leaf node contains two extra pointers these pointers point to tree nodes that are to its left and to its right in the tree if any respectively these pointers are used to facilitate insertion and deletion of entries we do not discuss their use here to locate a file in a directory the directory b tree is searched starting with its root node the files name is compared with the file name in the first index entry in the node if it is lexicographically smaller than the file name in the entry the pointer in the index entry is used to locate another tree node where the search is continued otherwise the search is continued with the next index entry in the node if any and so on if the next index entry is the last index entry in the mounting of file systems there can be many file systems in an operating system each file system is constituted on a logical disk ie on a partition of a disk files contained in a file part file systems and io management system can be accessed only when the file system is mounted the mount operation is what connects the file system to the systems directory structure an unmount operation disconnects a file system the mount and unmount operations are performed by the system administrator these operations provide an element of protection to files in a file system mounting creates an effect analogous to that provided by a link the difference is that mounting does not permanently alter the directory structure its effect lasts only until the file system is unmounted or until the system is booted again mounting of file systems is useful when there are multiple file systems in the os see section or when a user of a distributed system wishes to access files located in a remote machine see chapter a mount point is a directory in which a file system can be mounted a mount operation is performed by issuing the command mount fsname mountpointname where fsname and mountpointname both of which are path names designate the root of the file system to be mounted and the mount point respectively when the mount operation is performed the root of the mounted file system assumes the name mountpointname thus any file with the relative path name api in the directory fsname can be accessed by the path name mountpointnameapi if a file system is mounted in a directory that already contains some files these files become invisible to the user until the file system is unmounted the next example illustrates the effect of executing a mount command example mounting of a file system in figure a aadmin is a mount point in a directory structure and meeting is the root directory of another file system figure b shows the effect of the command mount meetingaadmin file items can now be accessed as aadminagendaitems a meeting a admin agenda admin time agenda items time items a b figure directory structures a before a mount command b after a mount command file protection a user would like to share a file with collaborators but not with others we call this requirement controlled sharing of files to implement it the owner of a file specifies which users can access the file in what manner the file system stores this information in the protection info field of the files directory entry see figure and uses it to control access to the file different methods of structuring the protection information of files are discussed in chapter in this section we assume that a files protection information is stored in the form of an access control list each element of the access control list is an access control pair of the form username listofaccessprivileges when a process executed by some user x tries to perform an operation opn on file alpha the file system searches for the pair with username x in the access control list of alpha and checks whether opn is consistent with the listofaccessprivileges if it is not the attempt to access alpha fails for example a write attempt by x will fail if the entry for user x in the access control list is x read or if the list does not contain an entry for x the size of a files access control list depends on the number of users and the number of access privileges defined in the system to reduce the size of protection information users can be classified in some convenient manner and an access control pair can be specified for each user class rather than for each individual user now an access control list has only as many pairs as the number of user classes for example unix specifies access privileges for three classes of users the file owner users in the same group as the owner and all other users of the system in most file systems access privileges are of three kinds read write and execute a write privilege permits existing data in the file to be modified and also permits new data to be added one can further differentiate between these two privileges by defining a new access privilege called append however it would increase the size of the protection information the execute privilege permits a user to execute the program contained in a file access privileges have different meanings for directory files the read privilege for a directory file implies that one can obtain a listing of the directory while the write privilege for a directory implies that one can create new files in the directory the execute privilege for a directory permits an access to be made through it that is it permits a file existing in the directory to be accessed a user can use the execute privilege of directories to make a part of his directory structure visible to other users allocation of disk space as mentioned in section a disk may contain many file systems each in its own partition of the disk the file system knows which partition a file belongs to but the iocs does not hence disk space allocation is performed by the file system early file systems adapted the contiguous memory allocation model see section by allocating a single contiguous disk area to a file when it was created this model was simple to implement it also provided data access efficiency by reducing disk head movement during sequential access to data in a file however contiguous allocation of disk space led to external fragmentation interestingly it also suffered from internal fragmentation because the file system found it prudent to allocate some extra disk space to allow for expansion of a file contiguity of disk space also necessitated complicated arrangements to avoid use of bad disk blocks the file system identified bad disk blocks while formatting the disk and noted their addresses it then allocated substitute disk blocks for the bad ones and built a table showing addresses of bad blocks and their substitutes during a readwrite operation the iocs checked whether the disk block to be accessed was a bad block if it was it obtained the address of the substitute disk block and accessed it modern file systems adapt the noncontiguous memory allocation model see section to disk space allocation in this approach a chunk of disk space is allocated on demand ie when the file is created or when its size grows because of an update operation the file system has to address three issues for implementing this approach managing free disk space keep track of free disk space and allocate from it when a file requires a new disk block avoiding excessive disk head movement ensure that data in a file is not dispersed to different parts of a disk as it would cause excessive movement of the disk heads during file processing accessing file data maintain information about the disk space allocated to a file and use it to find the disk block that contains required data the file system can maintain a free list of disk space and allocate from it when a file requires a new disk block alternatively it can use a table called the disk status map dsm to indicate the status of disk blocks the dsm has one entry for each disk block which indicates whether the disk block is free or has been allocated to a file this information can be maintained in a single bit and so a dsm is also called a bit map figure illustrates a dsm a in an entry indicates that the corresponding disk block is allocated the dsm is consulted every time a new disk block has to be allocated to a file to avoid dispersing file data to different parts of a disk file systems confine the disk space allocation for a file either to consecutive disk blocks which form an extent also called a cluster or consecutive cylinders in a disk which form cylinder groups see section use of a disk status map rather than a free chapter file systems disk block address didi disk block status disk block is allocated disk block is free figure disk status map dsm file location name info free list pointer alpha data beta metadata directory figure linked allocation of disk space list has the advantage that it allows the file system to readily pick disk blocks from an extent or cylinder group we discuss two fundamental approaches to noncontiguous disk space allocation they differ in the manner they maintain information about disk space allocated to a file linked allocation a file is represented by a linked list of disk blocks each disk block has two fields in it data and metadata the data field contains the data written into the file while the metadata field is the link field which contains the address of the next disk block allocated to the file figure illustrates linked allocation the location info field of the directory entry of file alpha points to the first disk block of the file other blocks are accessed by following the pointers in the list of disk blocks the last disk block contains null information in its metadata field thus file alpha consists of disk blocks and while file beta consists of blocks and free space on the disk is represented by a free list in which each free disk block contains a pointer to the next free disk block when a disk block is needed to store new data added to a file a disk block is taken off the free list and added to the files list of disk blocks to delete a file the files list of disk blocks is simply added to the free list linked allocation is simple to implement and incurs a low allocation deallocation overhead it also supports sequential files quite efficiently however files with nonsequential organization can not be accessed efficiently reliability is also poor since corruption of the metadata field in a disk block may lead to part file systems and io management file location name info free alpha end beta free end free directory file allocation table figure file allocation table fat loss of data in the entire file similarly operation of the file system may be disrupted if a pointer in the free list is corrupted we discuss these reliability issues in section file allocation table fat msdos uses a variant of linked allocation that stores the metadata separately from the file data a file allocation table fat of a disk is an array that has one element corresponding to every disk block in the disk for a disk block that is allocated to a file the corresponding fat element contains the address of the next disk block thus the disk block and its fat element together form a pair that contains the same information as the disk block in a classical linked allocation scheme the directory entry of a file contains the address of its first disk block the fat element corresponding to this disk block contains the address of the second disk block and so on the fat element corresponding to the last disk block contains a special code to indicate that the file ends on that disk block figure illustrates the fat for the disk of figure the file alpha consists of disk blocks and hence the directory entry of alpha contains the fat entry for disk block contains and the fat entry for disk block indicates that the file ends on that disk block the file beta consists of blocks and the fat can also be used to store free space information the list of free disk blocks can be stored as if it were a file and the address of the first free disk block can be held in a free list pointer alternatively some special code can be stored in the fat element corresponding to a free disk block eg the code free in figure use of the fat rather than the classical linked allocation involves a performance penalty since the fat has to be accessed to obtain the address of the next disk block to overcome this problem the fat is held in memory during file processing use of the fat provides higher reliability than classical linked allocation because corruption of a disk block containing file data leads to limited damage however corruption of a disk block used to store the fat is disastrous indexed allocation in indexed allocation an index called the file map table fmt is maintained to note the addresses of disk blocks allocated to a file in its simplest form an chapter file systems file location name info alpha beta fmtalpha directory fmtbeta figure indexed allocation of disk space fmt can be an array containing disk block addresses each disk block contains a single field the data field the location info field of a files directory entry points to the fmt for the file see figure in the following discussion we use the notation fmtalpha for the fmt of the file alpha if the size of the file alpha grows the dsm is searched to locate a free block and the address of the block is added to fmtalpha deallocation is performed when alpha is deleted all disk blocks pointed to by fmtalpha are marked free before fmtalpha and the directory entry of alpha are erased the reliability problem is less severe in indexed allocation than in linked allocation because corruption of an entry in an fmt leads to only limited damage compared with linked allocation access to sequentialaccess files is less efficient because the fmt of a file has to be accessed to obtain the address of the next disk block however access to records in a directaccess file is more efficient since the address of the disk block that contains a specific record can be obtained directly from the fmt for example if address calculation analogous to shows that a required record exists in the ith disk block of a file its address can be obtained from the ith entry of the fmt for a small file the fmt can be stored in the directory entry of the file it is both convenient and efficient for a medium or large file the fmt will not fit into the directory entry a twolevel indexed allocation depicted in figure may be used for such fmts in this organization each entry of the fmt contains the address of an index block an index block does not contain data it contains entries that contain addresses of data blocks to access the data block we first access an entry of the fmt and obtain the address of an index block we then access an entry of the index block to obtain the address of the data block this arrangement resembles a multilevel page table see section the index blocks resemble pages of a page table for the file and the fmt resembles a higherlevel page table such an fmt is compact hence even fmts of large files may fit into a directory entry however access to data blocks is slower since two levels of indirection are involved some file systems use a hybrid fmt organization that includes some of the features of both classical and multilevel indexed allocation figure shows such an organization the first few entries in the fmt say n entries point to part file systems and io management data blocks fmt index blocks figure a twolevel fmt organization data blocks n m data blocks fmt index blocks figure a hybrid organization of fmt data blocks as in the conventional indexed allocation other entries point to index blocks the advantage of this arrangement is that small files containing n or fewer data blocks continue to be accessible efficiently as their fmt does not use index blocks medium and large files suffer a marginal degradation of their access performance because of multiple levels of indirection the unix file system uses a variation of the hybrid fmt organization performance issues two performance issues are associated with the use of a disk block as the unit of disk space allocation size of the metadata ie the control data of the file system and efficiency of accessing file data both issues can be addressed by using a larger unit of allocation of disk space hence modern file systems tend to use an extent also called a cluster as a unit of disk space allocation an extent is a set of consecutive disk blocks use of large extents provides better access efficiency however it causes more internal fragmentation to get the best of both worlds file systems prefer to use variable extent sizes their metadata contains the size of an extent along with its address interface between file system and iocs the file system uses the iocs to perform io operations and the iocs implements them through kernel calls the interface between the file system and the iocs consists of three data structures the file map table fmt the file control block fcb and the open files table oft and functions that perform io operations use of these data structures avoids repeated processing of file attributes by the file system and provides a convenient method of tracking the status of ongoing file processing activities as discussed earlier in section the file system allocates disk space to a file and stores information about the allocated disk space in the file map table fmt the fmt is typically held in memory during the processing of a file a file control block fcb contains all information concerning an ongoing file processing activity this information can be classified into the three categories shown in table information in the file organization category is either simply extracted from the file declaration statement in an application program or inferred from it by the compiler eg information such as the size of a record and number of buffers is extracted from a file declaration while the name of the access method is inferred from the type and organization of a file the compiler puts this information as parameters in the open call when the call is made during execution of the program the file system puts this information in the fcb directory information is copied into the fcb through joint actions of the file system and the iocs when a new file is created information concerning the current state of processing is written into the fcb by the iocs this information is continually updated during the processing of a file the open files table oft holds the fcbs of all open files the oft resides in the kernel address space so that user processes can not tamper with it when a table fields in the file control block fcb category fields file organization file name file type organization and access method device type and address size of a record size of a block number of buffers name of access method directory information information about the files directory entry address of parent directorys fcb address of the file map table fmt or the file map table itself protection information current state of processing address of the next record to be processed addresses of buffers part file systems and io management internal idalpha file alpha fcbalpha is opened fmtalpha alpha fmtalpha file alpha is closed directory open files table oft file system data structures in memory figure interface between file system and iocs oft fcb and fmt file is opened the file system stores its fcb in a new entry of the oft the offset of this entry in the oft is called the internal id of the file the internal id is passed back to the process which uses it as a parameter in all future file system calls figure shows the arrangement set up when a file alpha is opened the file system copies fmtalpha in memory creates fcbalpha which is an fcb for alpha in the oft initializes its fields appropriately and passes back its offset in oft which in this case is to the process as internalidalpha the file system supports the following operations open filename processingmode fileattributes close internalidoffile readwrite internalidoffile recordinfo ioareaaddr filename is an absolute or relative path name of the file to be opened processingmode indicates what kind of operations will be performed on the file the values input create and append of it have obvious meanings while update indicates that the process intends to update existing data in place fileattributes is a list of file attributes such as the files organization record size and protection information it is relevant only when a new file is being created attributes from the list are copied into the directory entry of the file at this time recordinfo indicates the identity of the record to be read or written if the file is being processed in a nonsequential mode ioarea addr indicates the address of the memory area where data from the record should be read or the memory area that contains the data to be written into the record the iocs interface supports the following operations iocsopen internalidoffile directoryentryaddress iocsclose internalidoffile directoryentryaddress iocsreadwrite internalidoffile recordinfo ioarea addr each of these operations is a generic operation for the various file organizations supported by the file system it works in two parts it performs some chapter file systems open files table oft internal idalpha directory u alpha fcbalpha file declaration open alpha open alpharead read file attributes read alpha read internal idalpha file recordinfo recordinfo system iocs xyz adxyz close alpha close internal idalpha source program compiled program file system and iocs actions figure overview of file processing actions that are common to all file organizations and invokes a module of the access method mentioned in the fcb of the file for performing special actions required for specific file organizations the iocsopen and iocsclose operations are specialized read and write operations that copy information into the fcb from the directory entry or from the fcb into the directory entry the iocsreadwrite operations access the fcb to obtain information concerning the current state of the file processing activity such as the address of the next record to be processed when a write operation requires more disk space iocswrite invokes a function of the file system to perform disk space allocation see section figure is a schematic diagram of the processing of an existing file alpha in a process executed by some user u the compiler replaces the statements open read and close in the source program with calls on the file system operations open read and close respectively the following are the significant steps in file processing involving the file system and the iocs shown by numbered arrows in figure the process executes the call open alpha read fileattributes the call returns with internalidalpha if the processing mode read is consistent with protection information of the file the process saves internalidalpha for use while performing operations on file alpha the file system creates a new fcb in the open files table it resolves the path name alpha as described later in section locates the directory entry of alpha and stores the information about it in the new fcb for use while closing the file thus the new fcb becomes fcbalpha the file system now makes a call iocsopen with internalidalpha and the address of the directory entry of alpha as parameters file processing in this section we discuss the processing of structured files in which readwrite operations are performed on a record file system actions at open the purpose of a call open pathname processingmode file attributes where pathname is an absolute or relative path name for a file filename is to set up the processing of the file as described in section open performs the following actions it aborts the process if processingmode is not consistent with the protection information for the file otherwise it creates an fcb for the file filename in the oft and puts relevant information in its fields if filename is a new file it also writes fileattributes into its directory entry it passes the internal id of the file filename back to the process for use in file processing actions if the file filename is being created or appended to it makes provision to update the files directory entry when a close call is made by the process the procedure called path name resolution traverses all path components in a path name and checks the validity of each component it uses two pointers called the file fcb pointer and the directory fcb pointer during the traversal it points the file fcb pointer at the fcb of the file corresponding to the current component in the path name and the directory fcb pointer at the fcb of its parent directory at the end of path name resolution the file fcb pointer is used to chapter file systems determine the internal id of the file path name resolution consists of the following steps if an absolute path name is used locate the fcb of the file system root directory in the oft otherwise locate the fcb of the current directory this step assumes that the fcbs of these directories have already been created in the oft if not they should be created in this step set directory fcb pointer to point to this fcb a search for the next path component of the path name in the directory represented by directory fcb pointer indicate an error if the component does not exist or if the process owner lacks privileges to access it b create an fcb for the file described by the path component store this fcb in a free entry of the oft copy the directory fcb pointer in this fcb c set the file fcb pointer to point to this fcb d if this is not the last path component in the path name initialize the newly created fcb using information from the directory entry of the file set directory fcb pointer file fcb pointer and repeat step a if the file already exists copy the file size and the pointer to the fmt from the directory entry of the file into the fcb pointed to by file fcb pointer b if the file does not already exist create the fmt of the file and store its address in the fcb this action may involve allocating a disk block for the fmt set internal id of the file to the offset of file fcb pointer in the oft copy the directory fcb pointer into the fcb of the file return internal id to the process apart from the actions described above the file system may perform some other actions in the interest of efficiency for example while opening an existing file it may copy a part or all of the files fmt into memory see step a this action ensures efficient access to data in the file also only the fcbs pointed to by directory fcb pointer and file fcb pointer are needed during file processing so other fcbs created during path name resolution may be destroyed the following example illustrates the data structures built by the file system when a file is opened implementation of the open operation example figure shows the result of the file system actions after executing the call openinfoalpha the path name used in the open call is an absolute path name the file system searches for the name info in the root directory and creates an fcb part file systems and io management directory root info directory info alpha directory fcbinfo fcb pointer fcbalpha file fcb pointer fmtalpha oft figure file system actions at open for info in the oft it now searches for the name alpha in info and creates an fcb for alpha in the oft directory fcb pointer points to fcbinfo and file fcb pointer points to fcbalpha since alpha is an existing file its fmt pointer is copied into fcbalpha from the directory entry of alpha the call returns with the internal id of alpha which is the mount command mount fsname mountpointname mounts fsname at the mount point see section a simple way to implement mounting is to temporarily change the directory entry of mountpointname in its parent directory to point to the directory entry of fsname when a mount point is crossed during path name resolution the file system has to switch from the directory structure of the mount point to the directory structure of the mounted file system or vice versa to facilitate this while processing a mount command the file system puts the value m in the flags field of the directory entry of fsname and maintains a mount table to store pairs of the form fsname mountpointname for example when the call mount meetingaadmin of section is executed the file system adds the pair meeting aadmin to the mount table during path name resolution this table is consulted when a mount point is encountered during traversal of the directory structure from parent to child for the slash operator in the path name or child to parent for the operator the file system also has to ensure that disk space allocation performed during the processing of a mounted file is in the mounted file system rather than in the host file system chapter file systems file system actions during a file operation after opening a file filename a process executed by user u performs some read or write operations on it each such operation is translated into a call opn internal id record id ioarea addr where internal id is the internal id of filename returned by the open call and record id is absent if the operation is performed on a sequentialaccess file because the operation is necessarily performed on the next record in the file the file system performs the following actions to process this call locate the fcb of filename in the oft using internal id search the access control list of filename for the pair u give an error if the protection information found in the files fcb does not permit user u to perform opn on the file make a call on iocsread or iocswrite with the parameters internal id record id and ioarea addr for nonsequentialaccess files the operation is performed on the indicated record for sequentialaccess files the operation is performed on the record whose address is in the fcb field address of the next record to be processed and the contents of this field are updated to point to the next record in the file in step the iocs and the access method invoked by it obtains the fmt of the file from its fcb and uses it to convert record id into a pair of the form disk block id byte offset if it runs out of disk space during a write operation it calls a module of the file system which allocates a new disk block to the file and adds its address to the fmt implementation of readwrite operations example following the open call of example a call read alpha by the process where is record id would lead to the call iocsread if disk blocks have a size of bytes each and a record is bytes in length the iocs will convert record id into disk block number and record number in the disk block which implies a byte offset of now the address of the third disk block allocated to alpha is obtained from its fmt and this block is read to obtain the desired record file system actions at close the file system performs the following actions when a process executes the statement close internal id if the file has been newly created or appended to a if it is a newly created file create an entry for the file in the directory pointed to by the directory fcb pointer if the directory entry format file sharing semantics as discussed in section the owner of a file may authorize some other users to access the file processes created by authorized users can read write or execute chapter file systems the file in accordance with access privileges granted to them in essence they share the files to which they have access the file system provides two methods of file sharing so that processes can choose the one that permits them to collaborate and build on each others work effectively sequential sharing processes access a shared file one after another thus file modifications made by one process if any are visible to processes that access the file afterwards concurrent sharing two or more processes access a file over the same period of time file sharing semantics is a set of rules that determine the manner in which results of file manipulations performed by concurrent processes are visible to one another sequential sharing of a file can be implemented through the lock field in the files directory entry see figure if the lock field of the files directory entry has the value reset an open operation would succeed and change the value to set otherwise the open operation would fail and would have to be repeated a close operation would change the value in the lock to reset to facilitate concurrent sharing of a file the file system has to ensure that file processing activities of processes do not interfere accordingly it creates a separate fcb for each process by simply following the procedure of section every time a file is opened several fcbs may thus be created for concurrent sharing of file alpha we use the notation fcbpal pha for the fcb of alpha created for process p table summarizes three modes of concurrent file sharing provided in file systems sharing immutable files when the file alpha is shared as an immutable file none of the sharing processes can modify it hence the processes sharing file alpha are independent of one another creation of an fcbalpha for each sharing process is adequate to implement this form of file sharing table modes of concurrent file sharing mode description immutable files the file being shared can not be modified by any process singleimage mutable files all processes concurrently sharing a file see the same image of the file ie they have an identical view of files data thus modifications made by one process are immediately visible to other processes using the file multipleimage mutable processes sharing a file may see different images of files the file thus updates made by a process may not be visible to some concurrent processes the file system may maintain many images of a file or it may reconcile them in some manner to create a single image when processes close the file part file systems and io management fcbapl pha data blocks of alpha fcbapl pha fmtalpha oft figure concurrent sharing of a singleimage mutable file by processes p and p sharing singleimage mutable files a single copy of the file is shared by processes accessing it hence modifications made by one process are immediately visible to other processes to implement this form of sharing it is essential that a single copy of the fmt be used by all sharing processes hence it is best to keep a pointer to the fmt rather than the fmt itself in an fcb figure shows concurrent sharing of file alpha using such an arrangement the fcbs fcbpal pha and fcbpal pha are created when alpha is opened by processes p and p both fcbs point to the same copy of fmtalpha each fcb contains the address of the next record to be accessed by a process if the sets of records processed by p and p overlapped their modifications would be visible to one another race conditions could also arise in such situations and updates made by processes might be lost a typical file system does not provide any means of tackling this problem the processes have to evolve their own synchronization conventions for this purpose the unix file system supports singleimage mutable files we discuss unix file sharing semantics in section sharing multipleimage mutable files when a multipleimage mutable file alpha is shared by several processes each process that modifies the file creates a new version of alpha that is distinct from versions created by other concurrent processes in this scheme there has to be a distinct fmtalpha for each fcb and each fmt must point to an exclusive copy of the file this requirement is best implemented by making a copy of alpha and its fmt for each process concurrently accessing it figure illustrates the arrangement for implementing multipleimage mutable files processes p and p are engaged in updating alpha alphap represents the copy of alpha made for process p processing by p uses fcbapl pha and fmtapl pha to access alphap while processing by p uses fcbpal pha and fmtapl pha to access alphap alphap and alphap are thus two versions of alpha to arrive at a unique implementation scheme the file sharing semantics must specify how alpha would be accessed by processes that wish only to read it ie which version of alpha they would access sharing of multipleimage mutable files has special features that may not be valid or applicable in many applications hence it can be used only in applications file system reliability file system reliability is the degree to which a file system will function correctly even when faults such as data corruption in disk blocks and system crashes due to power interruptions occur the two principal aspects of file system reliability are ensuring correctness of file creation deletion and updates preventing loss of data in files the former concerns consistency and correctness of metadata ie the control data of the file system while the latter concerns consistency and correctness of data stored in files reliability literature distinguishes between the terms fault and failure a fault is a defect in some part of the system a failure is a system behavior that is erroneous or that differs from its expected behavior occurrence of a fault causes a failure thus corruption of a disk block due to a damaged disk head or a power outage is a fault whereas inability of the file system to read a faulty block is a failure chapter discusses these terms formally loss of file system consistency file system consistency implies correctness of metadata and correct operation of the file system loss of consistency arises if the metadata of the file system is lost or damaged it is interesting to see how this can happen consider operation of a process that updates a file alpha to ensure efficient operation the file system maintains some of its metadata in memory thus fcbalpha which exists in the part file systems and io management open files table part of fmtalpha and part of the disk status map or free list would be in memory some of this metadata like fmtalpha are written on a disk when alpha is closed in addition the file system may periodically copy the disk status map or free list on the disk however metadata is modified constantly so disk copies of metadata generally do not contain uptodate information during system operation when power fails metadata maintained in memory is lost and when a disk fails metadata stored on the disk is lost these situations may result in one or more of the following failures some data from file alpha may be lost part of file alpha may become inaccessible contents of two files may get mixed up it is easy to visualize a situation of the first kind for example suppose a fault occurs after a new disk block has been added to the file alpha the disk copy of fmtalpha will not contain this blocks id and so data in the newly added block will be lost when the fault occurs the second and third kind of situation can arise in a file system that does not employ any reliability techniques we illustrate these situations in a file system that uses linked allocation of disk space and employs algorithm to add a new disk block to a file the third kind of situation can also arise in a file system that uses indexed allocation of disk space algorithm add block dj between blocks d and d input d d dj record next id of next block data end dj next dnext dnext address dj write d to disk write dj to disk algorithm adds a new disk block dj between blocks d and d of the file figure illustrates how parts of file alpha may become inaccessible due to a fault figures a b show the file before and after a normal execution of the algorithm figures c shows the file if a fault occurs between steps and of algorithm new contents have been written into disk block d but not into disk block dj hence dnext points to dj whereas dj does not contain correct metadata in its next field disk blocks d d would not be accessible as parts of the file any more contents of two files may get mixed up if the file system writes metadata to the disk only while closing a file and not after every file operation consider the following situation a process p deletes a disk block dk from some file beta dk will be returned to the free list or will be marked free in the disk status map now process p adds a new record to file alpha the file system allocates a new disk block dj for this purpose and adds it ahead of disk block dm in file alpha chapter file systems before adding dj after adding dj after a fault d d d d dj d d d d dj a b c figure inconsistencies in metadata due to faults ab before and after adding dj during normal operation c after a fault dh dk dl dh d beta d dj d d dj d alpha a b figure files alpha and beta a after adding dj during normal operation b if dj dk alpha is closed and a power outage occurs see figure a now consider the situation when dj dk and the following events occur in the system file alpha is closed the file system updates the disk copy of file alpha it involves adding disk block dj to alpha a power outage occurs note that file beta was not closed before the power outage occurred so the disk contains an old copy of beta that contains block dk and the new copy of alpha that contains block dj since dj dk alpha and beta now share disk block dj and all other blocks accessible through it see figure b all disk blocks of file beta that were previously accessible through dk ie block dl and other blocks accessible through it are now inaccessible in effect some data is common to files alpha and beta while some data of beta has been lost approaches to file system reliability by means of the two approaches described in table operating systems ensure that user files are reliably stored over a period of time recovery is a classic approach that is activated when a failure is noticed it restores the data and metadata of the file system to some previous consistent state the file system now resumes its operation from this state thus deviations from correct behavior do occur but system operation is rectified when deviations are noticed fault tolerance on the other hand provides correct operation of the file system at all times ie it ensures that faults do not lead to failures it achieves this ability through some special techniques part file systems and io management table approaches to file system reliability approach description recovery restore data and metadata of the file system to some previous consistent state fault tolerance guard against loss of consistency of data and metadata due to faults so that system operation is correct at all times ie failures do not occur to see the difference between the two approaches consider the example of a disk block that becomes unreadable inability of the file system to read the block is a failure under the recovery approach the data in the block would be restored to an earlier value when a failure is noticed with fault tolerance each data unit would be recorded in two blocks a primary block and an alternative block if a failure occurs while the primary block is being read the file system would automatically read the alternative block of course fault tolerance is not absolute the system can tolerate only those faults that it is designed to for example when a data unit is recorded in two blocks the system can tolerate a fault in the primary block but not faults in both primary and alternative blocks recovery techniques the file system state at some time instant ti is the collection of all data and metadata in the file system at ti a backup of the file system is a recording of the file system state to support recovery the file system periodically produces backups during its operation let tlb represent the time at which the latest backup was produced in the event of a failure say at time tf the file system is restored to the state recorded in its latest backup file updates performed between tlb and tf are lost operations that performed these updates need to be reprocessed after recovery recovery using backups thus involves two kinds of overheads overhead of creating backups and overhead of reprocessing reprocessing overhead in recovery can be reduced through a combination of backups and incremental backups of a file system an incremental backup contains copies of only those files or disk blocks that were modified after the last backup or incremental backup was created the file system creates backups at large intervals of time eg a day a few days or a week incremental backups are created at shorter intervals and are discarded when the next backup is created for example an incremental backup may be created when a process closes a file after updating it the incremental backup would contain a copy of only that file use of incremental backups increases the overhead of the backing up activity the space overhead is also high because backups and incremental backups coexist and some files may exist in more than one incremental backup however the reprocessing overhead is low for the following reason after a crash the system could be restored from the latest backup and incremental backups would then be processed in the same order in which they were created this action would restore chapter file systems all files whose modification was completed before the last of the incremental backups was created only the file processing activities that were in progress at the time of the failure would have to be repeated to reduce the recovery overhead the file system could be restored by processing all incremental backups and the latest backup in the reverse order taking care not to restore a file that has been already restored from a later incremental backup this approach would reduce overhead by restoring each file exactly once however it would be effective only if the file system metadata is consistent at the time of a failure recovery in a file system example figure illustrates a system in which backups were taken at times t and t and incremental backups were taken at t and t the incremental backups contain and disk blocks respectively because disk blocks were updated between t and t and disk blocks were updated between t and t if a failure occurs after t the system would be restored to the state recorded in the backup taken at t however if a failure occurred between t and t the system would have been restored by using the backup taken at t and the incremental backups taken at t and t creating backups the key issue in creation of backups is consistency of metadata recorded in a backup consider the following scenario during operation of a file system the free list data structure is written in the backup a record is added to a file phi which requires a new disk block to be allocated to phi from the free list file phi is now written in the backup here recording of the free list and file phi in the backup would be mutually inconsistent it could lead to a mixup of data in files as discussed in section similar problems would arise even if these three actions are performed in the reverse order inconsistencies of metadata could be prevented by freezing all activities in the file system while a backup is created however this method is intrusive and it would cause delays in processes an alternative is to create a backup during normal operation of a system but use some simplifications like not writing the free list in a backup when the state of the file system is restored from such a backup the file system could scan the complete disk and build the free list anew however in this scheme files would have been recorded in the backup at different times so they would suffer loss of data to different extents if the file system is restored by using this backup another issue is the backing up of a file that is being processed when a backup is initiated either its backing up should be delayed until its processing is complete or the user would not precisely know how much of the files processing would be lost if the file system is restored by part file systems and io management time file system backup media kind of backup t backup t incremental backup t incremental backup t backup figure backups and incremental backups in a file system using the backup an incremental backup that is created when a file is closed does not face any of these consistency problems because only modified files are written into the backup so file system metadata like free lists would not be written into it what about the overhead of creating a backup when disk space was expensive backups were typically created on slower io devices like tapes however disk space is affordable in modern computer systems so it is possible to create backups on disks when indexed allocation of disk space is used it is possible to create an ondisk backup of a file cheaply by means of a technique that resembles the copyonwrite technique of virtual memory figure illustrates this technique file location file location name info name info phi phi bphi bphi fmtphi fmtphi fmtbphi directory directory fmtbphi a d db d figure creating a backup a after backing up file phi b when phi is modified chapter file systems when file phi is to be backed up the file system creates a copy of the directory entry of phi and names the new file appropriately say bphi now the fmt pointers of phi and bphi are identical see figure a so file bphi is a copy of phi as desired if contents of the second disk block allocated to phi change from to because of a file update the file system would perform the following actions see figure b if the fmt pointers of phi and bphi are identical make a copy of the fmt and make the directory entry of bphi point to the copy allocate a new disk block to file phi change the appropriate pointer in fmtphi to point to the new disk block write the new contents into the new disk block thus only the fmt and the disk block whose contents are updated after the backup is created would be copied this arrangement conserves both disk space and time fault tolerance techniques file system reliability can be improved by taking two precautions preventing loss of data or metadata due to io device malfunction and preventing inconsistency of metadata due to faults these precautions are implemented by using the fault tolerance techniques of stable storage and atomic actions respectively stable storage lampson proposed the technique of redundant recording of data to ensure reliability it is called stable storage because it can tolerate one fault in the recording of a data item two copies of a record called its primary and secondary copy are maintained on a disk a write operation updates both copies the primary copy is updated first followed by the secondary copy a read operation accesses the disk block containing the primary copy if it is unreadable the block containing the secondary copy is accessed since only single faults are assumed to occur one of the blocks is sure to contain readable data figure illustrates operation of the stable storage technique if faults occur at times t t t or t respectively while a process pi is executing an update operation on some data d parts ad show timing charts and values in the primary and secondary copies of d when faults occur in part a a fault occurs at time t ie before the primary copy is updated hence the primary copy containing the old value of the data is accessible after a fault in part b a fault occurs while the primary copy is being updated so that the primary copy becomes unreadable the old value of the data is accessible from the secondary copy in part c a fault occurs after the primary copy is updated but before the secondary copy is updated new data is accessible in the primary copy after the fault occurs in part d a fault occurs after both copies have been updated hence both copies are accessible the stable storage technique can be applied to entire files lampson called this technique disk mirroring however it is different from the disk mirroring we will come across in section however stable storage incurs high space and time overhead which makes it unsuitable for general use in a file system part file systems and io management primary secondary copy is copy is primary secondary updated updated copy copy a old old b unreadable old c new old d new new t t t t time figure fault tolerance using the stable storage technique so processes may use it selectively to protect some of their own data also while stable storage guarantees that one copy of data will survive a single fault it can not indicate whether this value is old or new see parts a d of figure hence the user does not know whether to reexecute the update operation in pi when system operation is restored an atomic action overcomes this problem atomic actions an action may involve manipulation of many data structures eg consider algorithm of section these data structures may become inconsistent if a fault interrupts execution of the action an atomic action is a method of avoiding such ill effects of faults definition atomic action an action that consists of a set of subactions and whose execution has the property that either the effects of all of its subactions are realized or the effects of none of its subactions are realized thus an atomic action has an allornothing property this property avoids data inconsistency when faults occur consistency of file system metadata can be preserved by updating all file system data structures by using atomic actions database systems use a concept called an atomic transaction or a database transaction that ensures certain additional properties such as serializability our discussion is restricted to atomic actions for file system reliability only the subactions in an atomic action are enclosed between the statements begin atomic action and end atomic action execution of the atomic action begins when the begin atomic action statement is executed the action can end in two ways it can either fail or succeed it fails if it loses interest in continuing its execution and executes an abort statement or if a fault occurs before the statement end atomic action is executed if it fails the state of each file or metadata used by it should be left as it was prior to execution of the begin atomic action statement an atomic action succeeds when it executes the end atomic action statement it is said to chapter file systems begin atomic action addablock dj next dnext d next addressdj write d write dj end atomic action addablock figure atomic action addablock commit at this time all updates made by it are guaranteed to survive any faults after it commits figure shows algorithm of section coded as an atomic action named addablock it differs from algorithm only in the use of the statements begin atomic action and end atomic action if the atomic action addablock commits disk block dj is added to file alpha and alpha now consists of disk blocks d dj d if it fails disk block dj is not added to file alpha ie alpha continues to consist of disk blocks d d thus it avoids the problem described in section and illustrated in figure atomic actions can be implemented in many ways in one implementation approach files or metadata are not updated during execution of the atomic action they are updated only after the atomic action commits this arrangement automatically tolerates faults that occur before an atomic action commits since no updates will have been made in files thus it implements the nothing part of the allornothing property to implement the all part of the allornothing property it is necessary to ensure that all updates will be made even if faults occur two data structures called intentions list and commit flag are maintained to ensure this both data structures are maintained in stable storage to protect them against data corruption and loss due to faults every time the atomic action modifies a file or metadata the file system makes an entry of the form disk block id new contents in the intentions list to indicate that new contents should be written in the disk block with the id disk block id the file system uses the information in the intentions list to update the files when the atomic action commits this action is called commit processing the commit flag contains two fields transaction id and value this flag is created when the statement begin atomic action of an atomic action ai is executed and its fields are initialized to ai and not committed respectively the value in the commit flag is changed to committed when end atomic action is executed the flag is destroyed after all updates described in the intentions list have been carried out if a failure occurs the file system checks for the presence of commit flags when its operation is resumed if a commit flag exists for ai and has the value not committed the file system simply destroys the commit flag and the intentions list and executes atomic action ai again starting with the statement begin atomic action existence of a commit flag for ai with the value committed implies that commit processing of ai was in progress when occurrence of a fault led to part file systems and io management disk new block contents dj d transaction id value d d d addablock nc d commit flag intentions list dj a b figure a before and b after commit processing note nc means not committed a failure since it is not known whether any entries of the intentions list were processed before the fault the entire commit processing is now repeated if faults occur during commit processing some entries of the intentions list may be processed many times however it does not pose any data consistency problems because the operation of writing new contents into disk block id is an idempotent operation which has the property that executing it many times has the same effect as executing it once the following algorithm summarizes all actions concerning implementation of an atomic action algorithm implementation of an atomic action execution of an atomic action ai a when the statement begin atomic action is executed create a commit flag and an intentions list in stable storage and initialize them as follows commit flag ai not committed intentions list empty b for every file update made by a subaction add a pair d v to the intentions list where d is a disk block id and v is its new content c when the statement end atomic action is executed set the value of ais commit flag to committed and perform step commit processing a for every pair d v in the intentions list write v in the disk block with the id d b erase the commit flag and the intentions list on recovering after a failure if the commit flag for atomic action ai exists a if the value in commit flag is not committed erase the commit flag and the intentions list reexecute atomic action ai b perform step if the value in commit flag is committed example implementation of an atomic action figure a shows the file alpha the commit flag and the intentions list when algorithm is applied to the atomic action addablock of journaling file system as discussed in section a file system keeps some part of file data as well as metadata such as file control blocks file map tables and free lists of disk blocks in memory during its operation when a file systems operation is shut down by a system administrator the file system copies all the data and metadata held in memory onto the disk so that the copy on disk is complete and consistent however when a power outage occurs or when the system is switched off abruptly the file system does not get an opportunity to copy the file data and metadata from memory to disk such a shutdown is called an unclean shutdown it results in loss of file data and metadata that was held in memory traditionally file systems relied on recovery techniques to protect against loss of data and metadata because they were so simple to implement thus backups were created periodically and files were recovered from backups when failures were detected metadata was recovered by laborious searches to find and fix inconsistencies use of recovery techniques imposed little overhead during normal operation of the system when a failure was detected however cpu overhead was incurred in checking consistency of metadata and the system was unavailable during recovery as well as an example consider what happened when a unix system using the ext file system was shut down uncleanly on rebooting the file system would realize that it was shut down uncleanly and hence its metadata was likely to be inconsistent it would invoke the fsck program to recover the metadata fsck would look through every file system data structure on the disk and try to fix any inconsistencies it could find operation of the os was delayed while fsck executed a modern file system uses fault tolerance techniques so that it can resume its operation quickly after an unclean shutdown a journaling file system implements fault tolerance by maintaining a journal which resembles the intentions list used to implement atomic actions see section the file system records actions that it is about to perform in the journal before actually performing them when operation of a file system is restored after an unclean shutdown it consults the journal to identify actions that were not performed as a result of the shutdown and performs them thus ensuring correctness of file data and metadata the ext virtual file system users have diverse requirements of a file system such as convenience high reliability fast response and access to files on other computer systems a single file system can not provide all these features so an operating system provides a virtual file system vfs which facilitates simultaneous operation of several file systems this way each user gets to use the file system he prefers a virtual file system vfs is an abstraction that supports a generic file model the abstraction is implemented by a vfs layer that is situated between a process and a file system see figure the vfs layer has two interfaces an interface with the file systems and an interface with processes any file system that conforms to the specification of the vfsfile system interface can be installed to work under the vfs this feature makes it easy to add a new file system the vfs process interface provides functionalities to perform generic open close read and chapter file systems processes virtual file system metadata vfs metadata file data metadata file data metadata file data file systems of type x file systems of type y file systems of type z figure virtual file system write operations on files and mount unmount operations on file systems these functionalities are invoked through system calls the vfs determines which file system a file actually belongs to and invokes the open close read and write functionalities of the specific file system through the vfsfile system interface it also invokes functions of the specific file system to implement mount and unmount operations all file systems operating under the vfs are available for use simultaneously in the system of figure one process may use a file system of type x while another process simultaneously uses a file system of type y the virtual file system can also be used to compose a heterogeneous file system for example a user can mount a file system of type x in a directory of a file system of type y this feature is useful with removable media like cds it permits a user to mount the file system that exists in a cd in his current directory and access its files without any concern for the fact that file data is recorded in a different format this feature is also useful in a distributed environment for mounting a remote file system into a file system of a computer it is described in section as shown in the schematic diagram of figure the virtual file system does not contain any file data it merely contains data structures that constitute vfs metadata each file system contains its own metadata and file data the key data structure used by the virtual file system is the virtual node popularly called vnode which contains the information needed for performing operations on a file it can be looked upon as a file object with the following three parts filesystemindependent data such as a file id that is unique within the domain of the vfs which may be the individual computer system or a network the file type eg directory data file or a special file and other fields such as an open count lock and flags filesystemspecific data such as the file map table addresses of functions in the file system that contains this file these functions implement the open close read and write operations on files of this file type case studies of file systems unix file system the design of the unix file system is greatly influenced by the multics file system in this section we describe important features common to most versions of unix in the context of the generic description of file processing in sections and inodes file descriptors and file structures the information that constituted the directory entry of a file in figure is split in unix between the directory entry and the inode of the file the directory entry contains only the file name and the inode number the bulk of the information concerning a file is contained in its inode files are considered to be streams of characters and are accessed sequentially the system administrator can specify a disk quota for each user it prevents a user from occupying too much disk space the inode data structure is maintained on disk some of its fields contain the following information file type eg whether directory link or special file number of links to the file file size id of the device on which the file is stored inode serial number user and group ids of the owner access permissions allocation information the splitting of the conventional directory entry into the directory entry and the inode facilitates creation and deletion of links a file can be deleted when its number of links drops to zero note the similarity between fields of the inode and those of the fcb see table figure illustrates the arrangement in memory during the processing of a file it consists of inodes file structures and file descriptors a file structure contains two fields the current position in an open file which is in the form of an offset from the start of the file and a pointer to the inode for the file thus an inode and a file structure together contain all the information necessary to access the file a file descriptor points to a file structure file descriptors are stored in a perprocess table this table resembles the open files table oft described in section when a process opens a file alpha the directory entry for alpha is located a directory lookup cache is employed to speed up this operation once the entry of alpha is located its inode is copied into memory unless memory already chapter file systems offset inode pointer perprocess file disk table of structure blocks file descriptors of alpha inode for alpha figure unix file system data structures contains such a copy the arrangement shown in figure is now set up and the index of the file descriptor in the file descriptors table which is an integer is passed back to the process that opened the file the process can use it in a manner that resembles use of the internal id of a file in the generic arrangement of sections and when a process creates a child process a table of descriptors is created for the child process and the file descriptors of the parent process are copied into it thus more than one file descriptor may point to the same file structure processes owning these file descriptors share the offset into the file a read or write by one process will modify the offset for the other processes as well file sharing semantics several processes may independently open the same file in that case the arrangement of figure is set up for each process thus two or more file structures may point to the same inode processes using these file structures have their own offsets into the file so a read or write by one process does not modify the offset used by other processes unix provides singleimage mutable file semantics for concurrent file sharing as shown in figure every process that opens a file points to the copy of its inode through the file descriptor and file structure thus all processes sharing a file use the same copy of the file changes made by one process are immediately visible to other processes sharing the file implementation of these semantics is aided by the fact that unix uses a disk cache called buffer cache rather than buffers for individual file processing activities see section to avoid race conditions while the inode of a shared file is accessed a lock field is provided in the memory copy of an inode a process trying to access an inode must sleep if the lock is set by some other process processes concurrently using a file must make their own arrangements to avoid race conditions on data contained in the file disk space allocation unix uses indexed disk space allocation with a disk block size of kb each file has a file allocation table analogous to an fmt which is maintained in its inode the allocation table contains entries see figure twelve of these entries directly point to data blocks of the file the next entry in the allocation table points to an indirect block ie a block that itself contains pointers to data blocks the next two entries point to double and triple indirect part file systems and io management single indirection double indirection triple indirection figure unix file allocation table blocks respectively in this manner the total file size can be as large as bytes however the file size information is stored in a bit word of the inode hence file size is limited to bytes for which the direct single and double indirect blocks of the allocation table are adequate for file sizes smaller than kb this arrangement is as efficient as the flat fmt arrangement discussed in section such files also have a small allocation table that can fit into the inode itself the indirect blocks permit files to grow to large sizes although their access involves traversing the indirection in the file allocation table a survey of unix file sizes conducted in reported that the average file size in unix was kb and over percent of files had sizes smaller than kb thus the unix file allocation table is as efficient as the flat fmt for most files unix maintains a free list of disk blocks each entry in the list is similar to an indirect block in an fmt it contains addresses of free disk blocks and the id of the next disk block in the free list this arrangement minimizes the overhead of adding disk blocks to the free list when a file is deleted only marginal processing is required for files that contain only direct and single indirect blocks a lock field is associated with the free list to avoid race conditions when disk blocks are added and deleted from it a file system program named mkfs is used to form the free list when a new file system is created mkfs lists the free blocks in ascending order by block number while forming the free list however this ordering is lost as disk blocks are added to and deleted from the free list during file system operation the file system makes no effort to restore this order thus blocks allocated to a file may be dispersed throughout a disk which reduces the access efficiency of a file bsd unix uses cylinder groups to address this issue see section multiple file systems the root of a file system is called the superblock it contains the size of the file system the free list and the size of the inode list in the interest of efficiency unix maintains the superblock in memory but copies it onto the disk periodically this arrangement implies that some part of file system state is lost in the event of a system crash the file system can reconstruct some of this information eg the free list by analyzing the disk status this is done as a part of the system booting procedure chapter file systems there can be many file systems in a unix system each file system has to be kept on a single logical disk device hence files can not span different logical disks a physical disk can be partitioned into many logical disks and a file system can be constructed on each of them such partitioning provides some protection across file systems and also prevents a file system from occupying too much disk space a file system has to be mounted before being accessed only a user with the root password typically a system administrator can mount a file system mounting and unmounting of file systems works as follows a logical disk containing a file system is given a device special file name this name is indicated as fsname in a mount command see section when a file system is mounted the superblock of the mounted file system is loaded in memory disk block allocation for a file in the mounted file system is performed within the logical disk device of the mounted file system files in a mounted file system are accessed as described in section a file open call in unix specifies three parameters path name flags and mode flags indicate what kind of operations will be performed on the file whether read write or readwrite the mode parameter is provided only when a file is being created it specifies the access privileges to be associated with the file this information is typically copied from the file creation mask of the user the owner of a file can change the file protection information any time through a chmod command berkeley fast file system the berkeley fast file system ffs for unix was developed to address the limitations of the file system sfs it supports a symbolic link which is merely a file that contains a reference to another file if the symbolic link is encountered during path name resolution the path name resolution is simply continued at the referenced file it also includes several innovations concerning disk block allocation and disk access which we describe in the following ffs permits use of large disk blocks blocks can be as large as kb different file systems can use different block sizes however block size can not vary within one file system a large block size makes larger files accessible through the direct blocks in the file allocation table a large block size also makes io operations more efficient and makes efficient use of the disk however a large block size leads to large internal fragmentation in the last disk block of a file ffs counters this effect by allocating a part of a disk block to the last portion of a file this way a disk block may be shared by many files to facilitate such allocation a disk block is divided into equalsize parts called fragments the number of fragments in a disk block is a parameter of a file system and is either or ffs uses a bit map to keep track of free fragments of a block file growth requires special attention in this scheme because a file may need more fragments which might not be available in the same disk block in such cases all its fragments are moved to another disk block and the previously allocated fragments are freed ffs uses the notion of cylinder groups to reduce the movement of disk heads see section to reduce disk head movement further it puts all inodes of a file system in the same cylinder group and tries to put the inode of a file and part file systems and io management the file itself in the same cylinder group it also prevents a file from filling up a cylinder group if a file grows to a size that would violate this constraint it relocates the entire file into a larger cylinder group this technique increases the possibility that concurrently accessed files will be found within the same cylinder group which would reduce disk head movement ffs tries to minimize rotational latency while reading a sequential file as described later in section a certain period of time elapses between the end of a disk read operation and start of the next disk read operation during this time the next few disk blocks inevitably pass under the disk head even if a command to read the next disk block is issued immediately the block can therefore be read only during the next revolution of the disk to ensure that consecutively numbered blocks on a track can be read during the same disk revolution ffs separates them by putting a few other disk blocks between them this feature is similar to the technique of interleaving of sectors in a track discussed later in section as illustrated there this technique has a significant impact on disk throughput linux file system linux provides a virtual file system vfs which supports a common file model that resembles the unix file model this file model is implemented by using unixlike data structures such as superblocks and inodes when a file is opened the vfs transforms its directory entry into a dentry object this dentry object is cached so that the overhead of building it from the directory entry is avoided if the file is opened repeatedly during a computing session the standard file system of linux is called ext the file system ext incorporates journaling which provides integrity of file data and metadata and fast booting after an unclean shutdown see section ext provides a variety of file locks for process synchronization advisory locks are those that are supposed to be heeded by processes to ensure mutual exclusion however the file system does not enforce their use unix file locks belong to this category of locks mandatory locks are those that are checked by the file system if a process tries to access data that is protected by a mandatory lock the process is blocked until the lock is reset by its holder a lease is a special kind of file lock that is valid for a specific amount of time after another process has tried to access the data protected by it it is implemented as follows if a process accesses some data that is protected by a lease the holder of the lease is intimated by the file system it now has a stipulated interval of time to finish accessing the file and release the lease if it does not do so its lease is broken and awarded to the process that tried to access the data protected by it design of ext was influenced by bsds fast file system see section ext uses the notion of a block group which is a set of consecutive disk blocks to reduce the movement of disk heads when a file is opened and its data is accessed it uses a bit map to keep track of free disk blocks in a block group when a file is created it tries to allocate disk space for the inode of the file within the same block group that contains its parent directory and also accommodates the file chapter file systems data within the same block group every time a file is extended through addition of new data it searches the bit map of the block group to find a free disk block that is close to a target disk block if such a disk block is found it checks whether a few adjoining disk blocks are also free and preallocates a few of these to the file if such a free disk block is not found it preallocates a few contiguous disk blocks located elsewhere in the block group this way it is possible to read large sections of data without having to move the disk head when the file is closed preallocated but unused disk blocks are freed this strategy of disk space allocation ensures use of contiguous disk blocks for contiguous sections of file data even when files are created and deleted at a high rate it contributes to high file access performance solaris file system the solaris file system provides unixlike file access permissions in which three access control pairs exist in each access control list for the file owner for other users in the file owners group and for all other users in the system see section to provide flexibility that is lacking in this basic scheme it also permits new pairs containing listofuserids and listofaccessprivileges to be added to the access control list of a file the system administrator specifies a new pair through the setfacl command solaris offers convenience and flexibility in file processing through a virtual file system as described in section and through a variety of file processing modes an exclusive open operation on a file fails if the file already exists otherwise it creates the file and returns its descriptor in a single indivisible action this operation avoids race conditions while a new file is created it is used by processes that create a lock file to synchronize their activities recordlevel locking is provided to implement finegrained synchronization between processes that concurrently access a file when a process tries to access a record whose lock has been set by another process it is blocked until the lock is reset the nonblocked io mode is provided to avoid indefinite waits due to this feature in this mode an io operation that tries to access a record that is locked by another process simply fails the process issuing the operation now has an opportunity to perform some other actions and retry the io operation later an asynchronous io mode is provided in which a process is not blocked for its io operation to complete this mode is useful in realtime applications in the direct io mode the file system does not buffer or cache file data this mode facilitates applications such as database systems that wish to perform their own buffering or caching data synchronization and file integrity flags can be set in the directory entry of a file to obtain reliable operation when some of these flags are set for a file io operations on the file ensure the integrity of metadata andor the file data in a manner resembling the journaling modes summarized in table windows file system the ntfs file system of windows is designed to meet the requirements of servers and workstations it provides support for clientserver applications for file and part file systems and io management database servers a key feature of ntfs is recoverability of the file system which we will discuss later in this section a partition is a large collection of contiguous sectors on a disk a volume is a logical partition on a disk ie it is a virtual disk a simple volume contains a single partition while a multipartition volume called a spanned volume may contain up to partitions located on one or more disks ntfs performs disk space allocation in units called clusters each cluster is a group of contiguous sectors the number of sectors in a cluster is a power of a cluster on a volume is assigned a logical cluster number lcn whereas that in a file is assigned a virtual cluster number vcn an ntfs volume contains a boot sector a master file table mft some system files and user files the presence of a boot sector makes every volume bootable the mft typically contains a kb record for each file and directory on the volume though large files may need multiple mft records the mft also contains information about unused areas on the volume each file on a volume has a unique file reference which consists of two components a bit file number which is simply the record number of the mft record occupied by it and a bit sequence number which is a count of the number of times the mft record has been used to date the sequence number is used to prevent mixups between two files that have used the same mft record at different times each file has a set of attributes where each attribute is an independent byte stream that can be edited some standard attributes are common to all files in addition a file may have special attributes required in an application each file has an mft record called its base file record which contains the file reference of the file the time of its last update and its attributes an unnamed data attribute of a file contains file data this arrangement permits the data in a small file or directory to be stored in its base file record itself which provides high file access efficiency if an attribute can not fit in the files base file record it is stored as a nonresident attribute it is stored in another mft record and a pointer to it is put in its base file record if the nonresident attribute itself can not fit in one mft record it is stored in clusters on the disk and the mft record pointed to by the files base file record contains a vcntolcn mapping for its clusters when a process opens a file ntfs sets up a stream control block scb for each of its attributes an scb contains a pointer to a file control block for the file which contains its file reference and an offset into an attribute when the process wishes to access an attribute of a file ntfs uses the scb to locate the files base file record finds information about location of the attribute and then applies the offset to access the required portion of the attribute a directory is organized as a b tree with files as its leaf nodes and it is implemented by using an index file the b tree data structure has the property that the length of each path in the tree is the same this feature facilitates efficient search for a file in a directory see section ntfs provides hard links to set up multiple paths to a file it also supports symbolic links called junctions that redirect path name translation from a directory to an alternative one this feature provides an effect that is analogous to mounting of file systems chapter file systems ntfs employs two techniques to save disk space if a file is sparse it does not allocate disk space to that portion of the file into which either no data has been written or the written data is such that one or more complete sectors contain zeroes it performs data compression for nonsparse files using consecutive virtual clusters in a file as a unit it replaces them by a compressed form only if that action would save at least one cluster and notes this fact so that it can automatically perform decompression when the file is accessed ntfs stores its metadata also in files some of these files are as follows the mft file contains mft records the log file contains information used for recovery its use is described later in this section the attribute definition table contains information about attributes a bit map file indicates which clusters in a volume are allocated and which are free the boot file contains the boot sector a bad clusters file keeps track of clusters that are unusable due to hardware problems ntfs provides robustness by ensuring consistency of the metadata when a crash occurs it is achieved by treating every modification of the metadata as an atomic transaction from the discussion of atomic actions in section it would appear that atomic transactions can be implemented simply by writing the intentions of a transaction in a writeahead log file and actually carrying out the intentions when the transaction commits however certain actions like creation of a new files record in the mft can not be delayed until a transaction commits so ntfs uses a combined redoundo log that contains two kinds of records the collection of redo records in the log resembles the intentions list of section while the undo records pertain to actions that have been already performed by transactions that are yet to commit during normal operation only the redo records are used they are processed to actually perform modification of ntfss metadata when a transaction commits the undo records are used only during recovery from a crash as described in the following ntfs performs recovery as follows it modifies its metadata according to the redo entries in the log pertaining to transactions that had committed prior to the crash it then processes the undo entries to undo the modifications performed by transactions that had not committed prior to the crash the metadata is in a consistent state at the end of these actions so ntfs now resumes normal operation this feature provides the write behind capabilities of journaling file systems discussed in section in principle log entries pertaining to a transaction can be discarded after all of its actions are carried out during normal operation or recovery or after all of its actions are undone during recovery however ntfs can not discard log entries in this manner for two reasons it stores its metadata in files and it uses a file cache see section to speed up file processing activities thus changes made in a file containing metadata while processing the redo or undo entries in the log would remain in the file cache for a long time and may be lost if a crash performance of file systems file systems employ five techniques to provide high file access performance use of efficient data structures directories are organized by using data structures that facilitate fast search effective disk space allocation disk space is allocated to a file in such a manner that little disk head movement and rotational delays are involved in processing of a sequential file caching part of memory is used as a cache for data stored on an io device as discussed in section caching speeds up accesses to information that exhibits either temporal locality or spatial locality that is data that is either repeatedly accessed or located in proximity of previously accessed data buffering a buffer is a memory area that is used to store data temporarily the file system loads data from an io device into a buffer before a process needs it so that the process can access the data without having to wait for an io operation to complete converse actions are performed when a process wishes to write data in a file disk scheduling io operations on a disk are performed in an order that reduces disk head movement it ensures high throughput of a disk figure summarizes how a file system uses these techniques to speed up file processing hash tables and b trees enable fast searches in a directory see section disk space allocation of a file is confined to extents and cylinder chapter file systems cached and buffered data and metadata process pi phi directories open phi fmts read phi file data operation techniques employed for speedup directory access directory cache directory search hashtables b trees accessing file map table file map table cache in memory accessing a disk block disk block allocation in extents and cylinder groups disk block cache in memory disk scheduling disk block cache in io device accessing data buffering and blocking of data or use of a file cache figure techniques employed to provide high file access performance groups to reduce disk head movement and rotational delays see section the other techniques provide fast access to file data and metadata of a file system such as directory entries and file map tables directories are cached in memory when accessed for the first time thus a directory used to resolve a path name is retained in the cache to speed up future references to files located in it this cache is called a directory names cache a file map table is buffered in memory when the file is opened in anticipation of accesses to it it may be cached after its first access buffering may not be feasible if a file map table is large in size in that case parts of it may be cached in memory when first referenced a disk cache stores disk blocks in memory following their first use in a file processing activity hit ratios better than are possible in the disk cache hence its use reduces the number of io operations on a disk significantly an access method uses buffering and blocking of file data or stores file data in a file cache to reduce the wait time involved in an io operation disk scheduling is used to reduce disk head movement and the average wait time for io operations these techniques are employed by the iocs they are discussed later in chapter as technology advances techniques that were developed for use in software become implemented in the hardware modern io device technology incorporates some of the techniques mentioned in figure thus scsi disks part file systems and io management provide disk scheduling in the device itself raid units contain a disk block buffer which can be used to both buffer and cache disk blocks these technologies are discussed later in chapter logstructured file system disk caching reduces the number of read operations directed at a disk hence disk usage is dominated by disk head movement and write operations disk head movement can be reduced through disk scheduling and through the use of cylinder groups in disk space allocation for files however these techniques are less effective when files located in different parts of a disk are processed simultaneously which is the case most of the time in a shared computer system for example in a unix system write operations to a disk consume only about percent of the disk time the rest of the time is spent in disk head movement which leads to poor throughput of a disk a logstructured file system reduces disk head movement through a radically different file organization it writes file data of all files together in a single sequential structure that resembles a journal we call it the log file when an update or write operation is performed on any file the new data is simply added to the end of the log file hence little disk head movement is involved in this operation the file system writes special index blocks into the log file to contain metadata about the location of each files data in the log file these index blocks are used when file data has to be read off the disk thus little disk head movement is required for reading data that was written into a file recently however more disk head movement is involved for older data performance studies on the sprite logstructured file system showed that disk head movement accounted for only percent of the disk time consumed during file processing and its performance was superior to the conventional file system for frequent small writes example illustrates operation of a logstructured file system example logstructured file system figure a is a schematic diagram of the arrangement used in a logstructured file system for simplicity it shows the metadata and file data of a single file in the log file the data blocks in the log file are numbered for convenience the directory entry of a file points to an index block in the log file we assume the index block to contain the fmt of the file when file data residing in block is updated the new values are written into a new disk block ie block similarly some file data is written into disk block when the data in block is updated the file system now writes a new index block that contains the updated fmt of the file and sets the fmt pointer in the directory entry of the file to point to the new index block the new fmt contains pointers to the two new data blocks and to data block that has not been modified see figure b the old index block and disk blocks and are now free summary computer users have many expectations of a file organizations suit sequential and random access system convenience good performance of a file to records in a file respectively several hybrid processing activity and efficient use of io devices organizations such as the index sequential organito deal with these concerns effectively the file syszation are also widely used second a file system tem is structured into two layers the file system allows users to group related files logically and conlayer deals with convenience issues such as sharing veniently by creating files and directories to any and protection of files and reliability the inputdesired level third it allows a user to specify which output control system iocs layer implements file other users may access his files in what manner operations and deals with efficiency issues in this which facilitates sharing and protection of files chapter we discussed the techniques of file systems the file system allocates disk space to a file a file may be a structured file ie it may consuch that fragmentation of disk space is avoided tain records of data or it may be an unstructured and file data can be accessed efficiently indexed or byte stream file a file system provides conallocation of disk space to a file uses a disk block venience to its users through three means first or an extent as the unit of disk space for allocait provides different file organizations where each tion the disk blocks or extents allocated to a file organization suits a specific pattern of accessing are confined to cylinder groups to ensure efficient records in a file it provides a method of arranging access to file data information concerning the disk records of a file on an io device and accessspace allocated to a file is stored in a file map table ing them efficiently the sequential and direct file fmt part file systems and io management before reading from or writing into a file a inconsistent by faults such as power outages it is process has to open the file by specifying its path achieved through an atomic action which ensures name in the directory structure the file system that all actions in a set of related actions are comtraverses the path name determines which file is pleted even if faults occur an atomic action incurs being opened and sets up a file control block fcb considerable overhead therefore journaling file systo contain information such as the files type and tems provide a menu of reliability modes that guard organization address of its fmt and address of its data and metadata to different extents so that a next record when the process wishes to perform system administrator can choose the mode that is a read or write operation the file system passes costeffective for a computing environment the fcb to the iocs and the iocs implements a virtual file system vfs is a software layer the operation using the information accessible that permits several file systems to be in operathrough the fcb the file system specifies the file tion on a computer system simultaneously so that sharing semantics which determine how the results a user can choose the file system that is most of a file update made by a process should be visible suitable for his application the vfs provides a to other processes using the file concurrently unified method of accessing different file systems the file system ensures reliability of operaa process invokes the vfs layer using generalized tion by ensuring that the file data and metadata commands to access files and the vfs layer directs such as fmts and fcbs are not lost or made the commands to the appropriate file system test your concepts classify each of the following statements as true g during creation of a new file in a mounted or false file system the file is allocated disk space a allocation of contiguous disk space for a in the logical disk used by the mounted file sequentialaccess file leads to more efficient system file processing than allocation of noncontiguh the effect of mounting a file system is similar ous disk space to that of setting up a link in the directory b cycles in the directory structure create diffistructure except that the effect is obliterated culties with the file deletion operation when the file system is unmounted c absolute path names for two different files i when a user updates the data in a singlecannot be identical whereas their relative image mutable file changes made to the file path names could be identical are not immediately visible to users concurd the purpose of the file control block fcb rently using the file is to facilitate a file open operation the fcb j when a fault occurs a single incremental can be deleted immediately after the file is backup is adequate for restoring the entire opened file system to a previous consistent state e when a file is closed after updating the k journaling incurs overhead during operation directory containing the file may have to be of a file system updated as well l a virtual file system permits use of many file f maintaining a files file map table fmt systems in a computer however these file in memory while the file is being processed systems can not be used concurrently reduces the number of disk accesses during select the appropriate alternative in each of file processing the following questions chapter file systems a the file control block fcb of a file b the stable storage technique is alpha i a fault tolerance technique that is used to i contains only information copied from recover from two faulty blocks on a disk the directory entry of alpha ii a recovery technique used to recover the ii is used to avoid frequent accesses to the file system after a power failure directory entry of alpha iii a fault tolerance technique that is used to iii is used only to protect file alpha recover from one faulty block on a disk against invalid accesses iv none of the above exercises a file named data is frequently accessed by if all records in the index sequential file have the users in a system the following alternatives are same probability of being accessed show that proposed to simplify access to data access efficiency of the file will be affected by the a set up links from every users home directory presence of records in the overflow area can to data access efficiency be restored by rewriting the file b copy data into every users home directory as a new file that does not contain any overflow compare the advantages and drawbacks of these records approaches the amoeba distributed operating system uses an index sequential file contains records contiguous allocation of disk space when a file its index contains entries each index entry is updated it writes the updated file as a new file describes an area of the file that contains and deletes its old copy records if all records in the file have the comment on the advantages and drawbacks of same probability of being accessed calculate this approach the average number of disk operations involved does noncontiguous allocation of disk space in accessing a record compare this number influence the feasibility and effectiveness of with the number of disk operations required if the fundamental file organizations discussed in the same records were stored in a sequential section file a file system uses indexed disk space allocation consider the index sequential file of figure the size of each disk block is kb and each disk the following problem arises when a new record block address is bytes in length the size of the say record for employee number we will call fmt is one disk block it contains pointers it record is added to it there is no space to data blocks all other pointers point to index to store the new record on the track hence the blocks access method takes out record from the track a sequential file info contains records and shifts records and to make space for each of size kb characteristics of the disk and the new record record is now put into an of a process that reads and processes all records overflow area a new field called overflow area in file info are as follows pointer is added to each entry in the track index average time to read a disk block ms this field in the first entry of the track index is average time to process a record ms set to point to record in the overflow area if more records overflow out of the first track they calculate the elapsed time of the process under are put into a linked list and the overflow area the following conditions pointer of the track index points to the head of a the file system keeps the fmt in memory the list similar linked lists may be formed for but does not keep any index blocks in memory several tracks over a period of time while processing info part file systems and io management b the file system keeps the fmt and one index directory structure rooted at x is mounted in block of info in memory directory ya it should be possible to access a new record is to be added to the file info of file d as yabd problem what is the minimum number b multiple mounts the directory structure of disk operations required to reflect this change rooted at some directory say w is mounted in info on the disk what is the maximum at many mount points simultaneously number when indexed allocation is used for files explain a file system uses indexed allocation of disk how a disk block may occur in more than one space however it permits a sequential file file if a fault occurs to contain partially full disk blocks what let algorithm be rewritten as follows are the advantages and disadvantages of this dj next dnext scheme dnext address dj a file system uses contiguous allocation of disk write dj to disk space the sequential access method handles write d to disk bad blocks on a disk as follows if an error occurs does this modified algorithm prevent mixup while readingwriting a block it consults the bad between files in the event of a fault blocks table that is itself stored on the disk and explain how the byte offset into a unix file can accesses the alternative disk block assigned to be converted into the pair disk block id the bad block assuming all disk accesses to byte offset require identical access times calculate degra by default unix assigns the files stdin and dation in file access performance if percent stdout to the keyboard and terminal respecof the disk blocks allocated to a file are bad tively a user can use the redirection operators blocks suggest a method to improve the access and in a command to override the default performance assignments and use some other files for input to reduce the overhead of file access validaand output the redirect and append operator tion see step of section an os appends the output of a process to the end designer proposes to perform validation only at of an existing file the default assignments of file open time as mentioned in section the files are restored at the end of the command the open statement specifies the kind of accesses these features can be implemented by permawhich will be made to the file eg open abc nently associating fcbs for stdin and stdout with read is a single access validation each process check at file open time adequate if not explain a describe the file system actions involved in why in either case suggest an implementation implementing the default assignments for outline stdin and stdout and the redirection operators step of section creates an fcb for every and directory appearing in a path name b describe the file system actions involved in a is this arrangement adequate when a relative implementing the operator path name is used disk blocks allocated to a file are added to b are these entries necessary if a file is being the free list when the file is deleted write an opened for reading algorithm to perform this operation in unix c can the number of fcbs created per file be the unix file system associates a lock field with reduced the free list see section classify the explain how the following features can be incorfollowing statement as true or false locking porated in a file system of the free list is necessary due to the nature a cascaded mounts directory c contains a of unix processes such locking is unnecesfile d the directory structure rooted at c sary in an os using the conventional process is mounted at mount point xb later the model chapter file systems bibliography organick is historically the most important paper bina e j and p a emrath a faster on directory structures since the multics directory fsck for bsd unix proceedings of the winter structure has influenced most contemporary file sys usenix technical conference tems like unix linux solaris and windows usenix bovet d p and m cesati understanding contains proceedings of a file system workthe linux kernel rd ed oreilly sebastopol shop grosshans weiderhold and livadas calif discuss file organizations and file systems burrows m c jerian b lampson and mckusick et al describes a memorybased t mann online data compression in a file system which provides memorymapped files and logstructured file system acm sigplan notices directory structures implemented in pageable mem ory levy and silberschatz discusses file shar florido j i s journal file systems ing semantics lampson describes the stable linux gazette issue storage technique for reliability of disk data while grosshans d file systems design and svobodova surveys how atomic actions are implementation prentice hall englewood cliffs performed in various file servers florido disnj cusses design of journaling file systems kleiman kleiman s r vnodes an architecture describes the virtual file system design vahalia for multiple file system types in sun unix describes the unix virtual file system interface rosenproceedings of the summer usenix blum and ousterhout discusses design of the technical conference sprite logstructured file system while matthews et al kowalski t fsck the unix system discusses adaptive methods for improving the check program bell laboratories murray hill performance of logstructured file systems mckusick nj et al discusses the logstructured file system lampson b w atomic transactions in of unix bsd distributed systems architecture and bach and vahalia describe the unix implementation an advanced course goos g file system kowalski describes the unix program and j hartmanis eds springer verlag berlin used to check file system integrity this program looks through every file system data structure on disk bina levy h m and a silberschatz and emrath discusses how the file system integrity distributed file systems concepts and checks can be speeded up in the unix file system beck examples acm computing surveys et al and bovet and cesati discuss the ext file system of linux mauro and mcdougall livadas p file structures theory and discusses the solaris file system nagar and russipractice prentice hall englewood cliffs nj novich and solomon describe the ntfs file love r linux kernel development system of windows nd ed novell press matthews j n d roselli a m costello r y wang and t e anderson bach m j the design of the unix improving the performance of logstructured file operating system prentice hall englewood systems with adaptive methods proceedings of cliffs nj sixteenth symposium on operating systems beck m h bohme m dziadzka u kunitz principles r magnus c schroter and d verworner mauro j and r mcdougall solaris linux kernel programming pearson internals nd ed prenticehall englewood education new york cliffs nj part file systems and io management mckusick m k k bostic m karels and logstructured file system acm transactions on j s quarterman the design and computer systems implementation of the bsd operating system russinovich m e and d a solomon addison wesley reading mass microsoft windows internals th ed microsoft mckusick m k m karels and k bostic press redmond wash a pageable memory based filesystem svobodova l file servers for proceedings of the summer usenix networkbased distributed systems acm technical conference computing surveys nagar r windows nt file system usenix proceedings of the file systems internals oreilly sebastopol calif workshop ann arbor mich may organick e i the multics system vahalia u unix internals the new mit press cambridge mass frontiers prentice hall englewood cliffs nj rosenblum m and j k ousterhout weiderhold g file organization for the design and implementation of a database design mcgrawhill new york layers of the inputoutput control system the schematic of figure shows how the inputoutput control system iocs implements file operations processes pi and pj are engaged in file processing activities and have already opened some files when one of these processes makes a request to read or write data from a file the file system passes on the request to the iocs recall from section that the iocs holds some file data in memory areas called buffers the file cache or the disk cache to speed up file processing activities for a read operation the iocs checks whether the data required by the process is present in memory if so the process can access the data straightaway otherwise the iocs issues one or more io operations to load the data into a file buffer or the disk cache and the process has to wait until this io part file systems access methods physical iocs process pi io operations process file buffers disk disk blocks pj file cache or scheduling contain records disk cache figure implementation of file operations by the iocs process file system layer access method layer layers of the iocs physical iocs layer kernel figure layers of the file system and the iocs operation completes since many processes perform io operations concurrently the io operations are scheduled by a disk scheduling algorithm which aims to provide high throughput of the disk thus the iocs implements io operations in a manner that provides efficiency of file processing activities in processes and high throughput of io devices the iocs is structured into two layers called the access method and the physical iocs the access method layer provides efficient file processing and the physical iocs layer provides high device throughput this structure of the iocs separates processlevel concerns in efficient implementation of file operations from devicelevel concerns figure shows the hierarchy of file system and iocs layers the number of iocs layers and their interfaces vary across operating systems in older operating systems the physical iocs was typically a part of the kernel however modern operating systems put it outside the kernel to enhance extensibility and reliability of the os we will assume that the physical iocs is invoked through system calls and it invokes other functionalities of the kernel also through system calls overview of io organization section contained an overview of io organization three modes of performing io operations programmed mode interrupt mode and direct memory access dma mode were summarized in table we focus on the dma mode of io operations figure showed how io devices are connected to device controllers which are in turn connected to the dma controller each device controller has a unique numeric id similarly each device connected part file systems to it has a unique numeric device id a device address is a pair of the form controllerid deviceid an io operation involves the following details operation to be performed read write etc address of the io device number of bytes of data to be transferred addresses of areas in memory and on the io device that are to participate in the data transfer when an io operation is performed in the dma mode the cpu initiates the io operation but it is not involved in data transfer between an io device and memory to facilitate this mode of io an io operation is initiated by executing an io instruction the cpu the dma controller the device controller and the io device participate to realize an io instruction the io instruction points to a set of io commands that specify the individual tasks involved in the data transfer implementation of an io command requires participation of the dma controller the device controller and the io device but does not require participation of the cpu this way the cpu is free to execute other instructions while the io operation is in progress typically io commands are stored in memory and the address of the memory area containing io commands is used as an operand in the io instruction in some computers the address is picked up from a standard memory location when the io instruction is executed when the io instruction is executed the cpu passes this address to the dma controller the dma controller now realizes the io commands the next example provides details of this arrangement example io operations the io operation to read the data recorded in a disk block with the id trackid blockid is performed by executing the following io instruction ioinit controllerid deviceid iocommandaddr where iocommandaddr is the start address of the memory area containing the following two io commands position disk heads on track trackid read record recordid into the memory area with the start address memoryaddr the arrangement called third party dma works as follows device controllers are connected to the dma controller as shown in figure when an io instruction is executed the dma controller passes details of the io commands to the device controller of the io device the device delivers the io devices io devices operate under a variety of principles such as electromechanical signal generation and electromagnetic or optical data recording io devices work with different io media serve different purposes and organize and access data in different ways so they can be classified through the following criteria purpose input print and storage devices nature of access sequential and randomaccess devices data transfer mode character and block mode devices the information written or read in one io command is said to form a record a sequentialaccess device uses its io medium in a sequential manner hence an operation is always performed on a record that adjoins the record accessed in the previous operation access to any other record requires additional commands to skip over intervening records a randomaccess device can perform a read or write operation on a record located in any part of the io medium a keyboard a mouse a network and a tape drive are sequentialaccess devices disks can be accessed in both sequential and random manner a unit of io medium is called an io volume thus a tape cartridge and a disk can be called a tape volume and a disk volume respectively io volumes part file systems for some io devices are detachable eg floppy disks compact disks cds or digital audiotape dat cartridges while those for other io devices like hard disks are permanently fixed in the device data transfer modes the data transfer mode of a device depends on its speed of data transfer a slow io device operates in the character mode ie it transfers one character at a time between memory and the device the device contains a buffer register that can store one character the device controller raises an interrupt after an input device reads a character into the buffer or an output device writes a character from the buffer device controllers of such devices can be connected directly to the bus the keyboard mouse and printer are character mode devices a device capable of a high data transfer rate operates in the block mode of data transfer it is connected to a dma controller tapes and disk drives are block mode devices a block mode device needs to read or write data at a specific speed two kinds of problems would arise if a data transfer is delayed because of contention for the bus data would be lost during a read operation if the bus were unable to accept data from an io device at the required rate for transfer to memory a write operation would fail if the bus were unable to deliver data to the io device at the required rate to prevent problems due to contention for the bus data is not transferred over the bus during the operation instead it is transferred between an io device and a buffer during an input operation the data delivered by the io device is stored in a buffer in the dma controller which we will call the dma buffer it is transferred from the dma buffer to memory after the io operation completes to perform an output operation data to be written onto the io device is first transferred from memory to the dma buffer during the io operation it is transferred from the dma buffer to the io device data transfer between the cpu and an io device can also be realized by using memorymapped io in this approach a set of memory addresses are reserved for an io device these addresses are mapped into some of the registers of the io device such that when the cpu writes some data into a memory location with one of the reserved addresses the data is actually written into the corresponding register of the io device similarly when the cpu executes an instruction that reads data from a memory location with one of the reserved addresses the data actually gets read from the corresponding register of the io device this way the transfer of data takes place without a dma yet it does not load the cpu much memorymapped io is implemented as follows an io device listens on the bus on which memory is connected when one of its reserved addresses appears on the bus it simply transfers data between the bus and the register corresponding to the reserved address memorymapped io is popular on the pcs because a special io bus is not needed and the cpu does not have to provide any special instructions for initiating io operations and for checking the status of io devices which reduces the cost of the cpu however more hardware is needed on the memory bus to decode the reserved addresses chapter implementation of file operations access time and transfer time we use the following notation while discussing io operations tio io time ie time interval between the execution of an instruction to initiate an io operation and completion of the io operation ta access time ie time interval between the issue of a read or write command and the start of data transfer tx transfer time ie time taken to transfer the data fromto an io device during a read or write operation it is the time between start of transfer of the first byte to end of transfer of the last byte the io time for a record is the sum of its access time and transfer time ie tio ta tx figure illustrates the factors influencing tio the access time in a sequential device is a constant because the device can only read or skip a record on either side of its current position the access time in a randomaccess device varies because it can read or write any record in an io volume so it must reposition either the readwrite head or the io medium before commencing a read or write operation error detection and correction errors might arise during recording or reading of data or transferring it between an io medium and memory to facilitate detection and correction of such errors data being recorded or transmitted is viewed as a bit stream ie as a stream of s and s and special codes are used to represent the bit stream we discuss some of these codes in the following error detection is performed through recording of redundancy information with data this information which we will call error detection information is derived from the data by using a standard technique when data is read off an io medium this information is also read off the medium now error detection information is computed again from the read data using the same technique and it is compared with the error detection information read off the medium a mismatch indicates some recording errors error correction is performed analogously except that more powerful algorithms are used to generate the error correction information this information can both detect an error and indicate tio device readied for data transfer data transfer in progress ta tx readwrite command data transfer data transfer is issued starts ends figure access and transfer times in an io operation part file systems how it can be corrected recording and reading of redundant information causes an overhead error correction incurs more overhead than error detection figure describes two approaches to error detection and correction in the parity bits approach np parity bits are computed for nd bits of data the parity bits are put in fixed locations in a record they are indistinguishable from data except to the error detectioncorrection algorithm in the cyclic redundancy check crc approach an nc bit number called the crc is recorded in the crc field of a record a key difference between the two approaches is that np depends on nd while nc is independent of nd both approaches use modulo arithmetic this arithmetic is analogous to binary arithmetic except that it ignores carries or borrows generated in any bit position this property makes it very fast a modulo addition is represented as an exclusiveor operation it uses the following rules and a popular variant of the parity bits approach used in rams and older magnetic tapes associates a single parity bit with a byte of data as described in figure it is generated from all bits of a byte by using the operation it can detect a single error in a byte but fails if two errors occur it also can not correct any errors the error detection overhead is parity bit for bits of data ie percent a hamming code can detect up to two errors in a record and can correct a single error the correct technical name of the code is nd np nd hamming code comparison of the parity bit values in a record read off the medium with parity values computed from the read data by applying the rules of the code indicates which bit is in error the value in this bit is inverted to correct the error figure gives the rules for determining the number of parity bits and computing their values a hamming code can perform error detection and correction for byte it uses ie parity bits thus the overhead is percent the overhead decreases with the number of data bits eg parity bits are adequate for bytes of data the crc is computed from data that is to be transmitted or recorded and it is put into the crc field of a record it can indicate whether one or more errors have occurred in any byte of data or if bytes have been swapped or reordered when a record is read a crc is computed from its data field and compared with the number in its crc field an error exists if the two do not match a practical value of nc is or bits irrespective of the value of nd with nc nd error detection is not foolproof because two bit streams say s and s could generate the same crc if one of them is transformed into the other due to errors the errors can not be detected using crc the probability of this happening is nc hence reliability of crc is nc for a bit crc the reliability is percent for a bit crc reliability is percent magnetic tapes the io medium in a tape or cartridge is a strip of magnetic material on which information is recorded in the form of s and s using principles of electromagnetic recording the recording on a tape is multitrack each track records a bit chapter implementation of file operations data and parity data crc nd np bits parity bit nd bits nc bits parity bits approach crc approach calculating a parity bit a parity bit is computed from a collection of data bits by modulo arithmetic ie by using the exclusive or operator for example the parity bit for data bits bi bj bk and bl is computed as follows p bi bj bk bl c where c is a constant which is for odd parity and for even parity hamming code step determine the number of parity bits as the smallest value of np which satisfies nd np np fix parity bit positions as powers of ie positions b b b b in a record where bits are numbered as b b from the start of the record step compute the parity bit occupying the nth position from the following bits excepting itself for each value of c take n consecutive bits starting on bit position n c n where c has values etc thus parity bit b is computed from b b b is computed from b b b b b and b is computed from b b b b b b b step when a record is received or read compute parity bits and compare them with the parity bit values in the record form a binary number e e e as follows ei is if the received and computed values of parity bit bi are different otherwise it is no error has occurred if this number is zero if a single error exists this number indicates the position of the bit which is in error example if bit data is to be transmitted or recorded parity bits are used they occupy positions b b b and b the record contains where the parity bits have been underlined if the record is read as the error word is indicating that the error has occurred in position cyclic redundancy check crc step a bit stream is looked upon as a binary polynomial ie a polynomial each of whose coefficients is either a or a for example a bit stream is looked upon as a binary polynomial x x x x ie x x here a is interpreted as modulo addition ie an exclusiveor operation step the data in a received record is augmented by adding nc zeroes at its end the polynomial obtained from the augmented data is divided by a predefined polynomial of degree nc the remainder of this division is a polynomial of degree nc coefficients in this polynomial form the crc for example the crc for data using a predefined bit polynomial is step when a record is received the receiver computes the crc from the data part of the record and compares it with the crc part of the record a mismatch indicates errors alternatively the receiver computes the crc from the entire record an error exists if the computed crc is not figure approaches to error detection and correction part file systems of a byte or a parity bit a readwrite head is positioned on each track tape drives are sequentialaccess devices the operations that can be performed on these devices are readwrite a specified number of bytes skip and rewind because of the sequential nature tapes and dat cartridges are popularly used for archival purposes which involve reading or writing of all records on the medium in older tape technologies adjoining records on a tape are separated by an interrecord gap this gap provides for the startstop motion of the medium between the reading or writing of successive records the access time ta during a read or write operation is caused by both the need to achieve uniformvelocity motion of the io medium before the data transfer can be initiated and the need to position the next record under the readwrite head total io time for a record of size s bytes is given by the formula s tio ta d v where d recording density v velocity of the io medium interrecord gaps cause heavy penalties they lead to poor use of the recording medium and slow down file processing activities despite the drawback of poor utilization of the recording medium in the s tapes offered a cost per megabyte that was onetenth of that offered by disks however tapes lost this edge in the subsequent decade because disk technology made rapid progress and large disks became both practical and cheap to regain the cost advantage a streaming tape technology was developed a streaming tape contains a single record that is stored without a break irrespective of its size hence interrecord gaps do not exist even when a large volume of data is recorded on a tape a streaming tape device contains a buffer a write operation is started after putting some data in the buffer the device writes the data from the buffer onto the tape to keep the streaming tape operating at full speed it is important to put new data into the buffer at a speed that matches the writing speed of the tape the tape drive stops writing when it finds that the buffer is empty when new data is put into the buffer the tape drive resumes the write operation to avoid creating an interrecord gap the tape is first moved back and then moved forward again so that it can gather recording velocity by the time the head passes over the last bit it has written it now resumes writing effectively resumption of writing consumes a few milliseconds the streaming tape provides a high data transfer rate if the buffer is not allowed to become empty at any time however if the tape stops frequently the effective writing speed can drop to a much smaller value the physical iocs has to ensure that this does not happen the stopstartresume operation of the tape also requires precise positioning and alignment which makes streaming tapes expensive magnetic disks the essential storage element of a magnetic disk is a flat circular object called a platter which rotates on its axis the circular surfaces of a platter are covered with chapter implementation of file operations magnetic material a single readwrite head records on and reads from a surface so a byte is recorded serially along a circular track on the disk surface the read write head can move radially over the platter for each position of the head the recorded information forms a separate circular track parity information is not used in a disk a crc is written with each record to support error detection a startoftrack position is marked on each track and records of a track are given serial numbers with respect to this mark the disk can access any record whose address is specified by the pair track number record number the access time for a disk record is given by ta ts tr where ts seek time ie time to position the head on the required track tr rotational latency ie time to access desired record on the track the seek time is the time required for the mechanical motion of the head rotational latency arises because an io operation can start only when the required record is about to start passing under the head the average rotational latency is the time taken for half a disk revolution representative values of the average rotational latency are ms seek times are in the range of ms and data transfer rates are of the order of tens of megabytes per second variations in disk organization have been motivated by the desire to reduce the access time of a disk increase its capacity and data transfer rate and reduce its price the cheapest disk is a floppy disk which is slow and has a small capacity a hard disk has a higher capacity still higher capacities are obtained mainly through mounting of many platters on the same spindle one readwrite head is provided for each circular surface of a platter that is one above and one below each platter all heads in the disk pack are mounted on a single access arm which is called the actuator and so at any moment all heads are located on identically positioned tracks of different surfaces the set of such identically positioned tracks outlines a cylinder see figure a form that can be exploited for data organization all the tracks in a cylinder are accessible from the same position of the access arm thus cylinders make several disk tracks accessible without requiring any movement of the disk heads and so io operations on records situated in the same cylinder can be performed without incurring seek times a hard disk can be looked upon as consisting of a set of concentric cylinders from the innermost to the outermost a records address can thus be specified by the triple cylinder number surface number record number the necessary commands for operation of a disk device are seek cylinder number surface number and readwrite a specified record disk capacity can be increased by increasing the number of platters however more platters require more disk heads which in turn require a heavier actuator and impose more mechanical stresses hence disks tend to have only a few platters when a very large capacity is desired applications use multiple disk drives in section we discuss how arrangements using multiple disk part file systems readwrite heads platter access arm cylinder track figure a disk pack drives can also be exploited to provide high data transfer rates and high reliability seek times can be reduced by using higher rotational speeds but high speeds increase the cost of mechanical components and so fast disks tend to have smaller platters to compensate pcs and desktop computers tend to use cheaper disks these disks have large platters which provide large capacity and comparatively low rotational speeds in contrast servers tend to use costlier disks that are smaller and rotate faster to optimize use of disk surface tracks are organized into sectors a sector is a standardsized slot in a track for a disk record the sector size is chosen to ensure minimum wastage of recording capacity due to interrecord gaps on the surface sectoring can be made a part of the disk hardware hard sectoring or could be implemented by the software soft sectoring data staggering techniques recall from section that the data read off an io device during a read operation is stored in the dma buffer from where the dma transfers it to memory as a single block but while this transfer is under way the disk continues to revolve and one or more following sectors may pass under the head by the time the transfer is completed hence if a read operation on the next consecutive sector is issued immediately after the previous one the required sector may have passed under the head by the time the dma can initiate the read operation such a read operation can be performed only in the next disk revolution analogously during a write operation recording of the data is initiated only after data is transferred from memory to the dma buffer so recording in the next sector can not take place in the same revolution if the sector passes under the readwrite head before the data transfer is completed a similar problem is caused by head switching time which is the time taken to switch operation between heads positioned on different platters by this time a few sectors of the next platter have passed under the readwrite head the seek time to move the head to the next cylinder also causes a similar problem all these problems adversely affect the throughput of a disk chapter implementation of file operations the techniques of sector interleaving head skewing and cylinder skewing address the problems caused by data transfer time head switch time and seek time respectively these techniques collectively called data staggering techniques ensure that the next consecutively numbered sector will not pass under the read write head before the head will be in a position to perform a readwrite operation on it so that the operation can be performed in the current revolution of the disk sector interleaving staggers sectors along a track in such a way that consecutively numbered sectors are separated by a few other sectors this arrangement permits the io operation for a sector to be completed by the time the sector with the next consecutive address passes under the head head skewing staggers the start of track positions on different platters of a cylinder so that the times when the last sector of a track and the first sector of the next track pass under their respective heads are separated by the head switch time cylinder skewing analogously staggers the start of track positions on consecutive cylinders to allow for the seek time after reading the last sector on a cylinder figure illustrates how the techniques of sector interleaving head skewing and cylinder skewing reduce rotational delays through data staggering it is assumed that the disk has five sectors in a track and uses ten platters so a cylinder has sectors in it for each data staggering technique the left and right parts of the figure show operation of the disk without and with data staggering the first line in each part shows which sectors pass under the readwrite heads of the disk at different times the next few lines show what activities involved in an io operation are in progress as the disk rotates they constitute a timing diagram for the io operation figure a illustrates sector interleaving we assume the disk head is positioned immediately before the first sector on the first cylinder where a file is stored so the command to read the first sector does not incur any seek or rotational latency reading of the sector into the dma buffer completes a little before time t and the transfer of this data to memory by the dma controller completes a little after time t the command to read the next sector is issued immediately after reading of the previous sector completes ie a little after time t by that time the head is positioned somewhere over sector so sector can not be read immediately a rotational delay now occurs that lasts until sector passes under the head in the next revolution of the disk ie until time t the right part of the figure shows the arrangement of sectors when sector interleaving is employed sectors and are separated by sector on the track when the command to read sector is issued the readwrite head is located almost at the end of sector now the rotational delay lasts only until time t when sector starts passing under the head figure b illustrates head skewing here we show the arrangement of sectors in the first two tracks allocated to a file the read command on sector which is the last sector on the first track is issued at time t the reading of this sector and transfer of the data to memory completes before time t so the read command for sector is issued some time before t however it involves head switching because t is located on a different track head switching is not completed before time t when sector starts passing under the head part file systems without data staggering with data staggering a sector interleaving sectors of first track seek rotational latency read memory transfer t t t t t t t t t t t t b head skewing sectors of first two tracks seek and head switching rotational latency read memory transfer t t t t t t t t t t t t t t c cylinder skewing sectors of first tracks of cylinders seek rotational latency read memory transfer t t t t t t t t t t t t t t figure effect of data staggering a sector interleaving b head skewing and c cylinder skewing chapter implementation of file operations so reading of sector can not be commenced immediately it has to wait until sector starts passing under the head in the next revolution of the disk at time t this rotational delay is reduced by staggering the recording on the second track by one sector position as shown in the right half of the figure now the reading of sector can commence at time t thus incurring a much smaller rotational delay figure c illustrates cylinder skewing here we show the arrangement of sectors in the first track of the first two cylinders allocated to a file the seek operation for reading sector results in movement of the readwrite head by one cylinder the seek operation completes a little before t however sector has passed under the readwrite head by that time hence a rotational delay is incurred until sector passes under the head in the next revolution at time t as shown in the right half of the figure data staggering by two sector positions enables sector to be read starting at time t sector interleaving had a dramatic impact on the throughput of older disks modern disks have controllers that transfer data to and from memory at very high rates so that sector interleaving is not needed however we discuss sector interleaving because it provides an insight into optimizing the peak disk throughput through data staggering head and cylinder skewing are still used to optimize the peak disk throughput figure illustrates sector interleaving the interleaving factor fint is the number of sectors that separate consecutively numbered sectors on the same disk track part b of figure illustrates the arrangement when fint ie consecutively numbered sectors have two other sectors between them interleaving is uniform that is each pair of consecutively numbered sectors are separated by the same number of sectors if either n or n is a multiple of fint where n is the number of sectors on a track the arrangement in the figure where there are sectors to a track is uniform whereas interleaving with fint or would not be uniform see the second column in table some consecutive sectors are separated by more than fint sectors as we shall see in example a performance penalty is incurred when interleaving is not uniform let tst be the time taken to transfer one sectors data between the dma controller and memory and let tsect be the time taken for one sector to pass under the disk head optimal performance is obtained if tst fint tsect since io on the next sector can be started immediately after the dma finishes transferring a b figure sectors in a disk track a without interleaving b with interleaving factor part file systems the previous sectors data if tst fint tsect the next sector would pass under the head before the dma finishes data transfer for the previous sector hence the next sector can be accessed only in the next revolution of the disk tst fint tsect implies that the disk would be idle for some time before the next sector is accessed in the same revolution disk throughput suffers in both these cases analogously throughput would suffer when other data staggering techniques are employed if data is staggered by too little or too much the following example illustrates the variation of peak disk throughput with the sector interleaving factor example sector interleaving a disk completes one revolution in ms and has sectors on a track each containing bytes the values of tst and tsect satisfy the relation tsect tst tsect to obtain the peak disk throughput for a value of fint we read the sectors in the order over and over again and observe the number of bytes transferred in one second figure shows variation of peak disk throughput for different values of fint table shows the arrangement of sectors for different values of fint and the corresponding disk throughput represented in units of kbs where kbs is bytes per second interleaving with fint or is not uniform for fint the arrangement of sectors on the track is after reading sector sector can not be read in the same revolution hence the disk takes ms to read sector similarly sectors and require ms sectors and are separated by sectors hence they can be read in the same revolution of the disk the disk takes only ms to read sector after sector has been read reading of sectors and requires ms each while reading of sector requires ms figure shows the variation of throughput with different values of fint fint is adequate to satisfy tst fint tsect and so the throughput increases sharply values of fint are counterproductive since the disk spends some idle time before the next sector passes under the head hence the throughput dips for fint peak disk throughput interleaving factor figure variation of throughput with sector interleaving factor chapter implementation of file operations table sector arrangement and performance in sector interleaving arrangement of average peak throughput fint sectors tio for sectors ms tio ms kbs disk attachment technologies eide and scsi interfaces enhanced integrated device electronics eide and small computer system interconnect scsi are the leading disk interfaces for attaching disks to computers disks attached this way have come to be called hostattached storage integrated device electronics ide also called advanced technology attachment or ata was the predecessor of eide before eide was developed the different features of ide and scsi made each of them ideal for specific applications for example ide was considered to provide excellent performance for sequential io while scsi was considered to be superior for random io accordingly ide disks were used in the lowcost pc and desktop environment while scsi disks were used in the server environment with eide the gap in randomaccess performance has narrowed considerably both retain their traditional niche areas but eide and scsi now compete in some application segments such as backup storage media both kinds of disks provide a large buffer of a few megabytes ide disks primarily worked with programmed io modes though they supported a dma mode as well eide supports new dma modes including the first party ie bus mastering dma mode the ultra ata mode of eide supports transfer rates of mb per second which is times faster than the ide data transfer rate eide disks use larger platters rotate relatively slowly and are cheap up to two disks can be connected to eide however only one of them can operate at a time scsi supports several dma modes the fastest of these provides a data transfer rate of mb per second scsi permits up to disks to be connected to it scsi is called an interface but technically it is an io bus because it permits simultaneous operation of many disks connected to it scsi disks are smaller rotate faster and are more expensive accordingly they provide smaller seek times and higher data transfer rates a scsi disk supports scattergather io wherein it can transfer data from a disk block into noncontiguous areas of memory or collect data from noncontiguous areas and write them into a disk block see section it also provides several part file systems functionalities that were traditionally performed by the iocs including the following disk scheduling a scsi disk accepts several io requests concurrently and stores them into a queue of requests it uses its knowledge of the current position of disk heads and the rotational position of the platters to select an io operation that involves the minimum delay due to seek and rotational latency this feature is described in section bad block recovery a scsi disk detects bad disk blocks and assigns substitute disk blocks for them it maintains a table showing addresses of bad blocks and their substitutes if an io command is directed toward a bad disk block the disk automatically redirects it at the substitute block this feature speeds up io operations by performing bad block management in the device rather than in the access method layer of iocs prefetching of data a scsi disk contains a buffer at every io operation it reads the next few disk blocks into the buffer this action speeds up subsequent read operations during processing of a sequential file networkattached storage and storage area networks host attachment of disks suffers from poor scalability because disk sizes are limited by prevailing technologies and the number of disks that can be attached to a host is limited by the interface therefore organizations have to constantly replace disks or add more servers to meet their requirements for more storage this problem is addressed by facilitating use of remote disks through a network this approach enables the storage capacity to be increased incrementally and seamlessly and storage to be shared by applications operating on many servers a networkattached storage nas is a disk or a redundant array of inexpensive disks raid which is discussed in the next section attached directly to a local area network lan see figure a nas is an inexpensive method of providing large disk capacities because it employs the hardware and software existing in a lan environment functionalities such as a file server or a distributed file system see chapter can be provided by using the nas however use of nas faces some difficulties in practice lans use protocols that optimize applicationtoapplication data transfers whereas the file server or distributed file system requires use of a filebased protocol like the sun nfs protocol discussed in section or microsofts common interface file system cifs protocol the load created by the filebased protocol slows down networking applications a storage area network san is an alternative arrangement that avoids slowdown of networking applications a san is a network composed of disks that provides a high bandwidth see figure b the network could be a dedicated fiber channel that uses the scsi protocol or an internet protocol ip network that uses the iscsi protocol several servers can be connected to a san each server can access the entire storage this feature facilitates formation of highperformance clusters of computer systems see section data integrity and availability is provided through the redundancy of disks and servers connected to the san chapter implementation of file operations clients clients networklocal area local area attached network lan network lan storage server server storage area network san a b figure a networkattached storage b storage area network new technologies that employ the iscsi protocol over an ip network to combine the features of the nas and san technologies are emerging these technologies support both blockaccessed san devices and fileaccessed nas devices without incurring the cost of a fiber channel raid computer users constantly clamor for disks with larger capacity faster access to data higher data transfer rate and higher reliability all these issues are addressed through arrangements involving multiple disks the redundant array of inexpensive disks raid technology was originally employed for providing large disk capacities at a low cost through use of several inexpensive disks however the recent trend is to enhance disk capacities through networkattached storage and storage area networks see section hence todays raid technology is used for providing fast access high data transfer rates and high reliability it is more appropriately called redundant array of independent disks the raid technology spreads the data involved in an io operation across several disks and performs io operations on these disks in parallel this feature can provide either fast access or a high data transfer rate depending on the arrangement employed high reliability is achieved by recording redundant information however the redundancy employed in a raid is qualitatively different from that employed in conventional disks a conventional disk provides reliability only by writing a crc at the end of every record see section whereas redundancy techniques in a raid employ extra disks to store redundant information so that data can be recovered even when some disks fail access to redundant information does not cost additional io time because both data and redundant information can be accessed in parallel part file systems recording in a raid is performed as follows a disk strip is a unit of data on a disk which can be a sector a disk block or a disk track identically positioned disk strips on different disks form a disk stripe a file is allocated an integral number of disk stripes the data residing in the strips of the same stripe can be read or written simultaneously because they exist on different disks if the disk array contains n disks theoretically the data transfer rate could be n times that of a single disk practical values of data transfer rates depend on overhead and on any factors that may limit the parallelism of io operations while processing a file several raid organizations using different redundancy techniques and disk striping arrangements have been proposed these organizations are called raid levels table summarizes the properties of various raid levels raid levels and which are hybrid organizations based on raid levels and and raid level are the most popular raid organizations raid level level employs only disk striping it is not really a raid organization because it does not involve redundant recording of data it provides high data transfer rates particularly if each disk is under a separate disk controller however it suffers from low reliability data becomes inaccessible even if a single disk is inoperative also lack of redundancy implies that data is lost if a disk fails and so reliability still has to be achieved by means other than the raid organization raid level level raid organization writes identical information on two disks it is called disk mirroring when a process writes or updates a record in a file one copy of the record is written on each disk this way raid incurs percent overhead however one copy of a record is guaranteed to be accessible even if a single fault occurs during a read the raid simply reads the copy that can be accessed earlier high data transfer rates can be achieved during read operations because both disks could operate in parallel when no errors arise hybrid organizations that use the features of raid levels and are often used in practice to obtain both high data transfer rates as in raid level and high reliability as in raid level raid employs disk striping as in raid and mirrors each stripe as in raid raid first mirrors each disk and then performs striping these organizations provide different kinds of fault tolerance in raid a single error in a copy of a stripe makes the entire copy inaccessible so errors in both copies of a stripe would make the stripe inaccessible in raid an error on one disk would be tolerated by accessing its mirror disk a stripe would become inaccessible only if both a disk and its mirror disk have errors raid level this raid organization uses bit striping ie it stores each bit of data or redundancy information on a different disk when data is to be written the ith data strip contains the ith bit of each byte and a parity strip contains one of the parity bits computed from corresponding bits in all strips of the stripe an error correcting code is used to compute and store redundancy information for each byte see section thus disks are used to record the bits of a byte chapter implementation of file operations table raid levels level technique description level disk striping data is interleaved on several disks during an io operation the disks are accessed in parallel potentially this organization can provide an nfold increase in data transfer rates when n disks are used level disk mirroring identical data is recorded on two disks during reading of data the copy that is accessible faster is used one of the copies is accessible even after a failure occurs read disk disk operations can be performed in parallel if errors do not arise level error correction codes redundancy information is recorded to detect and correct errors each bit of data or redundancy information is stored on a different disk and is read or written in parallel d d p p provides high data transfer rates level bitinterleaved parity analogous to level except that it uses a single parity disk for error correction an error that occurs while reading data from a disk is detected by its device controller the d d p parity bit is used to recover lost data level blockinterleaved parity writes a block of data ie consecutive bytes of data into a strip and computes a single parity strip for strips of a stripe provides high data transfer rates for large read d d p operations small read operations have low data transfer rates however many such operations can be performed in parallel level blockinterleaved analogous to level except that the parity information distributed parity is distributed across all disk drives prevents the parity disk from becoming an io bottleneck as in level also provides better read performance than level level p q redundancy analogous to raid level except that it uses two independent distributed parity schemes supports recovery from failure of two disks d d p p note d and p indicate disks that contain only data and only parity information respectively indicates a strip indicates bits of a byte that are stored on different disks and their parity bits indicates a strip containing only parity information part file systems and a few more disks are used to record redundancy information for example the hamming code which is adequate for recovery from a single failure would require redundancy bits the raid arrangement employing this code would consist of data disks and disks containing redundancy information each storing bit of data or parity information this raid arrangement can readwrite data times faster than a single disk however it is expensive because several disks are needed to store redundancy information hence it is not practical raid level level employs disk striping with a bitinterleaved parity scheme ie it employs bit interleaving it writes the bits of a byte on different disks and employs a single parity bit per byte the data strips of a stripe are stored on data disks and the parity strip is stored on the parity disk thus raid level employs a significantly smaller amount of redundant information than raid level a read operation is performed as follows the disk controller checks whether an error exists within a strip if so it ignores the entire strip and recovers the data in the strip using the parity strip the value of a data bit is the modulo difference between the parity bit and the modulo sum of corresponding bits of other strips in the stripe all data disks participate in an io operation this feature provides high data transfer rates however it also implies that only one io operation can be in progress at any time another drawback of raid level is that parity computation can be a significant drain of the cpu power hence parity computation is offloaded to the raid itself raid level level is analogous to level except that it employs blockinterleaved parity each strip accommodates a block of data ie a few consecutive bytes of data if an io operation involves a large amount of data it will involve all data disks as in raid level hence raid level can provide high data transfer rates for large io operations a faultfree read operation whose data fits into one block will involve only a single data disk so small io operations have small data transfer rates however several such io operations can be performed in parallel a write operation involves computation of parity information based on data recorded in all strips of a stripe this can be achieved by first reading data contained in all strips of a stripe replacing the data in some of the strips with new data that is to be written computing the new parity information and writing the new data and parity information on all disks however this procedure limits parallelism because all disks are involved in the write operation even when new data is to be written into a single block blocki of stripe stripei hence the parity information is computed by a simpler method that involves the exclusive or of three items the old information in the parity block the old data in block blocki and the new data to be written in block blocki this way only the disks containing the blocks to be written into and the parity block are involved in the write operation and so several small faultfree read operations involving other disks can be performed in parallel with the write operation raid level level uses block level parity as in level but distributes the parity information across all disks in the raid this technique permits small chapter implementation of file operations write operations that involve a single data block to be performed in parallel if their parity information is located on different disks small faultfree read operations can be performed in parallel as in raid level hence this organization is particularly suitable for small io operations performed at a high rate larger operations can not be performed in parallel however the organization provides high data transfer rates for such operations it also provides higher peak disk throughput for read operations than level because one more disk can participate in read operations raid level this organization uses two independent distributed parity schemes these schemes support recovery from failure of two disks peak disk throughput is slightly higher than in level because of the existence of one more disk optical disks data is recorded on an optical disk by creating changes in reflectivity of the disk and it is read by a laser and a photosensitive assembly that picks up changes in reflectivity of the surface under the disk head a compact disc cd is an optical disk the disk writer stores a by causing a change in reflectivity compared with the data bit in the preceding position and stores a by retaining the same reflectivity as the preceding bit recording on a cd can be performed by various means massproduced prerecorded cds that contain music are produced by mechanical means they are called stamped cds recording can also be performed by using a laser beam a laserrecorded cd contains three layers a polycarbonate layer a polymer dye and a reflective metallic layer when a strong laser beam is directed at a spot on the cd it heats the dye and creates a permanent mark on the disk called a pit which has a lower reflectivity this is why the recording process is called burning a cd data is recorded in a shallow spiral groove on a cd that extends from the inside diameter of the disk to its outside diameter a cd contains spiral revolutions which are about microns apart each revolution is called a track speed control and absolute time information are prerecorded on a cd a cd contains several regions reserved for use by a cd recorder the power calibration area is used to calibrate the power of the writing laser the program memory area stores track information for all sessions in the cd it is followed by leadin program and leadout areas for each session a leadin area is a table of contents of a session it indicates the number of tracks track start and stop points and the length of the session the program area contains data tracks of the session the leadout area indicates end of a session two features of a cd are important from an operating system viewpoint recording of data and creation of a file system data is recorded in the form of sectors on a track a cdrom intended for computer use contains sectors of kb it has a capacity of about mb a dvd digital versatile disk on the other hand has a capacity of about gb data is recorded on either devicelevel io four functions are involved in implementing io at the level of an io device initiating an io operation performing readwrite operations checking the status of an io device and handling interrupts raised by devices the first three of these functions are performed through io instructions and io commands described in section table describes features in the computer system that support chapter implementation of file operations table computer system features supporting functions in devicelevel io function description of computer system feature supporting it initiating an the io instruction ioinit cu d commandaddress initiates io operation an io operation see example the ioinit instruction sets a condition code to indicate whether the io operation has been initiated successfully performing devicespecific io commands implement tasks like positioning readwrite of readwrite heads over a record and reading of a record checking the io instruction iostatus cu d obtains status information device status for an io device the information indicates whether the device is busy free or in an error state and cause of the error if any handling the interrupt hardware implements the interrupt action interrupts described in section the cpu is switched to the physical iocs when an io completion interrupt occurs these functions we assume that io operations are performed in the dma mode see section in section we discuss details of devicelevel io and in section we discuss the facilities provided by the physical iocs to simplify devicelevel io io programming we use the term io programming to describe all actions involved in performing an io operation to understand two key aspects of io programming namely io initiation and io completion processing we consider the program of figure which is an assembly language version of the following program in a higherlevel language read a b result a b the program uses a bare machine ie a computer system that does not have any software layers between the program and the machines hardware the program uses the flag ioflag to indicate whether the io operation is in progress it sets the ioflag to initiates an io operation and loops until the io operation completes before performing its computations io initiation when the ioinit instruction of figure is executed the cpu sends the device address to the dma controller the dma controller finds whether the device is available for the io operation and informs the cpu accordingly the cpu sets an appropriate condition code in the condition code field also called the flags field of the psw if the device is available the dma also starts the io operation by accessing and decoding the first io command the part file systems set ioflag to indicate that io is in progress retry ioinit cu d commands read a b bc cc inprogress branch if io initiation is successful bc cc retry loop if the device is busy bc cc error error inform system administrator inprogress comp ioflag check whether io is still in progress bc eq inprogress loop if io is in progress perform result ab commands io commands iointrpt set ioflag interrupt processing io is complete figure io programming ioinit instruction is now complete the io operation if initiated will proceed in parallel with the cpus execution of instructions in the next few instructions the program examines the condition code set by the ioinit instruction to handle any exceptional situations that might have occurred when the ioinit instruction was executed the instruction bc cc inprogress is a conditional branch instruction condition code cc would have been set if io initiation was successful in that event the io operation would have already started and so execution of the program is diverted to the instruction with the label inprogress condition code cc indicates that the device was busy so the program would retry the io instruction until io initiation succeeds condition code cc indicates that an io error occurred so the program would report the error to the system administrator these details are not shown in figure io completion processing the program can not perform the computation result ab until the io operation completes however the programs execution can not be suspended because it is executing on a bare machine the program addresses this problem by using the flag ioflag to indicate whether the io operation has completed to start with it sets the value of ioflag to to indicate that io is in progress after starting the io operation it enters a loop at inprogress where it repeatedly checks this flag this is a busy wait see section when an io interrupt occurs indicating the end of the io operation control is transferred to the instruction with the label iointrpt by the interrupt action see section this is the start of the io interrupt servicing routine which changes ioflag to and returns this action ends the busy wait at inprogress the physical iocs the purpose of physical iocs is to simplify the code of user processes by hiding the complexity of io operations and to ensure high system performance it is achieved through the following three functions handling devicelevel io the physical iocs provides an interface for devicelevel io that eliminates the complexity of io programming discussed earlier in section synchronizing a process with completion of an io operation this synchronization avoids the busy wait following io initiation in figure and releases the cpu for use by other processes io scheduling the physical iocs schedules the io operations to be performed on a device in a suitable order to provide high device throughput handling devicelevel io while requesting initiation of an io operation a process needs to specify only the device address and details of the io operation the physical iocs initiates an io operation immediately if the io device is available otherwise it notes the request for io initiation and initiates it sometime later in either case control is returned to the process that made the io request when an interrupt arises the physical iocs notes which io operation has completed and initiates another operation on the io device if one is pending synchronizing a process with completion of an io operation the physical iocs provides an await io completion functionality to block a process until an io operation completes its parameters are the address of the io device and details of the io operation when a process invokes this functionality the physical iocs checks whether the io operation has already completed if it has not it requests the kernel to block the process this action avoids the busy wait of figure the state of the process is changed to ready when the io operation completes io scheduling the throughput of an io device can be computed as the number of bytes of data transferred per unit time or the number of io operations performed per unit time throughput can be optimized by minimizing the access times suffered during io operations in disk devices it can be achieved by reducing the rotational latency and mechanical motion of disk heads by performing io operations in a suitable order this function is called io scheduling it is performed automatically by the physical iocs it is not explicitly invoked by a process logical devices a logical device is an abstraction that is employed for a variety of useful purposes in the simplest case a logical device is merely a name for a physical io device use of a logical device in the code of a process solves a practical difficulty the address of a physical device that a process will use is not known when its code is part file systems written while creating a process that uses a logical device the kernel assigns a physical device to the logical device when the process performs an operation on the logical device the physical iocs implements the operation on the physical device assigned to the logical device a logical device can also be a virtual device as described in section in this case the kernel has to map the logical device into a part of a physical device many logical disks may be mapped into a physical disk in this manner the io operations directed at the logical disks would all be performed on the same physical disk physical iocs data structures the physical iocs uses the following data structures see figure physical device table pdt logical device table ldt io control block iocb io queue ioq the physical device table pdt is a systemwide data structure each entry in it contains information about one io device the ioq pointer field of an entry points to the queue of io operations that are to be performed on the device each entry in the queue is a pointer to an io control block iocb which contains information concerning one io operation the current operation field points to the io control block that contains information concerning the io operation logical physical device device device device ioq current name address address type pointer operation stdout disk stderr logical device physical device table ldt table pdt of process pi io queue ioq kernel space user space logical device io status name details flag io control block iocb figure data structures of the physical iocs chapter implementation of file operations that has been initiated on the device this information is useful in processing completion of the io operation the logical device table ldt is a perprocess data structure there is one copy of the ldt for every process in the system this copy is accessible from the process control block pcb of the process the ldt contains one entry for each logical device used by the process the field physical device address in the entry contains information concerning the current assignment if any for the logical device note that many logical devices possibly belonging to different user processes may be assigned the same physical device such as a disk an io control block iocb contains all information pertaining to an io operation the important fields in an iocb are logical device name io details and status flag the io details field contains the address of the first io command the status flag indicates whether an io operation is in progress or completed it is the equivalent of ioflag in figure the io queue ioq is a list of all io operations pending on a physical device each entry of the ioq contains a pointer to an io control block information in the ioq is used for io scheduling the pdt is formed at system boot time by obtaining details of all devices connected to the system the size of the ldt is specified at boot time an ldt is formed when a process is created an io control block is allocated when an io operation is to be initiated the ioq is shown as an array of pointers in figure however it is more practical to organize it as a linked list of iocbs the pdt ldt and ioq data structures are found within the kernel whereas a process creates an iocb in its own address space initializes its fields and uses it as a parameter in a call on a physical iocs module the iocbs presence in the address space of the process permits the process to check the status of an io operation without having to invoke the kernel organization of physical iocs figure shows organization of the physical iocs modules above the dashed line execute with the cpu in the user mode while those below this line execute with the cpu in the kernel mode the physical iocs is activated in one of two ways through calls on the physical iocs library modules startio or awaitio by a process with an io control block as a parameter through occurrence of an io completion interrupt when a process invokes startio startio invokes the io initiator through a system call the io initiator obtains the address of the physical device on which the io operation is to be performed enters the io operation in the ioq of the physical device and passes control to the io scheduler the io scheduler invokes the io initiator to start the io operation immediately if no other io operations exist in the ioq of the device control is then passed to the process scheduler which returns it to the process that had requested the io operation when the awaitio module of the physical iocs is invoked it determines the status of the io operation from the status flag of the io control block if part file systems process startio awaitio obtain ioq physical device address io io io io process interrupt completion scheduler initiator scheduler handler error data recovery control figure organization of the physical iocs the io operation is complete control is immediately returned to the process otherwise the awaitio module makes a system call to block the process at an io completion interrupt from a device an error recovery routine is invoked if an io error has occurred otherwise the status flag in the io control block describing the current operation on the device is set to completed the ecbpcb arrangement of example is used to activate a process if any awaiting completion of the io operation and the io scheduler is invoked it selects one of the io operations pending on the device and hands it over to the io initiator the io initiator initiates the io operation and passes control to the process scheduler implementation of physical iocs recall from section that the compiler replaces the file processing statements in a program with calls on the file system operations open read and close as seen in section the file system operation read makes a call on the iocs library module seqread seqread contains code that contributes to efficient processing of a file more about it later in this chapter this code makes a call on the physical iocs library module startio to perform devicelevel io the linker links all these modules of the file system iocs and the physical iocs with the compiled program a process representing execution of the linked program makes a call on the file system operation open to open a file named alpha open constructs a file control block fcb for alpha ie fcbalpha in the open files table oft and returns internal idalpha which is the offset of the fcb in the oft see section the following actions take place when the process wishes to read a record of alpha see figure the process calls the file system module read which invokes the iocs module seqread with internal idalpha as a parameter chapter implementation of file operations load reg seq read startio adopn internal idalpha adopn si intcode file system module iocs module physical iocs library read seq read module startio figure invocation of the physical iocs library module startio in a process when seqread decides to read a record of alpha it uses internal idalpha to access fcbalpha obtains the address of fmtalpha and finds the address of the disk block that contains the desired record it now forms an io control block for the io operation and calls startio with the address of the io control block as a parameter the io control block is named opn in figure startio loads the address of the io control block in a generalpurpose register and executes an si instruction with an appropriate code to invoke the physical iocs io initiation when invoked through a system call the physical iocs obtains the address of the iocb from the generalpurpose register and performs the following actions sets the status flag field of the iocb to in progress enters the address of the io control block in the ioq of the physical device initiates the io operation if the io device is not busy returns control to the process to enter the io control block address in the correct ioq the physical iocs extracts the logical device id from the iocb and accesses the logical device table ldt of the process to obtain the address of the physical device assigned to the logical device it then obtains the address of the ioq for the physical device from its entry in the physical device table pdt and adds the iocb address at the end of the ioq the io operation can be initiated immediately if there are no other entries in the ioq if other entries exist presumably one of the previous io operations is in progress so the io operation can not be initiated now io initiation is performed as described in section the status flag field of the io control block is used in a manner analogous to the use of ioflag in figure address of the io control block is stored in the current operation field of the devices entry in the physical device table io completion handling the io completion handler is implicitly invoked at the occurrence of an io completion interrupt the interrupt hardware provides the address of the physical device raising the io interrupt the io completion handler queries the device to obtain an io status code describing the cause of the interrupt it now performs the following actions if the io operation was device drivers in the physical iocs design described in previous sections the physical iocs handles io initiation io completion and error recovery for all classes of io devices within the system consequently addition of a new class of io devices requires changes to the physical iocs which can be both complex and expensive because the physical iocs may be a part of the kernel modern operating systems overcome this problem through a different arrangement the physical iocs provides only generic support for io operations and invokes a specialized device driver dd module for handling devicelevel details for a specific class of devices thus device drivers are not part of the physical iocs this arrangement enables new classes of io devices to be added to the system without having to modify the physical iocs device drivers are loaded by the system boot procedure depending on the classes of io devices connected to the computer alternatively device drivers can be loaded whenever needed during operation of the os this feature is particularly useful for providing a plugandplay capability figure illustrates how device drivers are used by the physical iocs the entry of a device in the physical device table pdt shows the name of its chapter implementation of file operations device dd ioq address name pointer tapedd tapedd diskdd physical device table table of pdt entry points ioq ioinit diskdd intproc figure use of device drivers device driver in the dd name field the diskdd the device driver for the system disk has been loaded at system boot time the tapedd would be loaded on demand so it is shown as a dashed box a device driver contains functionalities of the four physical iocs modules shown in figure namely io scheduler io initiator io completion handler and error recovery a table of entry points located at the start of its code contains start addresses of these functionalities when the physical iocs is invoked for initiating an io operation it locates the pdt entry of the device and performs the generic function of entering details of the io operation into the ioq of the device it now consults the dd name field of the pdt entry obtains the identity of the device driver and loads the device driver in memory if it is not already in memory it now obtains the address of the entry point for io initiator in the device driver by following the standard conventions and passes control to it the device driver performs io initiation processing and returns control to the physical iocs which passes control to the process scheduler when the physical iocs is invoked implicitly at an io interrupt it performs similar actions to identify the device driver entry point for handling interrupts and passes control to it after servicing the interrupt the device driver returns control to the physical iocs which passes it to the process scheduler devicelevel optimization one important optimization is disk scheduling to ensure good throughput which is discussed in the next section another optimization is reducing the number of seek operations in a disk this optimization can be performed in various ways one simple way is to read several adjoining disk blocks when a read operation is to be performed it amounts to buffering of data which is useful in sequential files device drivers for raid units reduce the number of seek operations by combining several io operations into a single one a device driver can also support a novel or nonstandard io device a good example of the former is a ram disk which is simply a virtual disk maintained in the ram of a computer system an area in ram is reserved for use as a disk all read and write operations directed at the disk are actually performed on relevant parts of the ram operation of the ram disk is extremely fast however data disk scheduling the seek time of a disk block depends on its position relative to the current position of the disk heads consequently the total seek time involved in performing a set of io operations depends on the order in which the operations are performed the throughput of a disk defined as the number of io operations performed per second also depends on the order in which io operations are performed hence the physical iocs and device drivers for disks employ a disk scheduling policy to perform disk io operations in a suitable order we shall discuss the following disk scheduling policies before describing disk scheduling in modern systems firstcome firstserved fcfs scheduling select the io operation that was requested earliest shortest seek time first sstf scheduling select the io operation whose seek time from the current position of disk heads is the shortest scan scheduling this policy moves the disk heads from one end of the platter to the other servicing io operations for blocks on each track or cylinder before moving on to the next one it is called a scan when the disk heads reach the other end of the platter they are moved in the reverse direction and newly arrived requests are processed in a reverse scan a variant called look scheduling reverses the direction of disk heads when no more io operations can be serviced in the current direction it is also called the elevator algorithm circular scan or cscan scheduling this policy performs a scan as in scan scheduling however it never performs a reverse scan instead it moves the heads back to that end of the platter from where they started and initiates another scan the circular look variant we will call it clook scheduling moves the heads only as far as needed to service the last io operation in a scan before starting another scan the fcfs disk scheduling policy is easy to implement but does not guarantee good disk throughput to implement the shortest seek time first sstf policy the physical iocs uses a model of the disk to compute the seek time of the disk block involved in an io operation given the current position of the disk heads however the sstf policy is analogous to the shortest request next srn scheduling policy so while it achieves good disk throughput it may starve some io requests sstf and the various scan policies can be efficiently implemented if the ioqs are maintained in sorted order by track number example describes the operation of various disk scheduling policies for a set of five io operations the look policy completes all io operations of this chapter implementation of file operations example in the shortest amount of time however none of these policies is a clear winner in practice because the pattern of disk accesses can not be predicted disk scheduling policies example figure summarizes the performance of the fcfs sstf look and clook disk scheduling policies for five io operations on a hypothetical disk having tracks the requests are made at different instants of time it is assumed that the previous io operation completes when the system clock reads ms the time required for the disk heads to move from track to track is assumed to be a linear function of the difference between their positions thm tconst track track tpt where tconst is a constant tpt is the pertrack head movement time and thm is the total head movement time we assume the rotational latency and data transfer times to be negligible tconst ms and tpt ms a practical value of tconst is ms also the formula for thm is not linear in practice figure shows the following details for each decision time at which the decision is made pending requests and head position at that time the scheduled io operation and its seek time the last column shows the total seek time for each policy the plots in the lower half of the figure show the disk head movement for each policy note that the total seek times in different scheduling policies vary greatly sstf is better than fcfs however look has the smallest total seek time in this example it is better than clook because it can reverse the direction of diskhead traversal after completing the io operation on track and service the operations on tracks and whereas clook starts a new scan with the operation on track scheduling in the disk itself can surpass scheduling in the physical iocs because the disk uses a more precise model that considers the seek time as well as the rotational latency of a disk block hence it can make fine distinctions between two io commands that would appear equivalent to the physical iocs as an example consider io commands that concern disk blocks that are n and n tracks away from the current position of the disk heads both commands have equal seek times the physical iocs would have to make a random choice between them however given the current rotational position of the platters and the position of the required disk block or sector the disk may find that the block that is n tracks away may already be passing under the heads by the time the heads are positioned on that track it would mean that the disk block can be read only in the next revolution of the disk the disk block that is n tracks away on the other hand might pass under the heads sometime after the heads have been positioned on that track hence its rotational latency would be smaller than that of the disk block that is n tracks away such finer distinctions can contribute to higher throughput of the disk part file systems tconst and tpt ms and ms respectively current head position track direction of last movement toward higher numbered tracks current clock time ms requested io operations serial number track number time of arrival scheduling details scheduling decisions seek policy details time fcfs time of decision pending requests head position selected request seek time sstf time of decision pending requests head position selected request seek time look time of decision pending requests head position selected request seek time clook time of decision pending requests head position selected request seek time track no time fcfs sstf look clook figure disk scheduling summary using the fcfs sstf look and clook policies os interaction with the computer and user programs to respond readily to events an os uses an arrangement in which every event causes an interrupt in this section we discuss how the os interacts with the computer to ensure that the state of an interrupted program is saved so that its execution can be resumed at a later time and how an interrupt servicing routine obtains information concerning the event that had caused an interrupt so that it can perform appropriate actions we also discuss how a program invokes the services of the os through a software interrupt a system call is the term used for this method of invoking os services controlling execution of programs to control execution of user programs the os has to ensure that various fields of the psw contain appropriate information at all times when user programs are in execution which includes the time when a new programs execution is initiated and also times when its execution is resumed after an interruption from the discussion in section the key points in this function are at the start of execution of a user program the psw should contain the following information a the program counter field pc field should contain the address of the first instruction in the program b the mode field m field should contain a such that the cpu is in the user mode part overview c the memory protection information field mpi field should contain information about the start address and size of the memory area allocated to the program d the interrupt mask field im field should be set so as to enable all interrupts when a user programs execution is interrupted the cpu state which consists of the contents of the psw and the generalpurpose registers should be saved when execution of an interrupted program is to be resumed the saved cpu state should be loaded into the psw and the generalpurpose registers the os maintains a table to contain information relevant to this function for now we will use the generic name program table for it in later chapters we will discuss specific methods of organizing this information such as the process control block pcb each entry in the table contains information pertaining to one user program one field in this entry is used to store information about the cpu state the kernel puts information mentioned in item into this field when the programs execution is to be initiated and saves the cpu state into this field when the programs execution is interrupted it achieves this by copying information from the saved psw information area when the program is interrupted information stored in this field is used while resuming operation of the program effectively relevant fields of the psw would contain the information mentioned in items bd whenever the cpu is executing instructions of the program interrupt servicing as mentioned in section for simplicity we assume that an interrupt vector has the same format as the psw the kernel forms the interrupt vectors for various classes of interrupts when the operating system is booted each interrupt vector contains the following information a in the mode m field to indicate that the cpu should be put in the kernel mode the address of the first instruction of the interrupt servicing routine in the program counter pc field a and the size of memory in the memory protection information mpi field so that the interrupt servicing routine would have access to the entire memory and an interrupt mask in the interrupt mask im field that either disables other interrupts from occurring or enables only higherpriority interrupts to occur in accordance with the philosophy of nested interrupt servicing employed in the operating system we discuss details of this philosophy later in this section figure contains a schematic of operation of the kernel it gets control only when an interrupt occurs so its operation is said to be interruptdriven the interrupt action actually transfers control to an appropriate interrupt servicing routine also called an isr which perform the actions shown in the dashed box it first saves information about the interrupted program in the program table for use when the program is scheduled again this information consists of the psw chapter the os the computer and user programs occurrence of an interrupt context save interrupt servicing event handling scheduling exit from kernel figure interruptdriven operation of the kernel table event handling actions of the kernel interrupt event handling action arithmetic exception abort the program memory protection violation abort the program software interrupt satisfy the programs request if possible otherwise note it for future action end of io operation find which program had initiated the io operation and note that it can now be considered for scheduling on the cpu initiate a pending io operation if any on the device timer interrupt update the time of the day take appropriate action if a specified time interval has elapsed saved by the interrupt action contents of gprs and information concerning memory and resources used by the program it is called the execution context or simply context of a program the action that saves it is called the context save action the interrupt servicing routine now takes actions appropriate to the event that had caused the interrupt as mentioned in section the interrupt code field of the saved psw provides useful information for this purpose table summarizes these actions which we call the event handling actions of the kernel the scheduling routine selects a program and switches the cpu to its execution by loading the saved psw and gprs of the program into the cpu depending on the event that caused the interrupt and the state of other programs it may be the same program that was executing when the interrupt occurred or it may be a different program part overview example illustrates interrupt servicing and scheduling when an interrupt occurs signaling the end of an io operation example interrupt servicing in a hypothetical kernel figure a shows the arrangement of interrupt vectors and interrupt servicing routines in memory while figure b shows contents of the psw at various times during servicing of an io interrupt the interrupt vectors are formed by the os boot procedure each interrupt vector contains the address of an interrupt servicing routine an interrupt mask and a in the mode field a user program is about to execute the instruction that exists at the address ddd in memory when an interrupt occurs signaling the end of an io operation on device d the leftmost part of figure b shows the psw contents at this time step of the interrupt action puts d in the ic field of the psw and saves the psw in the saved psw information area the saved psw contains a in the mode field ddd in the pc field and d in the ic field the contents of the interrupt vector for the io completion interrupt are loaded into the psw effectively the cpu is put in the kernel mode of operation and control is transferred to the routine that has the start address bbb which is the io interrupt servicing routine see the arrow marked aiin figure a and the psw contents shown in figure b the io interrupt servicing routine saves the psw and contents of the gprs in the program table it now examines the ic field of the saved psw finds that device d has completed its io operation and notes that the program that had initiated the io operation can be considered for scheduling it now transfers control to the scheduler see the arrow marked biin figure a the scheduler happens to select the interrupted program itself for execution so the kernel switches the cpu to execution of the program by loading back the saved contents of the psw and gprs see arrow marked ciin figure a the program would resume execution at the instruction with the address ddd see the psw contents in the rightmost part of figure b nested interrupt servicing figure a diagrams the interrupt servicing actions of example in the simplest form interrupt servicing routine a handles the interrupt and the scheduler selects the interrupted program itself for execution if another interrupt occurs however while interrupt servicing routine a is servicing the first interrupt it will lead to identical actions in the hardware and software this time execution of interrupt servicing routine a is the program that will be interrupted the cpu will be switched to execution of another interrupt servicing routine say interrupt servicing routine b see figure b this situation delays servicing of the first interrupt and it also requires careful coding chapter the os the computer and user programs memory aaa bbb ccc save psw save psw b and gprs and gprs select a program handle the handle the event event load gprs transfer to branch ccc branch ccc user program program interrupt io interrupt servicing servicing c scheduler routine routine io pc m pc m interrupt bbb ddd a vector d interrupt action im ic switches the cpu pc m when an program aaa io interrupt interrupt occurs vector im interrupt vectors saved psw kernel area information area kernel switches previous instruction the cpu to return ddd next instruction from interrupt servicing user program a interrupt vectors and interrupt servicing routines pc m pc m pc m pc m ddd bbb ccc ddd d d d ic ic ic ic before interrupt after action a after action b after action c b psw contents at various times figure servicing of an io interrupt and return to the same user program of the kernel to avoid a mixup if the same kind of interrupt were to arise again also see exercise however it enables the kernel to respond to highpriority interrupts readily operating systems have used two approaches to nested interrupt servicing some operating systems use the interrupt mask im field in the interrupt vector part overview user interrupt user interrupt interrupt program routine a program routine a routine b a b figure simple and nested interrupt servicing to mask off all interrupts while an interrupt servicing routine is executing see figure this approach makes the kernel noninterruptible which simplifies its design because the kernel would be engaged in servicing only one interrupt at any time however noninterruptibility of the kernel may delay servicing of highpriority interrupts in an alternative approach the kernel sets the interrupt mask in each interrupt vector to mask off less critical interrupts it services more critical interrupts in a nested manner such a kernel is called an interruptible kernel or a preemptible kernel data consistency problems would arise if two or more interrupt servicing routines activated in a nested manner update the same kernel data so the kernel must use a locking scheme to ensure that only one interrupt processing routine can access such data at any time user program preemption in the scheme of figure preemption of a user program occurs implicitly when an interrupt arises during its execution and the kernel decides to switch the cpu to some other programs execution recall from example that the interrupted programs context is stored in the program table so there is no difficulty in resuming execution of a preempted program when it is scheduled again system calls a program needs to use computer resources like io devices during its execution however resources are shared among user programs so it is necessary to prevent mutual interference in their use to facilitate it the instructions that allocate or access critical resources are made privileged instructions in a computers architecture this way these instructions can not be executed unless the cpu is in the kernel mode so user programs can not access resources directly they must make requests to the kernel and the kernel must access resources on their behalf the kernel provides a set of services for this purpose in a programmer view a program uses a computers resources through statements of a programming language the compiler of a programming language implements the programmer view as follows while compiling a program it chapter the os the computer and user programs memory kernel call openinfo compiled code of the program call readinfo open info library function system call to open file read info library function system call to read data a b figure a schematic of system calls a a program and b an execution time arrangement replaces statements concerning use of computer resources by calls on library functions that implement use of the resources these library functions are then linked with the user program during execution the user program calls a library function and the library function actually uses the resource through a kernel service we still need a method by which a library function can invoke the kernel to utilize one of its services we will use system call as a generic term for such methods figure shows a schematic of this arrangement the program shown in figure a opens file info and reads some data from it the compiled program has the form shown in figure b it calls a library function to open i the library function invokes the file this call is shown by the arrow marked the kernel service for opening a file through a system call see the arrow marked i the kernel service returns to the library function after opening the file which returns to the user program the program reads the file analogously through a i call on a library function which leads to a system call see arrows marked and i a system call is actually implemented through the interrupt action described earlier hence we define it as follows definition system call a request that a program makes to the kernel through a software interrupt we assume that the software interrupt instruction mentioned in section has the format si intcode part overview where the value of intcode which is typically an integer in the range indicates which service of the kernel is being requested a program interrupt occurs when a program executes this instruction and step of the interrupt action as shown in figure copies intcode into the interrupt code ic field of the psw the interrupt servicing routine for program interrupts analyzes the interrupt code field in the saved psw information area to know the request made by the program a system call may take parameters that provide relevant information for the invoked kernel service eg the system call to open a file in figure would take the filename info as a parameter and the system call to read data would take parameters that indicate the filename number of bytes of data to be read and the address in memory where data is to be delivered etc several different methods can be used to pass parameters parameters can be loaded into registers before the system call is made they can be pushed on the stack or they can be stored in an area of memory and the start address of the memory area can be passed through a register or the stack the next example describes execution of a system call to obtain the current time of day example system call in a hypothetical os a hypothetical os provides a system call for obtaining the current time let the code for this timeofday service be when a program wishes to know the time it executes the instruction si which causes a software interrupt is entered in the interrupt code field of the psw before the psw is saved in the saved psw information area thus the value d in the ic field of the saved psw in figure would be as shown in figure the interrupt vector for program interrupts contains aaa in its pc field hence the cpu is switched to execution of the routine with the start address aaa it finds that the interrupt code is and realizes that the program wishes to know the time of the day according to the conventions defined in the os the time information is to be returned to the program in a standard location typically in a data register hence the kernel stores this value in the entry of the program table where the contents of the data register were saved when the interrupt occurred this value would be loaded into the data register when the cpu is switched back to execution of the interrupted program in accordance with the schematic of figure we will assume that a program written in a programming language like c c or java calls a library function when it needs a service from the os and that the library function actually makes a system call to request the service we will use the convention that the name of the library function is also the name of the system call for example in example a c program would call a library function gettimeofday to obtain the time of day and this function would make the system call gettimeofday through the instruction si as described in example buffering of records to process the records in a sequential file using the physical iocs a process initiates a read operation on a record by invoking the startio module and immediately invokes the awaitio module to check whether the read operation has completed the awaitio module blocks the process until the io operation completes see section thus the process suffers a wait time for each record which affects its performance an access method for sequential files reduces the wait times faced by a process through the technique of buffering of part file systems records which tries to overlap the io and cpu activities in the process it is achieved through two means prefetching an input record into an io buffer or postwriting an output record from an io buffer where an io buffer or simply a buffer is a memory area that is temporarily used to hold the data involved in an io operation in prefetching the io operation to read the next record into a buffer is started sometime before the record is actually needed by the process it may be started while the process is engaged in processing the previous record this arrangement overlaps a part of the time spent in reading the next record with processing of the previous record which reduces the wait time for the next record in postwriting the record to be written is simply copied into a buffer when the process issues a write operation and the process is allowed to continue actual writing is performed from the buffer sometime later it can overlap with a part of processing of the next record we use the following notation while discussing the technique of buffering tio io time per record see eq tc copying time per record ie the amount of cpu time required to copy a record from one memory area to another tp processing time per record ie the amount of cpu time consumed by the process in processing a record tw wait time per record ie the amount of time for which the process has to wait before the next record is available for processing tee effective elapsed time per record ie the interval between the time when a process wishes to start processing a record and the time when the processing of the record is completed tw and tee are analogously defined for an output file consider a program that reads and processes records from a sequential file f we consider three versions of the program named unbufp singlebufp and multibufp that use zero one and n buffers n respectively we assume tio ms tp ms and tc ms figure illustrates the operation and performance of processes that represent executions of unbufp singlebufp and multibufp for convenience we assume a process to have the same name as the program it executes each column of the figure shows the code of a program illustrates the steps involved in reading and processing a record and shows a timing chart depicting performance of the process executing it the statements start an io operation and await io completion in the programs are translated into calls on the physical iocs modules startio and awaitio with appropriate operands the start io statement reads the next record of f if any into a memory area if there are no more records in f the endoffile condition is set when an await io statement is executed unbufp uses a single area of memory named recarea to read and process a record of file f see figure a it issues a read operation and chapter implementation of file operations programs program unbufp program singlebufp program multibufp start an io operation for start an io operation for for i to n read f recarea read f buffer start an io operation await io completion await io completion for read f bufi while not endoffilef while not endoffilef await io completion on begin begin buf process recarea copy buffer into recarea k start an io operation for start an io operation for while not endoffilef read f recarea read f buffer copy bufk into recarea await io completion process recarea start an io operation for end await io completion read f buf k end process recarea k k mod n await io completion on buf k end io copying and processing activites upunbufp spsinglebufp mpmultibufp recarea buffer buf up recarea buffer recarea buf recarea buf recarea buffer mp up sp recarea recarea recarea buffer recarea buf buffer buf recarea sp recarea mp recarea timing diagrams i io operation c copying p processing i c buf i i c c buffer buf i c c p p p recarea recarea recarea a b c figure unbuffered and buffered file processing note the endoffile condition is set when the statement await io completion is executed for an operation that tried to read past the end of a file part file systems awaits its completion before processing the record in recarea itself the timing diagram shows that io is performed on recarea from to ms and cpu processing of the record held in recarea occurs between ms and ms hence tw tio and tee tio tp this sequence of operations repeats times hence the elapsed time of the process is ms seconds figure b illustrates operation of singlebufp which uses a single buffer area named buffer the process issues a read operation to read the first record into buffer and awaits its completion it now enters the main loop of the program which repeats the following fourstep procedure times copy the record from buffer into recarea initiate an io operation on buffer process the record held in recarea await end of io operation on buffer as shown in the timing diagram of figure b the process faces an io wait in step until the read operation on buffer completes it now performs steps hence after copying the record into recarea it initiates a read operation for the second record and starts processing the first record these two activities proceed in parallel thus overlapping processing of the first record with io for the second record we depict this parallelism by drawing a rectangular box to enclose these two actions in the activities part of figure b step ie copying of the next record from buffer to recarea is performed only after both reading of the next record and processing of the current record complete it is once again followed by processing of a record and reading of the next record in parallel hence the wait time before processing each of records is tw tio tp tc if tio tp tc if tio tp and so buffering is more effective when tio tp for records effective elapsed time per record tee is given by tee tw tp tc max tio tp thus the process goes through three distinct phases the startup phase when the first record is read the steady state when a record is copied and processed while the next record is read in parallel and the final phase when the last record is copied and processed accordingly the total elapsed time of the process is given by total elapsed time tio number of records tee tc tp from eqs and tee is ms and total elapsed time of the process is ms seconds if tio had been ms the total elapsed time of the process would have been seconds figure c illustrates operation of the process multibufp which uses buffer areas named buf buf buf n at the start of file processing chapter implementation of file operations multibufp initiates io operations on all n buffers inside the file processing loop it uses the buffers in turn following the four steps of the program loop for processing a record in a buffer the statement k k mod n ensures that the buffers are used in a cyclic manner the process waits for io to complete on the next buffer copies the record from the buffer into recarea invokes startio for reading the next record in the buffer and then processes the record in recarea presence of multiple buffers causes one significant difference between operations of multibufp and singlebufp consider processing of the first two records by multibufp see figure c when io on buf completes multibufp would copy the first record from buf into recarea and start processing it a read operation on buf would have been requested earlier so the physical iocs would initiate this read operation when the io on buf completes hence this operation would overlap with the copying out of the first record from buf in figure c we depict this parallelism as follows the dashed rectangular box around copying and processing of the record from buf is meant to indicate that these actions are performed sequentially the rectangular box enclosing this box and the io operation on buf indicates that these two activities are performed in parallel accordingly the effective elapsed time per record is given by tw tio tp if tio tc tp tc if tio tc tp tee maxtio tc tp from eq tee ms the total elapsed time which is governed by eq is ms seconds which is marginally better than singlebufps elapsed time of seconds the ratio of the elapsed times of unbufp and multibufp is the speedup factor due to use of multiple buffers considering the steady state the speedup factor is approximately tio tp max tio tc tp from eq it can be seen that its best value is obtained when tio tc tp this value has the upper bound of consider the operation of multibufp when more than one buffer is used figure illustrates a typical situation during execution of multibufp the cpu has recently copied the record from buf i into recarea and started an io operation on buf i thus io operations have been initiated on all n buffers some of the io operations specifically those on bufi bufj are already complete io is currently in progress for bufj while bufj bufn buf bufi are currently in the queue for io initiation thus j i buffers are full at the moment io is in progress for one buffer and n j i buffers are in the queue for io the value of j i depends on the values of tio and tp if tio tp ie if the io operation for a record requires less time than its processing we can see blocking of records in unbuffered processing of a file by a process the time spent in performing io operations may dominate the elapsed time of the process even in buffered chapter implementation of file operations processing of a file tw if tio tp or tio tc tp see eqs and thus both unbuffered and buffered processing of files would benefit from a reduction in tio the technique of blocking of records reduces the effective io time per record by reading or writing many records in a single io operation from eq tio ta tx hence a program that processes two records from a file that does not employ blocking would incur the total io time of ta tx if blocking is employed and a process reads or writes two records in a single io operation the total io time would reduce to ta tx logical and physical records when several records are read or written together it is necessary to differentiate between how file data is accessed and processed in a process and how it is written on an io device a logical record is the unit of file data for accessing and processing in a process a physical record also called a block is the unit of data for transfer to or from an io device the blocking factor of a file is the number of logical records in one physical record a file is said to employ blocking of records if the blocking factor is greater than figure shows a file that uses a blocking factor of note that when blocking is employed interrecord gaps on the io media separate physical records ie blocks rather than logical records deblocking actions a read operation on a file containing blocked records transfers m logical records to memory where m is the blocking factor actions for extracting a logical record from a block for use in a process are collectively called deblocking actions figure shows a program that manipulates a file with blocked records in an unbuffered manner the main loop of the program reads one physical record in each iteration it contains an inner loop that extracts logical records from a physical record and processes them thus an io operation is initiated only after interrecord interrecord gap gap logical logical logical record record record physical record ie block figure a file with blocking factor start an io operation for read f recarea await io completion while not endoffilef for i to m extract i th record in recarea and process it start an io operation for read f recarea await io completion end figure processing of a file with blocked records in an unbuffered manner part file systems m records are processed a similar logic can be incorporated into the programs of figures b c to achieve buffered processing of a file containing blocked records choice of blocking factor generalizing on the previous discussionwe can say that if slr and spr represent the size of a logical and a physical record respectively spr m slr the io time per physical record tio pr and the io time per logical record tiolr are given by tiopr ta m tx tiolr ta tx m thus blocking reduces the effective io time per logical record which would benefit both buffered and unbuffered processing of a file if tx tp with an appropriate choice of m it is possible to reduce tiolr such that tiolr tp once it is achieved from eqs and it follows that buffering can be used to reduce the wait time per record to tc the next example illustrates how tiolr varies with the blocking factor example blocking of records table shows the variation of tiolr with m for a disk device with ta ms transfer rate of kbs where kbs bytes per second and slr bytes tx the transfer time per logical record is ms ie ms tiopr and tiolr are computed according to eqs and if tp ms m makes tiolr tp the value of m is bounded on the lower side by the desire to make tiolr tp on the higher side it is bounded by the memory commitment for file buffers and the size of a disk track or sector a practical value of the blocking factor is the smallest value of m that makes tiolr tp the next example illustrates processing of a file employing both blocking and buffering of records table variation of tiolr with blocking factor blocking ta m tx tiopr tiolr factor m block size ms ms ms ms access methods as mentioned in section an access method provides support for efficient processing of a class of files that use a specific file organization for the fundamental file organizations discussed in section the iocs may provide access methods for the following kinds of processing unbuffered processing of sequentialaccess files buffered processing of sequentialaccess files processing of directaccess files unbuffered processing of index sequentialaccess files buffered processing of index sequentialaccess files access methods for buffered processing of sequentialaccess and index sequentialaccess files incorporate the buffering technique illustrated in figure c these access methods also optionally perform blocking of records using the technique shown in figure we assume that each access method module provides three entry points with the following parameters amopen internalid amclose internalid amreadwrite internalid recordinfo ioarea addr disk and file caches a generic technique of speeding up access to file data is to use a memory hierarchy consisting of a part of memory and files stored on a disk recall from the principles of memory hierarchies discussed in section that memory would contain some parts of the file data stored on the disk other parts would be loaded in memory when required in essence memory would function as a cache between files on the disk and processes both physical iocs and access methods use this chapter implementation of file operations principle the physical iocs uses a disk cache which treats all files stored on a disk uniformly and holds some data of some files in memory at any time an access method on the other hand uses a file cache which focuses on keeping some part of the data in a specific file in memory the access method maintains a separate file cache for each file the unit of data kept in a disk or file cache is typically a few consecutive disk blocks for simplicity we assume it to be a single disk block we will call the memory area used to store a unit of data a buffer the cache is thus a collection of buffers managed in the software each buffer has two parts the header part indicates what data is contained in it and the data part actually contains data the header contains the following information address of the disk blocks from where data has been loaded in the buffer a dirty flag information needed for performing replacement of data in the buffer such as the time of last reference made to it when a process issues a read operation it specifies the offset of the required data in the file the iocs determines the address of the disk block that contains the required data and searches the cache to check whether contents of that disk block are present in a buffer if so the required data is copied from the buffer into the address space of the process otherwise an io operation is initiated to load the data from the disk block into a buffer in the cache and it is copied into the address space of the process when the io operation completes when a process performs a write operation the iocs checks whether contents of the disk block that contains old values of the data are present in a buffer if so it copies the values to be written from address space of the process into the buffer and sets the dirty flag of the buffer to true otherwise it copies the disk block address and values of the data to be written into a new buffer and sets its dirty flag to true in either case contents of the buffer would be written on the disk by the procedure described in the following to facilitate speedy search in the cache the buffer headers are stored in an efficient data structure such as a hash table for example the hashwithchaining organization used in the inverted page table of the virtual memory handler could be adapted for use in the cache see figure in section in this organization the address of a disk block whose data is contained in a buffer is hashed to obtain an entry number in the hash table all buffers that contain disk blocks whose addresses hash into the same entry of the hash table are entered into a linked list called a chain and the hash table entry is made to point to the chain to check whether data from a disk block is present in the cache the address of the disk block is hashed to obtain an entry number in the hash table and the chain pointed to by this entry is searched to check whether a copy of the disk block is contained in one of the buffers if it is not present in the cache it is loaded in a free buffer in the cache and the buffer is added to the chain if the cache is full a policy such as lru replacement is employed to decide which buffer should be used to load the required data if the dirty flag of the buffer is true its contents would be written in the disk block whose address is contained in its header before new data is loaded in part file systems the buffer such an arrangement used in the unix buffer cache is described later in section loading of whole disk blocks which are a few kb in size in the cache captures spatial locality because data that adjoins previously accessed data would exist in the cache this effect is analogous to blocking of records discussed previously in section studies mentioned in section indicate that disk cache hit ratios of or more can be obtained by committing a small amount of memory to the disk cache a file cache can exploit temporal locality further by preloading the next few disk blocks of a sequentialaccess file in the cache which is analogous to buffering of records discussed in section use of a cache has some drawbacks too an io operation involves two copy operations one between the disk and the cache and the other between the cache and the address space of the process that initiated the io operation use of a cache also leads to poor reliability of the file system because modified data exists in a buffer in the cache until it is written to the disk this data will be lost in the event of a crash file cache a file cache is implemented in an access method and aims to provide efficient access to data stored in a file as shown in figure a the access method invokes the cache manager which checks whether the required data is available in the file cache it invokes the physical iocs only if the file cache does not already contain the required data the key advantage of a file cache over a disk cache is that the cache manager can employ filelevel techniques to speed up accesses to file data such a technique exploits properties of a files organization to speed up data accesses eg it can perform prefetching of data for sequentialaccess files however a key disadvantage is that a separate file cache has to be implemented for each file so the iocs has to decide how much memory to commit to each individual file cache disk cache the disk cache is implemented in the physical iocs or device driver of a disk its purpose is to speed up accesses to data stored on the disk as shown in figure b a request for an io operation is passed to the io scheduler only if the required data is not present in the disk cache the key advantage of a file file system system access cache file access method manager cache method cache disk physical physical manager cache iocs iocs io scheduler a b figure a file cache b disk caches unified disk cache apart from disk or file caches the os also maintains implicitly or explicitly another cache called the page cache in the virtual memory handler use of several caches may increase the number of copy operations that have to be performed to access data stored on a disk the time and memory overhead introduced by multiple copy operations motivates use of a unified disk cache figure a is a schematic diagram showing use of the disk cache and the page cache the page cache contains all code and data pages of processes that are presently in memory including pages of any memorymapped files a new page is loaded into the page cache when a page fault occurs since the page size is typically a few disk blocks this operation involves reading a few blocks from a program file or a swap file this is file io hence the disk blocks get read into the disk cache and they have to be copied into the page cache when a modified page is to be removed from memory it is first copied into the disk cache from there it is written to the disk sometime in the future thus two copy operations are involved in each pagein and pageout operation one copy operation between a disk and the disk cache and another between the disk cache and the demand paging process process memorymapped file file files accesses accesses page cache file system demand paging file system memorymapped files disk cache unified disk cache disk disk a b figure disk caching a separate disk and page caches b unified disk cache case studies unix unix supports two types of devices block devices and character devices block devices are randomaccess devices that are capable of reading or writing blocks of data such as various kinds of disks while character devices are serialaccess devices such as keyboards printers and mice a block device can also be used as a serial device unix files are simply sequences of characters and so are io devices so unix treats io devices as files thus a device has a file name has an entry in the directory hierarchy and is accessed by using the same calls as files viz open close read and write the unix iocs consists of two main components device drivers and a buffer cache these are described in the following sections device drivers a unix device driver is structured into two parts called the top half and the bottom half the top half consists of routines that initiate io operations on a device in response to open close read or write calls issued by a process while the bottom half consists of the interrupt handler for the device class serviced by the driver thus the top half corresponds to the io scheduler and io initiator modules in chapter implementation of file operations figure while the bottom half corresponds to the io completion handler and error recovery modules a device driver has an interface consisting of a set of predefined entry points into the device driver routines some of these are ddnameinit device driver initialization routine ddnamereadwrite routines to read or write a character ddnameint interrupt handler routine the ddnameinit routine is called at system boot time it initializes various flags used by the device driver it also checks for the presence of various devices sets flags to indicate their presence and may allocate buffers to them character io is performed by invoking the ddnameread and ddnamewrite routines the device driver has to provide a strategy routine for block data transfers which is roughly equivalent to the io scheduler shown in figure a call on the strategy routine takes the address of an io control block as a parameter the strategy routine adds this io control block to an ioq and initiates the io operation if possible if immediate initiation is not possible the io operation is initiated subsequently when an io completion interrupt occurs buffer cache the buffer cache is a disk cache as described in section it is organized as a pool of buffers where each buffer is the same size as a disk block each buffer has a header containing three items of information a device address disk block address pair gives the address of the disk block that is present in the buffer a status flag indicates whether io is in progress for the buffer and a busy flag indicates whether some process is currently accessing the contents of the buffer a hash table is used to speed up the search for a required disk block see figure the hash table consists of a number of buckets where each bucket points to a list of buffers when a disk block with address aaa is loaded into a buffer with the address bbb aaa is hashed with function h to compute a bucket number e haaa in the hash table the buffer is now entered in the list of buffers in the eth bucket thus the list contains all buffers that hold disk blocks whose addresses hash into the eth bucket hash table buffers bucket bucket bucket free list pointer bucket figure unix buffer cache part file systems the following procedure is used when a process pi performs a read operation on some file alpha form the pair device address disk block address for the byte required by pi hash disk block address to obtain a bucket number search the buffers in the bucket to check whether a buffer has a matching pair in its header if there is no buffer with a matching header allocate a free buffer put the device address disk block address information in its header enter the buffer in the list of the appropriate bucket set its status flag to io in progress queue the buffer for io and put pi to sleep on completion of io if a buffer with matching header exists return to pi with its address if flags indicate that the io operation on the buffer is complete and the buffer is not busy otherwise put pi to sleep on completion of a read operation on the buffer or buffer not busy condition if free buffers exist check whether the next disk block allocated to alpha is already present in a buffer if not allocate a free buffer to it and queue it for a read operation this procedure does not allocate buffers on a perprocess basis so processes that concurrently access a file can share the file data present in a buffer this arrangement facilitates unix file sharing semantics see section at the same time prefetching of data is performed on a perprocess basis by initiating an io for the next disk block of the file see step which provides buffering on a perprocess basis the benefits of blocking of records are inherent in the fact that a complete disk block is readwritten when any byte in it is accessed buffers in the buffer pool are reused on an lru basis as follows all buffers are entered in a free list a buffer is moved to the end of the list whenever its contents are referenced thus the least recently used buffers move toward the head of the free list in step the buffer at the head of the free list is allocated unless it contains some modified data that is yet to be written into the disk block in that case a write operation for the buffer is queued and the next buffer in the list is allocated example unix buffer cache figure illustrates the unix buffer cache disk blocks and hash into the first entry of the hash table hence they are entered in the linked list starting on this entry similarly and form the linked lists starting on the second and third entries of the hash table all buffers are also entered in the free list if a process accesses some data residing in disk block the buffer containing block is moved to the end of the free list if the process now accesses data in disk block the first buffer in the free list ie the buffer containing block is allocated if its contents have not been modified since it was loaded the buffer is added to an appropriate list in the hash table after block is loaded in it it is also moved to the end of the free list chapter implementation of file operations the effectiveness of the unix buffer cache has been extensively studied a study reported that a mb cache on an hp system provided a hit ratio of and a mb cache on another system provided a hit ratio of thus a comparatively small memory commitment to the buffer cache can provide a high hit ratio linux the organization of linux iocs is analogous to that of unix iocs thus blockand charactertype io devices are supported by individual device drivers devices are treated like files and a buffer cache is used to speed up file processing however many iocs specifics are different we list some of them before discussing details of disk scheduling in linux linux kernel modules which include device drivers are dynamically loadable so a device driver has to be registered with the kernel when loaded and deregistered before being removed from memory for devices the vnode data structure of the virtual file system vfs see section contains pointers to devicespecific functions for the file operations open close read and write each buffer in the disk cache has a buffer header that is allocated in a slab of the slab allocator see section dirty buffers in the disk cache are written to the cache when the cache is too full when a buffer has been in the cache for a long time or when a file directs the file system to write out its buffers in the interest of reliability io scheduling in linux uses some innovations to improve io scheduling performance a read operation needs to be issued to the disk when a process makes a read call and the required data does not already exist in the buffer cache the process would get blocked until the read operation is completed on the other hand when a process makes a write call the data to be written is copied into a buffer and the actual write operation takes place sometime later hence the process issuing a write call does not get blocked it can go on to issue more write calls therefore to provide better response times to processes the iocs performs read operations at a higher priority than write operations the io scheduler maintains a list of pending io operations and schedules from this list when a process makes a read or a write call the iocs checks whether the same operation on some adjoining data is pending if this check succeeds it combines the new operation with the pending operation which reduces the number of disk operations and the movement of disk heads thereby improving disk throughput linux provides four io schedulers the system administrator can choose the one that best suits the workload in a specific installation the noop scheduler is simply an fcfs scheduler the deadline scheduler uses look scheduling as its basis but also incorporates a feature to avoid large delays it implements look scheduling by maintaining a scheduling list of requests sorted by track numbers and selecting a request based on the current position of disk heads however part file systems look scheduling faces a problem when a process performs a large number of write operations in one part of the disk io operations in other parts of the disk would be delayed if a delayed operation is a read it would cause considerable delays in the requesting process to prevent such delays the scheduler assigns a deadline of second to a read operation and a deadline of seconds to a write operation and maintains two queues one for read requests and one for write requests according to deadlines it normally schedules requests from the scheduling list however if the deadline of a request at the head of the read or write queue expires it schedules this request and a couple of more requests from its queue out of sequence before resuming normal scheduling the completely fair queuing scheduler maintains a separate queue of io requests for each process and performs round robin between these queues this approach avoids large delays for processes a process that performs synchronous io is blocked until its io operation completes such a process typically issues the next io operation immediately after waking up when look scheduling is used the disk heads would most probably have passed over the track that contains the data involved in the next io operation so the next io operation of the process would get serviced only in the next scan of the disk this causes delays in the process and may cause more movement of the disk heads the anticipatory scheduler addresses this problem after completing an io operation it waits a few milliseconds before issuing the next io operation this way if the process that was activated when the previous io operation completed issues another io operation in close proximity to the previous operation that operation may also be serviced in the same scan of the disk file processing in windows the schematic of figure shows the file processing arrangement used in windows the cache manager performs file caching the io manager provides generic services that can be used to implement subsystemspecific io operations through a set of device drivers and also performs management of io buffers as described in section subsystem dlls linked to a user application invoke functions in the io manager to obtain subsystemspecific io behavior the vm manager was described in section the file cache is organized as a set of cache blocks each of size kb the part of a file held in a cache block is called a view a virtual address control block vacb describes each view it contains the virtual address associated with the view the offset of its first byte in the file and the number of read or write operations currently accessing the view presence of the virtual address and file offset information in the vacb helps in implementing file sharing semantics it ensures that processes making concurrent accesses to a file would see the result of the latest update operation irrespective of whether the file was memorymapped or accessed directly the cache manager sets up a vacb index array for a file when the file is opened for a sequential file the index array would contain only one pointer that points to the vacb covering the current offset into the file for chapter implementation of file operations user application dll cache io request file manager system driver data cache driver transfer flushing stacks data disk vm transfer driver manager page loading io manager figure file processing in windows a random file the vacb index array would contain pointers to vacbs that cover several recent accesses made to the file an io operation is performed by a layered device driver it is represented as a linked list of device drivers called a driver stack when a thread requests an io operation the io manager constructs an io request packet irp for it and passes it to the first device driver in the appropriate driver stack the device driver may perform the complete io operation itself write a status code in the irp and pass it back to the io manager alternatively it may decide on additional actions required to complete the io operation write their information in the irp and pass the irp to the next device driver in the stack and so on until the io operation actually gets implemented this model permits device drivers to be added to provide additional features in the io subsystem for example a device driver could be added between the file system driver which we discuss in the following and the disk driver to perform disk mirroring such a driver is called a filter driver drivers such as the disk driver are called function drivers they contain functionalities for initialization scheduling and initiation of io operations interrupt servicing and dynamic addition of new devices to facilitate the plugandplay capability a file system is also implemented as a file system driver fsd it invokes other drivers that implement the functionalities of the access method and the device drivers this arrangement permits many file systems to coexist in the host the io manager thus provides the functionalities of a virtual file system see section when a subsystem dll linked to a thread requests a file operation the io manager invokes an appropriate file system driver to handle the request the request typically contains the byte offset of the file data involved in the io operation the file system driver consults the file map table for the concerned file which is accessible from the files base file record in the master file table mft to convert the byte offset within the file into a byte offset within part file systems a data block on a device and invokes the device driver for it if the concerned device is a raid the device driver is actually a volume manager which manages the raid it converts the byte offset within a data block into one or more units containing a disk number sector number and a byte offset and passes the request to the disk driver windows supports striped volumes which are level raid systems mirrored volumes which are level raid systems and level raid systems in this manner it supports spanned volumes described in section analogously when a thread makes a request to read from a file the io manager passes this request to the file system driver which passes it to the cache manager the cache manager consults the vacb index array for the file and determines whether the required bytes of the file are a part of some view in the cache if not it allocates a cache block creates a view that covers the required bytes from the file in the cache block and constructs a vacb for it this operation involves reading the relevant part of the file into the cache block the cache manager now copies the required data from the cache block into the callers address space converse actions are performed at a write request if a page fault arises while copying data to or from the callers address space the virtual memory manager invokes the disk driver through the file system to read the required page into the memory this operation is performed in a noncached manner thus a file system must support both cached and noncached file io to facilitate efficient manipulation of metadata the file system driver uses kernellevel readwrite operations which access the data directly in the cache instead of first copying it tofrom the logical address space of the file system driver the cache manager keeps information about the last few io requests on a file if it can detect a pattern from them such as sequential accesses to the file it prefetches the next few data blocks according to this pattern it also accepts hints from user applications concerning the nature of file processing activities and uses them for the same purpose file updates take place in an asynchronous manner the data to be written into a file is reflected into the view of the file held in the cache manager once every second the lazy writer which is a system thread created by the cache manager queues onequarter of the dirty pages in the cache for writing on a disk and nudges the virtual memory manager to write out the data recall that an os typically finds out the devices connected to it at boot time and builds its device data structures accordingly this arrangement is restrictive as it requires rebooting of the system when a device is to be connected to it or disconnected from it windows supports a plugandplay pnp capability which permits devices to be connected and disconnected to the system dynamically it is achieved by coordinating the operation of io hardware the operating system and the concerned device driver the hardware cooperates with the boot software to construct the initial list of devices connected to the system and also coordinates with the pnp manager when devices are added or disconnected the pnp manager loads a device driver for a new device if necessary determines the resources such as specific interrupt numbers that may be required for its operation and ensures the absence of conflicts by assigning or reassigning required resources it now summary during a file processing activity the file system io operation has completed which enables the implements sharing and protection of files while kernel to switch the cpu to another process it the input output control system iocs actually uses disk scheduling to perform the io operations implements file operations the iocs is itself directed at a disk in an order that would reduce the structured into two layers called access methods movement of readwrite heads of the disk which and physical iocs that ensure good performance increases the throughput of the disk and reduces of a file processing activity and good throughput the average wait time of io operations of io devices respectively in this chapter we studan access method improves the performance of ied the techniques employed by the access methods a file processing activity within a process through and the physical iocs the techniques of buffering and blocking of records good throughput of an io device is achieved a buffer is a memory area used to temporarily hold through joint actions of the io device and the data that has been read off a device or that is to be iocs the io device is designed such that it written on it for an input file the technique of is reliable and io operations incur short access buffering tries to prefetch the data so that it would time which is the time spent in positioning the be available to a process without having to perform io media or the readwrite heads prior to data an io operation which reduces or eliminates the transfer and achieve high data transfer rates wait time for an output file it copies the data into data staggering techniques disk attachment techthe buffer and lets the process continue its opernologies and redundant arrays of inexpensive disks ation the actual writing is performed later the raid are relevant in this context technique of blocking reads more data off a device even with fast access and high data transin a single io operation than required by a process fer rates of io devices a process performing an at a time it reduces the number of io operations io operation incurs considerable wait time until to be performed the io operation completes the physical iocs caching is the technique of keeping some of provides two basic capabilities to enhance systhe file data in memory so that it can be accessed tem performance it blocks a process until its without having to perform an io operation part file systems caching reduces the number of io operations the virtual memory handler also uses a cache performed to access data stored in files thereby called a page cache which contains pages of proimproving performance of file processing activicesses to improve virtual memory performance ties in processes and also improving performance however since the swap areas of processes are of the system the physical iocs implements a implemented on a disk use of the page cache disk cache to reduce the number of io operaand the disk cache involves copying of pages tions performed for accessing the files stored on between the two caches which consumes cpu time a disk an access method implements a file cache and ties up memory because of multiple copies of a to reduce the number of io operations performed page operating systems therefore use a unified disk during the processing of an individual file in a cache to reduce copying and eliminate the need for process multiple copies of pages test your concepts classify each of the following statements as true ii increase the capacity of a disk or false iii none of iii a when parity bits are used for reliable recordb data staggering techniques are used to ingreading of data an error correction code i reduce the number of disk operations requires more parity bits than an error detecwhile a file is processed tion code ii reduce disk head movement between disk b restricting the disk space allocation for a file blocks having adjoining addresses to a cylinder group which is a group of coniii reduce rotational delays while disk secutive cylinders on a disk reduces disk blocks having adjoining addresses are head movement while the file is processed accessed c raid level which uses blockinterleaved iv improve effectiveness of buffering and parity provides parallelism between small blocking of file records io operations c disk scheduling d blocking of records speeds up processing of i reduces the number of io operations sequential files performed on a disk e buffering of records speeds up processing of ii reduces the average disk head movement directaccess files per io operation f the scan disk scheduling policy suffers iii aims at speeding up processing of a file from starvation d a program executes a read statement on a g the physical iocs provides a method to file alpha times during its execution avoid the busy wait condition while a process however only io operations are actually awaits completion of its io operation performed to read data from the file alpha h if tx tp it is possible to reduce tw to tc this is possible if through appropriate buffering and blocking i the access method used for the file i using a blocking factor of m reduces the alpha employs buffering without effective io time per logical record by a blocking factor of m ii the access method does not employ select the correct alternative in each of the blocking and the physical iocs does not following questions employ a disk cache a a disk cache is used to iii either the access method employs blocki reduce the number of io operations on ing or the physical iocs employs a disk a disk cache chapter implementation of file operations exercises explain how and whether buffering and blockalgorithm on the conventional disk described in ing of records is beneficial for the following kinds example of files a process manipulates a sequentialaccess file a a sequentialaccess file the io and processing times for each record in b an index sequentialaccess file the file are as follows c a directaccess file access time of device ms an update file is one that is read and modified transfer time per record ms during processing a program reads a record largest number of records records modifies it in memory and writes it back into required together the file processing time per record ms a which io device is best suited for recording an update file a if two buffers are used find the value of the b is buffering and blocking of records useful smallest blocking factor that can minimize for an update file the wait time per record justify your answers b if two buffers and a blocking factor of discuss how the throughput of a disk device are used what is the minimum number of can be optimized in a file system that performs records that are present in memory at any noncontiguous allocation of disk blocks to files time assume that a process initiates an io hint think of organization of blocks in the free operation on a buffer after processing the last list data staggering and cylinder groups record in it see figure a sectored disk has the following characteristics a sequential file is recorded by using blocking time for revolution ms a process manipulates it by using two buffers tsect ms the io and processing times are as follows tst ms access time average ms sector size bytes transfer time per record ms plot the peak disk throughput against the sector largest number of records records interleaving factor fint required together comment on the effectiveness of a a disk cache processing time per record ms and b a ram disk for speeding up processing determine optimal values of the blocking facof sequentialaccess and directaccess files tor and the number of buffers what changes if requests for io operations on the following any would you make in your design if the largest tracks are pending at time ms number of records that the process is likely to require together is i records ii records hint see example if the requests are made in the above order con one buffer is used in processing the file info of struct a table analogous to table for the exercise calculate the elapsed time of the disk of example process if the copying time per record is ms a biased disk is a hypothetical disk device whose explain your calculations seek time for track n is a linear function in n for classify the following statement as true or false example seek time n seqi is the set by judicious selection of the blocking factor of io operations requested over a certain period and the number of buffers it is always possible of time is the order in which io operations are to reduce the wait time to tc scheduled on a biased disk by the sstf algo a process is expected to open a file before accessrithm identical to the order in which the same ing it if it tries to access a file without opening io operations would be scheduled by a scan the file system performs an open before part file systems implementing the access a system programwith the special io technique in section mers handbook warns all programmers to open the speedup factor due to buffering was shown a file before accessing it or suffer a performance to have an upper bound of develop a forpenalty explain the nature and causes of the mula for speedup factor when a process does performance penalty not use buffers while processing a file containing how do different disk scheduling algorithms blocked records can the value of this speedup influence the effectiveness of io buffering factor exceed if so give an example a process manipulates an input file using many develop a formula for speedup factor when a buffers which of the following statements are process uses two buffers while processing a file accurate explain your reasoning containing blocked records and tp tx a of all the disk scheduling algorithms fcfs describe the implications of a file or disk cache disk scheduling is likely to provide the best for file system reliability unix supports a syselapsed time performance for the process tem call flush that forces the kernel to write b data staggering is effective only during buffered output onto the disk can a programreading of the first few records in the file it is mer use flush to improve the reliability of his not effective during reading of other records files in the file the lseek command of unix indicates the off a magnetic tape has a recording density of set of the next byte in a sequentialaccess file bitscm along a track the tape moves at to be read or written when a process wishes a velocity of meters per second while readto perform a read or write operation it issues an ingwriting data the interrecord gap is cm lseek command this command is followed by wide and the access time of the tape is ms a an actual read or write command sequential file containing records each of a what are the advantages of using the lseek size bytes is stored on this magnetic tape command calculate the length of the magnetic tape occub what is the sequence of actions the file system pied by the file and the total io time required and the iocs should execute when a process to read the file if the file is recorded a without issues an lseek command blocking and b with a blocking factor of show that division of the binary polynomial a process uses many buffers while manipulating formed from nd nc bits in a record where a file containing blocked records a system failnd is the number of data bits and nc is the ure occurs during its execution is it possible to number of crc bits by the crc polynomial resume execution of the process from the point will yield a remainder hint a term of xi of failure i nd in the polynomial for the nd the speedup factor resulting from the use of a bits of data is the term of xinc in the polynospecial io technique is the ratio of the elapsed mial for the nd nc bits in the record also note time of a process without blocking or buffering that modulo addition and subtraction produce of records to the elapsed time of the same process identical results bibliography tanenbaum describes io hardware ruemmler disk drives lumb et al discusses how backand wilkes presents a disk drive model that can ground activities like disk reorganization can be perbe used for performance analysis and tuning teorey formed during mechanical positioning of disk heads and pinkerton and hofri compare varfor servicing foreground activities and the effect of ious disk scheduling algorithms while worthington disk scheduling algorithms on effectiveness of this et al discusses disk scheduling for modern approach chapter implementation of file operations chen and patterson and chen et al chau a and a w fu a gracefully describe raid organizations while wilkes et al degradable declustered raid architecture with and yu et al discuss enhancements to raid sysnear optimal maximal read and write tems alvarez et al discusses how multiple failures parallelism cluster computing can be tolerated in a raid architecture while chau chen p m and d patterson and fu discusses a new layout method to evenly maximizing performance in a striped disk distribute parity information for declustered raid array proceedings of th annual international gibson et al discusses file servers for networksymposium on computer architecture attached disks nagle et al discusses integration may of userlevel networking with networkattached storage chen p m e k lee g a gibson nas curtis preston discusses nas and storr h katz and d a patterson age area networks sans while clark is devoted raid high performance reliable secondary to the san technology toigo discusses modern storage computing surveys disks and future storage technologies clark t designing storage area disk caching is discussed in smith networks a practical reference for implementing braunstein et al discusses how file accesses are fibre channel and ip sans nd ed addison speeded up when virtual memory hardware is used to wesley professional look up the file buffer cache curtis preston w using sans and nas mckusick et al discusses the berkeley fast oreilly sebastopolcalif file system for unix bsd bach and vahalia custer h inside the windows nt file discuss other unix file systems ruemmler and system microsoft press redmondwash wilkes presents performance studies concerning gibson g a d nagle k amiri f w chang various characteristics of disk accesses made in the unix e m feinberg h gobioff c lee b ozceri file system beck et al and bovet and cesati e riedel d rochberg and j zelenka discuss the io schedulers of linux love file server scaling with networkattached secure describes the io schedulers in linux custer disks measurement and modeling of computer describes the windows nt file system while systems russinovich and solomon discusses ntfs for hofri m disk scheduling fcfs vs windows sstf revisited communications of the acm alvarez g a w a burkhard f cristian iyer s and p druschel anticipatory tolerating multiple failures in raid scheduling a disk scheduling framework to architectures with optimal storage and uniform overcome deceptive idleness in synchronous io declustering proceedings of the th annual proceedings of the th acm symposium on international symposium on computer operating systems principles architecture lampson b atomic transactions in bach m j the design of the unix distributed systems architecture and operating system prenticehall englewood implementation an advanced course goos g cliffs nj and j hartmanis eds springer verlag berlin beck m h bohme m dziadzka u kunitz r magnus c schroter and d verworner love r io schedulers linux journal linux kernel programming pearson education new york love r linux kernel development nd bovet d p and m cesati understanding ed novell press the linux kernel rd ed oreilly sebastopol lumb c r j schindler g r ganger and calif d f nagle towards higher disk head braunstein a m riley and j wilkes utilization extracting free bandwidth from busy improving the efficiency of unix buffer caches disk drives proceedings of the th symposium on acm symposium on os principles operating systems design and implementation part file systems mckusick m k k bostic m j karels and teorey t j and t b pinkerton j s quarterman the design and a comparative analysis of disk scheduling implementation of the bsd operating system policies communications of the acm addison wesley reading mass nagle d g ganger j butler g gibson and toigo j avoiding a data crunch c sabol network support for scientific american networkattached storage proceedings of hot vahalia u unix internals the new interconnects frontiers prentice hall englewood cliffs n j ruemmler c and j wilkes unix disk wilkes j r golding c staelin and t sullivan access patterns proceedings of the winter the hp autoraid hierarchical storage usenix conference system acm transactions on computer ruemmler c and j wilkes an systems introduction to disk drive modeling ieee worthington b l g r ganger and y n patt computer scheduling algorithms for modern disk russinovich m e and d a solomon drives proceedings of the acm sigmetrics microsoft windows internals th ed microsoft conference on measurement and modeling of press redmond wash computer systems smith a j disk cachemiss ratio yu x b gum y chen r y wang k li analysis and design considerations acm a krishnamurthy and t e anderson transactions on computer systems trading capacity for performance in a disk array proceedings of the symposium on tanenbaum a s structured computer operating systems design and implementation organization rd ed prentice hall englewood cliffs n j overview of security and protection ensuring noninterference with the computations and resources of users is one of the three fundamental goals of an os mentioned in section a resource could be a hardware resource such as an io device a software resource such as a program or data stored in a file or a service offered by the os several kinds of interference can arise during operation of a computer system we call each of them a threat some of the threats depend on the nature of specific resources or services and the manner of their use while others are of a generic nature part file systems and io management unauthorized access to resources is an obvious threat in an os persons who are not registered users of a computer system may try to access its resources while registered users may try to access resources that they have not been authorized to use such persons may maliciously try to corrupt or destroy a resource this is a potent threat for programs and data stored in files a less obvious threat is interference in legitimate access of resources and services by users it tends to disrupt computational activities of users by preventing them from using resources and services of an os this threat is called denial of service in this chapter we discuss how an os counters generic threats and threats concerning programs and data stored in files operating systems use two categories of techniques to counter threats to data and programs security measures guard a users data and programs against interference from persons or programs outside the operating system we broadly refer to such persons and their programs as nonusers protection measures guard a users data and programs against interference from other users of the system table describes two key methods used by operating systems for implementing security and protection authentication which is aimed at security consists of verifying the identity of a person computerbased authentication rests on either of two kinds of assumptions one common assumption is that a person is the user he claims to be if he knows something that only the os and the user are expected to know eg a password it is called authentication by knowledge the other authentication method relies on things that only the user is assumed to possess for example biometric authentication is based on some unique and inalterable biological feature such as fingerprints retina or iris authorization is the key method of implementing protection it consists of granting an access table terminology used in security and protection of information term explanation authentication authentication is verification of a users identity operating systems most often perform authentication by knowledge that is a person claiming to be some user x is called upon to exhibit some knowledge shared only between the os and user x such as a password authorization authorization has two aspects granting a set of access privileges to a user for example some users may be granted read and write privileges for a file while others are granted readonly privileges verifying a users right to access a resource in a specific manner chapter security and protection login id and user authentication information kernel authentication authentication service token authentication process database requests and security responses requests and setup responses changes in privileges protection authentication setup token service and authorization resource yesno service manager authorization database figure generic security and protection setups in an operating system privilege for a resource to a user which is a right to access the resource in the specified manner see chapter and determining whether a user possesses the right to access a resource in a specific manner figure shows a generic scheme for implementing security and protection in an operating system the security setup is shown in the dashed box in the upper part of the figure it consists of the authentication service and the authentication database the authentication database contains a pair of the form login id validating information for every registered user of the operating system where the validating information is typically an encrypted form of a users password to log into the system a person submits his login id and password to the kernel the kernel passes this information to the authentication service which encrypts the password and compares it with the validating information for the user stored in the authentication database if the check succeeds the authentication service generates an authentication token for the user and passes it back to the kernel the authentication token is typically the user id assigned to the user whenever the user or a process initiated by the user makes a request to access a resource the kernel appends the users authentication token to the request to facilitate making of protection checks the protection setup is shown in the dashed box in the lower part of figure it consists of the authorization service and the authorization database the authorization database contains triples of the form authentication token resource id privileges when a user wishes to grant access privileges for one of his files to some users or withdraw some previously granted access privileges for the file he makes a request to the kernel as shown in figure the kernel passes on the request to the authorization service along with the authentication token for the user the authorization service now makes appropriate changes in the authorization database to access a resource a user or his process makes a resource request to the service and resource manager the request contains the part file systems and io management id of a resource the kind of access desired to it and the authentication token of the user the service and resource manager passes the request to the authorization service which determines whether the user possesses the privilege to use the resource in the desired manner and sends a yesno reply to the service and resource manager depending on this reply the service and resource manager decides whether the users request should be granted not all operating systems incorporate all the elements shown in figure in their security and protection setups for example in most modern operating systems the authorization information is typically maintained and used by the file system so the operating system does not maintain the authorization database and does not perform authorization the distinction between security and protection provides a neat separation of concerns for the os in a conventional operating system the security concern is limited to ensuring that only registered users can use the system a security check is performed when a person logs in it decides whether the person is a user of the os and determines his user id following this check all threats to information stored in the system are protection concerns the os uses the user id of a person to determine whether he can access a specific file in the os in a distributed system however security concerns are more complex because of the presence of the networking component see chapter we confine the discussion in this chapter to conventional operating systems only mechanisms and policies table describes mechanisms and policies in security and protection security policies specify whether a person should be allowed to use a system protection policies specify whether a user should be allowed to access a specific file both these policies are applied outside the os domain a system administrator decides whether a person should be allowed to become a user of a system and a user specifies what users may access his files security and protection mechanisms implement these policies by maintaining the authentication and authorization databases and using their contents to make specific checks during system operation table policies and mechanisms in security and protection security policy whether a person can become a user of the system the system administrator employs the policy while registering new users mechanisms add or delete users verify whether a person is a registered user ie perform authentication perform encryption to ensure confidentiality of passwords protection policy the file owner specifies the authorization policy for a file it decides which user can access a file and in what manner mechanisms set or change authorization information for a file check whether a file processing request conforms to the users privileges chapter security and protection table goals of computer security and protection goal description secrecy only authorized users should be able to access information this goal is also called confidentiality privacy information should be used only for the purposes for which it was intended and shared authenticity it should be possible to verify the source or sender of information and also verify that the information has been preserved in the form in which it was created or sent integrity it should not be possible to destroy or corrupt information for example by erasing a disk goals of security and protection table describes the four goals of security and protection namely secrecy privacy authenticity and integrity of information of the four goals only privacy is exclusively a protection concern an os addresses privacy through the authorization service and the service and resource manager see figure the authorization service verifies whether a user possesses the privilege to access a resource in a specific manner and the service and resource manager disallows requests that do not conform to a users privileges it is up to users to ensure privacy of their information by using this setup a user who wishes to share his data and programs with a few other users should set the authorization for his information according to the wellknown needtoknow principle only those persons who need to use some information for a legitimate function should be authorized to access it secrecy authenticity and integrity are both protection and security concerns as protection concerns secrecy authenticity and integrity are easy to satisfy because the identity of a user would have already been verified and the service and resource manager would use the authorization information which is a part of the protection setup shown in figure however elaborate arrangements are needed to satisfy secrecy authenticity and integrity as security concerns these are discussed in chapter security and protection threats to see how and when security and protection threats arise in an os first consider a conventional os its authentication procedures ensure that only registered users can log into the system and initiate processes hence the os knows which user has initiated a specific process and with that knowledge it can readily check whether a process should be allowed to use a specific resource when processes communicate with other processes os actions concerning communication are also confined to the same computer system hence an illegal access to a resource or a service by security attacks attempts to breach the security of a system are called security attacks and the person or the program making the attack is called an adversary or intruder two common forms of security attacks are masquerading assuming the identity of a registered user of the system through illegitimate means denial of service preventing registered users of the system from accessing resources for which they possess access privileges in a successful masquerading attack the intruder gains access to resources that the impersonated user is authorized to access hence he can corrupt or destroy programs and data belonging to the impersonated user at will the obvious way to launch a masquerading attack is to crack a users password and use this knowledge to pass the authentication test at log in time another approach is to perform masquerading in a more subtle manner through programs that are imported into a software environment we discuss this approach in section a denialofservice attack also called a dos attack is launched by exploiting some vulnerability in the design or operation of an os a dos attack can be launched through several means some of these means can be employed only by users of a system while others may be employed by intruders located in other systems many of these means are legitimate which makes it easy to launch dos attacks and hard for an os to detect and prevent them for example a dos attack can be launched by overloading a resource through phantom means to such an extent that genuine users of the resource are denied its use if the kernel of an os limits the total number of processes that can be created in order to control pressure on kernel data structures a user may create a large number of processes so that no other users can create processes use of network sockets may be similarly denied by opening a large number of sockets a dos attack can also be launched by corrupting a program that offers some service or by destroying some configuration information within the kernel eg use of an io chapter security and protection device can be denied by changing its entry in the physical device table of the kernel see section a network dos attack may be launched by flooding the network with messages intended for a particular server so that network bandwidth is denied to genuine messages and the server is so busy receiving messages that it can not get around to responding to any messages a distributed dos attack is one that is launched by a few intruders located in different hosts in the network it is even harder to detect and prevent than a nondistributed one many other security attacks are launched through the message communication system reading of messages without authorization which is also called eavesdropping and tampering with messages are two such attacks these attacks primarily occur in distributed operating systems so we discuss them in chapter trojan horses viruses and worms trojan horses viruses and worms are programs that contain some code that can launch a security attack when activated table summarizes their characteristics a trojan horse or a virus enters a system when an unsuspecting user downloads programs over the internet or from a disk on the contrary a worm existing in one computer system spreads to other computer systems by itself a trojan horse is a program that has a hidden component that is designed to cause havoc in a computer system for example it can erase a hard disk in the computer which is a violation of integrity collect information for masquerading or force a system to crash or slow down which amounts to denial of service a typical example of a trojan horse is a spoof login program which provides a fake login prompt to trick a user into revealing his password which can be used later for masquerading since a trojan horse is loaded explicitly by an unsuspecting user it is not difficult to track its authorship or origin table security threats through trojan horses viruses and worms threat description trojan horse a program that performs a legitimate function that is known to an os or its users and also has a hidden component that can be used later for nefarious purposes like attacks on message security or masquerading virus a piece of code that can attach itself to other programs in the computer system and spread to other computer systems when programs are copied or transferred worm a program that spreads to other computer systems by exploiting security holes in an os like weaknesses in facilities for creation of remote processes part file systems and io management a virus is a piece of code that infects other programs and spreads to other systems when the infected programs are copied or transferred a virus called an executable virus or file virus causes infection as follows the virus inspects the disk selects a program for infection and adds its own code which we will call the viral code to the programs code it also modifies the programs code such that the viral code is activated when the program is executed a simple way to achieve it is to modify the first instruction in the programs code ie the instruction whose address is the execution start address of the program see section to transfer control to the viral code when the viral code gets activated it inspects the disk looking for other programs to infect after infecting these programs it passes control to the genuine code of the program since the infection step does not consume much cpu time and the infected programs functioning is not affected a user has no way of knowing whether a program has been infected the way a virus attaches itself to another program makes it far more difficult to track than a trojan horse a virus typically sets up a back door that can be exploited for a destructive purpose at a later date for example it may set up a daemon that remains dormant until it is activated by a trigger which could be a specific date time or message and performs some destructive acts when activated different categories of viruses infect and replicate differently apart from the file virus described above a bootsector virus plants itself in the boot sector of a hard or floppy disk such a virus gets an opportunity to execute when the system is booted and gets an opportunity to replicate when a new bootable disk is made executable and bootsector viruses thrived when programs were loaded through floppies use of cds that can not be modified has curtailed their menace however newer viruses have switched to more sophisticated techniques to breach a computers defenses an email virus enters a computer system through an email and sends spurious mails to users whose email ids can be found in an address book the melissa virus of used a viral code that was a word document posted on an internet newsgroup the virus was triggered when a user opened a downloaded copy of the word document and it sent the document itself to persons whose email ids were found in the users address book the back door in this case was a tiny code fragment that was associated with the word document using the language called visual basic application vba it was triggered by the autoexecute feature of microsoft word which automatically executes the program associated with a word document when the document is opened the i love you virus of year was an email virus that attached viral code as an attachment in an email this code executed when some user doubleclicked on the attachment it sent emails containing its own copies to several others and then corrupted files on the disk of the host where it executed both melissa and i love you viruses were so powerful that they forced large corporations to completely shut off their email servers until the viruses could be contained viruses use various techniques to escape detection by antivirus software these techniques include changing their form compressing or encrypting their code and data hiding themselves in parts of the os etc chapter security and protection a worm is a program that replicates itself in other computer systems by exploiting holes in their security setup it is more difficult to track than a virus because of its selfreplicating nature worms are known to replicate at unimaginably high rates thus loading the network and consuming cpu time during replication the code red worm of spread to a quarter of a million hosts in hours using a buffer overflow attack the morris worm of spread to thousands of hosts through three weaknesses in the unix system the unix remote login facility rsh enabled a user to set up an arrangement through which he could log into a remote host without having to supply a password the worm searched for files that stored names of remote hosts that could be accessed through rsh and used these files to move to remote hosts the buffer overflow technique described later in section forces a daemon on an unprotected server to accept and execute a piece of code the morris worm used this attack on the finger daemon of a remote unix host to send its own code to the remote host and achieve its execution on that host it used the debug facility in the sendmail program of unix to mail a copy of its code to another host and execute it there the security attacks launched through trojan horses viruses or worms can be foiled through the following measures exercising caution while loading new programs into a computer using antivirus programs plugging security holes as they are discovered or reported loading programs from original disks on which they are supplied by a vendor can eliminate a primary source of trojan horses or viruses this approach is particularly effective with the compact disk cd technology since such disks can not be modified a genuine program can not be replaced by a trojan horse or a vendorsupplied disk can not be infected by a virus antivirus programs analyze each program on a disk to see if it contains any features analogous to any of the known viruses the fundamental feature it looks for is whether the execution start address of the program has been modified or whether the first few bytes of a program perform actions resembling replication eg whether they attach code to any programs on a disk os vendors post information about security vulnerabilities of their operating systems on their websites periodically and provide security patches that seal these loopholes a system administrator should check such postings and apply security patches regularly it would foil security attacks launched through worms the buffer overflow technique the buffer overflow technique can be employed to force a server program to execute an intrudersupplied code to breach the host computer systems security it has been used to a devastating effect in mail servers and other web servers the basic idea in this technique is simple most systems contain a fundamental vulnerability some programs do not validate the lengths of inputs they receive part file systems and io management from users or other programs because of this vulnerability a buffer area in which such input is received may overflow and overwrite contents of adjoining areas of memory on hardware platforms that use stacks that grow downward in memory eg the intel x architecture such overflows provide an opportunity to execute a piece of code that is disguised as data put in the buffer this code could launch a variety of security attacks figure illustrates how an intruder can launch a security attack through the buffer overflow technique a web server is in operation when one of its top of stack other local data start address of sample of beta variable beta end address bytes bytes of beta parameters copied into bytes beta occupy return address this area top of bytes stack local data of local data of calling function calling function direction direction of growth of growth stack before function stack after function sample is called sample is called how a buffer overflow can be used to launch a security attack the stack grows downward ie toward smaller addresses in memory it looks as shown on the left before the currently executing function calls function sample the code of the calling function pushes a return address and two parameters of sample onto the stack each of these occupies four bytes the code of sample allocates the variable beta and other variables on the stack the stack now looks as shown on the right notice that the start address of beta is at the low end of the memory allocated to it the end address of beta adjoins the last byte of the parameters the function sample copies bytes into the variable beta the first bytes contain code whose execution would cause a security violation bytes contain the start address of this code these four bytes overwrite the return address in the stack the function sample executes a return statement control is transferred to the address found in the stack entry that is expected to contain the return address effectively the code in variable beta is invoked it executes with the privileges of the calling function figure launching a security attack through the buffer overflow technique formal aspects of security to formally prove that a system can withstand all forms of security attacks we need a security model comprising security policies and mechanisms a list of threats a list of fundamental attacks and a proof methodology the list of attacks must be provably complete in the sense that it should be possible to produce any threat in the list of threats through a combination of the fundamental attacks the proof methodology should be capable of ascertaining whether the security model can withstand certain forms of attack early work in security was performed along these lines in the takegrant model of computer security landwehr processes were given privileges for objects and for other processes a privilege for an object entitled the holder of the privilege to access the object in a specific manner a privilege for another process entitled the holder of the privilege to take an access privilege possessed by the other process a take operation or to transfer one of its own access privileges to the other process a grant operation the proof took the form of ascertaining whether a specific process could obtain a specific access privilege for a specific object through a series of take and grant operations in the following example we discuss how a security flaw can be detected through the formal approach part file systems and io management example detection of a security flaw in an organization employing militarylike security all documents are classified into three security levels unclassified confidential and secret persons working in the organization are given security clearances called u unclassified c confidential and s secret with the proviso that a person can access all documents at his level of security classification and at lower levels of classification thus a person with c classification can access confidential and unclassified documents but is forbidden from accessing secret documents the organization uses a unix system and persons in the organization use unix features to access files containing documents this way it is expected that a program executed by a user can access a document at a specific security level only if the user possesses the appropriate security clearances to check whether document security is foolproof all operations in the system are modeled and a check is made to see whether a person can access a document that is at a higher level of classification than his security clearance it is found that a combination of indiscriminate assignment of the execute privilege for programs to users and use of the setuid feature of unix can enable a user to access a forbidden document it can happen because the setuid feature permits a user to execute a program with the privileges of the programs owner see section so if a user can execute a program owned by an individual with a higher security clearance he can take the security clearance of the programs owner this security flaw can be eliminated by either forbidding use of the setuid feature or confining the execute privilege for a program only to users whose security clearance is not lower than that of the programs owner the security flaw in example could also have been discovered through manual procedures however manual procedures become less reliable as systems grow more complex formal methods construct feasible sequences of operations and deduce or verify their properties this way they can discover sequences of operations that have disastrous consequences or assert that such sequences of operations do not exist the formal approach also has some drawbacks as the size of the system to be analyzed grows the computing and storage requirements of formal methods exceed the capabilities of contemporary computer systems the formal approach is also hard to apply because it requires a complete specification of a system and a comprehensive list of fundamental attacks it is not possible to develop such a list for modern operating systems it also requires a clear statement of security policies this requirement is hard to meet because most security policies consist of rules that are informally stated so that everyone in an organization can understand them however this is where the formal approach contributes substantially to the field of security it emphasizes the need for precise specifications encryption encryption is application of an algorithmic transformation to data when data is stored in its encrypted form only a user or his process that knows how to recover the original form of data can use it this feature helps in preserving confidentiality of data protection and security mechanisms use encryption to guard information concerning users and their resources however it could also be used to guard information belonging to users cryptography is the branch of science dealing with encryption techniques table summarizes key terms and definitions used in cryptography the original form of data is called the plaintext form and the transformed form is called the encrypted or ciphertext form we use the following notation pd plaintext form of data d cd ciphertext form of data d where pd d encryption is performed by applying an encryption algorithm e with a specific encryption key k to data data is recovered by applying a decryption algorithm d with a key k in the simplest form of encryption called symmetric encryption decryption is performed by using the same key k in advanced encryption techniques called asymmetric encryption a different key k is used to decrypt a ciphertext figure illustrates symmetric encryption we represent encryption and decryption of data by using algorithms e and d with key k as application of functions ek and dk respectively thus cd ekd pd dkcd obviously the functions ek and dk must satisfy the relation dkekd d for all d thus a process must be able to perform the transformation dk in order to obtain the plaintext form of encrypted data in practice encryption is performed by standard algorithms e and d hence effectiveness of encryption depends on whether an intruder can determine the encryption key through trial and error later in this section we see how it is impractical for an intruder to discover the encryption key because of the large number of trials involved however theoretically it is not impossible to do so this property makes encryption effective in a probabilistic sense though not in an absolute sense confidentiality of encrypted data follows from this property confidentiality provided through encryption also helps to verify integrity of data if the encrypted form of data is tampered with by an intruder its decryption by a process having the correct decryption algorithm and key would yield unintelligible data which would reveal that it has been altered in an unauthorized manner because of this property of encrypted data we use the term decryption is unsuccessful for the situation where decryption by the correct key yields unintelligible data part file systems and io management table cryptography terms and definitions term description encryption encryption is application of an algorithmic transformation ek to data where e is an encryption algorithm and k is an encryption key it is used to protect confidentiality of data the original data is recovered by applying a transformation dk where d is a decryption algorithm and k is a decryption key a scheme using kk is called symmetric encryption and one using k k is called asymmetric encryption plaintext data to be encrypted ciphertext encrypted form of plaintext confusion shannons principle of confusion requires that changes caused in a ciphertext due to a change in a plaintext should not be easy to find diffusion shannons principle of diffusion requires that the effect of a small substring in the plaintext should be spread widely in the ciphertext attacks on an attack is a series of attempts by an intruder to find a cryptographic decryption function dk in a ciphertext only attack the intruder systems can examine only a set of ciphertexts to determine dk in a known plaintext attack the intruder has an opportunity to examine the plaintext and ciphertext form of some data whereas in a chosen plaintext attack the intruder can choose a plaintext and obtain its ciphertext form to perform the attack oneway a function computation of whose inverse is expensive enough to function be considered impractical its use as an encryption function makes cryptographic attacks difficult block cipher a block cipher technique substitutes fixedsize blocks of plaintext by blocks of ciphertext it introduces some confusion but does not introduce sufficient diffusion stream cipher both a plaintext and the encryption key are considered to be bit streams bits in the plaintext are encrypted by using an equal number of bits in the encryption key a stream cipher does not introduce confusion and introduces limited diffusion however some of its variants can introduce a high level of diffusion des the data encryption standard of the national bureau of standards adopted in uses a block cipher technique and provides cipher block chaining as an option it contains iterations which perform complex transformations on the plaintext or the intermediate ciphertext aes the advanced encryption standard is the new standard adopted by the national institute of standards and technology formerly known as the national bureau of standards in it performs between and rounds of operations each involving only substitutions and permutations on plaintext blocks of or bits chapter security and protection encryption decryption key k key k plaintext encryption ciphertext decryption plaintext pd algorithm algorithm pd e cd d figure symmetric encryption of data d attacks on cryptographic systems an attack on a cryptographic system consists of a series of attempts to find the decryption function dk since dkekd d dk is the inverse of ek hence an attack implies finding the inverse of ek if we define the quality of encryption to mean its ability to withstand attacks the aim of an encryption technique is to perform highquality encryption at a low cost the encryption quality is best if the function ek is a oneway function ie if computation of its inverse through an attack involves an impractical amount of effort and time an intruder who may be within an os or outside it can launch a variety of attacks on a cryptographic system the nature of an attack depends on the position that an intruder can occupy within the system if an intruder can not invoke the encryption function and can only examine data in the ciphertext form he has to depend on guesswork this is a trialanderror approach in which the function dk is guessed repeatedly until its application to a ciphertext produces intelligible output this attack is called an exhaustive attack because all possibilities for dk may have to be tried out an exhaustive attack involves a very large number of trials for example trials would be needed to break an encryption scheme employing a bit key by using an exhaustive attack the huge number was believed to make such a scheme computationally secure and the quality of encryption was believed to improve with an increase in the number of bits in an encryption key however powerful mathematical techniques like differential analysis may be employed to find dk much more easily than in an exhaustive attack intruders may also employ the attacks described below that involve fewer trials than an exhaustive attack we see examples of these attacks when we discuss password security in section in the ciphertext only attack an intruder has access only to a collection of ciphertexts consequently to make the attack more efficient than an exhaustive attack the intruder relies on clues extracted through analysis of strings in ciphertexts and information about features of plaintexts such as whether they consist only of words in a dictionary in the known plaintext attack an intruder knows the plaintext corresponding to a ciphertext this attack is feasible if an intruder can gain a position within the os from which both a plaintext and the corresponding ciphertext can be observed collecting a sufficient number of plaintextciphertext pairs provides clues for determining dk in the chosen plaintext attack an intruder is able to supply a plaintext and observe its encrypted form ie choose a d and observe ekd it allows the intruder to systematically part file systems and io management build a collection of plaintextciphertext pairs to support guessing and refinement of guesses during the attack encryption techniques encryption techniques differ in the way they try to defeat intruder attempts at finding dk the fundamental approach is to mask the features of a plaintext ie ensure that a ciphertext does not reveal features of the corresponding plaintext without incurring a very high cost of encryption consider the simplest encryption technique the classical substitution cipher which substitutes each letter in a plaintext by some other letter of the alphabet it does not mask features of a plaintext well enough so frequency analysis provides a simple method for finding dk arrange letters of the alphabet in the order of decreasing frequency of usage in a collection of ciphertexts take standard data for frequency of letters in english texts and organize the letters in the order of decreasing frequency now a good guess for dk is a function that simply maps a letter of the first list into the corresponding letter of the second list so how to mask features of a plaintext during encryption shannon formulated two principles for design of highquality encryption techniques these principles are called confusion and diffusion the confusion principle recommends that it should not be easy to find what changes would occur in the ciphertext due to a change in a plaintext the diffusion principle recommends that the effect of a small substring in the plaintext should be spread throughout the ciphertext these principles ensure that features of a plaintext are masked effectively because individual parts of a plaintext and its ciphertext would not have a strong correlation between them in the following we describe four encryption schemes and discuss their confusion and diffusion properties block cipher the block cipher is an extension of the classical substitution cipher it performs substitution of fixedsize blocks of a plaintext by ciphertext blocks of equal size for example a block consisting of say n bits is encrypted with a key k to obtain an nbit block of the ciphertext see figure these blocks are assembled to obtain the ciphertext the block cipher technique is simple to implement however the confusion and diffusion introduced by it is confined to a block in the ciphertext hence identical blocks in a plaintext yield identical blocks in the ciphertext this feature makes it vulnerable to an attack based on frequency analysis and known plaintext or chosen plaintext attacks larger values of n can be used to make such attacks less practical encryption key k block cipher plaintext ciphertext figure block cipher chapter security and protection stream cipher a stream cipher considers a plaintext as well as the encryption key to be streams of bits encryption is performed by using a transformation that involves a few bits of the plaintext and an equal number of bits of the encryption key a popular choice of the transformation is a bitbybit transformation of a plaintext typically by performing an operation like exclusiveor on a bit of the plaintext and a bit of the encryption key a stream cipher is faster than a block cipher it does not provide confusion or diffusion when a bitbybit transformation is used a variant of this cipher called a vernam cipher uses a random stream of bits as the key stream whose size exactly matches the size of the plaintext hence identical substrings in a plaintext do not lead to identical substrings in the ciphertext the onetime pad that is famous for its use during the second world war was actually a vernam cipher wherein a key stream was used to encode only one plaintext it made the cipher unbreakable variants of the stream cipher have been designed to introduce diffusion such a cipher operates as follows an nbit key stream is used to encrypt the first n bits of the plaintext the next n bits of the key stream are the n bits of the ciphertext that were just produced and so on until the complete plaintext is encrypted thus a substring in the plaintext influences encryption of the rest of the plaintext which provides a high level of diffusion this cipher is called a ciphertext autokey cipher see figure if the key stream generator uses n bits of the plaintext that were just encrypted instead of its ciphertext the cipher is called a selfsynchronizing cipher the diffusion introduced by it is confined only to the next n bits of the ciphertext rc is a widely used stream cipher that uses a key stream that is a pseudorandom stream of bits it uses a pseudorandom stream generator that is initialized by using a key generated by the key scheduling algorithm it is fast as it requires only between and machine operations to generate byte in the key stream it is used in the wired equivalent privacy wep protocol for security in wireless networks and its successor the wifi protected access wpa protocol and in the secure sockets layer ssl protocol for the internet the key scheduling algorithm of rc was shown to possess weaknesses which led to breaking of the wep and wpa protocols however its use in the ssl protocol is considered secure because the ssl protocol itself generates the key used to initialize the pseudorandom stream generator encryption keyk key stream generator plaintext stream ciphertext cipher figure ciphertext autokey cipher part file systems and io management data encryption standard des des was developed by ibm for the national bureau of standards it uses a bit key to encrypt bit data blocks thus it is a block cipher however to overcome the problem of poor diffusion des provides a cipher block chaining cbc mode in this mode the first block of plaintext is combined with an initial vector using an exclusiveor operation and then enciphered the resulting ciphertext is then combined with the second block of the plaintext using an exclusiveor operation and then enciphered and so on des consists of three steps the initial permutation step the transformation step and the final permutation step the transformation step consists of iterations in each iteration the string input to the iteration is subjected to a complex transformation that involves a permutation operation on the string which achieves diffusion and a substitution operation through duplication and omission of some bits which achieves confusion figure illustrates operations performed in each iteration in the first iteration the input string is the plaintext in all other iterations the input string is the output of the previous iteration the input string is split into two halves of bits each the right half of the input string becomes the left half of the result string and a complex transformation involving the left and right halves is performed to obtain the right half of the result string transformation of the right half of the input string consists of the following steps the right half is first expanded to bits by permuting its bits and duplicating some of them it is combined with key ki using an exclusiveor operation see the function f in figure where key ki is derived by permuting the encryption key k using the iteration number i the result of this operation is split into eight groups of bits each each bit group is input to an sbox that substitutes a bit group for it the results of substitution are concatenated to obtain a bit string that is permuted to obtain another bit string this string is combined with the left half of the input string using an exclusiveor operation to obtain the right half of the result string the sbox introduces confusion the permutation introduces diffusion while the final exclusionor operation introduces confusion des achieves both encryption and decryption by using the same sequence of steps except that the keys are used in the reverse order during decryption ie iteration i uses key ki instead of key ki the bit key length used in des would have required trials in an exhaustive attack which was considered a li ri input string f ki k i li fri ki li ri result string figure an iteration in des indicates an exclusiveor operation authentication and password security authentication is typically performed through passwords using the scheme shown in figure for every registered user the system stores a pair of the form login id validatinginfo in a passwords table where validatinginfo part file systems and io management ekpassword to authenticate a user the system encrypts his password using ek and compares the result with his validating information stored in the passwords table the user is considered to be authentic if the two match if an intruder has access to the passwords table he can launch one of the attacks described earlier in section to determine ek alternatively the intruder may launch an attack to crack the password of an individual user in the scheme described above if two users use identical passwords the encrypted forms of their passwords would also be identical which would facilitate an intruders attempts at cracking of a password if the passwords table is visible to him hence the encryption function e takes two parameters one parameter is the encryption key k and the other parameter is a string derived from the users login id now identical passwords yield distinct encrypted strings intruders may use password cracking programs to discover passwords of individual users their task is simplified by users tendency to use passwords that are not difficult to guess such as dictionary words and vehicle numbers or use simple keyboard sequences for infrequently used accounts users often choose simple passwords that are easy to remember the common refrain being that they do not have many important files in that account however a password is the proverbial weakest link in the security chain any password that is cracked provides an intruder with opportunities for launching further security attacks consequently a large number of security problems relate to use of poor passwords operating systems use a set of techniques to defeat attacks on passwords table summarizes these techniques password aging limits the exposure of passwords to intruders which is expected to make passwords more secure systemchosen passwords ensure use of strong passwords which can not be cracked by table os techniques for defeating attacks on passwords technique description password aging encourage or force users to change their passwords frequently at least once every months it limits the exposure of a password to intruder attacks systemchosen a system administrator uses a methodology to generate and passwords assign strong passwords to users users are not allowed to change these passwords an intruder would have to use an exhaustive attack to break such passwords encryption of the encrypted form of passwords is stored in a system file passwords however the ciphertext form of passwords is visible to all users in the system an intruder can use one of the attacks described in section to find ek or launch an exhaustive attack to crack an individual users password encrypt and hide the encrypted form of passwords is not visible to any person password within or outside the system hence an intruder can not use information any of the attacks described in section protection structures a protection structure is the classical name for the authorization database discussed in section and illustrated in figure it contains information indicating which users can access which files in what manner we begin by discussing the nature of information contained in a protection structure and how the information is used to implement protection later in this section we discuss the key issues in organization of the protection structure recall from section that an access privilege for a file is a right to make a specific form of access to the file eg a read access or a write access a user may hold one or more access privileges for a file eg he may be permitted to only read a file or read and write a file but not execute it an access descriptor is a representation of a collection of access privileges for a file the access control information for a file is a collection of access descriptors it represents access privileges for the file held by all users in the system we use the notations r w and x to represent access privileges to read write and execute the data or program in a file an access descriptor can be represented as a set of access privileges eg the set r w indicates privileges to both read and write a file we will use a set representation in this chapter for simplicity however a set representation is expensive in terms of both memory requirements and access efficiency so operating systems actually use a bitencoding scheme for access descriptors in this scheme an access descriptor is a string of bits where each bit indicates the presence or absence of a specific access privilege for example in an os using only three access privileges r w and x the access descriptor could be used to indicate that the read and write privileges are present but the execute privilege is absent part file systems and io management as discussed in section the access control information for a file alpha is created and used as follows when a user a creates file alpha he specifies the access control information for it the file system stores it in the protection structure when a user x logs in he is authenticated the authentication service generates an authentication token for him which is typically a user id when a process initiated by user x wishes to open or access file alpha his authentication token is passed to the file system the file system uses the authentication token to find the access privileges of user x for file alpha in the protection structure and checks whether the kind of access desired by the process is consistent with the access privileges organization of the protection structure influences two key aspects of protection how much discrimination the owner of a file can exercise in step while specifying which other users can access the file and how efficiently the protection check of step can be implemented we discuss these issues in the following sections granularity of protection granularity of protection signifies the degree of discrimination a file owner can exercise concerning protection of files we define three levels of granularity in table coarsegrained protection implies that users are clubbed into groups and access privileges are specified for a group of users whereas mediumgrained protection implies that the owner of a file can specify access privileges individually for each user in the system finegrained protection permits access privileges to be specified for a process or for different phases in operation of a process this way different processes created by the same user may possess different access privileges for a file or the same process may possess different access privileges for the file at different times it helps in ensuring privacy of information see section table granularity of protection granularity description coarsegrained access privileges for a file can be specified only for groups of protection users each user in a group has identical access privileges for the file mediumgrained access privileges for a file can be specified individually for protection each user in the system finegrained access privileges for a file can be specified for a process or for protection a phase in operation of a process chapter security and protection files alpha beta gamma users jay r rw anita rwx r access privileges sheila r of anita access control information for alpha figure access control matrix acm users desire mediumor finegrained protection however such protection leads to a large size of the protection structure this is why operating systems resort to coarsegrained protection access control matrix an access control matrix acm is a protection structure that provides efficient access to both access privileges of users for various files and access control information for files each element of the acm contains access privileges of one user for one file each user has a row in the acm while each file has a column in it this way a row in the acm describes one users access privileges for all files in the system and each column describes the access control information for a file when a user ui wishes to access file fk the element acmui fk can be accessed efficiently to validate the kind of access being made by ui figure shows an acm user jay has read write access privileges for beta but only a read privilege for alpha the acm provides mediumgrained protection however it is large in size because an os has a large number of users and contains a large number of files accordingly a large area of memory has to be committed to hold the acm or parts of it in memory during system operation operating systems use two approaches to reduce the size of access control information in the first approach the number of rows is reduced by assigning access privileges to groups of users rather than to individual users this approach retains the basic advantage of the acm namely efficient access to both access privileges of users and access control information of files however it leads to coarsegrained protection because all users in a group have identical access privileges for a file the second approach to reducing size of the protection structure exploits the fact that a typical user possesses access privileges for only a few files thus most elements in an acm contain null entries so space can be conserved by organizing the protection information in the form of lists containing only nonnull access privileges this approach does not affect the granularity of protection however it compromises access efficiency of the protection structure we present two listorganized protection structures in the following sections access control lists acls the access control list acl of a file is a representation of its access control information it contains the nonnull entries that the files column would part file systems and io management file access control list name acl alpha jay r anitar w x beta jay r w gamma anita r sheila r figure access control lists acls alpha r w x gamma r figure capability list for user anita have contained in the acm it is stored as a list of pairs of the form userid accessprivileges figure shows access control lists for the files alpha beta and gamma of figure the acl for alpha is jay read anita read write execute which indicates that user jay can only read file alpha while anita can read write or execute the file user sheila is not permitted any kind of access to alpha since alphas acl does not contain an entry for sheila even though use of an acl eliminates the need to store null access privileges presence of a large number of users in a system leads to large acl sizes and thereby to large disk and memory overhead in the file system the time overhead is also high because the acl has to be searched for validating a file access both memory and cpu time can be conserved at the cost of using coarsegrained protection by specifying protection information for groups of users rather than for individual users such an acl could be small enough to be stored in the directory entry of a file for example if users jay and anita belong to the same group of users the acl of file alpha would contain a single pair it would now be easier to determine whether jay can access alpha however both jay and anita would have identical access privileges capability lists clists a capability list clist represents access privileges of a user for various files in the system it contains the nonnull entries that the users row in the acm would have contained each entry in the clist is a capability which represents access privileges for one file it is a pair of the form fileid accessprivileges figure shows a clist for user anita of figure anita can read write or execute file alpha and can read file gamma anita has no access privileges for file beta since no entry for beta exists in the clist clists are usually small in size this feature limits the space and time overhead in using them for protection of files we discuss how capabilities are used in a computer in section chapter security and protection protection domain the access control matrix access control list or capability list is used to confer access privileges on users this arrangement serves the secrecy goal of security and protection because only authorized users can access a file however the privacy goal of security and protection requires that information should be used only for intended purposes see section and this requirement could be violated as follows a user is granted an access privilege for a file because some process initiated by the user requires it however every other process initiated by the user also has the same access privilege for the file some of these processes may access the file in an unintended manner thus violating the privacy requirement the next example illustrates how privacy of information may be jeopardized privacy violation example a user ui has an execute privilege for a program invest owned by another user uj when ui executes invest invest operates as a process initiated by user ui it can access any file for which user ui holds an access privilege including files that have nothing to do with investments if uj so wishes he can code invest to obtain a listing of uis current directory and either copy or modify some of the files found in it violation of privacy raises a major reliability concern as the correctness of data would depend not only on correct manipulation by processes that are supposed to access it but also on harmlessness of the accesses made by processes that are not supposed to access it the concept of a protection domain is used to prevent privacy violations we can think of a protection domain as a conceptual execution environment access privileges are granted to a protection domain rather than to a user or his process a process operates within a protection domain and can access those files for which the protection domain has access privileges this arrangement facilitates implementation of the needtoknow principle with a fine granularity a process should be allowed to operate within a protection domain only if it needs to access the files for which the protection domain has access privileges the following example illustrates how this approach ensures privacy of information protection domains example figure shows three protection domains domain d has read and write privileges for the files personal and finance while d possesses only a read privilege for finance domain d has read and write privileges for the files memos and notes and a read privilege for the file project thus domains d and d overlap while domain d is disjoint with both of them capabilities dennis and van horn proposed the concept of a capability for use in sharing and protection a capability is a token representing some access privileges for an object where an object is any hardware or software entity in the system eg a laser printer a cpu a file a program or a data structure of a program a capability is possessed by a process its possession gives the process a right to access the object in a manner that is consistent with the access privileges in the capability figure shows the format of a capability it consists of two fields object id and access privileges each object has an unique object id in the system the chapter security and protection object access id privileges figure format of a capability p object object object access id address id privileges dcap alpha alpha alpha r clist object table alpha ot figure capabilitybased addressing access privileges field typically contains a bitencoded access descriptor a process may possess many capabilities these are stored in the capability list clist discussed earlier in section when some process pi creates an object oi the os forms a capability for oi that contains the entire set of access privileges defined in the system and passes this capability to pi using this capability pi can request the os to create subset capabilities for oi that contain fewer access privileges it can also make copies of the capability for oi that it received from the os thus many capabilities for oi may exist in the system process pi can share the object oi with other processes by passing capabilities for oi to other processes thus each process possesses capabilities for the objects it owns and some capabilities passed to it by other processes all these capabilities are obtained through legal means none can be stolen or fraudulently created by a process this is why a capability is often described as an unforgeable token that confers access privileges onto its holder we use the notation capkobji to refer to a capability for obji the subscript of cap is used simply to distinguish between different capabilities for an object it does not have any other significance for simplicity we omit the subscript in contexts where a single capability of an object is involved capabilitybased computer systems a capabilitybased computer system implements capabilitybased addressing and protection for all objects in the system ranging from longlife objects like files to shortlife objects like data structures and copies of programs in memory many capabilitybased systems were built for research the intel iapx was a capabilitybased commercial system figure is a schematic diagram of capabilitybased addressing of objects the system does not explicitly associate memory with processes it associates clists with processes each object has an unique id the object table ot is a part file systems and io management systemwide table that contains location information for all objects in the system the object address field of an ot entry indicates the address of the object in the computers primary or secondary memory access to an object is implemented as follows a process p performs an operation opi on an object by using an instruction of the form opi dcapobji where dcapobji is the displacement of capobji in ps clist the cpu locates the capability in ps clist using the displacement and verifies that the operation opi is consistent with access privileges in the capability the object id in the capability that is alpha is now used to locate alphas entry in the ot and the object address found there is used to implement opi capabilitybased addressing can be made more efficient by using buffers analogous to address translation buffers see section and special cache memories for address translation the capabilities in a clist may be used to access objects existing anywhere in the system ie in memory or on disk the location of an object is immaterial to a process this feature permits the os to move objects around in memory for better memory management or move them between memory and disk for costeffective access performance without affecting the manner in which a program accesses the objects thus longlife objects like files and shortlife objects like data structures can be accessed in a uniform manner operations on objects and capabilities a process may be given some capabilities by the os by default when it is created it may also inherit some capabilities from its parent process when the process performs the operation create a new object the cpu creates a new object and creates a new entry in the ot for it it puts the object id and address of the newly created object in the entry it now creates a capability containing the entire set of access privileges for the object and puts it in the clist of pi it also puts dcapobji in a register of the cpu process pi saves the contents of this register for use while accessing obji in future all operations performed by a process are subject to access privileges contained in its clist the operation of creating an object may itself be subject to an access privilege if so the os would confer this access privilege through one of the default capabilities it gives to each process table lists the operations a process can perform on objects and capabilities thus a process can create modify destroy copy or execute an object if it possesses a capability with the appropriate access privileges operations on a capability are also subject to access privileges in it for example a process may be able to create a subset capability of capobji only if capobji contains the access privilege create subset capability this feature controls the operations that processes can perform on capabilities sharing of objects occurs when a process passes a capability for an object to another process the process receiving the capability enters it in its clist sharing is implicit in the fact that both clists contain a capability for the object protection is implicit in the fact that these capabilities may confer different access privileges on the processes chapter security and protection table permissible operations on objects and capabilities operations on objects create an object read or modify the object destroy the object copy the object execute the object operations on capabilities make a copy of the capability create a subset capability use the capability as a parameter in a function procedure call pass the capability for use by another process delete the capability protection of capabilities protection using capabilities is based on the fundamental assumption that capabilities can not be forged or tampered with this assumption would be invalid if a process could access its clist and modify the capabilities existing in it for example process p of figure could alter the access privileges field of the capability for alpha to give itself a write access privilege and then use the modified capability to modify object alpha such tampering of capabilities is prevented by ensuring that arbitrary operations can not be performed on capabilities it is implemented using two approaches tagged architectures and capability segments in a computer with a tagged architecture the runtime representation of an entity consists of two fields a tag field and a value field the tag field describes the type of the entity the cpu is designed to perform only those operations on an entity that are consistent with its tag this way only the six operations on capabilities that are mentioned in table can be performed on a capability which ensures that a capability can not be tampered with in a computer using capability segments data objects and their capabilities are stored in different segments and instructions in the cpu are designed to access their operands from an appropriate segment only the six operations on capabilities that are mentioned in table would take their operands from the capability segment this way arbitrary operations can not be performed on a capability software capabilities the os for a noncapabilitybased computer can implement capabilities in the software the arrangement of objects and capabilities can be analogous to the arrangement shown in figure however manipulation and protection of objects can not be performed by the cpu of the system it is now performed by a component of the kernel called an object manager om a program indicates its object manipulation requirements to the object manager by making a call part file systems and io management object access id privileges number figure an amoebalike capability om opi capobji this call has the same effect as instruction the object manager implements opi only if capobji contains the necessary access privileges for performing it two important issues in software capabilities are a process may be able to bypass the capabilitybased protection arrangement while accessing objects and it may be able to tamper with or fabricate capabilities how can we prevent a process from manipulating objects without going through the object manager one way to achieve it is to hide objects from the view of user processes by encrypting the object table now processes will not know the locations of objects hence they will have to depend on the object manager to perform object manipulation tampering with capabilities can also be prevented by using encryption as an example we describe a simplified version of the capability protection scheme used in the distributed operating system amoeba capabilities in amoeba an object obji is assigned an encryption key keyi when it is created the encryption key is stored in the object table entry of obji the amoeba capability has the format shown in figure the number field contains information that is used to protect the capability a capability for obji is created using the following procedure the object id and access privileges fields of the capability are set appropriately the encryption key keyi is obtained from the object table entry of obji contents of the access privileges field are now concatenated with keyi and the resulting string is encrypted using keyi we denote this operation as ekeyi access privileges keyi where denotes concatenation the result of the encryption step is stored in the number field of the capability to manipulate obji a process must submit a capability for obji to the object manager the object manager verifies the validity of this capability as follows the encryption key keyi is obtained from the object table entry of obji the string access privileges is obtained from the capability and ekeyi access privileges keyi is compared with the number field in the capability the comparison in step would fail if the object id or access privileges field of a capability has been tampered with so the object manager aborts the process if the comparison fails comparison with capabilitybased systems the major strength of software capabilities their independence from the underlying hardware is also their chapter security and protection major weakness operations such as creation of subset capabilities which are performed by the hardware of a capabilitybased system need to be performed in the software each of these involves a system call to invoke the object manager in addition prevention of tampering requires validation of a capability before use these requirements lead to substantial time overhead problem areas in the use of capabilities use of capabilities has three practical problems need for garbage collection when can an object be destroyed confinement of capabilities how to ensure that processes do not pass capabilities to other processes indiscriminately revocation of capabilities how to cancel a capability or withdraw the access privileges conferred by it garbage collection the owner of an object may prepare subset capabilities for an object and pass them to other processes so that they can access the object before destroying such an object the owner must know that no process is currently using the object this information can be gained only through synchronization of the owner with all users of an object this approach is impractical when objects are created and used at a high rate or when objects with long lifetimes are shared two problems can arise if objects are destroyed without collecting such information dangling pointers can exist that is an object may be destroyed while some capabilities still exist for it or an object may exist long after capabilities for it have been destroyed preventing both these situations requires use of expensive garbage collection techniques confinement of capabilities confinement implies restricting the use of a capability to a given set of processes lack of confinement implies proliferation of capabilities throughout the system due to indiscriminate passing of capabilities it complicates garbage collection and prolongs the life of an object it can also undermine protection by violating the needtoknow principle confinement can be achieved by making the passing of a capability itself an access right if process pi turns off the pass access right in a capability while passing it to pj pj will not be able to pass the capability to any other process revocation of capabilities revocation of all capabilities for an object is the most difficult problem in a capabilitybased system since there is no way to know which processes hold capabilities for the object and there is no method of nullifying a capability however revocation is possible in the case of software capabilities because they are protected through encryption in amoeba all existing capabilities of an object would be invalidated when the encryption key assigned to the object is changed to selectively revoke some capabilities of an object the owner can invalidate all capabilities of the object by changing the encryption key and then issue fresh capabilities to only some processes however it is an expensive and classifications of computer security a security policy specifies the roles of entities whether individuals or programs in ensuring that resources of a computer system are used in a legitimate manner in the terminology of figure a security policy would specify roles of system administrators and programs used by them to maintain the authentication and authorization databases and the roles of os programs that constitute the authentication and authorization services ideally it should be possible to prove that the security policies can not be undermined however such proofs are difficult for the reasons mentioned in section so system developers must use other means to inspire confidence in the security capabilities of systems these means typically take the form of some evidence that the system implements access control to ensure that resources are used in a legitimate manner and an auditing capability which keeps information on how a securityrelated event has been handled by the relevant entity the us department of defense evolved trusted computer system evaluation criteria tcsec to determine how well a computer system conforms with the security and protection requirements these criteria classify computer systems into four divisions and several levels within each division see table the classification envisages that a computer system can be partitioned into two parts the trusted computing base tcb is that part of its hardware software table trusted computer system evaluation criteria division description and levels verified protection a system must support formal methods for verification of division a security mandatory a system must associate sensitivity labels with data and protection programs and implement mandatory access control rules division b through a reference monitor rm b labeled security protection b structured protection b security domains discretionary a system must implement needtoknow protection and protection provide audit capabilities for accountability of subjects and division c their actions c discretionary security protection c controlled access protection minimal a system that fails the requirements for a higher division protection division d chapter security and protection and firmware that implements securityrelated functionalities in the system the remainder of the system does not implement any securityrelated functions the classification of a computer system depends on whether its tcb meets the requirements of a specific division in the classification and all lower divisions division d is the lowest security classification it is awarded to systems that can not meet the requirements of any of the other divisions the tcb of a division c computing system possesses three key capabilities first it permits a user to specify which other users can access the data or programs owned by him it performs authentication of users to provide this capability second it facilitates auditing of securityrelated events in the system by keeping a record of events such as authentication attempts file openclose actions of system administrators etc third it provides object reuse protection to ensure that a users data can not be accidentally accessed by another user it is implemented by clearing the memory allocated to a data object before returning it to the tcbs pool of free objects or free memory levels c and c of division c correspond to the different protection granularities a system satisfies level c of the classification if a user can identify each individual user who can access the files owned by him otherwise the system satisfies level c thus a system implementing coarsegrained protection would earn a level c classification see section to earn a division b classification a computer system must assign sensitivity labels to all data and programs that reflect their security and protection levels and must use these labels to validate every access of a data or program which is called mandatory access control it must also control the propagation of access rights the system developer must furnish a security policy model on which the tcb is based this model must employ a reference monitor rm to validate each reference to a data or program by a user or his process the reference monitor should be tamperproof and should be small so that its completeness can be analyzed and tested division b consists of three levels which differ in the extent of mandatory protection resistance to penetration support for trusted facility management and structuring of the tcb into protectioncritical and nonprotectioncritical elements in level b mandatory access control must exist and the system administrator should be able to audit the actions of select users or actions related to select programs or data objects in level b mandatory access control should be extended to all users and all data and program objects the system should be resistant to penetration and should provide support for system administrator and operator functions the system should also provide a trusted path between a user and the tcb this path is typically used when a user wishes to log in its use eliminates masquerading attacks by a trojan horse program see section in level b the system should be highly resistant to penetration and must support a system administrator in collecting information on imminent security attacks and terminating events that could be parts of such attacks to qualify for the division a rating a system has to have the capabilities of level b and its developer has to furnish a formal proof of its security policy case studies in security and protection multics multics provides protection domains that are organized as concentric rings the rings are numbered from the innermost to the outermost see figure the access privileges of a domain include access privileges of all highernumbered domains in addition the domain may have a few other access privileges of its own each procedure of a program is assigned to a protection domain and can be executed only by a process that is in the same protection domain the code component of a process may consist of procedures in different protection domains an interrupt is raised when a process executing in protection domain di invokes a procedure that is assigned to a protection domain dj where dj di to execute the procedure the protection domain of the process should be changed to dj the kernel checks whether this is permissible according to the rule for change of protection domain a simplified view of this rule is as follows change of protection domain is permitted if a process running in some domain di invokes a procedure that exists in a highernumbered domain however to enter a lowernumbered domain a process must invoke a specially designated procedure called a gate an attempt to invoke any other procedure in a lowernumbered layer fails and the process is aborted if a procedure call satisfies this rule the protection domain of the process is temporarily changed to the domain in which the invoked procedure exists the invoked procedure executes in this protection domain and accesses resources according to the access privileges of the domain at return the protection domain of the process is reset to its earlier value ie to di the multics protection structure is complex and incurs substantial execution overhead due to checks made at a procedure call because of the requirement that access privileges of a protection domain should include access privileges of all highernumbered domains it is not possible to use domains whose access privileges are disjoint for example domains d d and d of figure can not be implemented in multics since domain d is disjoint with domains d and d this feature restricts users freedom in specifying protection requirements gates ring ring figure multics protection rings chapter security and protection unix as mentioned in section unix employs encryption for password security under an option it uses a shadow passwords file that is accessible only to the root which forces an intruder to use an exhaustive attack to crack passwords each unix user has an unique id in the system the system administrator creates nonoverlapping groups of users and assigns a unique group id to each group the credential of a user is composed of his user id and group id it is stored in the passwords table and becomes the authentication token of the user after the user is authenticated unix defines three user classes file owner user group and other users and provides only three access rights r w and x representing read write and execute respectively a bit bitencoded access descriptor is used for each user class and the access control list acl contains access descriptors for three user classes in the sequence file owner user group and other users this way the acl requires only bits it is stored in the inode of a file see section the identity of the file owner is stored in another field of the files inode figure shows the unix acls as reported in a directory listing the file sigma can be read by any user in the system but can be written only by its owner delta is a readonly file for all user classes while phi has the read write and execute privileges only for its owner the access privileges of a unix process are determined by its uid when the kernel creates a process it sets the uid of the process to the id of the user who created it thus the process operates in a protection domain determined by the id of the user who created it unix changes the protection domain of a process under two conditions when the process makes a system call and when the setuid or setgid feature is used a process has two distinct running states user running and kernel running see section while in the userrunning state a process has access to the memory space and other resources allocated to it and to files in the file system according to its uid the process makes a transition to the kernelrunning state through a system call in this state it can access kernel data structures and also contents of the entire memory it returns to the userrunning state when it returns from the system call thus a change of protection domain occurs implicitly when a process makes a system call and when it returns from a system call the setuid feature can be used in two ways a process can make a system call setuid id to change its uid to id and another setuid system call with its own id to revert to its original uid alternatively uid can be changed implicitly when a process performs an exec in order to execute a program the latter usage rwr r sigma r r r delta rwx phi file user other owner group users figure unix access control list part file systems and io management is realized as follows let a program p be stored in a file named p if the owner of p requests the kernel that p be executed under the setuid feature the kernel sets the setuid bit in the inode of file p when p is execed by some process that has an execute permission for it the kernel notices that the setuid bit of file p is set and temporarily changes the uid of the process executing p to the uid of ps owner this action effectively puts the process into a protection domain whose access privileges are identical with the access privileges of ps owner this feature can be used to avoid the privacy violation discussed in example as follows user uj sets the setuid bit of the program invest user ui provides uj with a read access to the file finance before invoking invest now the program invest executes with the uid of uj thus invest can access user uis file finance but it can not access any other files owned by ui the setgid feature analogously provides a method of temporarily changing the group id of a process linux linux authenticates a user at login time by adding a salt value to his password and encrypting the result through md under an option it employs a shadow passwords file that is accessible only to the root additionally linux provides pluggable authentication modules pams through which an application can authenticate a user at any time through a dynamically loadable library of authentication modules this arrangement provides flexibility because the authentication scheme used in an application can be changed without having to recompile the application an application developer can use pams to enhance application security in several ways to employ a password encryption scheme of his own choice to set resource limits to users so that they can not consume an excessive amount of a resource to launch a denialofservice attack and to allow specific users to log in only at specific times from specific locations the system administrator maintains a pam configuration file for each application that is authorized to employ pam each pam configuration file specifies how authentication is to be performed and what actions such as mounting of home directories or logging of the authentication event are to be taken after a user is authenticated the configuration file also names the mechanism that is to be employed when a user wishes to change his password pam permits several authentication modules to be stacked these modules are invoked one after another an application can use this facility to authenticate a user through several means such as passwords and biometric identification to enhance security linux provides file access protection based on user id and group id of a process when a server such as the nfs accesses a file on behalf of a user file protection should be performed using the user id and group id of the user rather than those of the server to facilitate this linux provides the system calls fsuid and fsgid through which a server can temporarily assume the identity of its client chapter security and protection as described in section the linux kernel supports loadable kernel modules this feature has been employed to provide enhanced access controls through loadable kernel modules called the linux security modules lsm use of lsms permits many different security models to be supported the basic schematic of lsm is simple the kernel invokes an access validation function before accessing an object an lsm provides this function which may permit or deny the access to go through the security enhanced linux selinux of the us national security agency has built additional access control mechanisms through lsm which provide mandatory access control the linux kernel provides the execshield patch which enables protection against exploitation of buffer overflows and data structure overwriting to launch security attacks security and protection in windows the windows security model has several elements of cand bclass systems according to the tcsec criteria see section it provides discretionary access control object reuse protection auditing of securityrelated events a security reference monitor srm that enforces access control and a trusted path for authentication that would defeat masquerading attacks launched through a trojan horse among other notable features it provides security for client server computing through access tokens which are analogous to capabilities see section windows security is based around the use of security identifiers sids a security identifier is assigned to a user a host or a domain which is composed of several hosts the important fields in an sid are a bit identifier authority value which identifies the host or domain that issued the sid and a few bit subauthority or relative identifier rid values that are used primarily to generate unique sids for entities created by the same host or domain each process and thread has an access token that identifies its security context recall that we use the term process as being generic to both a process and a thread an access token is generated when a user logs on and it is associated with the initial process created for the user a process can create more access tokens through the logonuser function an access token contains a user account sid and a group account sid these fields are used by the security reference monitor to decide whether the process holding the access token can perform certain operations on an object an access token also contains a privilege array indicating any special privileges held by the process such as a privilege for creating backups of files impersonating a client and shutting down a host it may also contain a few superprivileges for loading and unloading drivers taking ownership of objects and creating new access tokens an object such as a file has a security descriptor which contains the object owners id a discretionary access control list dacl and a system access control list sacl the dacl is used to specify which users can access the object in what manner while the sacl is used to generate an audit log of operations performed on the object both dacl and sacl are lists of access control entries part file systems and io management aces however an ace plays different roles in these lists an ace in a dacl either indicates that the specified user is allowed to access the object or indicates that the user is forbidden access to the object this arrangement permits mediumgrained protection and yet helps to make the dacl compact however the entire dacl has to be processed to determine whether a specific user is allowed to access the object in a specific manner an object that can contain other objects such as a directory is called a container object we will call the objects contained in it its child objects an ace in the dacl of a container object contains flags to indicate how the ace is to apply to a child object identically not at all or in some other manner an important option is that the ace may be inherited by a child object that is itself a container object but it may not be further inherited by objects that may be created within the child object this feature helps to limit the propagation of access control privileges an ace in the sacl indicates which operation on the object by which users or groups of users should be audited an entry is made in the audit log when any of these operations is performed the impersonation feature in the windows security model provides security in clientserver computing when a server performs some operations on objects on behalf of a client these operations should be subject to the access privileges of the client rather than those of the server otherwise the client may be able to realize operations on these objects that exceed its own access privileges analogously the security audit log that is generated when the server accesses an object on behalf of a client should contain the identity of the client rather than that of the server both these requirements are satisfied by letting the server temporarily assume the identity of the client through impersonation impersonation is implemented as follows when a client invokes a server it indicates the kind of impersonation it wishes the server to perform the server can not perform impersonation without the clients consent if impersonation is enabled an impersonation token is created from the clients token and given to the server the server presents the impersonation token rather than its own access token while performing operations on objects effectively the access token and the impersonation token act like the capabilities discussed in section further to ensure security the server may create a restricted token from an impersonation token such a token would contain a subset of the privileges contained in the impersonation token it is like a subset capability discussed in section the following new security features were added in windows vista to make it a more secure os defeating buffer overflow attacks in intel x architectures recall from section that the stack grows downward in memory in processors with these architectures vista places the return pointers and parameters of a function call higher in the stack than local data to prevent their corruption by an overflow attempts at executing code smuggled in as data are defeated using the noexecute nx feature of processors by flagging parts of memory used to hold data as noexecute zones detecting heap corruption an intruder can launch a buffer overflow attack in the heap as well to prevent it metadata such as pointers in the heap are summary a fundamental goal of an os is to ensure nonto disrupt legitimate use of resources by users interference in the computations and resources these means include a trojan horse virus or worm of users however users need to share some of or use of a buffer overflow threats imposed by their resources such as programs and data stored intruders are thwarted by exercising caution while in files with collaborators hence an important loading unknown programs into a computer and aspect of implementing noninterference is knowby removing security flaws ing what accesses to a resource are legitimate and the authentication service of an os keeps what accesses constitute interference threats of names of registered users and information used interference can arise outside a system or within it to identify them in a database it uses encryption measures employed to counter such threats constiwhich is an algorithmic transformation of data to tute security and protection respectively authentiprevent intruders from accessing and misusing the cation is the key technique of security it determines authentication database block cipher and stream whether a person is a registered user of a comcipher are widely used encryption techniques the puter system authorization is the key technique encryption standards digital encryption standard of protection it determines whether a user is perdes and advanced encryption standard aes mitted to access a resource in this chapter we have been widely deployed studied implementation of the authentication and the authorization service of an os has a authorization techniques protection structure which contains two kinds of a person or program posing a threat of interinformation an access privilege represents a users ference is called an intruder intruders may employ right to access a specific file in a specific manvarious means that exploit security flaws in a comner the protection information of a file indicates puter system either to masquerade as a user or which users can access the file in what manner part file systems and io management organization of the protection structure controls a computer system is assigned a security clasthe amount of discrimination a user can exersification based on how well it conforms to the cise while specifying which users can access his security and protection requirements its ability files in what manner it is called granularity of to provide finegrained protection and support protection access control lists capability lists the system administrator in implementing secuand protection domains are alternative protection rity policies are key determinants of its security structures classification test your concepts classify each of the following statements as true j password aging limits exposure of a password or false to attacks by an intruder a the authentication mechanism is used to disk two capabilities of an object may confer tinguish between users and nonusers of a identical access privileges on their holders computer system l encryption is used to protect software capab an authentication token contains the list of bilities access privileges held by a user which of the following is a protection violation c the authorization database is used by secua user x who possesses a write privilege for rity mechanisms file alpha of user y writes invalid data into d encryption of information ensures its alpha integrity b a nonuser manages to read the data stored in e masquerading is a security attack a file beta in a computer system f a virus launches a security attack only if c user x manages to read a file alpha of user explicitly downloaded by a user y even though he does not possess a read g the buffer overflow technique can be used to privilege for it launch a security attack d none of ac h when encrypted by a stream cipher identi pair the related items in each column cal substrings in a plaintext always lead to i unixstyle access i finegrained identical substrings in its ciphertext control list protection i to authenticate a user at login time an ii access control matrix ii coarsegrained os decrypts the encrypted form of the acm protection users password stored in the authentication iii protection domains iii mediumgrained database and compares the result with the protection password presented by the user exercises explain the procedure to be followed for making formulate a security rule that will eliminate the changes in the authentication and authorization security flaw of example databases of figure describe the conditions under which a cho list the security attacks that can not be presen plaintext attack can be launched against vented by encryption passwords discuss whether encryption can ensure secrecy comment on the impact of granularity of proprivacy and integrity of data tection on sizes of various protection structures chapter security and protection suggest methods of reducing the size of the an os creates servers to offer various services to access control list acl when mediumgrained users while handling a service request made by protection is to be implemented a user a server may need to access resources on a file is encrypted by using an encryption funcbehalf of the user such resource accesses must tion e and a key k no other protection checks be subject to access privileges of the user rather are made by the file system if the user wishes than access privileges of the server to share the file with another user he makes e a the following scheme is proposed for this and k available to the other user compare the purpose when a user invokes a service he above scheme for protecting files with a protecsends his own authentication token to the tion scheme using an access control list on the server when the server requests access to basis of a ease of granting access privileges to a resource it presents the users authenticausers or withdrawing them and b granting tion token rather than its own authenticadifferent kinds of access privileges to the same tion token to the authorization service this file way its use of resources would be subject to some old operating systems used to associate the users access privileges how to ensure passwords with files and permit any program that an intruder can not exploit this arrangethat presented a valid password for a file to ment to masquerade as a user hint ensure access it compare this protection scheme with a that a users authentication token can not be capabilitybased protection scheme on the same forged criteria as in exercise b design a scheme using capabilities capability review is the process by which an os explain how buffers analogous to address transfinds all processes that possess a capability for a lation buffers used in virtual memories see specific object obji describe how a review opersection or a cache memory can be used ation can be performed in a system that uses in the schematic of figure to speed up hardware or software capabilities object accesses an os performs validation of software capabili different nodes of a distributed system may conties as follows when a new capability is created currently create new objects describe a scheme the object manager stores a copy of the capathat can ensure uniqueness of object ids in a bility for its own use when a process wishes to distributed os perform an operation on an object the capa study relevant unix literature and describe the bility presented by it is compared with stored provisions in unix for a finding the id of the capabilities the operation is permitted only if a user who owns a file and b deciding whether a matching capability exists with the object manuser belongs to the same user group as the owner ager is this scheme foolproof does it permit of a file selective revocation of access privileges bibliography ludwig describes different kinds of viruses while shannon is the classical work in computer ludwig discusses email viruses spafford security it discusses the diffusion and confusion propdiscusses the morris internet worm that caused havoc in erties of cyphers denning and denning and and berghel describes the code red worm lempel contain good overviews of data security of and cryptology respectively schneier and fergulandwehr discusses formal models for comson and schneier are texts on cryptography while puter security voydock and kent discuss security pfleeger and pfleeger is a text on computer secuissues in distributed systems and practical techniques rity stallings discusses cryptography and network used to tackle them security part file systems and io management naor and yung discusses oneway hash funcrc proceedings of th annual workshop on tions rivest describes the md message digest selected areas in cryptography function the goal of md is to make it computation lampson b w protection operating ally infeasible to produce two messages with an identical systems review message digest or to produce a message with a given landwehr c e formal models for message digest md is extremely fast and resists cryptcomputer security computing surveys analysis attacks successfully rivest describes md which is more conservative and a little slower than lempel a cryptology in transition md preneel describes cryptographic primitives computing surveys for information authentication levy h m capabilitybased computer access matrixbased protection and protection systems digital press burlington mass domains are discussed in lampson and popek ludwig m a the giant black book of organick discusses the multics procomputer viruses nd ed american eagle tection rings the setuid feature of unix is described in show low ariz most books on unix ludwig m a the little black book of dennis and van horn is a widely referemail viruses american eagle show low enced paper on the concept of capabilities levy ariz describes a number of capabilitybased systems mul menezes a p van oorschot and s vanstone lender and tanenbaum and tanenbaum handbook of applied cryptography crc describe the software capabilities of amoeba anderpress boca raton fla son et al discusses software capabilities with a mullender s p and a tanenbaum the provision for containment design of a capabilitybased distributed operating the trusted computer system evaluation criteria system computer journal tcsec of the us department of defense offers a nachenberg c computer classification of security features of computer systems virusantivirus coevolution communications of it is described in dod the acm spafford et al discusses security in solaris naor m and m yung universal mac os linux and freebsd operating systems wright oneway hash functions and their cryptographic et al discusses the linux security modules russiapplications proceedings of the st annual novich and solomon discusses security features acm symposium on theory of computing in windows oppliger r internet security firewalls anderson m r d pose and c s wallace and beyond communications of the acm a passwordcapability system the computer journal organick e i the multics system berghel h the code red worm mit press cambridge mass communications of the acm pfleeger c p and s pfleeger security in denning d e and p j denning computing prentice hall englewood cliffs nj data security computing surveys popek g j protection structures dennis j b and e c van horn computer programming semantics for multiprogrammed preneel b cryptographic primitives for computations communications of the acm information authentication state of the art in applied cryptography lncs springer dod trusted computer system verlag evaluation criteria us department of defense rivest r the md message digest ferguson n and b schneier practical algorithm proceedings of advances in cryptography john wiley new york cryptology crypto lecture notes in fluhrer s i mantin and a shamir computer science volume spingerverlag weaknesses in the key scheduling algorithm of chapter security and protection rivest r the md message digest stiegler h g a structure for access algorithm request for comments rfc control lists software practice and experience russinovich m e and d a solomon microsoft windows internals th ed microsoft tanenbaum a s modern operating press redmond wash systems nd ed prentice hall englewood schneier b applied cryptography nd cliffs nj ed john wiley new york voydock v l and s t kent security shannon c e communication theory mechanisms in high level network protocols of secrecy systems bell system technical computing surveys journal october wofsey m m advances in computer spafford e h the internet worm crisis security management john wiley new york and aftermath communications of the acm wright c c cowan s smalley j morris and g kroahhartman linux security spafford g s garfinkel and a schwartz modules general security support for the linux practical unix and internet security rd kernel eleventh usenix security ed oreilly sebastopol calif symposium stallings w cryptography and network security principles and practice rd ed prentice hall nj part features of distributed systems a distributed system can consist of two or more computer systems each with its own clock and memory some networking hardware and a capability of part distributed operating systems table benefits of a distributed system feature description resource sharing an application may use resources located in different computer systems reliability a distributed system provides availability ie continuity of services despite occurrence of faults it is achieved through redundancies in the network and resources and in os services computation speedup parts of a computation can be executed in parallel in different computer systems thus reducing duration of an application ie its running time communication users or their subcomputations located at different nodes can communicate reliably by using os services incremental growth open system standards permit new subsystems to be added to a distributed system without having to replace or upgrade existing subsystems this way the cost of enhancing a capability of a distributed system is proportional to the additional capability desired performing some of the control functions of the os see definition benefits of a distributed system were discussed earlier in section these are summarized here in table use of distributed systems spread rapidly in s when computer hardware prices dropped and use of the open system standard facilitated incremental growth of a system an open system has welldefined and nonproprietary interfaces with its own components and with other systems these interfaces are typically developed or approved by a standards body so they have ready acceptance within the computer industry their use enables addition of new components and subsystems to a computer system thereby facilitating incremental growth the lan is an excellent example of an open system computer systems ranging from supercomputers to cheap pcs can be connected to it because they all use a standard interface when a distributed system is implemented by using a lan its computing capability can be enhanced incrementally by connecting new computer systems to the lan the benefits of distributed systems listed in table are realized using the following hardware and software components hardware components individual computer systems and networking hardware such as cables links and routers software components operating system components that handle creation and scheduling of distributed computations and use of distant resources os and programming language features that support writing of distributed computations and networking software which ensures reliable communication nodes of distributed systems a distributed system can contain different types of nodes a minicomputer node has a single cpu that is shared to service applications of several users a workstation node has a single cpu but services one or more applications initiated by a single user a node that is a multiprocessor system is called a processor pool node it contains several cpus and the number of cpus may exceed the number of users whose applications are serviced in parallel a cluster is a group of hosts that work together in an integrated manner a cluster constitutes a single node of a distributed system each individual host is a node within the cluster figure is a schematic diagram of a cluster the cluster is shown to have two nodes however more nodes may be added to provide incremental growth each node is a computer system having its own memory and io devices the nodes share disk storage such as a multihost raid which offers both high transfer rate and high reliability see section or a storage area network which offers incremental growth see section each node is connected to two networks a private lan to which only the nodes in the cluster are connected and a public network through which it can communicate with other nodes in the distributed system cluster software controls operation of all nodes in a cluster it can provide computation speedup by scheduling subtasks in an application on different nodes within the cluster and reliability by exploiting redundancy of cpus and resources within the cluster section describes how these features are implemented in the windows cluster server and the sun cluster raid node p cu cu cu cu p node m m private lan public network figure architecture of a cluster integrating operation of nodes of a distributed system to realize the benefits of resource sharing reliability and computation speedup summarized in table processes of an application should be scattered across various nodes in the system whenever possible to achieve computation speedup and efficiency of resources and whenever necessary to provide reliability it is achieved by integrating the operation of various nodes in the system through interactions of their kernels in this section we sample features of a few systems to illustrate different ways in which operation of nodes is integrated in section we discuss design issues in distributed operating systems network operating systems a network operating system is the earliest form of operating system for distributed architectures its goal is to provide resource sharing among two or more computer systems that operate under their own oss as shown in the schematic of figure the network os exists as a layer between the kernel of the local os and user processes if a process requests access to a local resource the network os layer simply passes the request to the kernel of the local os however if the request is for access to a nonlocal resource the network os layer contacts the network os layer of the node that contains the resource and implements access to the resource with its help many network operating systems have been developed on top of the unix operating system the newcastle connection also called unix united is a wellknown network os developed at the university of newcastle upon tyne it provided access to remote files by using system calls that are identical with those used for local files a network os is easier to implement than a fullfledged distributed os however local operating systems retain their identities and operate independently so their functioning is not integrated and their identities are visible to users in some network oss a user had to log into a remote operating system before he could utilize its resources this arrangement implies that a user must know where a resource is located in order to use it a network os can not balance or optimize utilization of resources thus some resources in a node may be heavily loaded while identical resources in other nodes may be lightly loaded or free the network os also can not provide fault tolerance a computation explicitly uses a resource id while accessing a resource so it has to be aborted if the resource fails user processes user processes network os network os computer layer layer computer system kernel of kernel of system local os local os figure a network operating system chapter distributed operating systems windows and sun cluster software cluster software is not a distributed operating system however it contains several features found in distributed operating systems it provides availability through redundancy of resources such as cpus and io devices and computation speedup by exploiting presence of several cpus within the cluster the windows cluster server provides fault tolerance support in clusters containing two or more server nodes an application has to use a special application program interface api to access cluster services basic fault tolerance is provided through raids of level or see section that are shared by all server nodes in addition when a fault or a shutdown occurs in one server the cluster server moves its functions to another server without causing a disruption in its services a cluster is managed by distributed control algorithms which are implemented through actions performed in all nodes see chapter these algorithms require that all nodes must have a consistent view of the cluster ie they must possess identical lists of nodes within the cluster the following arrangement is used to satisfy this requirement each node has a node manager which maintains the list of nodes in a cluster the node manager periodically sends messages called heartbeats to other node managers to detect node faults the node manager that detects a fault broadcasts a message containing details of the fault on the private lan on receiving this message each node corrects its list of nodes this event is called a regroup event a resource in the cluster server can be a physical resource a logical resource or a service a resource is implemented as a dynamic link library dll so it is specified by providing a dll interface a resource belongs to a group a group is owned by one node in the cluster at any time however it can be moved to another node in the event of a fault the resource manager in a node is responsible for starting and stopping a group if a resource fails the resource manager informs the failover manager and hands over the group containing the resource so that it can be restarted at another node when a node fault is detected all groups located in that node are pulled to other nodes so that resources in them can be accessed use of a shared disk facilitates this arrangement when a node is restored after a failure the failover manager decides which groups can be handed over to it this action is called a failback it safeguards resource efficiency in the system the handover and failback actions can also be performed manually the network load balancing feature distributes the incoming network traffic among the server nodes in a cluster it is achieved as follows a single ip address is assigned to the cluster however incoming messages go to all server nodes in the cluster on the basis of the current load distribution arrangement exactly one of the servers accepts the message and responds to it when a node fails its load is distributed among other nodes and when a new node joins the load distribution is reconfigured to direct some of the incoming traffic to the new node the sun cluster framework integrates a cluster of two or more sun systems operating under the solaris os to provide availability and scalability of services part distributed operating systems availability is provided through failover whereby the services that were running at a failed node are relocated to another node scalability is provided by sharing the load across servers three key components of the sun cluster are global process management distributed file system and networking global process management provides globally unique process ids this feature is useful in process migration wherein a process is transferred from one node to another to balance the computational loads in different nodes or to achieve computation speedup a migrated process should be able to continue using the same path names to access files from a new node use of a distributed file system provides this feature amoeba amoeba is a distributed operating system developed at the vrije universiteit in the netherlands during the s the primary goal of the amoeba project is to build a transparent distributed operating system that would have the look and feel of a standard timesharing os like unix another goal is to provide a testbed for distributed and parallel programming the amoeba system architecture has three main components x terminals a processor pool and servers such as file and print servers the x terminal is a user station consisting of a keyboard a mouse and a bitmapped terminal connected to a computer the processor pool has the features described in section the amoeba microkernel runs on all servers pool processors and terminals and performs the following four functions managing processes and threads providing lowlevel memory management support supporting communication handling lowlevel io amoeba provides kernellevel threads and two communication protocols one protocol supports the clientserver communication model through remote procedure calls rpcs while the other protocol provides group communication for actual message transmission both these protocols use an underlying internet protocol called the fast local internet protocol flip which is a network layer protocol in the iso protocol stack see section many functions performed by traditional kernels are implemented through servers that run on top of a microkernel thus actions like booting process creation and process scheduling are performed by servers the file system is also implemented as a file server this approach reduces the size of the microkernel and makes it suitable for a wide range of computer systems from servers to pool processors the concept of objects is central to amoeba objects are managed by servers and they are protected by using capabilities see section when a user logs in a shell is initiated in some host in the system as the user issues commands processes are created in some other hosts to execute the commands thus a users computation is spread across the hosts in the system there is no notion of a home machine for a user this disregard for machine boundaries shows how tightly all resources in the system are integrated amoeba uses the processor pool model of nodes in the system when a user issues a command the os allocates a few pool processors to the execution of the command where necessary pool processors are shared across users summary as mentioned in the first chapter a modern os can instructions in user programs at other times it service several user programs simultaneously the is achieved by sending a special signal called an os achieves it by interacting with the computer interrupt to the cpu interrupts are sent at the and user programs to perform several control funcoccurrence of a situation such as completion of an tions in this chapter we described relevant features io operation or a failure of some sort a software of a computer and discussed how they are used by interrupt known as a system call is sent when a the os and user programs program wishes to use a kernel service such as the operating system is a collection of rouallocation of a resource or opening of a file tines the instructions in its routines must be exethe cpu contains a set of control registers cuted on the cpu to realize its control functions whose contents govern its functioning the prothus the cpu should execute instructions in the gram status word psw is the collection of control os when a situation that requires the operating sysregisters of the cpu we refer to each control tems attention occurs whereas it should execute register as a field of the psw a program whose part overview execution was interrupted should be resumed at a larger random access memory ram which we later time to facilitate this the kernel saves the will simply call memory and a disk the cpu cpu state when an interrupt occurs the cpu accesses only the cache however the cache constate consists of the psw and programaccessible tains only some parts of a programs instructions registers which we call generalpurpose registers and data the other parts reside in memory the gprs operation of the interrupted program is hardware associated with the cache loads them into resumed by loading back the saved cpu state into the cache whenever the cpu tries to access them the psw and gprs the effective memory access time depends on what the cpu has two modes of operation confraction of instructions and data accessed by the trolled by the mode m field of the psw when cpu was found in the cache this fraction is called the cpu is in the user mode it can not execute the hit ratio sensitive instructions like those that load informathe inputoutput system is the slowest unit tion into psw fields like the mode field whereas of a computer the cpu can execute millions of it can execute all instructions when it is in the instructions in the amount of time required to kernel mode the os puts the cpu in the user perform an io operation some methods of permode while it is executing a user program and forming an io operation require participation of puts the cpu in the kernel mode while it is executthe cpu which wastes valuable cpu time hence ing instructions in the kernel this arrangement the inputoutput system of a computer uses direct prevents a program from executing instructions memory access dma technology to permit the that might interfere with other programs in the cpu and the io system to operate independently system the operating system exploits this feature to let the the memory hierarchy of a computer procpu execute instructions in a program while io vides the same effect as a fast and large memory operations of the same or different programs are though at a low cost it contains a very fast in progress this technique reduces cpu idle time and small memory called a cache a slower and and improves system performance test your concepts classify each of the following statements as true f a memory protection violation leads to a or false program interrupt a the condition code ie flags set by an g the kernel becomes aware that an io operainstruction is not a part of the cpu tion has completed when a program makes a state system call to inform it that the io operation b the state of the cpu changes when a prohas ended gram executes a noop ie no operation which of the following should be privileged ininstruction structions explain why c the software interrupt si instruction a put the cpu in kernel mode changes the mode of the cpu to kernel mode b load the size register d branch instructions in a program may lead c load a value in a generalpurpose register to low spatial locality but may provide high d mask off some interrupts temporal locality e forcibly terminate an io operation e when a dma is used the cpu is involved in data transfers to an io device during an io operation chapter the os the computer and user programs exercises what use does the kernel make of the interrupt a computer has two levels of cache memories code field in the psw which provide access times that are and the cpu should be in the kernel mode while times the access time of memory if the hit executing the kernel code and in the user mode ratio in each cache is the memory has an while executing a user program explain how it access time of microseconds and the time is achieved during operation of an os required to load a cache block is times the the kernel of an os masks off all interrupts duraccess time of the slower memory calculate the ing interrupt servicing discuss the advantages effective memory access time and disadvantages of such masking a computer has a cpu that can execute mil a computer system has the clocktickbased lion instructions per second and a memory that timer arrangement described in section has a transfer rate of million bytessecond explain how this arrangement can be used to when interrupt io is performed the interrupt maintain the time of day what are the limitaroutine has to execute instructions to transtions of this approach fer byte between memory and an io device an os supports a system call sleep which puts what is the maximum data transfer rate during the program making the call to sleep for the io operations implemented by using the follownumber of seconds indicated in the argument ing io modes a interrupt io and b dmaof the sleep call explain how this system call based io is implemented several units of an io device that has a peak a computer system organizes the saved psw data transfer rate of thousand bytessecond information area as a stack it pushes conand operates in the interrupt io mode are contents of the psw onto this stack during step nected to the computer in exercise how of the interrupt action see figure exmany of these units can operate at full speed at plain the advantages of a stack for interrupt the same time servicing a hypothetical os supports two system calls if the request made by a program through a for performing io operations the system call system call can not be satisfied straightaway the initio initiates an io operation and the syskernel informs the scheduling component that tem call awaitio ensures that the program would the program should not be selected for execuexecute further only after the io operation has tion until its request is met give examples of completed explain all actions that take place such requests when the program makes these two system calls a hypothetical os provides a system call for hint when none of the programs in the os can requesting allocation of memory an experiexecute on the cpu the os can put the cpu enced programmer offers the following advice into an infinite loop in which it does nothing it if your program contains many requests for would come out of the loop when an interrupt memory you can speed up its execution by comoccurs bining all these requests and making a single system call explain why this is so bibliography smith and handy describe cache memcomputer architecture and organization eg hayes ory organizations przybylski discusses cache patterson and hennessy hennessy and and memory hierarchy design memory hierarchy and patterson hamacher et al and stallings io organization are also covered in most books on part overview most books on operating systems discuss the sys hennessy j and d patterson computer tem calls interface bach contains a useful synarchitecture a quantitative approach rd ed opsis of unix system calls ogorman describes morgan kaufmann san mateo calif interrupt processing in linux beck et al bovet love r linux kernel development and cesati and love contain extensive disnd ed novell press cussions of linux system calls mauro and mcdougall mauro j and r mcdougall solaris describes system calls in solaris while russiinternals nd ed prentice hall englewood novich and solomon describes system calls in cliffs nj windows patterson d and j hennessy computer bach m j the design of the unix organization and design the hardwaresoftware operating system prenticehall englewood interface rd ed morgan kaufman san mateo cliffs nj calif beck m h bohme m dziadzka u kunitz przybylski a cache and memory r magnus c schroter and d verworner hierarchy design a performancedirected linux kernel programming rd ed approach morgan kaufmann san mateo pearson education new york calif bovet d p and m cesati understanding russinovich m e and d a solomon the linux kernel rd ed oreilly sebastopol microsoft windows internals th ed microsoft ogorman j linux process manager press redmond wash the internals of scheduling interrupts and smith a j cache memories acm signals john wiley new york computing surveys hamacher c z vranesic and s zaky stallings w computer organization and computer organization th ed mcgrawhill architecture th ed prentice hall upper saddle new york river nj handy j the cache memory book tanenbaum a structured computer nd ed academic press new york organization th ed prentice hall englewood hayes j computer architecture and cliffs nj organization rd ed mcgrawhill new york reliable interprocess communication in a conventional os processes that wish to communicate through messages exist in the same host and have unique ids assigned by its kernel however in a distributed system processes existing in different nodes may wish to communicate with one another hence the distributed os assigns globally unique names to processes it also provides an arrangement through which a process with a given name can be located in the system so that other processes can communicate with it we discuss both these features in section once the location of a destination process is determined a message meant for it can be sent to it over the network however message delivery may fail because of faults in communication links or nodes located in network paths to the destination process hence processes must make their own arrangement to ensure reliable delivery of messages this arrangement is in the form of an interprocess communication protocol ipc protocol which is a set of rules and conventions aimed at handling transient faults during message transmission the sender and destination processes invoke protocol routines when they execute the send and receive statements these routines perform necessary actions to ensure reliable delivery of messages table summarizes three key provisions in ipc protocols acknowledgments timeouts and retransmissions an acknowledgment informs the sender process that its message has been delivered to the destination process a timeout is said to have occurred if the sender process does not receive an acknowledgment in an expected interval of time the message is now retransmitted these steps are repeated until the sender process receives an acknowledgment the protocol is implemented as follows when a process sends a message the protocol routine invoked by it makes a system call to request an interrupt at the end of a specific time interval this interrupt is called a timeout interrupt when the message is delivered to the destination process the protocol routine invoked by the destination process sends an acknowledgment to the sender process to inform it that its message has been delivered if the timeout interrupt occurs table provisions for reliability in an ipc protocol provision description acknowledgment when a process receives a message the protocol routine invoked by it sends an acknowledgment to the sender of the message timeout the protocol specifies an interval of time within which it expects a sender process to receive an acknowledgment a timeout is said to have occurred if the acknowledgment is not received within this interval retransmission of a if a timeout interrupt occurs before the sender receives message an acknowledgment the protocol routine invoked by the sender retransmits the message part distributed operating systems in the senders site before an acknowledgment is received the protocol routine retransmits the message and makes a system call to request another timeout interrupt these actions are repeated until the sender receives an acknowledgment a similar arrangement may be used to ensure that a reply if any sent by the destination process reaches the sender process we discuss ipc protocols in sections naming of processes all entities in a distributed system whether processes or resources are assigned unique names as follows each host in a system is assigned a systemwide unique name which can be either numeric or symbolic and each process or resource in a host is assigned an id that is unique in the host this way the pair hostname processid is unique for each process and can be used as its name a process that wishes to send a message to another process uses a pair like humanresources pj as the name of the destination process where humanresources is the name of a host this name should be translated into a network address for sending the message to easily locate a host in the internet the internet is partitioned into a set of domains that have unique names each domain is partitioned into smaller domains that have unique names in the domain and so on a host has a unique name in the immediately containing domain but its name may not be unique in the internet so a unique name for a host is formed by adding names of all the domains that contain it separated by periods starting with the smallest domain and ending with the largest domain for example the host name everestcseiitbacin refers to the server everest in the computer science and engineering department of iit bombay which is in the academic domain in india the domain name space is hierarchically organized the top level in the hierarchy is occupied by an unnamed root domain this domain contains a small number of toplevel domains that represent either organizations of a specific kind or organizations within a country in the host name everestcseiitbacin in is the toplevel domain representing india and ac is the name of a domain containing academic organizations hence acin contains academic organizations in india ac is called a secondlevel domain because its name contains two domain names each host connected to the internet has a unique address known as the internet protocol address ip address the domain name system dns is a distributed internet directory service that provides the ip address of a host with a given name it has a name server in every domain which contains a directory giving the ip address of each host in the domain when a process operating in a host hi wishes to send a message to another process with the name hostname process id host hi performs name resolution to determine the ip address of host name host hi is called the resolver name resolution proceeds as follows the resolver knows the address of a name server for the root domain to resolve the name hostname the resolver sends it to the name server of the root domain this name server responds by returning the ip address of a name server for the chapter distributed operating systems toplevel domain in hostname the resolver now sends hostname to this name server which returns the address of a name server for the secondlevel domain and so on until a name server returns the address of the required host name resolution using name servers can be slow so each resolver can cache some name server data this technique speeds up repeated name resolution the same way a directory cache speeds up repeated references to the directory entry of a file see section an ip address can be kept in the cache for the amount of time specified as the time to live which is hour the name server of a domain is replicated to enhance its availability and to avoid contention ipc semantics ipc semantics is the set of properties of an ipc protocol ipc semantics depend on the arrangement of acknowledgments and retransmissions used in an ipc protocol table summarizes three commonly used ipc semantics atmostonce semantics result when a protocol does not use acknowledgments or retransmission these semantics are used if a lost message does not pose a serious threat to correctness of an application or if the application knows how to recover from such situations for example an application that receives periodic reports from other processes knows when a message is not received as expected so it may itself communicate with a sender whose message is lost and ask it to resend the message these semantics provide high communication efficiency because acknowledgments and retransmissions are not used atleastonce semantics result when a protocol uses acknowledgments and retransmission because a destination process may receive a message more than table ipc semantics semantics description atmostonce semantics a destination process either receives a message once or does not receive it these semantics are obtained when a process receiving a message does not send an acknowledgment and a sender process does not perform retransmission of messages atleastonce semantics a destination process is guaranteed to receive a message however it may receive several copies of the message these semantics are obtained when a process receiving a message sends an acknowledgment and a sender process retransmits a message if it does not receive an acknowledgment before a timeout occurs exactlyonce semantics a destination process receives a message exactly once these semantics are obtained when sending of acknowledgments and retransmissions are performed as in atleastonce semantics however the ipc protocol recognizes duplicate messages and discards them so that the receiver process receives the message only once part distributed operating systems once if an acknowledgment is lost or delayed because of congestion in the network a message received for the second or subsequent time is called a duplicate message an application can use atleastonce semantics only if processing of duplicate messages does not pose any correctness problems such as updating of data many times instead of only once exactlyonce semantics result when a protocol uses acknowledgments and retransmission but discards duplicate messages these semantics hide transient faults from both sender and receiver processes however the ipc protocol incurs high communication overhead due to handling of faults and duplicate messages ipc protocols an ipc protocol specifies what actions should be performed at the sites of sender and destination processes so that a message is delivered to a destination process and its reply is delivered to the sender process we describe how ipc protocols are classified and present a couple of examples reliable and unreliable protocols a reliable protocol guarantees that a message or its reply is not lost it achieves this through atleastonce or exactlyonce semantics for both messages and their replies an unreliable protocol does not guarantee that a message or its reply would not be lost it provides atmostonce semantics either for messages or for their replies as commented in the last section a reliable protocol incurs substantial overhead due to acknowledgments and retransmission of messages and replies whereas an unreliable protocol does not incur these overheads blocking and nonblocking protocols as discussed in chapter it is common to block a process that executes a receive system call if no messages have been sent to it there are no intrinsic reasons to block a process that executes a send system call however blocking of a sender process may simplify a protocol reduce its overhead and also add some desirable features to its semantics for example if a sender process is blocked until its message is delivered to a destination process the message would never have to be retransmitted after the sender is activated so the message need not be buffered by the protocol after the sender is activated also blocking of the sender helps to provide semantics similar to the conventional procedure call a protocol is a blocking protocol if a sender process is blocked until it receives a reply to its message otherwise it is a nonblocking protocol we assume that if a protocol does not block a sender process interrupts will be generated to notify the process of the arrival of a reply or an acknowledgment so that it can take appropriate actions blocking and nonblocking protocols are also called processsynchronous and asynchronous protocols respectively the requestreplyacknowledgment protocol the requestreplyacknowledgment rra protocol is a reliable protocol for use by processes that exchange requests and replies receipt of the reply implies that the destination process has received the request so a separate acknowledgment chapter distributed operating systems request buffer header reply reply buffer sender destination site site figure operation of a blocking version of the requestreplyacknowledgment rra protocol of the request is not needed the sender however sends an an explicit acknowledgment of the reply a blocking version of the rra protocol is presented as algorithm figure depicts its operation algorithm a blocking version of the rra protocol when a process makes a request the request is copied in a buffer called the request buffer in its site and also sent to the destination process in the form of a message a system call is made to request a timeout interrupt the sender process is blocked until a reply is received from the destination process when a destination process receives a message the destination process analyzes the request contained in the message and prepares a reply the reply is copied in a buffer called the reply buffer in the destination site and also sent to the sender process a system call is made to request a timeout interrupt when a timeout occurs in the sender process the copy of the request stored in the request buffer is retransmitted when the sender process receives a reply the sender process sends an acknowledgment to the destination process it also releases the request buffer if not already done when a timeout occurs in the destination process the copy of the reply stored in the reply buffer is retransmitted when the destination process receives an acknowledgment the destination process releases the reply buffer the sender process is blocked until it receives a reply so a single request buffer in the sender site suffices irrespective of the number of messages a process sends out or the number of processes it sends them to the destination process is not blocked on an acknowledgment so it could handle requests from other processes while it waits for an acknowledgment accordingly the destination site needs one reply buffer for each sender process the number of messages can be reduced through piggybacking which is the technique of including the acknowledgment of a reply in the next request to the same destination process since a sender process is blocked until it receives a reply an acknowledgment of a reply is actually implicit in the next request it makes hence only the reply to the last request would require an explicit acknowledgment message part distributed operating systems the rra protocol has the atleastonce semantics because messages and replies can not be lost however they might be delivered more than once as mentioned in table duplicate requests would have to be discarded in the destination site to provide exactlyonce semantics it can be achieved as follows a sender assigns ascending sequence numbers to its requests and includes them in its request messages the sequence number of a message is copied into its reply and acknowledgment and into the header field of the reply buffer in the destination site the destination process also separately preserves the sequence number of the last request received from the sender process if the sequence number in a request is not greater than the preserved sequence number the request is a duplicate request so the destination process simply retransmits the reply if its copy is present in the reply buffer otherwise either the copy of the reply in the reply buffer would have been discarded after receiving its acknowledgment in which case the request is an outdated retransmission or the destination process is still processing the request and would send its reply sometime in future in either of these cases the duplicate request is simply discarded the requestreply protocol the requestreply rr protocol simply performs retransmission of a request when a timeout occurs a nonblocking version of the rr protocol that provides the exactlyonce semantics is presented as algorithm figure depicts its operation algorithm a nonblocking version of the rr protocol when a process makes a request the request is copied in a request buffer in the sender site and also sent to the destination process in the form of a message a system call is made to request a timeout interrupt the sender process proceeds with its computation when the destination process receives a message if the message is not a duplicate request the destination process analyzes the request contained in the message and prepares a reply copies it in a reply buffer and also sends it to the sender process otherwise it simply locates the reply of the message in a reply buffer and sends it to the sender process request buffer header reply reply buffers sender destination site site figure operation of a nonblocking version of the requestreply rr protocol distributed computation paradigms data used in an application may be stored in different sites of a distributed system because of the following considerations data replication several copies of a data d may be kept in different sites of a distributed system to provide availability and efficient access data distribution parts of a data d may be kept in different sites of a system either because the data d is voluminous or because its parts originate in different sites or are frequently used in different sites part distributed operating systems table modes of accessing data in a distributed system mode of access description remote data access a computation accesses data over the network this mode of access does not interfere with organization or access of data and does not require restructuring of a computation however computations are slowed down by communication delays data migration the data is moved to the site where a computation is located data migration provides efficient data access however it may interfere with replication and distribution of data computation migration a computation or a part of it is moved to the site where its data is located it provides efficient data access without interfering with organization or access of data when data d is neither replicated nor distributed the os may position it such that the total network traffic generated by accesses to d by various applications is minimal table summarizes three modes of accessing data in a distributed system in remote data access the data is accessed in situ ie where it exists this mode of using data does not interfere with decisions concerning placement of the data however it is slow because of network latencies data migration involves moving data to the site of the computation that uses it this mode faces difficulties if data is used by many computations or if it has been replicated to provide high availability in the worst case it may force the data to be used strictly by one computation at a time computation migration moves a computation to the site where its data is located it does not interfere with replication or distribution of data operating systems provide some support for each data access mode summarized in table as described in section a network os supports remote data access the file transfer protocol ftp is a facility for data migration it performs transfer of files in an offline manner rather than during execution of a computation process migration is a feature for migrating a computation or a part of it while the computation is in progress it is described later in section a distributed computation is one whose parts can be executed in different sites for reasons of data access efficiency computation speedup or reliability a distributed computation paradigm is a model of useful practices for designing distributed computations the primary issues addressed by a distributed computation paradigm are manipulation of data and initiation of subcomputations in different sites of a distributed system table summarizes three distributed computation paradigms the clientserver computing paradigm focuses on remote data access and manipulation while the remote procedure call and remote evaluation paradigms provide different ways of performing computation migration chapter distributed operating systems table distributed computation paradigms paradigm description clientserver computing a server process provides a specific service to its clients a client process invokes its service by sending a message to it and the server returns its results in another message applications use the clientserver paradigm extensively to perform remote data access or remote data manipulation remote procedure a remote procedure resembles a conventional call rpc procedure except that it executes in a different node of the system a remote procedure is installed in a node by a system administrator and it is registered with a name server the remote procedure call has been used extensively for computation migration remote evaluation if a program uses the statement at node eval codesegment the compiler of the language in which the program is written makes a provision to transfer codesegment to the node designated by node execute it there and return its results there is no need to install the code segment in the remote node java provides a facility for remote evaluation client server computing a server is a process in a distributed system that provides a specific service to its clients typically the name of a server and a specification of its service are widely advertised in a system any process can send a message to a server and become its client a service may have a physical connotation like accessing or printing a file or it may have a computational connotation like evaluating mathematical functions in a math server accordingly the servers role ranges from mere data access to data manipulation in the latter case the server may even play a computational role in a distributed computation a server may become a bottleneck if the rate at which clients make requests exceeds the rate at which the server can service them figure depicts three methods of addressing this problem figure a shows many identical servers each with its own request queue the clients are partitioned in some way such that each client knows which server it should use this arrangement inherits the drawbacks of partitioning some servers may be heavily loaded while others are idle in figure b many servers dynamically share the same queue this arrangement is more flexible than partitioning the clients to use different servers figure c shows a multithreaded server a new thread is created to handle each request the threads compete with one another for cpu and other resources if the server function is iobound this arrangement can overlap servicing of several requests another way to eliminate the server bottleneck is to push most of the computational burden into a client process now the server can provide part distributed operating systems s s s s s s clients clients clients a b c figure servers with a independent and b shared queues c a multithreaded server better response times to clients design methodologies have been evolved to design such clientserver arrangements clientserver computing is a poor paradigm for distributed computing because methodologies for structuring a distributed computation in the form of a clientserver configuration have not been evolved the primary difficulty is that a distributed computation involves many entities with a symmetric relationship this relationship is hard to model with the clientserver paradigm in practice the clientserver paradigm is used extensively for noncomputational roles in a lan environment such as accessing files or handling simple database queries to make its implementation efficient simple protocols like the rr protocol are preferred over multilayered protocols like the iso protocol which is discussed in a later section remote procedure calls a remote procedure call rpc is a programming language feature designed for distributed computing as discussed earlier in section its syntax and semantics resemble those of a conventional procedure call in the remote procedure call call procid message procid is the id of a remote procedure and message is a list of parameters the call is implemented by using a blocking protocol the result of the call may be passed back through one of the parameters or through an explicit return value we can view the callercallee relationship as a clientserver relationship thus the remote procedure is the server and a process calling it is a client the schematic diagram of figure depicts the arrangement used to perform name resolution parameter passing and return of results during a remote procedure call the domain name system dns described in section is used to obtain the ip address of the called process the functions of the client and server stubs are as described earlier in section the client stub converts the parameters into a machineindependent form and the server stub converts them into the machinespecific representation suitable for the servers host whereas they play the converse roles for the results of the called procedure the circled chapter distributed operating systems name server client client server server process stub stub procedure client server site site figure implementation of a remote procedure call rpc numbers in figure denote the steps in implementing the remote procedure call details of these steps are as follows the client process calls the client stub with parameters this call is a conventional procedure call hence execution of the client process is suspended until the call is completed the client stub marshals the parameters and converts them into a machineindependent format it now prepares a message containing this representation of parameters the client stub interacts with the name server to find the identity of the site at which the remote procedure exists the client stub sends the message prepared in step to the site where the remote procedure exists using a blocking protocol this send operation blocks the client stub until a reply to its message arrives the server stub receives the message sent by the client stub it converts the parameters to the machinespecific format suitable for the server site the server stub now executes a call on the server procedure with these parameters this is a conventional procedure call hence execution of the server stub is suspended until the procedure call is completed the server procedure returns its results to the server stub the server stub converts them into a machineindependent format and prepares a message containing the results the message containing the results is sent to the client site the client stub converts the results into the format suitable for the client site the client stub returns the results to the client process step completes execution of the remote procedure call the client process is now free to continue its execution in step the client stub need not perform name resolution every time the rpc is executed it can do so the first time and save the information concerning site of the remote procedure in a name server cache for future use name resolution can even be performed statically ie before operation of the client process begins part distributed operating systems faults may occur during a remote procedure call either in the communication link in the server site or in the client itself if the client site crashes the call becomes an orphan because its result is not going to be of any use we discuss orphans and their handling later in section communication and server faults can be handled using an arrangement involving acknowledgments and retransmissions see section ideally rpcs should possess the exactlyonce semantics however it is expensive to implement these semantics atleastonce semantics are cheaper to implement however they require that either the actions of the remote procedure should be idempotent or that it must discard duplicate requests the remote procedure call feature can be used as a building block for distributed computations its advantages over the clientserver paradigm are due to two factors first it may be possible to set up a remote procedure by simply intimating its name and location to the name server it is much easier than setting up a server second only those processes that know of the existence of a remote procedure can invoke it so use of remote procedures provides more privacy and hence more security than use of the clientserver paradigm its primary disadvantage is a lack of flexibility the remote procedure has to be registered with a name server so its location can not be changed easily remote evaluation the remote evaluation paradigm was proposed by stamos and gifford the paradigm is implemented through the statement at node eval codesegment where node is an expression that evaluates to the identity of some node in the distributed system and codesegment is a segment of code possibly a sequence of statements when the at statement is encountered during operation of a process node is evaluated to obtain the identity of a node codesegment is executed in that node and its results if any are returned to the process this paradigm has several advantages over the clientserver and rpc paradigms it requires minimal support from the os most of the work is done by the compiler of the language in which the program is written with the help of the os the compiler makes a provision to transfer codesegment to the target node and to execute it there the os of the target node creates a process to execute the code and to return its results prior installation of codesegment or an elaborate setup of stub procedures is not needed the issues of naming and binding are also much simpler than in an rpc environment the decision about which node should be used to execute the code segment is taken dynamically this decision could use information concerning computational loads at various nodes codesegment can be any arbitrary section of code that can be executed remotely it need not have the syntactic shape of a procedure the remote evaluation paradigm can be used along with the clientserver or rpc paradigms ie the code segment could invoke procedures during its execution or it could itself be a procedure chapter distributed operating systems the remote evaluation paradigm can be used for computation speedup or for improving efficiency of a computation for example if a subcomputation involves considerable manipulation of data located at some specific node si the subcomputation can itself be executed at si it would reduce the amount of network traffic involved in remote data access similarly if a user wishes to send an email to a number of persons at si the mail sending command can itself be executed at si case studies sun rpc sun rpc was designed for clientserver communication in nfs the sun network file system nfs models file processing actions as idempotent actions so sun rpc provides the atleastonce semantics this feature makes the rpc efficient however it requires applications using rpc to make their own arrangements for duplicate suppression if exactlyonce semantics are desired sun rpc provides an interface language called xdr and an interface compiler called rpcgen to use a remote procedure a user has to write an interface definition for it in xdr which contains a specification of the remote procedure and its parameters the interface definition is compiled using rpcgen which produces the following a client stub the server procedure and a server stub a header file for use in the client and server programs and two parameter handling procedures that are invoked by the client and server stubs respectively the client program is compiled with the header file and the client stub while the server program is compiled with the header file and the server stub the parameter handling procedure invoked by the client stub marshals parameters and converts them into a machineindependent format called the external data representation xdr the procedure invoked by the server stub converts parameters from the xdr format into the machine representation suitable for the called procedure the sun rpc schematic has some limitations the remote procedure can accept only one parameter this limitation is overcome by defining a structure containing many data members and passing the structure as the parameter the rpc implementation also does not use the services of a name server instead each site contains a port mapper that is like a local name server it contains names of procedures and their port ids a procedure that is to be invoked as a remote procedure is assigned a port and this information is registered with the port mapper the client first makes a request to the port mapper of the remote site to find which port is used by the required remote procedure it then calls the procedure at that port a weakness of this arrangement is that a caller must know the site where a remote procedure exists java remote method invocation rmi a server application running on a host creates a special type of object called a remote object whose methods may be invoked by clients operating in other hosts the server selects a name for the service that is to be offered by a method of the remote object and registers it with a name server called the rmiregistry which runs on the servers networking the term networking includes both network hardware and network software thus it includes networking technology and design of computer networks as also software aspects of implementing communication between a pair of processes the basic issues in networking are summarized in table network type network topology and networking technology concern the design of networks all other issues concern message communication between processes finding the ip address of the node where a destination process is located deciding which route a message would follow to that node and ensuring that the message is delivered efficiently and reliably we discussed the domain name system dns that determines the ip address of a host in section all other issues in networking are discussed in this section types of networks a wide area network wan connects resources and users that are geographically distant when expensive mainframe computers were in use it made good sense to make them accessible to a large number of users from different organizations and different locations a wan made this possible the other motivation for wans was to enable communication and data sharing between users chapter distributed operating systems table issues in networking issue description network type the type of a network is determined by the geographical distribution of users and resources in the system two main types of networks are wide area networks wans and local area networks lans network topology network topology is the arrangement of nodes and communication links in a network it influences the speed and reliability of communication and the cost of network hardware networking technology networking technology is concerned with transmission of data over a network it influences network bandwidth and latency naming of processes using the domain name system dns the pair host name processid for a destination process is translated into the pair ip address processid connection strategy a connection strategy decides how to set up data paths between communicating processes it influences throughput of communication links and efficiency of communication between processes routing strategy a routing strategy decides the route along which a message would travel through the system it influences communication delays suffered by a message network protocols a network protocol is a set of rules and conventions that ensure effective communication over a network a hierarchy of network protocols is used to obtain a separation of various concerns involved in data transmission and reliability network bandwidth the bandwidth of a network is the rate at which data is and latency transferred over the network latency is the elapsed time before data is delivered at the destination site when inexpensive personal computers became available many organizations installed a large number of pcs within offices data used by pc users and resources like goodquality laser printers became critical resources so local area networks lans were set up to connect users and resources located within the same office or same building since all resources and users in a lan belonged to the same organization there was little motivation for sharing the data and resources with outsiders hence few lans were connected to wans though the technology for making such connections existed advent of the internet changed the scenario and most lans and wans are today connected to the internet figure illustrates wans and lans the lan consists of pcs printers and a file server it is connected to a wan through a gateway which is a computer that is connected to two or more networks and transfers messages between them part distributed operating systems workstations local area host network lan wide area cp network wan gateway file printer server cp cp host host figure types of networks bus star ring fully connected partially connected figure network topologies specialpurpose processors called communication processors cps are used in the wan to facilitate communication of messages between distant hosts lans use expensive highspeed cables like category or fiberoptic cables to provide high data transfer rates wans often use public lines for data transfer because of cost considerations so it is generally not possible to support high transfer rates network topology figure illustrates five network topologies these topologies differ in the cost of network hardware speed of communication and reliability the bus topology chapter distributed operating systems is similar to the bus in a pc all hosts are connected directly to the bus so the cost of network hardware is low only one pair of hosts can communicate over the bus at any time high transfer rates are achieved except when contention exists for the bus the bus topology is used in ethernetbased lans in the star topology each host is connected only to the host in the central site of the system this topology is useful when the distributed system contains one server and nodes contain processes that use this server reliability of a star network depends on reliability of the central host communication delays between a host and the central host or between two hosts depend on contention at the central host fast ethernet uses a star topology in a ring network each host has two neighbors when a host wishes to communicate with another host a message is passed along the ring until it reaches the destination host consequently the communication load on a host is high even when none of its processes is communicating in a unidirectional ring a link carries messages in only one direction whereas in a bidirectional ring a link can carry messages in both directions naturally unidirectional and bidirectional rings have different reliability characteristics a bidirectional ring network is immune to single host or link faults whereas a unidirectional ring network is not in a fully connected network a link exists between every pair of hosts consequently communication between a pair of hosts is immune to crashes of other hosts or faults in up to n links where n is the number of hosts in the network one or more hosts may become isolated if the number of faults exceeds n this situation is called network partitioning a partially connected network contains fewer links than a fully connected network it has a lower cost than a fully connected network however it may get partitioned with fewer host or link crashes than a fully connected network networking technologies we discuss three networking technologies the ethernet and token ring technologies are used for local area networks and the asynchronous transfer mode atm technology is used for isdn networks ethernet ethernet is a buslike network simple or branching bus using a circuit that consists of cables linked by repeaters several entities called stations are connected to the same cable data is transmitted in units called frames each frame contains addresses of its source and destination and a data field each station listens on the bus at all times it copies a frame in a buffer if the frame is meant for it otherwise it ignores the frame the original ethernet operated at a transmission rate of mbits per second fast ethernet which operates at mbits per second gigabit ethernet and gigabit ethernet are prevalent variants of ethernet a bridge is used to connect ethernet lans it is a computer that receives frames on one ethernet and depending on the destination addresses reproduces them on another ethernet to which it is connected since the basic ethernet topology is that of a bus only one conversation can be in progress at any time the carrier sense multiple access with collision part distributed operating systems detection csmacd technology ensures it as follows a station that wishes to send a message listens to the traffic on the cable to check whether a signal is being transmitted this check is called carrier sensing the station starts transmitting its frame if it does not detect a signal however if many stations find no signal on the cable and transmit at the same time their frames would interfere with one another causing abnormal voltage on the cable this situation is called a collision a station that detects a collision emits a special bit jam signal on receiving the jam signal any transmitting station that had not so far detected a collision becomes aware of a collision all the transmitting stations now back off by abandoning their transmissions and waiting for a random period of time before retransmitting their frames this procedure of recovering from a collision does not guarantee that the frames will not collide again however it helps in ensuring that eventually all frames will be transmitted and received without collisions the frame size must exceed a minimum that facilitates collision detection this size is bits for the mbps and mbps ethernets where mbps is an abbrevation of bits per second and bits for the gigabit ethernet token ring a token ring is a network with a ring topology that uses the notion of a token to decide which station may transmit a message at any time the token is a special message circulating over the network it has a status bit which can be either free or busy the status bit value busy indicates that a message is currently being transmitted over the network whereas the value free indicates that the network is currently idle any station that wishes to transmit a message waits until it sees the token with the status bit free it now changes the status to busy and starts transmitting its message thus a message follows a busy token so only one message can be in transit at any time a message can be of any length it need not be split into frames of a standard size every station that sees a message checks whether the message is intended for it only the destination station copies the message when the station that transmitted a message sees the busy token over the network it resets its status bit to free this action releases the network for another message transmission when early token release is supported the destination station resets the status bit of the token to free operation of the token ring comes to a halt if the token is lost because of communication errors one of the stations is responsible for recovering from this situation it listens continuously to the traffic on the network to check for the presence of a token and creates a new token if the token has been lost asynchronous transfer mode atm technology atm is a virtualcircuit oriented packetswitching technology see sections and the virtual circuit is called a virtual path in atm terminology and a packet is called a cell atm implements a virtual path between sites by reserving specific bandwidth in physical links situated in a network path between the sites that is by reserving a specific portion of the capacity of each physical link for the virtual path when a physical link is common to many virtual paths it multiplexes the traffic of the various virtual paths on a statistical basis such that each virtual path receives the specified portion of the bandwidth of the physical link this way cells to be chapter distributed operating systems transmitted on a virtual path do not face delays due to traffic on other virtual paths the principle of reserving bandwidth is carried one step further by hosts in an atm network a virtual path may be set up between two hosts say hosts x and y when a process pi in host x wishes to communicate with a process pj in host y the hosts may set up a virtual channel between pi and pj by reserving some bandwidth of the virtual path between x and y this twotier arrangement ensures that message traffic between a pair of processes does not incur delays due to message traffic between other pairs of processes the atm technology aims to provide realtime transport capabilities for multimedia applications incorporating diverse traffics such as voice video and highspeed data atm uses a cell size of bytes this size is a compromise between a small cell size that is desired in voice communication to ensure small delays and a largish cell size desired in data communication to reduce the overhead of forming packets for a message and assembling them back to form a message each cell contains a header of bytes and a data field of bytes the header contains two items of information a virtual path id vpi and a virtual channel id vci figure is a schematic diagram illustrating functioning of an atm switch the switch contains a routing table which has an entry for each virtual path defined in the switch the entry contains two fields the vpi field and the port field in figure the virtual path identifier of the incoming cell is n and the nth entry in the routing table contains m and p the switch copies m in the vpi field of the cell and sends out the modified cell on port p this simple arrangement ensures that the ids assigned to virtual paths need not be unique in the system they only need to be unique in the switch the switching actions are performed in the hardware of the switch they provide extremely fast switching of the order of low double digits of microseconds which makes it possible to provide lanlike transmission speeds over wide area networks while creating a new virtual path an application specifies the desired bandwidth the os sets up a virtual path by reserving the bandwidth in individual links choosing a unique virtual path identifier in each switch and updating its routing table while managing the traffic in virtual channels of the same virtual path hosts use statistical multiplexing to provide appropriate bandwidth to vpi port vpi vci p vpi vci n n m p m header data header data routing table atm switch figure an atm switch part distributed operating systems applications voice video data adaptation layer atm layer physical layer figure atm protocol reference model m pcm m m m m pi pj pi pj pi pcm pj m pcm circuit switching message switching packet switching a b c figure connection strategies circuit message and packet switching individual applications thus different applications can simultaneously transmit messages at different speeds over their virtual paths an atm network has a meshstar architecture atm switches are connected to one another in a mesh form hosts are connected to the atm switches as in a star network this strategy provides a path between every pair of nodes figure shows the protocol layers in the atm protocol reference model the physical layer performs transfer of cells across the network the atm layer performs transmission of messages between atm entities it performs multiplexing and demultiplexing of virtual channels into virtual paths cell scheduling and cell routing the atm adaptation layer provides different kinds of services to different kinds of traffic such as voice video and data communication it provides separate protocols for each kind of traffic connection strategies a connection is a data path between communicating processes a connection strategy also called a switching technique determines when a connection should be set up between a pair of processes and for how long it should be maintained choice of the switching technique influences efficiency of communication between a pair of processes and throughput of communication links figure illustrates three connection strategies we use the notation mi for a message and pcjmi for the jth packet of message mi where a packet has the meaning defined later in this section chapter distributed operating systems circuit switching a circuit is a connection that is used exclusively by a pair of communicating processes and carries all messages between them see figure a a circuit is set up when processes decide to communicate ie before the first message is transmitted and is destroyed sometime after the last message has been transmitted circuit set up actions involve deciding the actual network path that messages will follow and reserving communication resources accordingly each circuit is given a unique id and processes specify the circuit id while sending and receiving messages the advantage of circuit switching is that messages do not face delays once a circuit has been set up however a circuit ties up a set of communication resources and incurs set up overhead and delays so use of circuit switching is justified only if the overall message density in the system is low but mediumtoheavy traffic is expected between a pair of processes message switching a connection is established for every message exchanged between a pair of processes thus messages between the same pair of processes may travel over different paths in the system see figure b message switching incurs repetitive overhead and may cause delays due to the set up time of the connection so its use is justified if light message traffic exists between a pair of processes it does not tie up communication resources so other processes can use the same connection or some links in the connection for their communication traffic in the network should be heavy enough to exploit this possibility packet switching in packet switching a message is split into parts of a standard size called packets a connection is set up for each packet individually so packets of a message may travel along different paths see figure c and arrive out of sequence at a destination site use of packet switching incurs two kinds of overhead a packet has to carry some identification information in its header id of the message to which it belongs sequence number within the message and ids of the sender and destination processes and packets have to be assembled into messages in the destination site however use of fixedsize packets reduces the cost of retransmission when an error arises also links are not monopolized by specific pairs of processes hence all pairs of communicating processes receive fair and unbiased service these features make packet switching attractive for interactive processes because of the cost of setting up connections connectionless protocols are often used in practice for sending messages and packets in such a protocol the originating node simply selects one of its neighboring nodes and sends the message to it if that node is not the destination node it saves the message in its memory and decides which of the neighbors to send it to and so on until the message reaches the destination node this method is called the storeandforward method of transmitting a message a packet is transmitted similarly connectionless transmission can adapt better to traffic densities in communication links than message or packet switching because a node can make the choice of the link when it is ready to send out a message or packet it is typically implemented by exchanging traffic information among nodes and maintaining a table in each node that indicates which neighbor to send to in order to reach a specific part distributed operating systems destination node however each node should have a large memory for buffering messages and packets when its outgoing links are congested routing the routing function is invoked whenever a connection is to be set up it decides which network path would be used by the connection choice of the routing strategy influences ability to adapt to changing traffic patterns in the system figure illustrates three routing strategies fixed routing a path is permanently specified for communication between a pair of nodes see figure a when processes located in these nodes wish to communicate a connection is set up over this path fixed routing is simple and efficient to implement each node merely contains a table showing paths to all other nodes in the system however it lacks flexibility to deal with fluctuations in traffic densities and node or link faults hence its use can result in delays or low throughputs virtual circuit a path is selected at the start of a session between a pair of processes it is used for all messages sent during the session see figure b information concerning traffic densities and communication delays along different links in the system is used to decide the best path for a session hence this strategy can adapt to changing traffic patterns and node or link faults and it ensures good network throughput and response times dynamic routing a path is selected whenever a message or a packet is to be sent so different messages between a pair of processes and different packets of a message may use different paths see figure c this feature enables the routing strategy to respond more effectively to changes in traffic patterns and faults in nodes or links and achieve better throughput and response times than when virtual circuits are used in the arpanet which was the progenitor of the internet information about traffic density and communication delay along every link was constantly exchanged between nodes this information was used to determine the current best path to a given destination node pk pl pk pl pk m pl m m m m pi pj pi pj pi pj n n n n n m n fixed routing virtual circuit dynamic routing a b c figure routing strategies fixed routing virtual circuit and dynamic routing chapter distributed operating systems network protocols a network protocol is a set of rules and conventions used to implement communication over a network several concerns need to be addressed while implementing communication such as ensuring confidentiality of data achieving communication efficiency and handling data transmission errors therefore a hierarchy of network protocols is used in practice to provide a separation of concerns each protocol addresses one or more concerns and provides an interface to the protocols above and below it in the hierarchy the protocol layers are like the layers of abstraction in a model see section they provide the same benefits an entity using a protocol in a higher layer need not be aware of details at a lower layer accordingly lowerlevel protocols deal with datatransmissionrelated aspects such as detection of data transmission errors middlelevel protocols deal with formation of packets and routing and higherlevel protocols deal with semantic issues that concern applications eg atomicity of actions and confidentiality of data iso procotol the international organization for standardization iso developed an open systems interconnection reference model osi model for communication between entities in an open system this model consists of seven protocol layers described in table it is variously called the iso protocol the iso protocol stack or the osi model figure illustrates operation of the osi model when a message is exchanged by two application processes the message originates in an application which presents it to the application layer the application layer adds some control information to it in the form of a header field the message now passes through the presentation and session layers which add their own headers the presentation layer performs change of data representation and table layers of the iso protocol stack layer function physical layer provides electrical mechanisms for bit transmission over a physical link data link layer organizes received bits into frames performs error detection on frames performs flow control network layer performs switching and routing transport layer forms outgoing packets assembles incoming packets performs error detection and retransmission and flow control session layer establishes and terminates sessions provides for restart and recovery in applications presentation layer implements data semantics by performing change of representation compression and encryption decryption where necessary application layer provides network interface for applications part distributed operating systems application process application layer presentation layer session layer transport layer network layer data link layer physical layer sender receiver figure operation of the iso protocol stack encryptiondecryption the session layer establishes a connection between the sender and destination processes the transport layer splits the message into packets and hands over the packets to the network layer the network layer determines the link on which each packet is to be sent and hands over a link id and a packet to the data link layer the data link layer views the packet as a string of bits adds error detection and correction information to it and hands it over to the physical layer for actual transmission when the message is received the data link layer performs error detection and forms frames the transport layer forms messages and the presentation layer puts the data in the representation desired by the application the protocol layers are discussed in the following the physical layer is responsible for the mechanical electrical functional and procedural aspects of transmitting bit streams over the network it is implemented in the hardware of a networking device rsc and eiad are the common physical layer standards the data link layer provides error detection error correction and flow control facilities it splits the bit stream to be sent into fixedsize blocks called frames and adds a crc to each frame see section it provides flow control by sending frames at a rate that the receiver can handle hdlc highlevel data link control is a common protocol of this layer bridges and switches operate in this layer the network layer is responsible for providing connections and routes between two sites in a system it also collects information for routing popular protocols of this layer are the x protocol which is a connectionoriented protocol using virtual circuits and the internet protocol ip which is a connectionless protocol thus routing is the primary function of this layer and connection is an optional one routers operate in this layer the network layer is mostly redundant in lans and in systems with pointtopoint connections the transport layer provides errorfree transmission of messages between sites it splits a message into packets and hands them over to the network layer it handles communication errors like nondelivery of packets due to node or link faults this feature resembles the reliability feature of ipc protocols hence it is implemented analogously through timeouts and retransmissions chapter distributed operating systems see section the transport layer also performs flow control so that data is transferred at a rate that the receiver can handle the effective rate depends on the buffer space available in the receiver and the rate at which it can copy data out of the buffer iso has five classes of transport layer protocols named tp through tp other common transport layer protocols are the transport control protocol tcp which is a connectionoriented reliable protocol and user datagram protocol udp which is a connectionless unreliable protocol the session layer provides means to control the dialog between two entities that use a connectionoriented protocol it provides authentication different types of dialogs oneway twoway alternate or twoway simultaneous and checkpointrecovery facilities it provides dialog control to ensure that messages exchanged using nonblocking send primitives arrive in the correct order see section it also provides a quarantine service whereby messages are buffered at a receiver site until explicitly released by a sender this facility is useful in performing atomic actions in a file see section and in implementing atomic transactions see section the presentation layer supports services that change the representation of a message to address hardware differences between the sender and destination sites to preserve confidentiality of data through encryption and to reduce data volumes through compression the application layer supports applicationspecific services like file transfer email and remote log in some popular protocols of this layer are ftp file transfer protocol x email and rlogin remote login tcpip the transmission control protocol internet protocol tcpip is a popular protocol for communication over the internet it has fewer layers than the iso protocol so it is both more efficient and more complex to implement figure shows details of its layers the lowest layer is occupied by a data link protocol the internet protocol ip is a network layer protocol in the iso protocol stack it can run on top of any data link protocol the ip performs data transmission over the internet using the bit ip address of a destination host it is a connectionless unreliable protocol it does not guarantee that packets of a message will be delivered without error only once and in the correct order these properties are provided by the protocols occupying higher levels in the hierarchy iso layers file transfer protocol ftp email remote login or an applicationspecific protocol transmission control user datagram iso layer protocol tcp protocol udp iso layer internet protocol ip iso layer data link protocol figure the transmission control protocolinternet protocol tcpip stack part distributed operating systems protocols in the next higher layers provide communication between processes each host assigns unique bit port numbers to processes and a sender process uses a destination process address that is a pair ip address port number use of port numbers permits many processes within a host to send and receive messages concurrently some wellknown services such as ftp telnet smtp and http have been assigned standard port numbers by the internet assigned numbers authority iana other port numbers are assigned by the os in a host as shown in figure two protocols can be used in the layer above the ip which corresponds to the transport layer ie layer in the iso protocol stack the transmission control protocol tcp is a connectionoriented reliable protocol it employs a virtual circuit between two processes and provides reliability by retransmitting a message that is not received in an expected time interval see section for a discussion of acknowledgments and timeouts used to ensure reliable delivery of messages the overhead of ensuring reliability is high if the speeds of a sender and a receiver mismatch or if the network is overloaded hence the tcp performs flow control to ensure that a sender does not send packets faster than the rate at which a receiver can accept them and congestion control to ensure that traffic is regulated so that a network is not overloaded the user datagram protocol udp is a connectionless unreliable protocol that neither guarantees delivery of a packet nor ensures that packets of a message will be delivered in the correct order it incurs low overhead compared to the tcp because it does not have to set up and maintain a virtual circuit or ensure reliable delivery the udp is employed in multimedia applications and in video conferencing because the occasional loss of packets is not a correctness issue in these applications it only leads to poor picture quality these applications use their own flow and congestion control mechanisms such as reducing the resolution of pictures and consequently lowering the picture quality if a sender a receiver or the network is overloaded the top layer in the tcpip stack is occupied by an application layer protocol like the file transfer protocol an email protocol such as the smtp or a remote login protocol this layer corresponds to layers in the iso protocol when the udp is used in the lower layer the top layer can be occupied by an applicationspecific protocol implemented in an application process itself network bandwidth and latency when data is to be exchanged between two nodes network hardware and network protocols participate in data transfer over a link and communication processors cps store and forward the data until it reaches the destination node two aspects of network performance are the rate at which data can be delivered and how soon data can reach the destination node network bandwidth is the rate at which data is transferred over a network it is subject to various factors such as capacities of network links error rates and delays at routers bridges and gateways peak bandwidth is the theoretical maximum rate at which data can be transferred between two nodes effective bandwidth may be model of a distributed system a system model is employed to determine useful properties of a distributed system such as the impact of faults on its functioning and the latency and cost of message communication a distributed system is typically modeled as a graph s n e where n and e are sets of nodes and edges respectively each node may represent a host ie a computer system and each edge may represent a communication link connecting two nodes however as discussed later nodes and edges may also have other connotations the degree of a node is the number of edges connected to it each node is assumed to have an import list describing nonlocal resources and services that the node can utilize and an export list describing local resources of the node that are accessible to other nodes for simplicity we do not include the name server see section in the system model two kinds of graph models of a distributed system are useful in practice a physical model is used to represent the arrangement of physical entities in a distributed system in this model nodes and edges have the implications described earlier ie a node is a computer system and an edge is a communication link a logical model is an abstraction nodes in a logical model represent logical entities like processes and edges represent relationships between entities a logical model may use undirected or directed edges an undirected edge represents a symmetric relationship like twoway interprocess communication a directed edge represents an asymmetric relationship like the parentchild relationship between processes or oneway interprocess communication note that nodes and edges in a logical model may not have a onetoone correspondence with physical entities in a distributed system a system model is analyzed to determine useful properties of a system such as the ones described in table one important property is the resiliency of a system which is its ability to withstand faults without facing disruption a kresilient system can withstand any combination of up to k faults if n is the smallest degree of a node at least n faults must occur for a node to get isolated however fewer faults may be able to partition a system see exercise as illustrated in example analysis of the system model can be used as a network design technique as well part distributed operating systems table system properties determined by analyzing a system model property description impact of faults faults can isolate a node from the rest of the system or partition a system ie split it into two or more parts such that a node in one part can not be reached from a node in another part resiliency a system is said to be kresilient where k is a constant if k is the largest number of faults that the system can withstand without disruption latency between two the minimum latency of a communication path nodes depends on the minimum latency of each communication link in it the minimum latency between two nodes is the smallest of the minimum latencies across all paths between the nodes cost of sending the cost of this operation depends on topology of the information to every system and the algorithm used for sending the node information in a fully connected system containing n nodes the cost can be as low as n messages the cost may be more if the system is not fully connected example resiliency of a system if it is expected that only one or two sites in a system may suffer faults simultaneously and faults never occur in communication links availability of a resource is guaranteed if three units of the resource exist in three different sites in the system if communication links can also suffer faults but the total number of faults does not exceed two three units of each resource must exist and each site must have at least three communication links connected to it in such a system a resource becomes unavailable only if three or more faults occur when a node wishes to send some information to all other nodes in the system it can send the information to each of its neighbors in the form of a message and each neighbor receiving such a message for the first time can send similar messages to its neighbors and so on in this method a node would receive the information as many times as the number of edges connected to it so a total of e messages are required where e is the number of edges in the system however because a node needs to receive a message only once it is possible to use knowledge of the systems topology to manage with fewer messages for example if the system is fully connected it is possible to use a simpler protocol in which only the originator node sends messages to its neighbors this operation would require only n messages design issues in distributed operating systems the user of a distributed system expects its operating system to provide the look and feel of a conventional os and also provide the benefits of a distributed system summarized in table to meet these expectations the os must fully exploit the capabilities of all nodes by distributing data resources users and their computations effectively among the nodes of the system it gives rise to the following design issues transparency of resources and services transparency implies that names of resources and services do not depend on their locations in the system it enables an application to access local and nonlocal resources identically it also permits an os to change the location of a resource freely because a change in location does not affect the name of the resource and hence does not affect the applications that use the resource the os can exploit transparency to perform data migration to speed up applications reduce network traffic or optimize use of disks transparency also facilitates computation migration because the computation can continue to access resources as it did before it was migrated we discuss transparency in detail in chapter distribution of control functions a control function is a function performed by the kernel to control resources and processes in the system eg resource allocation deadlock handling and scheduling centralized control functions face two problems in a distributed system because of network latency it is not possible to obtain consistent information about the current state of processes and resources in all nodes of the system so the centralized function may not be able to arrive at correct decisions a centralized function is also a potential performance bottleneck and a single point of failure in the system to handle these problems a distributed os performs a control function through a distributed control algorithm whose actions are performed in several nodes of the system in a coordinated manner we discuss distributed algorithms for performing control functions such as deadlock detection scheduling and mutual exclusion in chapter system performance in addition to techniques of conventional oss a distributed os uses two new techniques to provide good system performance data migration and computation migration data migration is employed to reduce network latencies and improve response times of processes computation migration is employed to ensure that nearly equal amounts of computational load are directed at all cpus in the system this technique is called load balancing a distributed system typically grows in size over time through addition of nodes and users as the size of a system grows process response times may degrade part distributed operating systems because of increased loading of resources and services of the os and increased overhead of os control functions such degradation obstructs growth of a system so the performance of a distributed system should be scalable ie the delays and response times should not degrade with growth in system size and the throughput should increase with growth in system size an important scalability technique is to use selfsufficient clusters of hosts see section so that network traffic does not grow as more clusters are added to the system in chapter we discuss how the technique of file caching used in distributed file systems helps satisfy this requirement reliability fault tolerance techniques provide availability of resources and continuity of system operation when faults occur link and node faults are tolerated by providing redundancy of resources and communication links if a fault occurs in a network path to a resource or in the resource itself an application can use another network path to the resource or use another resource this way a resource is unavailable only when unforeseen faults occur consistency of data becomes an issue when data is distributed or replicated when several parts of distributed data are to be modified a fault should not put the system in a state in which some parts of the data have been updated but others have not been a distributed os employs a technique called twophase commit protocol to ensure that it does not happen see section parts of a computation may be performed in different nodes of a system if a node or link fault occurs during execution of such a computation the system should assess the damage caused by the fault and judiciously restore some of the subcomputations to previous states recorded in backups this approach is called recovery the system must also deal with uncertainties about the cause of a fault example illustrates these uncertainties example uncertainties about faults a distributed computation consists of two subcomputations represented by processes pi and pj executing in nodes n and n respectively see figure process pi sends a request to pj and waits for a response however a timeout occurs before it receives a reply the timeout could have been caused by any one of the following situations process pj never received the request so never started processing it the processing is taking longer than expected ie process pj is still processing the request process pj started processing the request but suffered a fault before completing it process pj completed the processing of the request but its reply to process pi was lost summary resource sharing reliability and computation and computation speedup such a computation speedup are the key benefits of distributed sysmay use data located in a distant node in three tems a distributed os realizes these benefits by ways remote data access uses the data over the integrating operation of individual computer sysnetwork data migration moves the data to the tems ensuring reliable network communication node where the computation exists and computaand effectively supporting operation of distributed tion migration moves a part of the computation to computations in this chapter we studied the relethe node where the data is located a distributed vant techniques of a distributed os computation paradigm is a model of distributed a distributed system consists of nodes concomputation that provides features for remote data nected to a network where a node could be an access data migration or computation migration individual computer system or a cluster which the clientserver paradigm provides remote data is a group of computers that share resources and access while the remote procedure call rpc and operate in an integrated manner a cluster can proremote evaluation paradigms provide computation vide computation speedup and reliability within migration a node processes located in different nodes of a disparts of a distributed computation can be exetributed system communicate by using an interprocuted in different nodes to achieve resource sharing cess communication protocol ipc protocol which part distributed operating systems is a set of rules for ensuring effective communicathe network and transmitting data at an approtion the protocol uses the domain name system priate rate effective network communication is dns to find the location of a destination proimplemented by a hierarchy of protocols called a cess ipc semantics describe the properties of an protocol stack in which each individual protocol ipc protocol a reliable protocol guarantees that a addresses a different concern in network communimessage would be delivered to the destination procation the iso protocol stack uses seven network cess in spite of faults in nodes and communication protocols the tcp and ip protocol stacks use links reliability is achieved as follows a process fewer protocols network performance is meathat receives a message returns an acknowledgment sured either as effective bandwidth which is the rate to the sender of the message the sender proat which data can be transferred over the network cess retransmits the message if an acknowledgment or as latency which is the delay involved in the is not received within the expected time intertransfer of data val in this protocol a message may be received a distributed system is modeled by a graph by the destination process more than once hence in a physical model nodes and edges of the graph it is called an atleastonce protocol a protoare nodes and links of the distributed system col would be called an exactlyonce protocol if respectively in a logical model they are proit arranges to recognize and discard duplicate cesses and relationships between processes respecmessages tively graph models of a system are used to network communication has to deal with trandetermine reliability properties of a system or sient faults in links and nodes of the system and as a basis for design of algorithms used by a network traffic densities in different parts of the distributed os network hence apart from ipc semantics the netnew design issues are faced by os designers in work software has to ensure reliability by detecting providing resource sharing reliability and perforand tolerating faults and ensure performance by mance in the distributed environment these issues finding an appropriate route for a message through are discussed in the next few chapters test your concepts classify each of the following statements as true f the sequence number in a message plays a or false role in implementing semantics of interproa failure of a single node partitions a ring cess communication network g in a reliable nonblocking interprocess comb when message switching is used all messages munication protocol a receiver process may between a pair of processes travel over the maintain only one reply buffer per sender same path in the network process c dynamic routing can adapt to link and node h a remote procedure call is useful for performfailures in a network ing data migration d a message sent using a virtual path in an i transferring n bytes between two nodes atm network might face a delay in a link requires only percent of the time required due to high traffic density to transfer n bytes e the atleastonce semantics are implemented by recognizing and discarding duplicate messages chapter distributed operating systems a b figure exercises for determining resiliency of distributed systems exercises discuss which process synchronization means until it receives an acknowledgment of its reply used in symmetrical multiprocessor systems can analyze the properties of this protocol be adapted for use in clusters see chapter a determine the i site faults and ii link explore the possibility of implementing the faults that the systems of figure a can blocking and nonblocking protocols through tolerate for interprocess communication monitors what are the difficulties in the impleb determine placement of copies of data d in mentation the systems of figure b if d is to be write a short note on factors that influence the available despite two sitelink faults in the duration of the timeout interval in the rra system protocol of section the diameter of a distributed system d is the develop schemes to discard duplicate replies largest number of links in any shortest path received in the sender site in the blocking and between nodes of the system if the maximum nonblocking versions of the rra protocol communication delay along any link in the sys requests made by nonblocking send calls may tem is what is the maximum communication arrive out of sequence at the destination site delay in the system explain the conditions when dynamic routing is used discuss how a under which it occurs nonblocking rr protocol should discard dupli compare the rpc and remote evaluation cate requests when this property holds refer to paradigms on the following basis section a flexibility one change is made in the rra protocol of b efficiency section a destination process blocks c security bibliography tanenbaum and van renesse is a survey article procedure calls lin and gannon discusses a on distributed operating systems it discusses blocking remote procedure call rpc schematic with exactlyand nonblocking communication protocols the texts once semantics stamos and gifford discusses by sinha tanenbaum and van steen and remote evaluation tanenbaum discusses the coulouris et al discuss the topics included in this iso protocol the clientserver model and the rpc chapter birman discusses the clientserver model and comer and stevens discusses the client the rpc server computing model birrell and nelson tanenbaum is a text devoted to computer discusses implementation of remote procedure calls networks it covers the iso protocol in great detail tay and ananda is a survey article on remote comer is a broad introduction to networking part distributed operating systems it explains the tcpip protocol stallings dis stallings w computer networking with cusses various networking protocols stevens and rago internet protocols prentice hall englewood describes network programming in unix cliffs nj birman k reliable distributed systems stamos j w and d k gifford remote technologies web services and applications evaluation acm transactions on programming springer berlin languages and systems birrell a d and b j nelson stevens w r and s a rago advanced implementing remote procedure calls acm programming in the unix environment nd ed transactions on computer systems addisonwesley professional comer d computer networks and tanenbaum a s modern operating internets th ed prentice hall englewood systems nd ed prentice hall englewood cliffs cliffs nj nj comer d and d stevens tanenbaum a s computer networks internetworking with tcpip vol iii th ed prentice hall englewood clientserver programming and applications cliffs nj linuxposix socket version prentice hall tanenbaum a s and m van steen englewood cliffs nj distributed systems principles and paradigms coulouris g j dollimore and t kindberg prentice hall englewood cliffs nj distributed systems concepts and tanenbaum a s and r van renesse design th ed addisonwesley new york distributed operating systems computing lin k j and j d gannon atomic surveys remote procedure call ieee transactions on tay b h and a l ananda a survey software engineering of remote procedure calls operating systems sinha p k distributed operating review systems ieee press new york notions of time and state time is the fourth dimension it indicates when an event occurred the state of an entity is the condition or mode of its being the state of an entity depends on its features eg the state of a memory cell is the value contained in it if an entity is states and events in a distributed system local and global states each entity in a system has its own state the state of a memory cell is the value contained in it the state of a cpu is the contents of its psw and generalpurpose registers the state of a process is its state tag state of the memory allocated to it the cpu state if it is currently scheduled on the cpu or contents of pcb fields if it is not scheduled on the cpu and the state of its interprocess communication which consists of information concerning the messages received and sent by it the state of an entity is called a local state the global state of a system at time instant t is the collection of local states of all entities in it at time t we denote the local state of a process pk at time t as skt where the subscript is omitted if the identity of the process is implicit in the context we denote the global state of a system at time t as st if a system contains n processes p pn st st st snt time clocks and event precedences let a global clock be an abstract clock that can be accessed from different sites of a distributed system with identical results if processes in two nodes of a distributed system perform the system call give current time at the same time instant they would obtain identical time values if they perform these system calls time units apart they would obtain time values that differ by exactly time units a global clock can not be implemented in practice because of communication delays requests for current time made in two different nodes at the same time instant would face different communication delays to reach the site where the clock is maintained consequently they would be given different time values similarly requests that are made time units apart may get time values that do not differ by exactly time units since a global clock can not be implemented we can explore an alternative arrangement that uses a clock in each process such a clock is called a local clock the local clock of a process would be accessed whenever the process performs a give current time system call to implement a practical timekeeping service using this idea local clocks should be reasonably well synchronized section discusses how clock synchronization can be achieved using the notion of event precedence part distributed operating systems event precedence the notation e e is used to indicate that event e precedes event e in time ie event e occurred before event e event ordering implies arranging a set of events in a sequence such that each event in the sequence precedes the next one in essence it implies determining the order in which events have occurred in a system a total order with respect to the precedes relation is said to exist if all events that can occur in a system can be ordered a partial order implies that some events can be ordered but not all events can be ordered to be precise at least two events exist that can not be ordered table summarizes the fundamental rules used to order events these rules can be explained as follows the os can readily determine precedence between events occurring within the same process events like execution of a send p message mi event in a process p and a receive event in p that receives message mi have a causal relationship ie a causeandeffect relationship consequently the send event in process p which is the cause precedes the receive event in process p which is its effect the precedes relation is transitive in nature hence e e if e e and e e this property can be used to determine precedences between some events that neither have a causal relationship nor occur within the same process for example an event ei preceding the send event for message mi in p precedes an event ej that follows the receive event for message mi in p because ei precedes the send event the send event precedes the receive event and the receive event precedes event ej using the rules of table precedence between any two events ei and ej can be classified as follows ei precedes ej if events ek and el exist such that ek el ei ek or ei ek and el ej or el ej ei follows ej if events eg and eh exist such that eg eh ej eg or ej eg and eh ei or eh ei ei is concurrent with ej if ei neither precedes nor follows ej a timing diagram is a plot of the activities of different processes against time processes are marked along the vertical axis in the plot and time is marked along table rules for ordering of events in a distributed system category description of rule events within a process the os performs event handling so it knows the order in which events occur within a process events in different in a causal relationship ie a causeandeffect processes relationship an event that corresponds to the cause precedes an event in another process that corresponds to the effect transitive precedence the precedes relation is transitive ie e e and e e implies e e chapter theoretical issues in distributed systems the horizontal axis we use the notation ekn for event en in process pk example demonstrates use of a timing diagram in determining event precedences by using transitiveness of the precedes relation it also illustrates why a total order over events does not exist in a distributed system event precedence example figure shows activities in processes p and p event e is a send event while e is a receive event for message m hence e e the transitive nature of leads to the precedence relations e e and e e transitiveness also yields e e and e e event e is concurrent with events e and e it is also concurrent with events e and e logical clocks an os needs a practical method of event ordering for purposes related to scheduling and resource allocation the method should be efficient so it should perform event ordering directly instead of working through causal relationships and transitivity it should also provide a total order over events so that the os can provide fcfs service to resource requests such an order can be obtained by incorporating event precedences into the event order arbitrarily ordering events that are concurrent eg the events e and e in figure timestamping of events provides a direct method of event ordering each process has a local clock that is accessible only to itself the timestamp of an event is its occurrence time according to the local clock of the process let tsei represent the timestamp of event ei event ordering is performed in accordance with the timestamps of events ie ei ej ei ej if tsei tsej and ej ei if tsei tsej however local clocks in different processes may show different times because of clock drift which would affect reliability of timestampbased event ordering for example if event ei occurred before event ej tsei should be tsej however if the clock at the process where event ei occurred is running faster than the clock at the process where ej occurred tsei may be tsej to avoid such situations it is necessary to synchronize the clocks of all processes p e e e m p e e e e figure event precedence via timing diagram part distributed operating systems synchronization of local clocks is achieved by using the causal relationship found in interprocess message exchange consider a message m sent by process pk to process pl let timestamps of the send and receive events be tk and tl respectively the causeandeffect relationship between the send and receive events implies that tk must be smaller than tl if it is not the situation can be corrected by increasing the time in the local clock of the receiver process to some value greater than tk before timestamping the receive event algorithm is the complete clock synchronization algorithm algorithm clock synchronization when a process pk wishes to send a message m to process pl pk executes a command send pl tssendm m where tssendm is a timestamp obtained just prior to sending message m when process pl receives a message process pl performs the actions if local clockpl tssendm then local clockpl tssendm timestamp the receive event where local clockpl is the value in the local clock of process pl and is the average communication delay in the network the clock synchronization achieved by algorithm is called loose synchronization because clocks of the two processes are mutually consistent at a message exchange but can drift apart at other times see exercise the quality of clock synchronization depends on the frequency of interprocess communication a higher frequency of communication provides tighter synchronization synchronization can be improved by using special synchronization messages that are exchanged at short intervals simply to keep the clocks synchronized note that clocks synchronized in this manner do not show real time for example if the clock at a sender process is fast the clock at the receiver process would also be advanced once we accept that the local clocks do not show real time there is no need to keep incrementing them all the time a process may increment its local clock by only when an event occurs in it and synchronize the local clock if necessary when it receives a message such clocks are called logical clocks we denote the logical clock of process pk by lck logical clocks are implemented according to the following rules r a process pk increments lck by whenever an event occurs in it r when process pk receives a message m containing tssendm pk sets its clock by the rule lck max lck tssendm the next example illustrates synchronization of logical clocks by these rules example synchronization of logical clocks figure contains the timing diagram for a system consisting of three processes the pair of numbers appearing in parentheses below an event indicate values in the logical clock of the process before and after the event the logical chapter theoretical issues in distributed systems e e e p m e m m p e e e m p e e figure synchronization of logical clocks clock in p contains when p decides to send message m to p it is incremented by rule r and the send event is timestamped so m contains the timestamp when p receives the message its clock reads it first increments its clock to using rule r and then synchronizes it using rule r the clock now reads when p decides to send message m its logical clock is incremented to m thus contains the timestamp when m reaches p p applies rules r and r rule r has no effect since ps logical clock value is much larger than the timestamp in the message when p sends m to p ps clock is synchronized to ps clock similarly ps clock gets synchronized to ps clock when p sends m to p timestamps obtained by using logical clocks have the property that tsei tsej if ei ej however the converse is not guaranteed for events occurring in different processes ie ei may not precede ej even if tsei tsej such a situation may arise if ei and ej occur in processes x and y respectively of the system and there has been no direct synchronization of the clocks of x and y either due to lack of message traffic between them or because the clock at process y is running faster than that at process x because more events occurred in process y than in process x we see this situation in figure where e occurs earlier than event e but has a larger timestamp than that of e obtaining unique timestamps events in different processes would obtain identical timestamps if the logical clocks in their processes happen to have identical time values when they occurred consequently these timestamps can not be used to obtain a total order over events this problem can be overcome by using a pair ptsei as the timestamp of an event ei where ptsei local time process id part distributed operating systems this way events can not have identical timestamps event ordering is now performed by defining event precedence as follows ei precedes ej iff i ptseilocal time ptsej local time or ii ptseilocal time ptsejlocal time and ptseiprocess id ptsej process id where ptseilocal time and ptseiprocess id are the local time and process id in ptsei respectively note that this notion of event precedence would provide an identical ordering of events ei and ej as that obtained through logical clocks if processes had different local times when events ei and ej occurred in them vector clocks a vector clock is a vector containing n elements where n is the number of processes in the distributed system we denote the vector clock of process pk by vck and its lth element by vckl elements of the vector clock vck have the following significance vc k k the logical clock of process pk vckl l k the highest value in the logical clock of process pl which is known to process pk that is the highest value of vcl l known to it the timestamp of an event ei occurring in a process pk is the value of vck when ei occurred thus the timestamp is a vector we call it the vector timestamp we denote the vector timestamp of event ei by vtsei and the lth element of vtsei by vtseil when process pk sends a message m to process pl it includes vtssendm in the message vector clocks are implemented according to the following rules r a process pk increments vckk by whenever an event occurs in it r when process pk receives a message m containing vtssendm pk sets its clock as follows for all l vckl max vckl vtssendml from these rules vckk vcl k for all l if ei ej are two consecutive events in process pk vtsej k vtsei k by rule r if ei ej were send and receive events for a message in processes pg and pk respectively process pk would increment vckk by rule r when ej occurs and then update vck by rule r before timestamping ej consequently vtseil vtsejl for all l and vtseik vtsej k we represent this condition as vtsei vtsej the precedence between events ei and ej is obtained as follows ei precedes ej for all l vtsei l vtsej l but for some k vtsei k vtsej k chapter theoretical issues in distributed systems ei follows ej for all l vtseil vtsej l but for some k vtseik vtsej k ei ej are concurrent for some k l vtseik vtsej k and vtseil vtsej l timestamping through vector clocks has two important properties every event has a unique timestamp as a result of rules r and r and vtsei vtsej if and only if ei ej the next example illustrates these properties synchronization of vector clocks example figure shows synchronization of vector clocks for the system of figure the vector timestamp after the occurrence of an event is shown below it when message m is received vc is incremented by and vc is updated to analogously when message m is received by process p vc is incremented by and vc and vc are updated events e and e are concurrent events because vtse vtse and vtse vtse the property that vtsei vtsej if and only if ei ej implies that vector clocks do not provide a total order over events total order can be obtained by using a pair pvtsei local time process id as the timestamp of an event ei and the following event ordering relation ei precedes ej iff i pvtseilocal time pvtsejlocal time or ii ei ej are concurrent events and pvtseiprocess id pvtsejprocess id where pvtseilocal time and pvtseiprocess id are the local vector time and process id in pvtsei respectively e e e p m e m m p e e e m p e e figure synchronization of vector clocks computing environments and nature of computations a computing environment consists of a computer system its interfaces with other systems and the services provided by its operating system to its users and their programs computing environments evolve continuously to provide better quality of service to users however the operating system has to perform more complex tasks as computer systems become more powerful their interfaces with io devices and with other computer systems become more complex and its users demand new services part overview the nature of computations in a computing environment and the manner in which users realize them depends on features of the computing environment in a typical modern computing environment a user initiates diverse activities simultaneously eg he may run a mail handler edit a few files initiate computations listen to music or watch a video and browse the internet at the same time the operating system has to provide the resources required by each of these activities such as the cpu and memory and io devices located either within the same computer system or in another computer system that can be accessed over the internet so that the activities progress to the users satisfaction we will begin the discussion of operating systems by taking a quick look at how computing environments evolved to their present form noninteractive computing environments these are the earliest forms of computing environments in these environments a user submits both a computation in the form of a program and its data together to the operating system the computation is performed by the operating system and its results are presented back to the user the user has no contact with the computation during its execution hence these computations can be viewed as passive entities to be interpreted and realized by the operating system examples of noninteractive computations are scientific computations involving number crunching and database updates performed overnight in these computing environments the operating system focuses on efficient use of resources computations used in a noninteractive environment are in the form of a program or a job a program is a set of functions or modules that can be executed by itself a job is a sequence of programs that together achieve a desired goal a program in a job is executed only if previous programs in the job have executed successfully for example consider compilation linking and execution of a c program a job to achieve these actions would consist of execution of a c compiler followed by execution of a linker to link the program with functions from libraries followed by execution of the linked program here linking is meaningful only if the program is compiled successfully and execution is meaningful only if linking is successful interactive computing environments in these computing environments a user may interact with a computation while it is in progress the nature of an interaction between a user and his computation depends on how the computation is coded eg a user may input the name of a data file to a computation during its execution or may directly input some data to it and the computation may display a result after processing the data the operating system focuses on reducing the average amount of time required to implement an interaction between a user and his computation a user also interacts with the os to initiate a computation typically each user command to the os calls for separate execution of a program here the notion of a job is not important because a user would himself consider the dependence of programs while issuing the next command for example if a c program is to be compiled linked and executed a user would attempt linking only if chapter overview of operating systems table computations in an os computation description program a program is a set of functions or modules including some functions or modules obtained from libraries job a job is a sequence of programs that together achieve a common goal it is not meaningful to execute a program in a job unless previous programs in the job have been executed successfully process a process is an execution of a program subrequest a subrequest is the presentation of a computational requirement by a user to a process each subrequest produces a single response which consists of a set of results or actions the program had compiled successfully hence operating systems for interactive environments deal exclusively with execution of programs not jobs os literature uses the term process for an execution of a program in an interactive environment in principle the term process is applicable in both noninteractive and interactive environments however we will follow the convention and use it only in the context of interactive computing environments a users interaction with a process consists of presentation of a computational requirement a subrequest by the user to the process and a response by the process depending on the nature of a subrequest the response may be in the form of a set of results or a set of actions such as file operations or database updates table describes the program job process and subrequest computations realtime distributed and embedded environments some computations have special requirements hence special computing environments are developed to service them a realtime computation is one that works under specific time constraints so its actions are effective only if they are completed within a specified interval of time for example a computation that periodically samples the data from an instrument and stores the samples in a file must finish storing a sample before it is due to take the next sample the operating system in a realtime environment uses special techniques to ensure that computations are completed within their time constraints the distributed computing environment enables a computation to use resources located in several computer systems through a network in the embedded computing environment the computer system is a part of a specific hardware system such as a household appliance a subsystem of an automobile or a handheld device such as a personal digital assistant pda and runs computations that effectively control the system the computer is typically an inexpensive one with a minimal configuration its os has to meet the time constraints arising from the nature of the system being controlled modern computing environments to support diverse applications the computing environment of a modern computer has features of several of the computing environments described earlier consequently its operating system has to employ recording the state of a distributed system as discussed in section the global state of a distributed system at a time instant t is the collection of local states of all entities in the system at time t however it is not possible to get all nodes to record their states at the same time instant because local clocks are not perfectly synchronized any other collection of local states may be inconsistent consider the distributed system shown in figure a banking application has a process p in node n and a process p in node n which perform the following actions process p debits to account a process p sends a message to process p to credit to account b process p credits to account b the recorded states of nodes n and n would be inconsistent if the balance in account a is recorded before step and that in account b is recorded after step a distributed os can not use such a state to perform its control functions in this section we present an algorithm for obtaining a consistent collection of local states such a collection of states is not a substitute for the global state however it has properties that facilitate some of the control functions in a distributed os consistent state recording a state recording is a collection of local states of entities in a system obtained through some algorithm a consistent state recording is one in which process states of every pair of processes in the system are consistent according to definition definition mutually consistent local states local states of processes pk and pl are mutually consistent if every message recorded as received from pl in pks state is recorded as sent to pk in pl s state and every message recorded as received from pk in pl s state is recorded as sent to pl in pks state in the state recording mentioned at the start of this section the state of p indicates that it has received the message from p concerning credit of in account b but the state of p does not indicate that it has sent such a message a b n n figure a funds transfer system chapter theoretical issues in distributed systems hence the state recording is inconsistent a state recording that shows any of the following situations would have been consistent accounts a and b contain and respectively accounts a and b contain and respectively accounts a and b contain and respectively in case none of the processes had received a message before its state was recorded so the process states are consistent in case the message recorded as received from p in ps state is recorded as sent in ps state in case again none of the process states records receipt of a message the message from p to p is in transit it will be delivered to process p sometime in future and process p will add to account b when it receives the message this is why mutual consistency of process states requires that every message recorded as received should be recorded as sent but not vice versa properties of a consistent state recording figure shows a model of a distributed computation the computation consists of four processes pp that communicate among themselves through messages an edge pi pj represents a channel chij ie an interprocess communication path that is used by process pi to send messages to process pj note that a channel is unidirectional a process either sends or receives messages along a channel but not both channels ch and ch together indicate that processes p and p send messages to one another figure shows the timing diagram of the computation table shows states of processes pp recorded at time instants tp tp tp and tp respectively these time instants are marked with the symbol in the figure the state of process p shows that it has received message m but not sent out any messages while the state of process p shows that it has sent out messages m and m before tp but not received any messages these states are mutually consistent according to definition however the states of p and p are not mutually consistent because the state of process p records message m as received but process ps state does not record it as sent hence the state recording of table is not a consistent state recording p p ch ch ch ch p p ch figure a distributed computation for state recording part distributed operating systems e e tp e p m tp p e e e e e m e tp m p e e e tp m m p e e figure a timing diagram for the distributed computation of figure table local states of processes process description of recorded state p no messages have been sent message m has been received p messages m and m have been sent no messages have been received p no messages have been sent message m has been received p no messages have been sent no messages have been received cut of a system the notion of a cut of a system helps in determining the consistency of a state recording let tpi be the point in a timing diagram at which the state of a process pi is recorded definition cut of a system a curve that connects the points in a timing diagram at which states of processes are recorded in increasing order by process number the cut of the distributed computation shown in figure represents the recorded state shown in table the term a cut is taken means that a collection of local states is recorded an event that had occurred in a process before the state of the process was recorded is said to occur to the left of the cut in the timing diagram such an event belongs in the past of the cut an event that would occur in a process after the state of the process was recorded is said to occur to the right of the cut in the timing diagram such an event belongs to the future of the cut a cut represents a consistent state recording of a system if the states of each pair of processes satisfy definition state of a channel the state of a channel chij is the set of messages contained in chij ie the messages sent by process pi that are not yet received chapter theoretical issues in distributed systems by process pj we use the following notation to determine the state of a channel chij recordedsentij the set of messages recorded as sent over channel chij in the state of pi recordedrecd ij the set of messages recorded as received over channel chij in the state of pj recordedsentij recordedrecd ij implies that all messages sent by pi have been received by pj hence the channel is empty recordedsentij recordedrecdij where represents the set difference operator implies that some messages sent by pi have not been received by pj these messages are still contained in channel chij recordedrecdij recordedsentij implies that process pj has recorded as received at least one message that is not recorded as sent by process pi this situation indicates inconsistency of the recorded local states of pi and pj according to definition a cut in the timing diagram may intersect with a message mk sent by process pi to process pj over channel chij the manner of the cut indicates whether the recorded states of pi and pj are consistent with respect to the sending and receipt of the message it also indicates the state of the channel three possibilities are no intersection with a message the message send and receive events are either both located to the left of the cut or both located to the right of the cut in either case the message did not exist in channel chij when the cut was taken forward intersection with a message the message send event is located to the left of the cut and the message receive event is located to the right of the cut hence the message existed in channel chij when the cut was taken the cut in the timing diagram of figure has a forward intersection with message m backward intersection with a message the message send event is located to the right of the cut and the message receive event is located to the left of the cut hence the message had been received but had not been sent when the cut was taken such a message indicates an inconsistency in the recorded state the cut in the timing diagram of figure has a backward intersection with message m from these observations we can formulate a consistency condition for a cut as follows cc a cut c represents a consistent state recording of a distributed system if the future of the cut is closed under the precedes relation on events ie closed under condition cc can be explained as follows a set of items i is said to be closed under a relation r if using the relation on any item in i yields an item that is also in i let i be the set of events in the future of a cut applying the relation to an event ei i gives us an event ej such that ei ej ie ei precedes ej if i is closed under this event also belongs to i that is it does not belong to the past of the cut this condition is equivalent to the restriction that a cut should part distributed operating systems c e c e c p p e e p e e e p e figure consistency of cuts cuts c c are consistent while c is inconsistent not have a backward intersection with a message a forward intersection does not violate condition cc since its send event is in the past and its receive event is in the future example consistency of a cut in figure cuts c and c are consistent cuts because there is no event ej belonging to the past of the cut that follows an event ei in the future of the cut cut c is inconsistent because event e follows event e because of the causeandeffect relationship between a send and a receive however e is in the future of c while e is in its past an algorithm for consistent state recording this section describes the state recording algorithm by chandy and lamport the algorithm makes the following assumptions channels are unidirectional channels have unbounded capacities to hold messages channels are fifo the assumption of fifo channels implies that messages received by a destination process must be the first few messages sent by a sender process and messages contained in a channel must be the last few messages sent by a process to initiate a state recording a process records its own state and sends a state recording request called a marker on every outgoing channel when a process receives a marker it records the state of the channel over which it received the marker if the marker is the first marker it received from any process it also records its own state and sends a marker on every outgoing chapter theoretical issues in distributed systems channel we use the following notation to discuss how the state of a channel is determined receivedij the set of messages received by process pj on channel chij before it received the marker on channel chij recordedrecdij the set of messages recorded as received over channel chij in the state of process pj algorithm chandylamport algorithm when a process pi initiates the state recording pi records its own state and sends a marker on each outgoing channel connected to it when process pj receives a marker over an incoming channel chij process pj performs the following actions a if pj had not received any marker earlier then i record its own state ii record the state of channel chij as empty iii send a marker on each outgoing channel connected to it b otherwise record the state of channel chij as the set of messages receivedij recordedrecdij rules of algorithm are executed atomically ie as indivisible operations recording of the channel state by the algorithm can be explained as follows let a process pi send messages mi mi min on channel chij before recording its own state and sending a marker on chij let process pj have two incoming channels chij and chkj if the marker on channel chij is the first marker pj received it would record its own state which would show recordedrecdij and recordedrecdkj as the messages received by it pj would also record the state of chij as empty because channels are fifo process pj would have received the marker after receiving messages mi mi min on chij so it is correct to record the state of channel chij as empty let pj receive two more messages mk and mk on chkj before it received the marker hence receivedkj recordedrecdkj mk mk and the state of channel chkj would be recorded as the set of messages receivedkj recordedrecdkj ie mk mk it is correct because process pk would have sent messages mk mk before it recorded its own state and sent the marker on channel chkj so if these messages were not received by pi by the time it recorded its own state they must have been in the channel example illustrates operation of the chandylamport algorithm operation of the chandylamport algorithm example figure a shows a distributed system at time process p has sent message m to p the message currently exists in ch at time process p sends message m to process p at time p decides to record the state of the system so it records its own state and sends markers on its outgoing channels part distributed operating systems p p p m p p m m p p m m p p p a b c figure example of the chandylamport algorithm system at times and table recorded states of processes and channels in figure entity description of recorded state p message m has been sent no messages have been received p no messages have been sent or received p messages m and m have been sent message m has been received ch empty ch empty ch contains the messages m and m figure b shows the situation at time message m is still in channel ch and m is in ch the bullets indicate markers the symbol indicates that the state of a process has been recorded process p receives the marker on ch at time records its own state and records the state of ch as empty process p sends message m to process p at time and receives the marker on ch at time it now records its own state records the state of ch as empty and sends a marker on ch figure c shows the situation at time states of all processes have been recorded states of channels ch and ch have also been recorded however the state of ch is yet to be recorded when the marker on ch reaches process p p will record the state of ch according to step b of algorithm it is recorded as messages m m because these messages are in received but not in recordedrecd table shows the state recording of the system properties of the recorded state let tb and te be the time instants when the state recording of system s begins and ends let rs be the recorded state of the system one would expect that system s would have been in the state rs at some time instant ti such that tb ti te however this may not be so that is the recorded chapter theoretical issues in distributed systems state rs may not match any global state of the system example illustrates this fact recorded state versus global state example figure shows the timing diagram of the distributed system of figure let p initiate state recording at time instant t the timing diagram of figure shows how the markers are sent and received by processes during state recording the markers are shown as dotted arrows table shows channel and process states recorded by the chandy lamport algorithm only message m is recorded as sent by p and received by p no other messages are recorded as sent or received however because the timing diagram of figure is drawn to scale it is clear that the system never existed in a state in which message m had been sent and received but no other messages had been sent the messagesend and messagereceive events e e and e had occurred before event e the messagereceive event for message m thus any global state that recorded message m as received would have also recorded message m as sent and received and message m as sent even though the system may not have existed in the recorded state at any point in time the recorded state is useful for applications that require only mutual p e e e m p e e e e m m p e e e m m p e e t figure state recording of the system of figures and table a recorded state that does not match any global state entity description of recorded state p no messages have been sent message m has been received p message m has been sent no messages have been received p no messages have been sent or received p no messages have been sent or received states of all channels are recorded as empty summary an operating system uses the notions of time and relationship between events that is causeandstate for controlling operation of user processes effect relationship to find which of the events and for organizing its own functioning however occurred earlier for example in interprocess meseach node in a distributed system has its own clock sage communication the sending of a message is a and memory so these notions can not be employed cause and its receipt is the effect hence the receive as simply as in a conventional operating system event of a message is known to occur after its send in this chapter we developed alternative notions event using transitivity it follows that an event of time and state for use in a distributed system that preceded a send event of a message must have an os uses the notion of time to know occurred before an event that followed its receipt the order in which events occurred so that it however for some pairs of events it is not possible can service events in timedependent orders such to know which of the events occurred earlier such as fcfs and lifo the notion of precedence events are called concurrent events of events which indicates which event occurred it is laborious to deduce the precedence of before which other event is transitive that is if events by using transitivity hence an os assoevent ei occurred before ej and event ej occurred ciates a timestamp ie occurrence time with each before ek then ei must have occurred before ek event and compares the timestamps of two events the os can determine precedence of events as folto know which of them occurred earlier to facillows if two events occurred in the same node itate timestamping the os maintains a clock in the os knows which of them occurred earlier for each process which is called the local clock of the events occurring in different nodes the os uses process and keeps the local clocks of processes well transitivity of events and the notion of a causal synchronized it performs clock synchronization chapter theoretical issues in distributed systems by using the causal relationship governing interof its nodes obtained at exactly the same instant of process messages as follows each message contime it is not possible to record the global state tains the timestamp of its send event the local by asking each node to record its local state at a clock in the process receiving the message must specific time instant because clocks in the nodes show a time larger than the timestamp in the mesare not perfectly synchronized an arbitrary collecsage if it is not so the local clock in the receiving tion of local states of nodes may be inconsistent process is incremented to a time that is larger than for example if nodes record their states spontathe timestamp in the message since local clocks neously the local state of node ni may be recorded do not show the real time they are called logical before it sent a message m to node nj and the local clocks timestamps using logical clocks have the state of nj may be recorded after it received mesproperty that if ti tj are the timestamps of events sage m this problem was overcome by chandy and ei ej respectively ti tj if event ei precedes ej lamport by using special messages called an alternative system of clocks called vector clocks markers to instruct individual nodes to record their has the additional property that ti tj implies that local states they showed that if interprocess comevent ei occurred before ej for concurrent events munication is fifo the local states of processes it is immaterial how we order them hence a pair recorded by their algorithm were mutually consisprocess id timestamp is used to obtain a total tent the collection of these local states can be used order on events to detect stable properties which are properties that the state of a node is called its local state the do not change with time such as presence of cycles global state of a system is a collection of local states in waitfor graphs test your concepts classify each of the following statements as true a if process pi sends messages to process pj or false but process pj does not send messages to proa events ei and ej are concurrent events only cess pi states of processes pi pj are mutually if tsei tsej where tsei tsej are the consistent local states only if timestamps of ei ej using logical clocks i all messages sent by process pi to process b even if tsei tsej event ei could have pj have been received by process pj occurred earlier than event ej ii some messages sent by process pi to c even if vtsei vtsej event ei could have process pj have not been received by occurred earlier than event ej process pj d a message from process pi to process pj that iii all messages received by process pj from intersects with a cut of a system is a mesprocess pi have been sent by process pi sage that has been sent by pi but not received iv none of iiii by pj in the states of processes pi and pj b if event ei in process pi is in the past of a represented by the cut cut ck e in a state recorded by the chandylamport i all events in the system that precede event algorithm the state of channel chij is likely ei are in the past of the cut ck to be nonempty only if process pj receives a ii some of the events that precede event ei marker on some other channel before receivmay be in the past of the cut ck ing the marker on channel chij iii all events that occur after event ei are in select the appropriate alternative in each of the the future of the cut ck following questions iv none of iiii part distributed operating systems exercises in example ps time is much larger than using causal relationships give an example of that of p or p list all conditions under which a system showing such events comment on this can happen the advantages and drawbacks of using rela the following events occur in a system consisting tion of three processes instead of using relation to obtain a total process p process p process p order using vector timestamps it is proposed to use the following relation event e event e event e ei precedes ej iff i pvtseilocal time pvtsej local time or send receive send message message message ii pvtseilocal time pvtsej local time and to p from p to p pvtseiprocess id pvtsej process id event e receive event e comment on the correctness of this proposal message ti and tj are timestamps of events ei and ej from p a give an example of a system in which ti tj receive when logical clocks are used but ti tj when message vector clocks are used from p b if ti tj when vector clocks are used show event e that ti tj when logical clocks are used send event e c if ti tj when logical clocks are used show message that ti tj when vector clocks are used to p vector timestamps of concurrent events ei a draw a timing diagram for the system and ej are such that vtsei k vtsej k b show event precedences in this system show that events ei and el are concurrent c list the concurrent events if vtsel g vtsej g for all g k and synchpi pj tk true if the logical clocks of pi vtsel k vtsej k and pj are reasonably consistent at time instant explain with the help of an example why the tk ie if the difference between their values is chandylamport algorithm requires channels for some small value if rpi k is the set to be fifo of processes from whom pi has received a mes a transitless state of a system is a state in which sage before tk and spi k is the set of processes no messages are in transit see table for an to which pi has sent a message before tk deterexample give an example of a system in which mine whether synchpi pj tk would be true in all states recorded by the chandylamport the following situations algorithm are necessarily transitless a rpi k rp j k a system consists of processes pi pj and chanb there exists a pg spi k such that nels chij and chji each process sends a message pj spg k to the other process every seconds every mesc pj spi k sage requires seconds to reach pj prove that d pj rpi k but pj rpi k if the state recording initiated by pi e pj spi k and pi spj k using the chandylamport algorithm can not f pj rpi k but pj rpi k and be transitless pi has not received any message from any give an example of a system in which the state process after the time it sent a message to pj recorded by the chandylamport algorithm is relation imposes a total order even one of the states in which the system existed if events can be only partially ordered by sometime during the execution of the algorithm chapter theoretical issues in distributed systems what will be the state recording in example the assumption concerning fifo channels can if the state recording request in channel ch is be removed from algorithm as follows a delayed and delivered to process p immediately flag field is added to each message this field after event e occurs contains the values before token or after token the chandylamport algorithm works cordepending on whether the message is sent before rectly if more than one node in a distributed or after sending a token on the same channel system spontaneously initiates a state recordif a process receives a message with the flag ing describe working of the algorithm if provalue after token before it receives a token on the cesses p and p of figure initiate state same channel it performs the same actions as it recording a before sending any messages b would have performed on receiving a token and after one message has been sent on each of ignores the token when it is received later forch ch and ch and no other messages mulate rules for recording the state of a channel are sent using this scheme bibliography lamport discusses ordering of events in a dis lai t h and t h yang on tributed system and defines a partial order on events distributed snapshots information processing mattern garg and attiya and welch letters discuss vector clocks and consistency of cuts lamport l time clocks and the consistency of cuts is also discussed in chandy and ordering of events in a distributed system lamport and knapp communications of the acm july chandy and lamport developed the dis tributed snapshot problem described in section li h f t radhakrishnan and k venkatesh which requires fifo channels li radhakrishnan and global state detection in nonfifo venkatesh lai and yang and mattern networks proceedings of the th international describe algorithms that do not require chanconference on distributed computing systems nels to be fifo lynch and tel discuss algorithms for global snapshots lynch n distributed algorithms attiya h and j welch distributed morgan kaufmann computing fundamentals simulations and mattern f virtual time and global advanced topics john wiley new york states of distributed systems m cosnard et al chandy k m and l lamport eds parallel and distributed algorithms distributed snapshots determining global elsevier science north holland states of distributed systems acm transactions spezialetti m and p kearns efficient on computer systems feb distributed snapshots proceedings of the th garg v k elements of distributed international conference on distributed computing wileyieee new york computing systems knapp e distributed deadlock tel g introduction to distributed detection computing surveys dec algorithms nd ed cambridge university press cambridge operation of distributed control algorithms a distributed operating system implements a control function through a distributed control algorithm whose actions are performed in several nodes of the system and whose data is also spread across several nodes this approach has the following advantages over a centralized implementation of control functions the delays and overhead involved in collecting the global state of a system are avoided the control function can respond speedily to events in different nodes of the system failure of a single node does not cripple the control function chapter distributed control algorithms table overview of control functions in a distributed os function description mutual exclusion implement a critical section cs for a data item ds for use by processes in a distributed system it involves synchronization of processes operating in different nodes of the system so that at most one process is in a cs for ds at any time deadlock handling prevent or detect deadlocks that arise from resource sharing within and across nodes of a distributed system scheduling perform load balancing to ensure that computational loads in different nodes of a distributed system are comparable it involves transferring processes from heavily loaded nodes to lightly loaded nodes termination detection processes of a distributed computation may operate in several nodes of a distributed system termination detection is the act of determining whether such a computation has completed its operation it involves checking whether any of the processes is active and whether any interprocess message is in transit between them election a coordinator also called a leader process is the one that performs some privileged function like resource allocation an election is performed when a coordinator fails or is terminated it selects one of the active processes to become the new coordinator and informs the identity of the new coordinator to all other processes a distributed control algorithm provides a service whose clients include both user applications and the kernel table describes control functions in a distributed os mutual exclusion and election are services provided to user processes deadlock handling and scheduling are services offered to the kernel while the termination detection service may be used by both user processes and the kernel in os literature names of these functions are generally prefixed with the word distributed to indicate that the functions are performed in a distributed manner note that fault tolerance and recovery issues are not discussed here they are discussed separately in chapter a distributed control algorithm operates in parallel with its clients so that it can respond readily to events related to its service the following terminology is used to distinguish between actions of a client and those of a control algorithm basic computation operation of a client constitutes a basic computation a basic computation may involve processes in one or more nodes of the system the messages exchanged by these processes are called basic messages part distributed operating systems kernel calls cpi control part control messages process request reply pi bpi basic part basic messages kernel calls figure basic and control parts of a process pi control computation operation of a control algorithm constitutes a control computation messages exchanged by processes of a control computation are called control messages to understand operation of a distributed control algorithm we visualize each process to consist of two parts that operate in parallel a basic part and a control part figure illustrates the two parts of a process pi the basic part of a process participates in a basic computation it exchanges basic messages with basic parts of other processes when it requires a service offered by a control algorithm it makes a request to the control part of the process all other requests are made directly to the kernel the control part of a process participates in a control computation it exchanges control messages with control parts of other processes and may interact with the kernel to implement its part in the control function the basic part of a process may become blocked when it makes a resource request however the control part of a process never becomes blocked this feature enables it to respond to events related to its service in a timely manner example basic and control parts of a process a distributed application consists of four processes pp let process p be currently in a cs for shared data ds when process p wishes to enter a cs for ds bp makes a request to cp which is a part of some distributed mutual exclusion algorithm discussed later in section to decide whether p may be allowed to enter a cs for ds cp exchanges messages with cp cp and cp from their replies it realizes that some other process is currently in a cs for ds so it makes a kernel call to block bp note that cp participates in this decision even while bp was executing in a cs when process p wishes to exit the cs bp makes a request to cp which interacts with control parts of other processes and decides that process p may enter a cs for ds accordingly cp makes a kernel call to activate bp correctness of distributed control algorithms processes of a distributed control algorithm exchange control data and coordinate their actions through control messages however message communication incurs delays so the data used by the algorithm may become stale and inconsistent and the algorithm may either miss performing correct actions or perform wrong actions accordingly correctness of a distributed control algorithm has two facets liveness an algorithm will eventually perform correct actions ie perform them without indefinite delays safety an algorithm does not perform wrong actions lack of liveness implies that an algorithm would fail to perform correct actions for example a distributed mutual exclusion algorithm might fail to satisfy the progress and bounded wait properties of section or a deadlock detection algorithm might not be able to detect a deadlock that exists in the system note that the amount of time needed to perform a correct action is immaterial for the liveness property the action must be performed eventually lack of safety implies that an algorithm may perform wrong actions like permitting more than one process to be in cs at the same time table summarizes the liveness and safety properties of some distributed control algorithms assuming a distributed control algorithm to consist of a set of distinct actions and a set of distinct conditions we can represent the algorithm as a set of rules of the form condition action where a rule specifies that the algorithm should perform action if and only if condition is true using table liveness and safety of distributed control algorithms algorithm liveness safety mutual exclusion if a cs is free and some at most one process will be processes have requested in a cs at any time entry to it one of them will enter it in finite time a process requesting entry to a cs will enter it in finite time deadlock if a deadlock arises it will deadlock will not be handling be detected in finite time declared unless one actually exists termination termination of a distributed termination will not be detection computation will be declared unless it has detected within a finite time occurred election a new coordinator will be exactly one process will be elected in a finite time elected coordinator distributed mutual exclusion a permissionbased algorithm the algorithm by ricart and agrawala grants entry to a critical section in fcfs order the algorithm is fully distributed in that all processes participate equally in deciding which process should enter a cs next a process that wishes to enter a cs sends timestamped request messages to all other processes and waits until it receives a go ahead reply from each of them if the system contains n processes n messages have to be exchanged before a process can enter the critical section safety of mutual exclusion follows from the fact that at most one process can obtain n replies at any time entry is granted in fcfs order hence every process gains entry to cs in finite time this feature satisfies the liveness property algorithm ricartagrawala algorithm when a process pi wishes to enter a cs pi sends request messages of the form request pi timestamp to all other processes in the system and becomes blocked when a process pi receives a request message from process pr a if pi is not interested in using a cs it immediately sends a go ahead reply to pr b if pi itself wishes to enter a cs it sends a go ahead reply to pr if the timestamp in the received request is smaller than the timestamp of its chapter distributed control algorithms own request otherwise it adds the process id found in the request to the pending list c if pi is in a cs it simply adds the request message to the pending list when a process pi receives n go ahead replies the process becomes active and enters a cs when a process pi exits a cs the process sends a go ahead reply to every process whose request message exists in its pending list table shows how steps of algorithm are implemented in the control part of a process the first column shows steps in the basic computation performed by a process it consists of a loop in which the process requests entry to a cs performs some processing inside the cs and exits from it the other columns show actions of the control part of the algorithm table basic and control actions of pi in a fully distributed mutual exclusion algorithm algorithm steps executed by the control part actions of basic part steps details repeat forever request cs entry b i send request messages request pi timestamp to all other processes and request the kernel to block the basic part ii when a request message is received from another process send a go ahead reply if the request has a smaller timestamp otherwise add the process id found in the request to the pending list iii count the go ahead replies received activate the basic part of the process after receiving n replies critical section c enter all received requests in the pending list perform cs exit send a go ahead reply to every process whose request message exists in its pending list rest of the cycle a when a request is received send a go ahead reply immediately end part distributed operating systems the number of messages required per cs entry can be reduced by requiring a process pi to obtain permissions from a subset ri of processes in the system ri is called the request set of pi safety must be ensured by forming request sets carefully the algorithm by maekawa uses request sets of size n and uses the following rules to ensure safety see exercise for all pi pi ri for all pi pj ri rj tokenbased algorithms for mutual exclusion a token represents the privilege to use a cs only a process possessing the token can enter the cs safety of a tokenbased algorithm follows from this rule when a process makes a request to enter a cs the mutual exclusion algorithm ensures that the request reaches the process possessing the token and that the token is eventually transferred to the requesting process this feature ensures liveness logical complexity and cost of a mutual exclusion algorithm depend on properties of the system model hence tokenbased algorithms use abstract system models in which edges represent the paths used to pass control messages and the graph formed by nodes and these edges has certain nice properties we discuss two algorithms that use abstract ring and tree topologies an algorithm employing the ring topology figure shows the logical model of a distributed computation and its abstract unidirectional ring model the token is an object ie a data structure containing a request queue in figure b the token is currently with process p p is in a cs and the request queue in the token contains p and p the algorithm works as follows a process that wishes to enter a cs sends a message containing its request and becomes blocked the message is routed along the ring until it reaches the token holder if the token holder is currently in a cs its control part enters the requesters id in the request queue contained in the token when the token holder finishes using the cs it removes the first process id from the request queue in the token and sends a message containing the token and the process id this message is routed along the ring until it reaches the process whose id matches the process id in the message the control part of this process extracts and keeps the token for future use and activates its basic part which enters a cs in figure b p p p p p p p p p p p p token p p a b figure a system model b abstract system model chapter distributed control algorithms process p would receive the token when p exits from its cs the algorithm is shown as algorithm the number of messages exchanged per cs entry is order of n where n is the number of processes algorithm tokenbased mutual exclusion algorithm for a ring topology when a process pi wishes to enter a cs the process sends a request message request pi along its outedge and becomes blocked when a process pi receives a request message from process pr if pi does not possess the token it forwards the message along its outedge if pi possesses the token and it is currently not in a cs it forms the message token pr and sends it along its outedge if pi is in a cs it merely enters pr in the request queue in the token when a process pi completes execution of a cs it checks whether the request queue is empty if not it removes the first process id from the queue let this id be pr it now forms a message token pr and sends it along its outedge when a process pi receives the message token pj pi checks whether pi pj if so it creates a local data structure to store the token becomes active and enters its cs if pi pj it merely forwards the message along its outedge raymonds algorithm raymonds algorithm uses an abstract inverted tree as the system model the inverted tree differs from a conventional tree in that a tree edge points from a node to its parent in the tree pholder designates the process in possession of the token raymonds algorithm has four key features invariants that ensure that a request reaches pholder a local queue of requesters in each node features to reduce the number of request messages and provisions to ensure liveness figure depicts the model of a distributed computation and its abstract inverted tree model process p holds the token so it is at the root of the tree processes p and p which are its children have outedges pointing to p similarly outedges p p p p and p p point from a process to its parent the algorithm maintains three invariants concerning the abstract inverted tree process pholder is the root of the tree each process in the system belongs to the tree p p pholder p p p p p p p p p p a b figure a system model b abstract system model part distributed operating systems each process pi pholder has exactly one outedge pi pj where pj is its parent in the tree these invariants ensure that the abstract system model contains a path from every process pi pholder to pholder this property is useful for ensuring that a request made by pi would reach pholder these invariants are maintained by changing edges in the abstract inverted tree when a process pk sends the token to another process say process pj the edge pj pk is reversed these actions reverse the direction of the tree edges along which the token is sent and establish a path from previous holder of the token to the new holder for example edge p p in figure b would be reversed when p sends the token to p each process maintains a local queue of requesters a request message contains a single field requesterid a process wishing to enter a cs puts its own id in its local queue and also sends a request message along its outedge when a process pi receives a request message it enters the requesting processs id in its local queue it now forms a request message in which it puts its own id ie pi and sends it along its outedge thus the request reaches pholder along a path ensured by invariant however the requester id is different in each edge of the path to reduce the number of request messages a process does not originate or send a request if a request sent earlier by it has not been honored yet it knows this because it would have received the token if its request had been honored pholder enters all requests it receives in its local queue on exiting the cs it removes the first process id from its local queue and sends the token to that process the process receiving the token sends it to the first process in its local queue unless its own id is at the head of the local queue this action is repeated until the token reaches a process that is at the head of its own local queue the control part of this process keeps the token with itself its basic part becomes active and enters a cs liveness requires that every process that requests entry to a cs gets to enter it in finite time to ensure this property a process transferring the token to another process checks whether its local queue is empty if the local queue still contains some requests it forms a new request with its own id in the requesterid field and sends it to the process to which it has sent the token this action ensures that it will receive the token sometime in future for servicing other requests in its local queue algorithm raymonds algorithm when a process pi wishes to enter a cs process pi enters its own id in its local queue it also sends a request message containing its own id along its outgoing edge if it has not sent a request message earlier or if its last request has been already honored when a process pi receives a request message from process pr process pi performs the following actions a put pr in its local queue b if pi pholder send a request message containing its own id ie pi along its outgoing edge if it has not sent a request message earlier or if its last request has been already honored distributed deadlock handling the deadlock detection prevention and avoidance approaches discussed in section make use of state information this section illustrates problems in extending these approaches to a distributed system and then describes distributed part distributed operating systems deadlock detection and distributed deadlock prevention approaches no special techniques for distributed deadlock avoidance have been discussed in os literature for simplicity the discussion in this section is restricted to the singleinstance singlerequest sisr model of resource allocation see section problems in centralized deadlock detection distributed applications may use resources located in several nodes of the system deadlocks involving such applications could be detected by collecting the waitfor graphs wfgs of all nodes at a central node superimposing them to form a merged wfg and employing a conventional deadlock detection algorithm to check for deadlocks however this scheme has a weakness it may obtain wfgs of individual nodes at different instants of time so the merged wfg may represent a misleading view of waitfor relationships in the system this could lead to detection of phantom deadlocks which is a violation of the safety property in deadlock detection example illustrates such a situation example phantom deadlock the sequence of events in a system containing three processes p p and p is as follows process p requests and obtains resource r in node n process p requests and obtains resource r in node n process p requests and obtains resource r in node n process p requests resource r in node n process p requests resource r in node n node n sends its local wfg to the coordinator node process p releases resource r in node n process p requests resource r in node n node n sends its local wfg to the coordinator node figures a and b show wfgs of the nodes at steps and respectively it can be seen that no deadlock exists in the system at any of these times however the merged wfg is constructed by superimposing the wfg of node n taken at step and wfg of node n taken at step see figure c so it contains a cycle p p and the coordinator detects a phantom deadlock distributed deadlock detection recall from chapter that a cycle is a necessary and sufficient condition for a deadlock in an sisr system whereas a knot is a necessary and sufficient chapter distributed control algorithms p p p p p r r r r r p r p p p p p p p p r p node n node n node n node n wfgs at step wfgs at step merged wfg a b c figure phantom deadlock in example node wfgs at steps and the merged wfg condition for a deadlock in an misr system in the distributed deadlock detection approach cycles and knots are detected through joint actions of nodes in the system and every node in the system has the ability to detect and declare a deadlock we discuss two such algorithms diffusion computationbased algorithm the diffusion computation was proposed by dijkstra and scholten for termination detection they called it the diffusing computation the diffusion computation contains two phases a diffusion phase and an information collection phase in the diffusion phase the computation originates in one node and spreads to other nodes through control messages called queries that are sent along all edges in the system a node may receive more than one query if it has many inedges the first query received by a node is called an engaging query while queries received later are called nonengaging queries when a node receives an engaging query it sends queries along all its outedges if it receives a nonengaging query subsequently it does not send out any queries because it would have already sent queries when it received the engaging query in the information collection phase each node in the system sends a reply to every query received by it the reply to an engaging query contains information pertaining to the node to which the engaging query was directed and about some other nodes connected to that node the reply to a nonengaging query typically does not contain any information it is called a dummy reply if the initiator receives its own query along some edge it sends a dummy reply immediately the chandylamport algorithm for consistent state recording of a distributed system discussed in section actually uses the first phase of a diffusion computation see exercise algorithm uses a diffusion computation to perform deadlock detection it was proposed by chandy misra and haas and works for both sisr and misr systems the diffusion computation spreads through edges in the wfg all steps in the algorithm are performed atomically so if a process receives two messages at the same time they will be processed one after another it is assumed that diffusion computations initiated by different processes are assigned distinct ids and that their queries and replies carry these ids this way different diffusion computations do not interfere with one another part distributed operating systems p p p p p p p p a b figure system for illustrating diffusion computationbased distributed deadlock detection algorithm diffusion computationbased distributed deadlock detection when a process becomes blocked on a resource request the process initiates a diffusion computation through the following actions a send queries along all its outedges in the wfg b remember the number of queries sent out and await replies to them c if replies are received for all the queries sent out and it has been in the blocked state continuously since it initiated the diffusion computation declare a deadlock when a process receives an engaging query if the process is blocked it performs the following actions a send queries along all its outedges in the wfg b remember the number of queries sent out and await replies to them c if replies are received for all the queries sent out and it has been in the blocked state continuously since it received the engaging query send a reply to the node from which it received the engaging query when a process receives a nonengaging query if the process has been in the blocked state continuously since it received the engaging query send a dummy reply to the node from which it received the nonengaging query consider an sisr system that contains four processes pp the wfg of figure a shows the system state immediately after process p requests a resource that is currently allocated to p p p and p are now in the blocked state whereas p is not p initiates a diffusion computation when it becomes blocked when p receives its query it sends a query to p which sends a query to p however p is not in the blocked state so it does not reply to ps query thus p does not receive a reply and consequently does not declare that it is in a deadlock let p now request the resource allocated to p and get blocked see the wfg of figure b p would now initiate a diffusion computation that would spread to processes p and p since these processes are blocked p will get the reply to its query and declare that it is involved in a deadlock the condition that a process should be continuously in the blocked state since the time it initiated the diffusion computation or since the time it received the engaging query ensures that a phantom deadlock would not be detected edge chasing algorithm in this algorithm a control message is sent over a waitfor edge in the wfg to facilitate detection of cycles in the wfg hence the name edge chasing algorithm it was proposed by mitchell and merritt each chapter distributed control algorithms name of rule precondition after applying the rule u x z x block z activate transmit u w w w uw u u u u detect u u figure rules of mitchellmerritt algorithm process is assigned two numerical labels called a public label and a private label the public and private labels of a process are identical when the process is created these labels change when a process gets blocked on a resource the public label of a process also changes when it waits for a process having a larger public label a waitfor edge that has a specific relation between the public and private labels of its start and end processes indicates existence of a deadlock figure illustrates rules of the mitchellmerritt algorithm a process is u represented as where u and v are its public and private labels respectively v figure illustrates rules of the mitchellmerritt algorithm a rule is applied when the public and private labels of processes at the start and end of a waitfor edge satisfy the precondition it changes the labels of the processes as shown to the right of details of the four rules are as follows block the public and private labels of a process are changed to a value z when it becomes blocked because of a resource request the value z is generated through the statement z incu x where u is the public label of the process x is the public label of the process for which it waits and function inc generates a unique value larger than both u and x activate the outedge of a process is removed from wfg when it is activated following a resource allocation its labels remain unchanged transmit if the public label of the process at the start of a waitfor edge u is smaller than the public label of the process at the end of the edge w then u is replaced by w detect a deadlock is declared if the public and private labels of a process at the start of a waitfor edge are identical and also equal to the public label of the process at the end of the edge operation of the algorithm can be explained as follows consider a path in uvii the wfg from pi to pk let labels of process pi be and let those of pk be uvkk according to the transmit rule applied to all edges in the path from pi to pk ui is greater than or equal to the public label of every process on the path from pi to pk let pk make a resource request that results in a waitfor edge pk pi according to the block rule public and private labels of pk assume a value given by incuk ui let this be n hence n ui according to the transmit distributed scheduling algorithms both system performance and computation speedup in applications would be adversely affected if computational loads in the nodes of a distributed system are uneven a distributed scheduling algorithm balances computational loads in the nodes by transferring some processes from a heavily loaded node to a chapter distributed control algorithms process pi state of pi process pi operates at is transferred operates at node n node n node n activities in node n activities in node n ti tj time figure migration of process pi from node n to node n lightly loaded node figure illustrates this technique which is called process migration process pi is created in node n at time t at time ti the scheduling function decides to transfer the process to node n so operation of the process is halted in node n and the kernel starts transferring its state to node n at time tj the transfer of state is complete and the process starts operating in node n to perform load balancing through process migration a scheduling algorithm needs to measure the computational loads in nodes and apply a threshold to decide which ones are heavily loaded and which ones are lightly loaded at appropriate times it transfers processes from heavily loaded nodes to lightly loaded nodes these nodes are called sender nodes and receiver nodes respectively cpu utilization is a direct indicator of the computational load serviced in a node however monitoring of cpu utilization incurs high execution overhead hence operating systems prefer to use the number of processes in a node or the length of the ready queue of processes as measures of computational loads these measures possess a good correlation with the average response time in a node and their use incurs a low overhead actual migration of a process can be performed in two ways preemptive migration involves suspending a process recording its state transferring it to another node and resuming operation of the process in the new node see figure it requires extensive kernel support in nonpreemptive migration the load balancing decision is taken during creation of a new process if the node in which a create process call is performed is heavily loaded the process is simply created in a remote node nonpreemptive migration does not require any special support in the kernel stability is an important issue in the design of a distributed scheduling algorithm an algorithm is unstable if under some load conditions its overhead is not bounded consider a distributed scheduling algorithm that transfers a process from a heavily loaded node to a randomly selected node if the node to which the process is sent is itself heavily loaded the process would have to be migrated once again under heavy load conditions this algorithm would lead to a situation that resembles thrashing the scheduling overhead would be high because part distributed operating systems process migration is frequent but processes being transferred would not make much progress a senderinitiated algorithm transfers a process nonpreemptively ie from a sender node to a receiver node while creating a process in a heavily loaded node it polls other nodes to find a lightly loaded node so that it can migrate the process to that node this action makes the scheduling algorithm unstable at high system loads because a sender that can not find a lightly loaded node would poll continuously and waste a considerable fraction of its cpus time instability can be prevented by limiting the number of attempts a sender is allowed to make to find a receiver if this number is exceeded the sender would abandon the process migration attempt and create the new process locally instability may also result if several processes are sent to the same receiver node which now becomes a sender node and has to migrate some of the processes it received this situation can be avoided by using a protocol whereby a node accepts a process only if it is still a receiver node see exercise a receiverinitiated algorithm checks whether a node is a receiver node every time a process in the node completes it now polls other nodes in the system to find a node that would not become a receiver node even if a process is transferred out of it and transfers a process from such a node to the receiver node thus process migration is necessarily preemptive at high system loads the polling overhead would be bounded because the receiver would be able to find a sender quickly at low system loads continuous polling by a receiver would not be harmful because idle cpu times would exist in the system unbounded load balancing overhead can be prevented by abandoning a load balancing attempt if a sender can not be found in a fixed number of polls however a receiver must repeat load balancing attempts at fixed intervals of time to provide the liveness property we discuss a symmetrically initiated algorithm that contains features of both senderinitiated and receiverinitiated algorithms it behaves like a senderinitiated algorithm at low system loads and like a receiverinitiated algorithm at high system loads each node maintains a status flag to indicate whether it is presently a sender a receiver or an ok node ie neither a sender nor a receiver it also maintains three lists called senders list receivers list and ok list to contain ids of nodes that are known to be senders receivers and ok nodes respectively algorithm symmetrically initiated load balancing algorithm when a node becomes a sender as a result of creation of a process change the status flag to sender if the receivers list is nonempty poll the nodes included in it subject to the limit on number of nodes that can be polled a if the polled node replies that it is a receiver node transfer a process to it examine local load and set the status flag accordingly b otherwise move the polled node to the appropriate list based on its reply when a node becomes a receiver as a result of completion of a process change the status flag to receiver poll the nodes included in the senders list chapter distributed control algorithms followed by those in the receivers list and ok list subject to the limit on number of nodes that can be polled a if the polled node replies that it is a sender node transfer a process from it examine local load and set the status flag accordingly b otherwise move the polled node to the appropriate list based on its reply when a node is polled by a receiver node move the polling node to the receivers list send a reply containing own current status when a node is polled by a sender node move the polling node to the senders list send a reply containing own current status when a process is transferred from or to a node examine local load and set the status flag accordingly instability would arise in this algorithm if too many processes are transferred to a receiver node simultaneously to prevent it a receiver node should change its flag in step by anticipating a transfer rather than in step as at present figure depicts comparative performance of distributed scheduling algorithms a senderinitiated algorithm incurs low overhead at low system loads because few senders exist in the system hence the system can provide good response times to processes as the load increases the number of senders increases and the overhead of the algorithm increases at high system loads the algorithm is unstable because a large number of senders exists in the system and few if any receivers exist consequently the response time increases sharply a receiverinitiated algorithm incurs a higher overhead at low system loads than a senderinitiated algorithm because a large number of receivers exists at low system loads hence the response time is not as good as when a senderinitiated algorithm is used at high system loads few receivers exist in the system so a receiverinitiated algorithm performs distinctly better than a senderinitiated algorithm the performance of a symmetrically initiated algorithm would resemble that of sender initiated response time receiver initiated system load figure performance of distributed scheduling algorithms classes of operating systems classes of operating systems have evolved over time as computer systems and users expectations of them have developed ie as computing environments have evolved as we study some of the earlier classes of operating systems we need to understand that each was designed to work with computer systems of its own historical period thus we will have to look at architectural features representative of computer systems of the period table lists five fundamental classes of operating systems that are named according to their defining features the table shows when operating systems of each class first came into widespread use what fundamental effectiveness criterion or prime concern motivated its development and what key concepts were developed to address that prime concern computing hardware was expensive in the early days of computing so the batch processing and multiprogramming operating systems focused on efficient use of the cpu and other resources in the computer system computing environments were noninteractive in this era in the s computer hardware became cheaper so efficient use of a computer was no longer the prime concern and the focus shifted to productivity of computer users interactive computing environments were developed and timesharing operating systems facilitated table key features of classes of operating systems os class period prime concern key concepts batch processing s cpu idle time automate transition between jobs multiprogramming s resource program priorities utilization preemption timesharing s good response time slice roundrobin time scheduling real time s meeting time realtime scheduling constraints distributed s resource sharing distributed control transparency chapter overview of operating systems better productivity by providing quick response to subrequests made to processes the s saw emergence of realtime applications for controlling or tracking of realworld activities so operating systems had to focus on meeting the time constraints of such applications in the s further declines in hardware costs led to development of distributed systems in which several computer systems with varying sophistication of resources facilitated sharing of resources across their boundaries through networking the following paragraphs elaborate on key concepts of the five classes of operating systems mentioned in table batch processing systems in a batch processing operating system the prime concern is cpu efficiency the batch processing system operates in a strict onejobatatime manner within a job it executes the programs one after another thus only one program is under execution at any time the opportunity to enhance cpu efficiency is limited to efficiently initiating the next program when one program ends and the next job when one job ends so that the cpu does not remain idle multiprogramming systems a multiprogramming operating system focuses on efficient use of both the cpu and io devices the system has several programs in a state of partial completion at any time the os uses program priorities and gives the cpu to the highestpriority program that needs it it switches the cpu to a lowpriority program when a highpriority program starts an io operation and switches it back to the highpriority program at the end of the io operation these actions achieve simultaneous use of io devices and the cpu timesharing systems a timesharing operating system focuses on facilitating quick response to subrequests made by all processes which provides a tangible benefit to users it is achieved by giving a fair execution opportunity to each process through two means the os services all processes by turn which is called roundrobin scheduling it also prevents a process from using too much cpu time when scheduled to execute which is called timeslicing the combination of these two techniques ensures that no process has to wait long for cpu attention realtime systems a realtime operating system is used to implement a computer application for controlling or tracking of realworld activities the application needs to complete its computational tasks in a timely manner to keep abreast of external events in the activity that it controls to facilitate this the os permits a user to create several processes within an application program and uses realtime scheduling to interleave the execution of processes such that the application can complete its execution within its time constraint distributed systems a distributed operating system permits a user to access resources located in other computer systems conveniently and reliably to enhance convenience it does not expect a user to know the location of resources in the system which is called transparency to enhance efficiency it may execute parts of a computation in different computer systems at the same time it uses distributed control ie it spreads its decisionmaking actions across different computers in distributed termination detection a process ties up system resources such as kernel data structures and memory the kernel releases these resources either when the process makes a terminate me system call at the end of its operation or when it is killed by another process this method is not adequate for processes of a distributed computation because they may not be able to decide when they should terminate themselves or kill other processes for example consider a distributed computation whose processes have a clientserver relationship a server would not know whether any more requests would be made to it because it would not know who its clients are and whether all of them have completed their operation in such cases the kernel employs methods of distributed termination detection to check whether the entire distributed computation has terminated if so it winds up all processes of the computation and releases the resources allocated to them we define two process states in our system model to facilitate termination detection a process is in the passive state when it has no work to perform such a process is dormant and waits for some other process to send it some work through an interprocess message a process is in the active state when it is engaged in performing some work it can be performing io waiting for a resource waiting for the cpu to be allocated to it or executing instructions the state of a process changes several times during its execution a passive process becomes active immediately on receiving a message sends an acknowledgment to the sender of the message and starts processing the message an active process acknowledges a message immediately though it may delay its processing until a convenient time an active process becomes passive when it finishes its current work and does not have other work to perform it is assumed that both control and basic messages travel along the same interprocess channels a distributed computation is said to have terminated if it satisfies the distributed termination condition dtc the dtc comprises two parts all processes of a distributed computation are passive no basic messages are in transit the second part is needed because a message in transit will make its destination process active when it is delivered we discuss two approaches to determining whether dtc holds for a distributed computation creditdistributionbased termination detection in this approach by mattern every activity or potential activity in a distributed computation is assigned a numerical weightage called credit a distributed computation is initiated with a known finite amount of credit c this credit is distributed among its processes the manner of its distribution is immaterial so long as each process pi receives a nonzero credit ci when a process sends a basic message to another process chapter distributed control algorithms it puts a part of its credit into the message again it is immaterial how much credit is put into a message so long as it is neither zero nor the entire credit of the process a process receiving a message adds the credit from the message to its own credit before processing the message when a process becomes passive it sends its entire credit to a special system process called the collector process which accumulates all credit it receives the distributed computation is known to have terminated when the credit accumulated by the collector process equals c this algorithm is simple and elegant however credit may be distributed indefinitely so a convenient representation of credit should be used in its implementation diffusion computationbased termination detection each process that becomes passive initiates a diffusion computation to determine whether the dtc holds thus every process has the capability to detect termination we discuss detection of the dtc in a system where the following three rules hold processes are neither created nor destroyed dynamically during execution of the computation ie all processes are created when the distributed computation is initiated and remain in existence until the computation terminates interprocess communication channels are fifo processes communicate with one another through synchronous communication ie the sender of a message becomes blocked until it receives an acknowledgment for the message rule simplifies checking for the dtc as follows the sender of a basic message becomes blocked it resumes its operation after it receives the acknowledgment it may enter the passive state only after finishing its work thus the basic message sent by a process can not be in transit when it becomes passive and the system can not have any basic messages in transit when all processes are passive hence it is sufficient to check only the first part of the dtc condition ie whether all processes are passive algorithm performs this check through a diffusion computation over a graph whose nodes represent processes and edges represent interprocess communication example illustrates operation of algorithm algorithm distributed termination detection when a process becomes passive the process initiates a diffusion computation through the following actions a send shall i declare distributed termination queries along all edges connected to it b remember the number of queries sent out and await replies c after replies are received for all of its queries declare distributed termination if all replies are yes when a process receives an engaging query if the process is in the active state it sends a no reply otherwise it performs the following actions a send queries along all edges connected to it excepting the edge on which it received the engaging query part distributed operating systems b remember the number of queries sent out and await replies c after replies are received for all of its queries if all replies are yes send a yes reply to the process from which it received the engaging query otherwise send a no reply when a process receives a nonengaging query the process immediately sends a yes reply to the process from which it received the query example distributed termination detection figure shows a distributed computation only processes p and p are active all other processes are passive now the following events occur process p becomes passive initiates termination detection and sends a query to process p process p sends a basic message to process p along the edge p p and becomes passive at the earliest opportunity the receive event in p for the basic message of p and events concerning sending and receipt of queries and their replies by the processes could occur in several different sequences two sequences of interest are as follows if process p received the query from p before it became passive it would send a no reply to p so p would not declare termination if process p received the query from p after it became passive according to rule it would have already received an acknowledgment to the basic message it had sent to process p in step so process p must have become active after receiving ps message before p became passive now when p receives the query from p it would send a query to each of pp p would send a no reply to p which would send a no reply to p so p would not declare termination if rules and of the system are removed the algorithm would suffer from safety problems in some situations distributed termination detection algorithms become complex when they try to remove rules of the system papers cited in the bibliography discuss details of such algorithms p p p p p p p figure illustration of distributed termination detection election algorithms a critical function like replacing the lost token in a tokenbased algorithm is assigned to a single process called the coordinator for the function typically priorities are assigned to processes and the highestpriority process among a group of processes is chosen as the coordinator for a function any process that finds that the coordinator is not responding to its request assumes that the coordinator has failed and initiates an election algorithm the election algorithm chooses the highestpriority nonfailed process as the new coordinator and announces its id to all nonfailed processes election algorithms for unidirectional ring topologies all links in the ring are assumed to be fifo channels it is further assumed that the control part of a failed process continues to function and simply forwards each received message along its outedge the election is performed by obtaining ids of all nonfailed processes in the system and electing the highestpriority process it is achieved as follows a process pi initiates an election by sending out an elect me pi message along its outedge a process pj that receives this message performs two actions it sends out an elect me pj message of its own and also forwards the elect me pi message immediately after its own message these messages reach process pi such that the elect me pi message follows all the other messages process pi examines process ids contained in all these messages and elects the highest priority process say phigh as the new coordinator it now sends a new coordinator phigh message along the ring to inform all processes about the outcome of the election it is assumed that failures do not occur during an election this assumption ensures identical results even if two or more processes initiate elections in parallel the algorithm requires an order of n messages per election the number of messages per election can be reduced as follows a process pj that receives an elect me pi message sends out only one message it sends an elect me pj message to start a new election if its own priority is higher than that of pi otherwise it simply forwards the elect me pi message this way only the highestpriority nonfailed process phigh would get back its own elect me message it would send a new coordinator phigh message to announce its election all other processes abandon their elections if any when they receive the new coordinator phigh message when this refinement is used the number of messages per election can be a maximum of n as follows the elect me pi message sent by the process that initiates an election needs a maximum of n messages to reach the highestpriority process the election initiated by the highestpriority process requires n messages to complete and another n messages are required to inform every process about the outcome of the election the time consumed by the election could be as high as n twc where twc is the worstcase message delivery time over a link bully algorithm a process pi that initiates an election sends an elect me pi message to all higherpriority processes and starts a timeout interval t if it does not hear from any of them before the timeout occurs it assumes that all of them have failed sends a new coordinator pi message to all lowerpriority processes and becomes the new coordinator if its elect me message reaches a practical issues in using distributed control algorithms resource management when a process requests access to a resource the resource allocator must find the location of matching resources in the system determine their availability chapter distributed control algorithms node ni node nk resource manager resource manager name resource resource name server allocator allocator server pi pk rk figure resource allocation in a distributed system and allocate one of the resources figure contains a schematic of resource allocation a resource manager exists in each node of the system it consists of a name server and a resource allocator the numbered arcs in the schematic correspond to steps in the following resource allocation procedure when process pi wishes to use a resource named resj it constructs a pair resj pi and forwards it to the resource manager in its node the resource manager forwards the request to the name server the name server locates resj using its name and attributes and constructs the triple rk nk pi where resj is resource rk at node nk it forwards the triple to the resource allocator the resource allocator finds whether resource rk of node nk is available if so it passes pk the id of the resource controller process for the resource to pi it also sends an allocation message containing the id of pi to pk if the resource is not available it stores the request in a queue of pending requests the request would be honored sometime in future when the resource becomes available process pk interacts with process pi to fulfill pis service requests after completing its use of the resource process pi makes a release request the resource manager sends a release message to pk and allocates the resource to a pending request if any the important issue in step is ensuring noninterference of resource allocators of different nodes it could be achieved either through a distributed mutual exclusion algorithm or through an election algorithm to elect a coordinator that would perform all allocations in the system use of a mutual exclusion algorithm would incur overhead at every allocation use of an election algorithm would avoid this overhead however it would require a protocol to ensure that resource status information would be accessible to a new coordinator if the present coordinator failed a simpler arrangement would be to entrust allocation of resources in a node to the resource allocator of that node this scheme would avoid the overhead of mutual exclusion election and fault tolerance it would also be more robust because a resource could be allocated to a process so long as the nodes part distributed operating systems containing the process and the resource and a network path between the two are functional the name server in each node would have to be updated when resources are added this problem can be solved through an arrangement of name servers as in the domain name service dns see section where only the name server of a domain needs to be updated when a resource is added process migration the process migration mechanism is used to transfer a process between nodes in a distributed system it is used to achieve load balancing or to reduce network traffic involved in utilizing a remote resource it may also be used to provide availability of services when a node has to be shut down for maintenance the schematic figure made process migration look deceptively simple however in reality it is quite complex for several reasons the state of a process comprises the following process identifier and ids of its child processes pending signals and messages current working directory and internal ids of files see section two kinds of problems are faced in transferring process state process state is often spread across many data structures in the kernel so it is difficult to extract it from kernel data structures process ids and internal ids of files have to be unique in the node where a process operates such information may have to be changed when a process is migrated this requirement creates difficulties in process synchronization and in io providing globally unique process ids as in the sun cluster see section and transparency of resources and services see section are important in this context when a message is sent to a process the dns converts the process name hostname processid into the pair ip address processid such a message may be in transit when its destination process is migrated so arrangements have to be made to deliver the message to the process at its new location each node could maintain the residual state of a process that was migrated out of it this state would contain the id of the node to which it was migrated if a message intended for such a process reaches this node the node would simply redirect the message to its new location if the process had been migrated out of that node in the meanwhile the node would similarly redirect the message using the residual state maintained by it in this manner a message would reach the process irrespective of its migration however the residual state causes poor reliability because a message would not be delivered if the residual state of its destination process in some node has been lost or has become inaccessible because of a fault an alternative scheme would be to inform the changed location of a process as also a change in the process id if any to all processes that communicate with it this way a message could be sent to the process directly at its new location if a message that was in transit when a process was migrated reached the old node where the process once existed the node would return a no longer here reply summary a distributed control algorithm is an algorithm for aware of its own local state and interacts with use in an os whose actions are performed in many other nodes to convey state information the cornodes of the distributed system an os uses disrectness of the algorithm depends on how state tributed control algorithms so that it can avoid the information is conveyed among nodes and how overhead of collecting state information about all decisions are made while performance depends entities in the system in one place be responsive to on the nature of the system model used by the events occurring in its nodes and provide reliable algorithm operation in the presence of node and link faults in mutual exclusion is performed by using either this chapter we discussed distributed control algoa fully connected logical model and timestamping rithms for mutual exclusion deadlock handling of requests or a token to represent the privilege scheduling electing coordinators for functions and to enter a critical section the former incurs small services and detecting termination of a distributed decision times while the latter requires fewer mescomputation sages distributed deadlock detection algorithms parts of a distributed control algorithm exeuse a logical model in which edges represent waitcuting in different nodes of a distributed system for relationships between processes and special reach a decision by interacting among themselves messages are sent over the edges for deadlock through interprocess messages this method of detection either a special algorithm called diffuoperation may delay decisions however the algosion computation is used to collect state informarithm must make the correct decision eventually tion from all relevant processes or presence of a since distributed algorithms do not have access cycle is inferred when a sender process receives to states of all relevant entities at the same time back its own deadlock detection message disthey must also ensure that they would not pertributed scheduling is performed by exchanging form a wrong action these two aspects of their state information among nodes of the system to correctness are called liveness and safety respecdecide whether processes should be transferred tively they have to be interpreted in the context between nodes to balance the execution loads of the function performed by a distributed conamong nodes trol algorithm for example in mutual exclusion a distributed computation terminates only liveness implies that the progress and bounded when all its processes are idle and no messages are wait conditions of section are satisfied while in transit between them distributed termination safety implies that at most one process is in the detection can be performed by using a diffusion cs at any time performance of a distributed computation to check whether any process is active control algorithm is measured in terms of the or any interprocess message is in transit alternumber of messages exchanged by the algorithm natively some known amount of credit can be and the delay incurred until a required action is distributed among processes and some of it can performed be put on every interprocess message termination a distributed control algorithm uses a syshas occurred if the total credit with idle processes tem model that is either a physical model of the equals the amount of credit with which the syssystem or a logical model in which nodes are protem started election algorithms use logical models cesses and an edge indicates that two processes and special messages to find the highestpriority exchange messages each node in the model is nonfailed process part distributed operating systems test your concepts classify each of the following statements as true a which of the following properties of a critical or false section implementation will ensure liveness a the control part of a process never blocks of a distributed mutual exclusion algorithm b the ricartagrawala algorithm is deadlockrefer to table free if timestamps are distinct i the progress property c in a tokenbased algorithm for mutual excluii the bounded wait property sion a requesting process sends its request to iii the progress and bounded wait every other process properties d in a diffusion computation model a process iv none of iiii does not send a reply to a nonengaging query b a process pi initiates a diffusion computae a centralized deadlock detection algorithm tion by sending out queries a process pk in may detect phantom deadlocks the system f a senderinitiated distributed scheduling i receives the query initiated by pi exactly algorithm is unstable at high system loads once g a distributed computation is said to have terii may not receive the query even once minated if all processes in the computation iii receives the query at least once but may are in the passive state receive it several times select the appropriate alternative in each of the iv none of iiii following questions exercises state and compare the liveness properties of a detection does not possess the liveness property a distributed mutual exclusion algorithm and if a killed process is given a new timestamp when b an election algorithm it is reinitiated step of the ricartagrawala algorithm is mod it is proposed to use an edge chasing deadified such that a process wishing to enter a cs lock detection algorithm for deadlocks arising does not send a go ahead reply to any other in interprocess communication when a process process until it has used its cs prove that this gets blocked on a receive message request modified algorithm is not deadlockfree a query is sent to the process from which it prove the safety property of maekawas algoexpects the message if that process is blocked rithm which uses request sets of size n on a receive message request it forwards the construct an example where raymonds algoquery to the process for which it is waiting and rithm does not exhibit fcfs behavior for entry so on a process declares a deadlock if it receives to a cs hint consider the following situation its own query comment on the suitability of this in example process p makes a request for algorithm for cs entry while p is still in cs a symmetric communication identify the engaging and nonengaging queries b asymmetric communication in the chandylamport algorithm for consis if use of the inc function in the block rule is omittent state recording algorithm extend ted from the mitchellmerritt algorithm show the algorithm to collect the recorded state inforthat the modified algorithm violates the liveness mation at the site of the node that initiated a requirement state recording prove correctness of the credit distribution prove that a resource allocator using the waitbased distributed termination detection ordie and woundorwait scheme for deadlock algorithm chapter distributed control algorithms a senderinitiated distributed scheduling algod the sender transfers a process when it rithm uses the following protocol to transfer a receives a yes reply process from one node to another e if it receives a no reply it selects another node a a sender polls all other nodes in the system and repeats steps be in search of a receiver node does this protocol avoid instability at high b it selects a node as the prospective receiver system loads and sends it a lock yourself for a process define the liveness and safety properties of a transfer message distributed scheduling algorithm hint will c the recipient of the message sends a no reply imbalances of computational load arise in a if it is no longer a receiver else it increases system if its scheduling algorithm possesses the length of its cpu queue by and sends liveness and safety properties a yes reply bibliography dijkstra and scholten and chang discuss chandy k m j misra and l m haas the diffusion computation model of distributed algodistributed deadlock detection acm rithms andrews discusses broadcast and token transactions on computer systems passing algorithms raymond and ricart and agrawala chang e echo algorithms depth discuss distributed mutual exclusion algorithms parallel operations on general graphs ieee dhamdhere and kulkarni discusses a faulttransactions on software engineering tolerant mutual exclusion algorithm the diffusion computationbased distributed deadlock detection algo dhamdhere d m and s s kulkarni a rithm algorithm is adapted from chandy et al token based kresilient mutual exclusion knapp discusses several distributed deadalgorithm for distributed systems information lock detection algorithms sinha and natarajan processing letters discuss an edge chasing algorithm for distributed dead dhamdhere d m s r iyer and e k k lock detection wu et al describes a distributed reddy distributed termination detection deadlock detection algorithm for the and model of dynamic systems parallel computing distributed termination detection is discussed in dijkstra and scholten mattern and dijkstra e w and c s scholten dhamdhere et al the bully algorithm for distermination detection for diffusing computatributed elections is discussed in garciamolina tions information processing letters smith discusses process migration techniques garg v k elements of distributed singhal and shivaratri and lynch computing wileyieee new york describe many distributed control algorithms in detail garciamolina h elections in tel and garg discuss election and termidistributed computing systems ieee nation detection algorithms attiya and welch transactions on computers discusses algorithms for the election problem knapp e deadlock detection in distributed databases computing surveys andrews g r paradigms for process interaction in distributed programs computing lynch n distributed algorithms surveys morgan kaufmann attiya h and j welch distributed mattern f global quiescence detection computing fundamentals simulations and based on credit distribution and recovery advanced topics john wiley new york information processing letters part distributed operating systems mitchell d p and m j merritt sinha m k and n natarajan a distributed algorithm for deadlock detection a priority based distributed deadlock detection and resolution proceedings of the acm algorithm ieee transactions on software conference on principles of distributed engineering computing august smith j m a survey of process obermarck r distributed deadlock migration mechanisms operating systems detection algorithm acm transactions on review database systems tel g introduction to distributed raymond k a treebased algorithm for algorithms nd ed cambridge university press distributed mutual exclusion acm transactions cambridge on computer systems wu h w chin and j jaffer ricart g and a k agrawala an efficient distributed deadlock avoidance an optimal algorithm for mutual exclusion in algorithm for the and model ieee computer networks communications of the transactions on software engineering acm singhal m and n g shivaratri advanced concepts in operating systems mcgrawhill new york faults failures and recovery a fault like a power outage or a memory read error may damage the state of a system for reliable operation the system should be restored to a consistent state and its operation should be resumed recovery is the generic name for all approaches used for this purpose part distributed operating systems system behavior state before expected fault behavior fault error state after unexpected fault behavior ie failure figure fault error and failure in a system a fault like a power outage is noticed readily whereas a fault like a damaged disk block becomes noticeable only when the resulting loss of data causes an unexpected behavior of the system or an unusual situation in it such unexpected behavior or situation is called a failure figure illustrates how a failure arises a fault causes an error which is a part of the system state that is erroneous an error causes unexpected behavior of the system which is a failure example discusses a fault an error and a failure in a banking system example fault error and failure bank accounts a and b contain and respectively a banking application transfers from account a to account b a power outage occurs after it deducts from the balance in account a but before it adds to the balance in account b the power outage is a fault the error is that has been deducted from account a but has not been added to account b the failure is that has vanished a recovery is performed when a failure is noticed figure illustrates the state of a system during normal operation after a fault and after recovery the system is initiated in state s at time a fault occurs at time t the consequent failure is detected at ti the system would have been in state si at time ti if the fault had not occurred however it is actually in state si a recovery procedure applies a correction s to the state and makes the system ready to resume its operation the resulting state would depend on the recovery procedure employed let the resulting state be called snew it would be ideal if snew si however the nature of a fault the failure caused by it and the recovery approach would determine whether it could be so chapter recovery and fault tolerance a failure a fault is noticed si occurs recovery s actions change snew the state s s si t ti time figure recovery after a fault classes of faults a fault may affect a computer system affect only a process in the system or affect hardware components such as memory and the communication hardware accordingly faults are classified into system process storage and communication faults within a class of faults a fault model describes those properties of a fault that determine the kinds of errors and failures that might result from a fault a system fault is a system crash caused by a power outage or by component faults system faults are classified into amnesia and partial amnesia faults depending on how much state information is lost when a fault occurs in an amnesia fault the system completely forgets the state it was in when the fault occurred in a partial amnesia fault the system forgets some components of its state when the fault occured file servers typically suffer partial amnesia faults because they lose the data or metadata that was stored in memory or on a disk that failed a failstop system fault is one that brings a system to a halt this characteristic is convenient in practice because it permits an external observer whether a person or a computer system to know when a fault has occurred it also provides an opportunity to recover or repair the system state before putting the system back into operation a process that suffers a byzantine fault may perform malicious or arbitrary actions it is not possible to undo such actions when a failure is noticed hence byzantine faults are handled by using redundant processes and agreement protocols in this approach several processes are created to perform the same computation if their results do not match the system uses an agreement protocol to decide which of them is the correct result processes producing incorrect results are identified and aborted before they perform any data updates others are permitted to perform updates and continue their operation a typical storage fault occurs because of a bad block on a storage medium it makes some data unreadable the occurrence of a storage fault may be detected by error checking techniques see section or it may be noticed when data is accessed storage faults are basically partial amnesia faults however they could be made nonamnesia faults by using software techniques such as disk mirroring part distributed operating systems communication faults are caused by link or transmission faults these faults are nonamnesia faults because the networking software includes sufficient buffering and error handling capability to ensure that messages are not lost in section we discuss how byzantine faults are handled in practice the rest of this chapter assumes faults to be nonbyzantine overview of recovery techniques for nonbyzantine faults recovery involves restoring a system or an application to a consistent state it involves reexecuting some actions that were performed before the fault occurred recovery techniques can be classified into data recovery process recovery fault tolerance and resiliency these techniques have different implications for reliability response times to computations and the cost of recovery table summarizes their features data recovery techniques guard against loss of data in a file through backups backups are created periodically during normal operation when a fault occurs a file is restored to the state found in its latest backup see section data recovery techniques incur substantial reexecution overhead if backups are created at large intervals and high overhead during normal operation if they are created frequently so deciding the frequency of backups involves a tradeoff process recovery techniques employ checkpoints to record the state of a process and its file processing activities this operation is called checkpointing when a fault occurs the recovery procedure sets the state of a process to that found in a checkpoint this operation is called a rollback of the process it incurs the cost of reexecuting the actions that were performed after the checkpoint was taken the tradeoff between the cost of a rollback and the overhead of checkpointing during normal operation is analogous to that in data recovery techniques table recovery techniques technique description data recovery a backup is a recording of the state of a file when a fault occurs the state of the file is set to that found in its latest backup see section process recovery a checkpoint is a recording of the state of a process and its file processing activities a process is recovered by setting its state to that found in a checkpoint that was taken before a fault occurred this action is called a rollback fault tolerance the error in state caused by a fault is corrected without interrupting the systems operation resiliency special techniques are employed to reduce the cost of fault tolerance fewer results that were produced in a computation before a fault occurred are recomputed after the fault byzantine faults and agreement protocols because of the difficulty in undoing wrong actions recovery from byzantine faults has been studied only in the restricted context of agreement between processes the agreement problem is motivated by the byzantine generals problem where a group of generals have to decide whether to attack the enemy the generals and their armies are located in different geographical locations hence generals have to depend on exchange of messages to arrive at a decision possible faults are that messages may get lost or some generals may be traitors who deliberately send out confusing messages an agreement protocol is designed to arrive at an agreement in spite of such faults three agreement problems have been defined in literature in the byzantine agreement problem one process starts the agreement protocol by broadcasting a single value to all other processes a process that receives the value broadcasts it to other processes a nonfaulty process broadcasts the same value that it receives a faulty process may broadcast an arbitrary value it may even send different values to different processes processes may have to perform many rounds of recovery a recovery scheme consists of two components the checkpointing algorithm decides when a process should take a checkpoint we will use the notation cij to denote the jth checkpoint taken by process pi the recovery algorithm rolls back some processes to their states recorded in checkpoints such that the new process states are mutually consistent example illustrates the fundamental issue in the design of checkpointing and recovery algorithms chapter recovery and fault tolerance c c c p cm c c p c c m m p tf figure checkpoints of processes in a distributed system checkpointing and recovery example figure shows the timing diagram of a distributed computation whose processes p p operate in nodes n n respectively c c and c are the checkpoints taken by process p similarly c c c and c c are the checkpoints taken by processes p and p respectively we denote the state recorded in checkpoint as statecheckpoint let processes p p and p be in the states s s and s respectively at time instant tf hence the distributed computation is in the state s s s s let a failure occur in node n at time instant tf a naive recovery algorithm simply rolls back process p to its latest checkpoint ie c however the new state of the computation s s statec is not a consistent state because p has received message m in state s but p has not sent m in statec which is its new state see definition from example it is clear that the state of a process can not be recovered in isolation a recovery algorithm should restore the state of the computation to a state s in which states of all pairs of processes are mutually consistent according to definition hence the goal of a recovery algorithm is to make the following decisions for each process pi in a distributed computation decide whether process pi should be rolled back if so identify a checkpoint cij to which pi should be rolled back in example the distributed computation could be recovered to the state s statec statec we discuss a basis for such recovery in the following definition orphan message a message mk sent by process pi to process pj is an orphan message in the state s s si sj sn of a system if sj the state of process pj records mk as received but si the state of process pi does not record it as sent an orphan message is a message that has been received by its destination process but it is disowned by its sender because of recovery hence the states of its sender and destination processes are inconsistent this inconsistency is removed efficiency system performance and user service measurement provides a method of assessing selected aspects of an operating systems functioning in chapter we defined efficiency of use and user convenience as two of the fundamental goals of an os however to a system administrator the performance of a system in its environment is more important than merely efficiency of use hence in this section we discuss measures of efficiency system performance and user service table summarizes these measures efficiency the way to evaluate efficiency of use of a resource is to see how much of the resource is unused or wasted and in the amount of resource that is used check how much of it is put to productive use as an example of efficiency consider use of the cpu some amount of cpu time is wasted because the cpu does not have enough work to do this happens when all user processes in the system are either performing io operations or waiting for the users to supply data of the cpu time that is used some amount of time is used by the os itself in performing interrupt servicing and scheduling this constitutes the overhead of os operation the remaining cpu time is used for executing user processes to evaluate efficiency of cpu use we should consider what fraction or percentage of the total cpu time is used for executing user processes efficiency of use of other resources such as memory and io devices can be similarly determined deduct the amount of unused resource and the os overhead from the total resource and consider what fraction or percentage the result is of the total resource using the notion of efficiency of use we briefly discuss the fundamental tradeoff between efficiency of use and user convenience a multiprogramming system has several user programs at any time and switches between them to obtain efficient use of both the cpu and io devices the cpu is given to the table measures of efficiency system performance and user service aspect measure description efficiency of use cpu efficiency percent utilization of the cpu memory efficiency percent utilization of memory system performance throughput amount of work done per unit time user service turnaround time time to complete a job or a process response time time to implement one subrequest chapter overview of operating systems highestpriority program in the system whenever it wants and it can use the cpu for as long as it wants a timesharing system however restricts the amount of cpu time a scheduled process can use it preempts a process that uses too much cpu time and schedules another process the preempted process may be scheduled again sometime in future this feature increases the os overhead in interrupt servicing and scheduling thereby affecting efficiency of cpu use however it provides good response times to all processes which is a feature desired by users of the os system performance once we decide on the suitable combination of cpu efficiency and user service it is important to know how well the os is performing the notion of performance depends on the computing environment and indicates the rate at which a computer system accomplishes work during its operation an operating system typically uses a measure of efficiency to tune its functioning for better performance for example if memory efficiency is low the operating system may load more user programs in memory in turn it may lead to better performance of the system by increasing the rate at which the system completes user computations if cpu efficiency is low the operating system may investigate its causes either too few programs in memory or programs spending too much time in waiting for io to complete and take corrective actions where possible system performance is characterized as the amount of work done per unit time it is typically measured as throughput definition throughput the average number of jobs programs processes or subrequests completed by a system in unit time the unit of work used for measuring throughput depends on the computing environment in a noninteractive environment throughput of an os is measured in terms of the number of jobs or programs completed per unit time in an interactive environment it may be measured in terms of the number of subrequests completed per unit time in a specialized computing environment performance may be measured in terms meaningful to the application for example in a banking environment it could be the number of transactions per unit time throughput can also be used as a measure of performance for io devices for example the throughput of a disk can be measured as the number of io operations completed per unit time or the number of bytes transferred per unit time user service some aspects of user convenience are intangible and thus impossible to measure numerically eg a feature like user friendly interfaces can not be quantified however there are some measurable aspects of user convenience so we can define appropriate measures for them user service which indicates how quickly a users computation has been completed by the os is one such aspect we define two measures of user service turnaround time in noninteractive computing environments and response time in interactive computing environments a smaller turnaround time or response time implies better user service fault tolerance techniques the basic principle in fault tolerance is to ensure that a fault either does not cause an error or the error can be removed easily in some earlier chapters and sections we saw how fault tolerance techniques ensure that no error in state would arise due to process storage and communication faults section described how process faults of a byzantine nature can be tolerated section discussed how the stable storage technique tolerates storage faults and section discussed an arrangement involving acknowledgment and retransmission of messages to tolerate communication faults in this section we discuss two facets of the tolerance of system faults that follow the failstop model fault tolerance for replicated data despite a fault data should be available and applications should see values resulting from the latest update operation fault tolerance for distributed data despite a fault mutual consistency of different parts of the data should not be affected logs forward recovery and backward recovery a log is a record of actions or activities in a process two kinds of logs are used in practice do logs a do log records those actions that should be performed to ensure correctness of state of an entity or a system a do log is also called a redo log chapter recovery and fault tolerance because actions recorded in it may be performed more than once if a fault occurs do logs are used to implement forward recovery undo logs an undo log contains a record of those actions that should be undone to remove an error in state caused by occurrence of a fault undo logs are used to implement backward recovery a writeahead logging principle is used to construct a log a process writes information concerning an action it intends to take into a log before performing the action this way the log would contain all information necessary to achieve the correct state should a fault occur before the action is completed a log could be an operation log which contains a list of actions to be performed so that entities in the system would achieve correct states or a value log which contains a list of values or data images that should be assigned to entities the implementation scheme for an atomic action discussed in section used an intentions list the intentions list is a value log that is used as a redo log being a value log recovery actions that use it are idempotent this property is needed because entries in the log would be processed more than once if faults occur during commit processing recovery using the intentions list constitutes forward recovery if the subactions in an atomic action directly updated data an undo log would have to be maintained so that the actions could be undone if a fault occurred before the atomic action could commit the undo log would contain data images taken before updates were performed its use to undo data updates constitutes backward recovery the idea of atomic execution of a sequence of operations on a file can be extended to operations involving several files a language construct called the atomic transaction is provided in a programming language or a database query language for this purpose it has the following syntax begin transaction transaction id access and modify files if condition then abort transaction access and modify files end transaction transaction id an atomic transaction has an allornothing property like an atomic action its execution commences when a process executes the begin transaction statement the atomic transaction is said to commit if the process executes the end transaction statement all files modified by the atomic transaction would be updated consistently at this time if the process executes the abort transaction statement or if a fault occurs before the transaction commits execution of the transaction would be aborted and no file updates would be made in this case all files would remain in their original states handling replicated data availability of data d can be provided through replication we can make n copies of d n and locate them strategically in the system such that at least one copy of d would be accessible from any node despite anticipated faults in the system part distributed operating systems if data d may be modified it is essential to use rules that would ensure correctness of data access and updates we use the following rules many processes can concurrently read d only one process can write a new value into d at any time reading and writing can not be performed concurrently a process reading d must see the latest value of d rules are analogous to rules of the readers and writers problem of section rule addresses a special issue in data replication quorum algorithms a quorum is the number of copies of d that must be accessed to perform a specific operation on d quorum algorithms ensure adherence to rules by specifying a read quorum qr and a write quorum qw two kinds of locks are used on d a read lock is a shared lock and a write lock is an exclusive lock a process requesting a read lock is granted the lock if d is presently unlocked or if it is already under a read lock request for a write lock is granted only if d is presently unlocked processes use read and write quorums while accessing d so a process can read d after putting a read lock on qr copies of d and can write d after putting a write lock on qw copies of d since a read lock is a shared lock any value of qr would satisfy rule for implementing rules and we choose qr and qw such that qw n qr qw n equation also ensures that a reader will always lock at least one copy that participated in the latest write operation this copy contains the latest value of d so eq also satisfies rule a choice of values that satisfies eqs and is qr and qw n with these quorums a read operation is much faster than a write operation it would be appropriate if read operations are more frequent than write operations many other quorum values are also possible if write operations are more frequent we could choose values of qr and qw such that eqs and are satisfied and qw is as small as possible if qw n a writer would not update all copies of d so a reader would access some copies of d that contain its latest value and some copies that contain its old values to be able to identify the latest value we could associate a timestamp with each copy of d to indicate when it was last modified the choice of qr and qw n is not fault tolerant qw n implies that a process would have to put locks on all n copies of d in order to perform a write operation hence a writer would be unable to write if even one node containing a copy of d failed or became inaccessible to it if a system is required to tolerate faults in up to k nodes we could choose qr k qw n k nk chapter recovery and fault tolerance these quorum sizes are large however it is unavoidable because eq is essential to ensure consistency of data and eq is essential to ensure that reading and writing are not performed concurrently handling distributed data a distributed transaction also called a multisite transaction is a facility for manipulating files located in different nodes of a distributed system in a mutually consistent manner each node participating in a distributed transaction ti contains a transaction manager it maintains information about data updates to be made on behalf of the transaction which could be similar to the intentions list of atomic actions see section in addition it also maintains a log that is local to it the node where the transaction was initiated contains a transaction coordinator the coordinator implements the allornothing property of transactions through the twophase commit protocol also called the pc protocol it initiates this protocol when the application executes the statement end transaction ti in the first phase the protocol checks whether each participating node can commit the updates of the transaction depending on responses from participating nodes it decides whether to commit or abort the transaction in the second phase it informs its decision to each participating node so that it could commit or abort accordingly the pc protocol is presented as algorithm algorithm twophase commit protocol phase actions of the transaction coordinator write the record prepare ti in the log set a timeout interval and send a prepare ti message to each participating node wait until either each participating node replies or a timeout occurs actions of a participating node on receiving a prepare ti message the participating node decides whether it is ready to commit if so it writes information about data updates to be made followed by the record prepared ti in its log and sends a prepared ti reply to the coordinator otherwise it writes the record abandoned ti in its log and sends an abandoned ti reply to the coordinator phase actions of the transaction coordinator if each participating node sent a prepared ti reply write the record commit ti in its log and send a commit ti message to each participating node if a participating node sent an abandoned ti message or a timeout occurred write the record abort ti in its log and send an abort ti message to each participating node in either case wait until an acknowledgment is received from each participating node and write a complete ti record in its log resiliency resiliency techniques focus on minimizing the cost of reexecution when faults occur the basis for resiliency is the property that failures in a distributed system are partial rather than total so some parts of a distributed computation or chapter recovery and fault tolerance the results computed by them may survive a failure use of such results after recovery would reduce reexecution and may even avoid it consider a distributed transaction that is initiated in node ni and involves computations in nodes nj and nk it has a transaction manager in each of these nodes the transaction would be aborted if the transaction manager in node nj does not respond to the prepare message from the coordinator in node ni because of the failure of node nj or link ni nj the aborted transaction would have to be reexecuted at some other time much of the reexecution would be wasteful if node nj had already completed the computation but was simply unable to participate in commit processing because of a link fault a nested transaction tik is an atomic transaction that is a part of another transaction ti transactions ti and tik have a parentchild relationship the transaction controller of ti initiates tik and assigns it a unique id the nested transaction can commit or abort just like an atomic transaction except for one difference when it reaches the commit point a tentative commit is performed for it a tentative commit is an intermediate stage between not committed and committed the log of the nested transaction is written in stable storage however it is not processed at this time the actual commit of the nested transaction which involves processing of the log is held in abeyance until the parent transaction commits when a parent transaction reaches its commit point it is committed by using a twophase commit protocol to ensure that all its child transactions can commit resiliency using nested transactions is implemented as follows consider a transaction ti that executes in node ni and initiates a nested transaction tik in node nj let node nj crash and recover sometime after tik has performed a tentative commit the transaction coordinator which is in node ni may find that the nested transaction tik is taking too long to complete or that the transaction manager in node nj is not responding to its prepare message so it may decide to initiate tik once again either in node nj itself or in another node if it reinitiates tik in node nj the transaction manager in node nj would check whether tik was initiated there in the past and had performed a tentative commit if so it would not reinitiate tik because it already has tiks results in the log it would simply use tiks results when the parent transaction ti commits thus reexecution of tik would be avoided if the transaction coordinator of ti decided to reinitiate the nested transaction in another node it would assign another id to the new nested transaction say til now transaction tik of node nj has become an orphan because its parent transaction is no longer interested in it if it has not performed a tentative commit it should be prevented from performing it in future if it has performed a tentative commit care should be taken not to include it in the pc when the results of ti are committed so that data consistency is not harmed through duplicate actions to implement this aspect the transaction coordinator for ti maintains a list of ids of nested transactions in which it is currently interested when it initiates nested transaction tik it would add tiks id to the list and when it reinitiates the nested transaction with the id til it would delete tik from this list and add til to it when tik wishes to perform a tentative commit its transaction manager would check with the transaction coordinator whether tiks id is present summary recovery and fault tolerance are two approaches to its operation whereas in forward recovery the reliability of a computer system these approaches error is removed from the systems state and its are generically called recovery the cost of a recovoperation is resumed backward recovery is impleery approach is determined by its overhead during mented as follows the states of processes are normal operation and the amount of reprocessing recorded periodically when a node fails a process which becomes necessary when a fault occurs in that was executing in it say process pi is rolled a distributed system a fault typically affects the back to a previous state if pi had sent a message operation of a single link or node hence special m that was received by another process pj pis techniques are employed to minimize the cost of a rollback makes message m an orphan message and recovery it gives rise to a third recovery approach causes an inconsistency in the states of pi and pj called resiliency in this chapter we studied to remove this inconsistency pj has to be rolled the recovery techniques of distributed operating back to some previous state in which it had not systems received message m this effect is called the domino a fault like an io device malfunction or a effect recovery is performed by rolling back propower outage causes an error in the state of the cesses in accordance with the domino effect until system it leads to an unexpected behavior of the all processes assume mutually consistent states system which is called a failure recovery is initia system implements fault tolerance by mainated when a failure is noticed it puts the system taining a log in which it writes information for into a new state from which its operation can be recovery an undo log contains information useful resumed the nature of a fault determines what for backward recovery while a do log which is also kind of recovery is possible a failstop fault brings called a redo log contains information for forward the system to a halt a partial amnesia fault makes it recovery fault tolerance is implemented through lose a part of its state while a byzantine fault makes an atomic transaction which ensures that if a fault it behave in an unpredictable manner and perform occurs either all actions in a specified sequence of wrong actions it may not be possible to undo actions would be performed or none of them would the effect of wrong actions performed because of be performed this way the system will never be in a byzantine fault in a process hence recovery is a state in which only some of the actions have been implemented as follows several processes are creperformed an atomic transaction can be impleated to perform the same computation in parallel mented by using a do log and forward recovery when a failure results from a byzantine fault the if a fault occurs while implementing its actions it state in which majority of the processes exist is can also be implemented by using an undo log and considered to be the correct state processes in the backward recovery the twophase commit protowrong state are aborted and others resume their col pc protocol is used to implement atomic operation transactions that involve data existing in differrecovery from nonbyzantine faults can be ent nodes of the system it ensures that actions of performed by using two approaches in backward the transaction are implemented only if all nodes recovery recovery is performed by rolling back the containing its data can carry out the required system to a previous consistent state and resuming updates chapter recovery and fault tolerance an atomic transaction that involves data in reduces the cost of reprocessing as follows a many nodes of the system can be implemented nested transaction of the failed transaction may by using nested transactions which are its parts have completed its operation in some other node that execute in different nodes if an atomic transhence it is not reinitiated even if its parent transaction is unable to complete because of a node action is reinitiated instead its results are simply fault it may be reinitiated the resiliency technique reused in the reinitiated parent transaction test your concepts classify each of the following statements as true a a fault occurs when a system is in state s and or false a process pi is in state si process pi is rolled a a power outage is a partial amnesia fault if back to a state si contained in a checkpoint no recovery techniques are used that was taken at time t a domino effect b use of a recovery technique incurs overhead arises if even during normal operation of a system i pi had received a message m some time ie even when no faults occur after time t c backward recovery is performed by using ii pi had sent a message m to a process pk backups and checkpoints some time after time t and in state s the d an orphan message is a message that has message is still in transit been sent but has not been received by its iii pi had sent a message m to a process destination process pk some time after time t and in state e the domino effect may be observed while s process pk has received the message recovering a system by using asynchronous b an atomic transaction can be implemented checkpoints by using f quorum algorithms are used for fault toleri a do log and backward recovery ance while updating distributed data ii a do log and forward recovery select the appropriate alternatives in each of iii an undo log and backward recovery the following questions iv an undo log and forward recovery exercises a checkpoint is said to be strongly consistent if can orphan messages arise if a process takes a i states of all pairs of processes are mutually checkpoint before receiving each message consistent and ii every message recorded as when asynchronous checkpointing is used sevsent by a sender process is recorded as received eral checkpoints for each process need to be by a receiver process discuss whether a synpreserved to support rollbacks in the presence chronous checkpoint is both consistent and of orphan messages to preserve disk space it strongly consistent is useful to know when if ever a specific check processes in a distributed computation perform point can be deleted without affecting recovery asynchronous checkpointing as follows each comment on the following proposals process takes a checkpoint immediately after a delete a checkpoint cij when another checksending a message prove that recovery using point is taken for process pi such checkpoints can be performed without b delete a checkpoint cij if another checkencountering the domino effect point cij is taken for process pi and no part distributed operating systems messages were sent by pi between the two can use of read and write quorums determined checkpoints by eq lead to deadlocks if so design a c delete a checkpoint cij if another checkpoint scheme to avoid deadlocks cij is taken for process pi and no mes because of large quorum sizes in handling replisages were received by pi between the two cated data it is proposed to use an approach checkpoints based on the notion of request sets of maekawa d delete all checkpoints for process pi taken see section comment on whether all prior to checkpoint cij if for every message four rules of section would be satisfied by mk recorded as received in cij the process this approach that sent message mk has taken a checkpoint comment on correctness of the following after sending it scheme for mutual exclusion of readers and the node in a distributed system in which a prowriters over replicated data cess pi operates fails what are the processes that a set qr and qw n where n is the number need to be rolled back due to recovery of pi of copies of data give an algorithm to recover from pis failure b when a writer wishes to update the data it while recovery from pis failure is in progress tries to set a write lock on each copy the node in which another process pj operates i if the copy is already locked it waits fails state the conditions under which recovery for the copys lock to be released by the from these two failures would be independent of process which had set it one another how should recovery from these ii if it can not access the copy it assumes failures be performed if these conditions are not that the node containing the copy has satisfied failed and reduces qw by give a scheme to implement an atomic transiii it proceeds to update the data when it action using an undo log in what order should finds that it has set a write lock on as entries in the undo log be processed if a transacmany copies as the current value of qw tion is aborted bibliography lamport et al discusses the byzantine generals discusses fault tolerance in distributed systems garg problem barborak et al surveys approaches that discusses recovery based on checkpointing and can be used to obtain agreement on a system status by the message logging faultfree segment of the processor population lynch tel and garg discuss consensus in barborak m m malek and a dahbura synchronous and asynchronous systems the consensus problem in fault tolerant the twophase commit protocol is discussed in computing computing surveys gray the threephase commit protocol avoids garg v k elements of distributed the blocking problem of the twophase commit protocomputing wileyieee new york col when the coordinator fails it permits participating gray j n the transaction concept nodes to roll forward such a transaction to completion virtues and limitations proceedings of the or to roll it back to an abort the threephase commit international conference on very large data protocol is discussed in skeen svobodova bases discusses resiliency in distributed computing lamport l r shostak and m pease venkatesh et al discusses optimal checkthe byzantine generals problem acm pointing and dominofree recovery this topic contransactions on programming languages and tinues to be much researched even today tel systems chapter recovery and fault tolerance lynch n distributed algorithms tel g introduction to distributed morgan kaufmann algorithms nd ed cambridge university press skeen d a formal model of crash cambridge recovery in a distributed system ieee venkatesh k t radhakrishnan and h f li transactions on software engineering optimal checkpointing and local recording for dominofree rollback recovery svobodova l resilient distributed information processing letters computing ieee transactions on software engineering design issues in distributed file systems a distributed file system dfs stores user files in several nodes of a distributed system so a process and a file being accessed by it often exist in different nodes of the distributed system this situation has three likely consequences a user may have to know the topology of the distributed system to open and access files located in various nodes of the system a file processing activity in a process might be disrupted if a fault occurs in the node containing the process the node containing the file being accessed or a path connecting the two chapter distributed file systems performance of the file system may be poor because of the network traffic involved in accessing a file the need to avoid these consequences motivates the three design issues summarized in table and discussed in the following transparency a file system finds the location of a file during path name resolution see section two relevant issues in a distributed file system are how much information about the location of a file should be reflected in its path name and can a dfs change the location of a file to optimize file access performance the notion of transparency has two facets that address these issues location transparency the name of a file should not reveal its location location independence the file system should be able to change the location of a file without having to change its name location transparency provides user convenience as a user or a computation need not know the location of a file location independence enables a file system to optimize its own performance for example if accesses to files stored at a node cause network congestion and result in poor performance the dfs may move some of those files to other nodes this operation is called file migration location independence can also be used to improve utilization of storage media in the system we discuss these two facets of transparency in section fault tolerance a fault disrupts an ongoing file processing activity thereby threatening consistency of file data and metadata ie control data of the file system a dfs may employ a journaling technique as in a conventional file table design issues in distributed file systems design issue description transparency high transparency of a file system implies that a user need not know much about location of files in a system transparency has two aspects location transparency implies that the name of a file should not reveal its location in the file system location independence implies that it should be possible to change the location of a file without having to change its name fault tolerance a fault in a computer system or a communication link may disrupt ongoing file processing activities it affects availability of the file system and also impairs consistency of file data and metadata ie control data of the file system a dfs should employ special techniques to avoid these consequences of faults performance network latency is a dominant factor of file access times in a dfs it affects both efficiency and scalability of a dfs hence a dfs should use techniques that reduce network traffic generated by file accesses part distributed operating systems system to protect consistency of metadata or it may use a stateless file server design which makes it unnecessary to protect consistency of metadata when a fault occurs to protect file data it may provide transaction semantics which are useful in implementing atomic transactions see section so that an application may itself achieve fault tolerance if it so desires we discuss fault tolerance issues in section performance performance of a dfs has two facets efficiency and scalability in a distributed system network latency is the dominant factor influencing efficiency of a file processing activity network latency typically exceeds the processing time for a file record so unlike io device latency it can not be masked by blocking and buffering of records see sections and a dfs employs the technique of file caching which keeps a copy of a remote file in the node of a process that accesses the file this way accesses to the file do not cause network traffic though staleness of data in a file cache has to be prevented through cache coherence techniques scalability of dfs performance requires that response times should not degrade when system size increases because of addition of nodes or users a distributed system is composed of clusters which are groups of computer systems having highspeed lans see section so caching a single copy of a file in a cluster ensures that file access performance for accesses from a computer system within a cluster would be independent of system size it also reduces network traffic both these effects help in enhancing scalability of dfs performance when several processes access the same file in parallel distributed locking techniques are employed to ensure that synchronization of the file processing activities scales well with an increase in system size we discuss dfs performance enhancement techniques in section overview of dfs operation figure shows a simplified schematic of file processing in a dfs a process in node n opens a file with path name alpha we call this process a client process of this file or simply a client of this file and call node n the client node through path name resolution the dfs finds that this file exists in node n so it sets up the arrangement shown in figure the file system component in node n is called a file server and node n is called the server node other nodes that were involved in path name resolution or that would be involved in transferring file data between nodes n and n are called intermediate nodes we refer to this model as the remote file processing model an arrangement analogous to rpc is used to implement file accesses through stub processes called file server agent and client agent see section when the client opens the file the request is handed over to the client agent the client agent communicates the request to the file server agent in node n which hands over the request to the file server the file server opens alpha and builds fcbalpha when file caching is not employed a read or write operation on alpha is implemented through a transparency in a conventional file system a user identifies a file through a path name he is aware that the file belongs in a specific directory however he is not aware of its location in the system the location info field of the files directory entry indicates the files location on disk this arrangement would be adequate to provide location transparency in a dfs as well a user would use a path name to access a file and the dfs would obtain the location of the file from its directory entry the dfs may choose to keep all files of a directory in the same node of the distributed system or disperse them to different nodes in the former case its metadata would be identical with that of a conventional file system in the latter case the location info field of the directory entry of a file would contain a pair node id location providing location independence would require the information in the location info field of a directory entry to change dynamically now the dfs could change the location of a file at will so long as it puts information about the new location in the location info field of the directory entry it should similarly change information in all links to the file see section to simplify these changes a dfs may use the following arrangement each file is assigned a globally unique file id the directory entry of the file contains this file id dfs maintains a separate data structure to hold file id file location pairs this way the dfs needs to change only one pair in this data structure when the location of a file is changed irrespective of the number of links to the file semantics of file sharing semantics of file sharing determine the manner in which the effect of file manipulations performed by concurrent users of a file are visible to one another recall from the discussion in section that all clients concurrently processing a singleimage mutable file have the same view of its contents so modifications made by one client are immediately visible to other clients processing the file clients processing a multipleimage mutable file can have different views of its contents when their file processing activities complete the file system can either reconcile these views in some manner to create a single image or support existence of many versions of the file in the latter case it has to ensure that any client that opens the file subsequently gets access to the correct version of the file table summarizes key features of three file sharing semantics unix semantics session semantics and transaction semantics unix semantics recall from section that unix file sharing semantics support a singleimage mutable file thus updates made by one client are visible to other clients immediately clients may optionally share the offset into a file this feature is useful if clients process a file jointly the unix semantics are easy and efficient to implement in a conventional file system however as discussed later in section they incur the overhead of cache coherence in a dfs that uses file caching table features of file sharing semantics semantics description unix semantics a singleimage mutable file is implemented the effect of a write operation in a file by one client is visible immediately to other clients of the file clients may optionally share the file offset ie pointer to the next record in a file session semantics a multipleimage mutable file is implemented only clients in a session share the same image of a file updates made by a client are visible to other clients in the same session immediately they are visible to other clients only after the file is closed transaction file processing performed by a client is implemented as an semantics atomic transaction so either all file operations are performed or none of them are performed this property simplifies fault tolerance fault tolerance file system reliability has several facets a file must be robust ie it must survive faults in a guaranteed manner it must be recoverable to an earlier state when a part distributed operating systems failure occurs it must also be available despite faults in the system ie a copy of the file should be accessible at all times and a client process should be able to open it for processing robustness and recoverability depend on how files are stored and backed up respectively while availability depends on how files are opened and accessed all these facets are independent of one another thus a file may be recoverable without being robust or available recoverable and robust without being available available without being recoverable or robust and so on robustness is achieved by using techniques for reliable storage of data eg the disk mirroring technique used in raid level see section recoverability and availability are achieved through special techniques discussed in this section faults in the server or intermediate nodes during a file open operation disrupt path name resolution such faults are tolerated through availability techniques the dfs maintains many copies of the information required for path name resolution and many copies of a file if a copy is inaccessible because of a fault the dfs uses another copy however availability techniques become very complex and expensive if faults that occur during file processing are to be tolerated see section for the quorumbased fault tolerance techniques to handle replicated data hence few if any distributed file systems handle such faults faults in the server or client nodes during file processing may result in loss of state as we shall see in section a file server can be designed such that its operation is not disrupted if state information is lost because of a fault however clients may not use special design techniques to protect against loss of state so client node crashes can be messy the only defense against client node crashes is the use of transaction semantics in the file server whereby the file would be restored to its state before the failed client had started its processing a fault in an intermediate node does not affect file processing if the communication system has sufficient resiliency ie if it can tolerate a few link and node faults hence file systems do not address these faults table summarizes fault tolerance techniques used in distributed file systems file replication and cached directories address faults in a file server and in intermediate nodes during an open operation the stateless file server design addresses faults in a file server during file processing following sections describe these techniques availability a file is said to be available if a copy of the file can be opened and accessed by a client ability to open a file depends on whether path name resolution can be completed ie whether the server node and all nodes involved in path name resolution are functional ability to access a file requires only the client and server nodes to be functional because a path between the two is guaranteed by resiliency of the network consider a path name abcd where directory files a b c and file d exist in nodes a b c and d respectively two approaches can be employed to resolve this path when the dfs finds that file b exists in node b it would send the path chapter distributed file systems table fault tolerance techniques of distributed file systems technique description cached directories a cached directory is a copy of a directory that exists at a remote site it helps the dfs to tolerate faults in intermediate nodes involved in path name resolution file replication several copies of a file are maintained in the interest of availability special techniques are used to avoid inconsistencies between the copies the primary copy technique permits client programs to readaccess any copy of a file but restricts file updates only to a special copy called the primary copy the results of these updates are propagated to other copies this method simplifies concurrency control stateless file server a conventional file server maintains information concerning state of a file processing activity in the metadata for example in file control blocks and file buffers a stateless file server does not maintain such information so it is immune to faults that lead to loss of state information name suffix bcd to node b at node b it would look up c in directory b and find that it exists at node c so it would send cd to node c and so on in an alternative approach the dfs would perform resolution of all path components in the client node itself when it finds that a path name component is the name of a directory in a remote node it would copy the directory from the remote node and continue path name resolution using it this way all directories would be copied into the client node during path name resolution as we shall see later these approaches have different implications for availability in either approach an access to file data does not involve the intermediate nodes involved in path name resolution file processing would not be affected if any of these nodes failed after the file was opened cached directories an anomalous situation may arise when path names span many nodes in the previous example let node c fail after file d was opened using path name abcd and its processing was underway if another client in node a tries to open abcz where file z also exists in node d it would fail because node c has failed so file z can not be processed even though its processing involves the same client and server nodes as file d the only way to avoid this anomaly is to cache remote directories accessed during path name resolution at the client node for the path name abcd it implies that the dfs would cache the directories ab and abc at node a while resolving path names involving the prefixes ab and abc the dfs would directly use the cached directories thus it would be able to resolve the path name abcz without having to access nodes b or c however information in cached directories may be outdated because of creation or deletion of files in some of the intermediate nodes so a cache updating protocol would have to be used we discuss a related issue in the next section part distributed operating systems file replication the dfs performs replication in such a way that it is transparent to clients replication of a file that is likely to be updated involves a tradeoff between cost and complexity of the protocol for updating and its implications for efficient use of the file a twophase commit protocol could be used to update all copies of a file at the same time this way stale and updated copies of a file would not coexist so a client would need only one copy of the file to implement a read access however an update operation may be delayed if some copies are in use by other processes or are inaccessible because of faults alternatives to this approach focus on speeding up the update operation by reducing the number of copies that need to be updated in the primary copy approach updates are directed at a single copy the primary copy other copies are invalidated when the primary copy is updated they would be replicated afresh when they are referenced alternatively the dfs can use a protocol similar to the readersandwriters protocol for replicated data see section to provide efficiency and fault tolerance it would make the read and write quorums as small as possible a timestamp would be associated with each copy to indicate when it was last updated these timestamps would be compared to identify the most recent copy of data in a read quorum file replication works best if the use of a stale copy is also meaningful because changes need not be propagated to all copies of a file immediately directories can be replicated in this manner all updates are made in the primary copy staleness of a directorys copy can cause two kinds of failures a file does not have an entry in the directory even though it has been created or an entry for a file exists in the directory even though the file has been deleted if the first kind of failure occurs the file server can immediately consult the primary copy to check whether the file actually exists and abort the process only if it does not the second kind of failure would occur when a read or write operation is attempted on the file the process would be aborted if it occurs client and server node failures as described in section a conventional file system stores information concerning the state of a file processing activity in metadata such as the file control block fcb of the file this state information provides an implicit context between the file system and a client using which a read or write operation on the file can be performed efficiently for example to read the next record or byte from a sequential file the file system simply accesses its fcb to obtain the id of the next record or byte to be read and accesses the file map table fmt to obtain the disk address of the next record or byte it does not have to access the directory entry of the file to obtain address of its fmt we refer to this design of a file system as a stateful design in a distributed file system the server node can maintain fcbs and the open files table oft in memory just as in a conventional file system this arrangement provides good performance however use of a stateful dfs design poses problems in the event of client and server crashes when a client crashes the file processing activity would have to be abandoned and the file would have to be restored to its previous state so that the chapter distributed file systems client can restart its file processing activity the server would have committed resources like the fcb and io buffers to service the file processing activity these resources would have to be released otherwise they would remain committed to the aborted file processing activity indefinitely these issues can be addressed as follows the client and the file server share a virtual circuit see section the virtual circuit owns the file processing actions and resources like file server metadata these actions and resources become orphans when a client or server crash breaks the virtual circuit so the actions would have to be rolled back and the metadata would have to be destroyed a clientserver protocol implementing transaction semantics may be used to ensure this if a dfs does not provide transaction semantics a client would have to make its own arrangements to restore the file to a previous consistent state when a file server crashes state information stored in server metadata is lost so an ongoing file processing activity has to be abandoned and the file has to be restored to its previous state the stateless file server design described in the next section can be used to avoid both these problems stateless file servers a stateless file server does not maintain any state information about a file processing activity so there is no implied context between a client and the file server consequently a client must maintain state information about a file processing activity and provide all relevant information in a file system call for example a client reading from a sequential file has to keep track of the id of the next record or byte to be read from the file so that it can issue the following call read alpha recordbyte id ioarea address at this call the file server opens file alpha locates its file map table and uses it to convert recordbyte id into the pair disk block id byte offset see section it then reads the disk block and provides the required record or byte to the client thus many actions traditionally performed only at file open time are repeated at every file operation if a file server crashes timeouts and retransmissions occur in the client the file server processes a retransmitted request when it recovers and provides a reply to the client thus the client perceives only a delayed response to a request and is unaware of a file server crash use of a stateless file server provides fault tolerance but it also incurs a substantial performance penalty for two reasons first the file server opens a file at every file operation and passes back state information to the client second when a client performs a write operation reliability considerations require that data should be written into the disk copy of a file immediately consequently a stateless file server can not employ buffering file caching see section or disk caching see section to speed up its own operation in section we discuss a hybrid design of file servers that avoids repeated file open operations a stateless file server is oblivious of client failures because it does not possess any state information for a client or its file processing activity if a client fails recovers and resends some requests to the file server the file server would batch processing systems computer systems of the s were noninteractive punched cards were the primary input medium so a job and its data consisted of a deck of cards a computer operator would load the cards into the card reader to set up the execution of a job this action wasted precious cpu time batch processing was introduced to prevent this wastage a batch is a sequence of user jobs formed for processing by the operating system a computer operator formed a batch by arranging a few user jobs in a sequence and inserting special marker cards to indicate the start and end of the batch when the operator gave a command to initiate processing of a batch the batching kernel set up the processing of the first job of the batch at the end of the job it initiated execution of the next job and so on until the end of the batch thus the operator had to intervene only at the start and end of a batch card readers and printers were a performance bottleneck in the s so batch processing systems employed the notion of virtual card readers and printers described in section through magnetic tapes to improve the systems throughput a batch of jobs was first recorded on a magnetic tape using a less powerful and cheap computer the batch processing system processed these jobs from the tape which was faster than processing them from cards and wrote their results on another magnetic tape these were later printed and released to users figure shows the factors that make up the turnaround time of a job user jobs could not interfere with each others execution directly because they did not coexist in a computers memory however since the card reader was the only input device available to users commands user programs and data were all derived from the card reader so if a program in a job tried to read more data than provided in the job it would read a few cards of the following job to protect against such interference between jobs a batch processing system required dfs performance inherent efficiency of file access mechanisms determines peak performance of a dfs measured as either average response time to client requests or throughput of client requests the dfs can achieve peak performance when all data accesses are local to client nodes ie when clients and file servers are located in the same node however network latencies can completely overshadow the efficiency of access mechanisms even when only a small fraction of file accesses cause network traffic this fact motivates measures to reduce network traffic caused by file processing activities a dfs design is scalable if dfs performance does not degrade with an increase in the size of a distributed system scalability is important for avoiding a situation in which a dfs that used to perform well in a users organization becomes a bottleneck when the organization becomes large scalability is achieved through special techniques that ensure that network traffic does not grow with size of the distributed system table summarizes techniques used to achieve high dfs performance these techniques are discussed in the following sections efficient file access inherent efficiency of file access depends on how the operation of a file server is structured we discuss two server structures that provide efficient file access multithreaded file server the file server has several threads each thread is capable of servicing one client request operation of several of these threads can be overlapped because file processing is an iobound activity this arrangement provides fast response to client requests and a high throughput the number of threads can be varied in accordance with the number of client requests that are active at any time and the availability of os resources such as thread control blocks chapter distributed file systems table performance techniques of distributed file systems technique description multithreaded file each thread in the file server handles one client request file server design processing is an iobound activity hence several threads can make progress in parallel thereby contributing to higher throughput hintbased file a hint is some information related to an ongoing file server design processing activity that may be maintained by a file server when a suitable hint is available the file server behaves like a stateful file server so that it can perform a file operation efficiently otherwise it behaves like a stateless file server file caching some part of a file located in a remote node is copied into the file cache in the client node file caching reduces network traffic during file processing by converting data transfers over the network into data transfers that are local to a client node semiindependent a cluster of nodes is a section of the distributed system that clusters of nodes contains sufficient hardware and software resources such that processes operating in a cluster rarely need resources located elsewhere in the system hintbased file server a hintbased file server is a hybrid design in that it has features of both a stateful and a stateless file server in the interest of efficiency it operates in a stateful manner whenever possible at other times it operates in a stateless manner a hint is some information concerning an ongoing file processing activity eg id of the next record in a sequential file that would to be accessed by a file processing activity see section the file server maintains a collection of hints in its volatile storage when a client requests a file operation the file server checks for presence of a hint that would help in its processing if a hint is available the file server uses it to speed up the file operation otherwise it operates in a stateless manner it opens the file and uses the recordbyte id provided by the client to access the required record or byte in either case after completing the file operation it inserts a part of the state of the file processing activity in its volatile storage as a hint and also returns it to the client as in a stateless file server the overall efficiency of the file server depends on the number of file operations that are aided by the presence of hints operation of a hintbased file server is fault tolerant because it would not be disrupted even if all hints in the servers volatile storage are lost because of a crash users will notice only a degradation of response times until the file server recovers and builds up a useful set of hints file caching the technique of file caching speeds up operation of a dfs by reducing network traffic it holds some data from a remote file in a buffer in a client node called part distributed operating systems the file cache the file cache and the copy of the file on a disk in the server node form a memory hierarchy see section so operation of the file cache and its benefits are analogous to those of a cpu cache chunks of file data are loaded from the file server into the file cache to benefit from spatial locality each chunk is large enough to service a few file accesses made by a client studies of file size distributions indicate small average file size so even an entire file can be copied into the file cache which is called wholefile caching studies by tanenbaum and others reported that percent of files in their system were smaller than kb in size and percent were smaller than kb in the andrew file system where the chunk size was varied on a perclient basis chunk size was frequently kb and contained an entire file and file cache hit ratios exceeded a dfs may use a separate attributes cache to cache information about file attributes figure contains a schematic diagram of file caching the cache manager exists on the path between a client and a file server it loads chunks of file data into the file cache supplies data from the cache to clients maintains the file cache using a replacement algorithm for chunks and writes modified chunks into the file copy in the server node key issues in the design of a file cache are location of the file cache file updating policy cache validation policy chunk size the file cache can be maintained in memory of a client node or on a disk attached to the client node organizing the file cache in memory would provide faster access to file data however it would result in low reliability because a crash of the client node would lead to loss of the file cache including any modified file data that is yet to be written to the file copy in the server locating the cache on the disk would slow down access to file data but would provide reliability as the file cache and the modified data contained in it would survive client server f node file server data traffic and cache validation traffic cache cache client manager file file manager client node cache cache node client client figure a schematic of file caching chapter distributed file systems node crashes redundancybased techniques like disk mirroring could be used to further enhance reliability of the file cache organized on a disk when a client performs a write operation on a disk the modified file data would have to be written into the file copy in the server the decision of whether to update the file copy immediately or at a later time involves a tradeoff between delay in the client and reliability of the dfs it is simplest to use the writethrough policy which updates the file cache in the client node and the file copy in the server node at the same time this method is reliable because the writethrough could be implemented as a transaction to ensure that it completes however it delays the client that performed the write operation to avoid delaying the client the update of the file copy could be performed at a later time provided arrangements are made to ensure that the modified data would not be lost if the client node failed in the meanwhile this policy is called the delayed write policy its variations perform the write operation at different times when the modified chunk is deleted from the file cache due to replacement or when the client closes the file when a file is processed by many clients in parallel copies of its data would exist in several file caches at the same time if one client performs a write operation copies in other clients caches become invalid ie stale the cache validation function identifies invalid data and deals with it in accordance with the file sharing semantics of the dfs for example when unix semantics are used file updates made by a client should be immediately visible to other clients of the file so the cache validation function either refreshes invalid data or prevents its use by a client chunk size in the file cache should be large so that spatial locality of file data contributes to a high hit ratio in the file cache however use of a large chunk size implies a higher probability of data invalidation due to modifications performed by other clients hence more delays and more cache validation overhead than when a small chunk size is used so the chunk size used in a dfs is a tradeoff between these two considerations a fixed chunk size may not suit all clients of a dfs so some distributed file systems notably the andrew file system adapt the chunk size to each individual client cache validation a simple method to identify invalid data is through timestamps a timestamp is associated with each chunk in a file and with each of its cached chunks the timestamp of a chunk indicates when it was last modified when a chunk of the file is copied into a cache its timestamp is also copied as the timestamp of the cached chunk at any time the cached chunk is invalid if its timestamp is smaller than the timestamp of the corresponding chunk in the file this way a write operation in some chunk x of a file by one client invalidates all copies of x in other clients caches data in such a chunk is refreshed ie reloaded at its next reference two basic approaches to cache validation are clientinitiated validation and serverinitiated validation clientinitiated validation is performed by the cache manager at a client node at every file access by a client it checks whether the required data is already in the cache if so it checks whether the data is valid if the check succeeds the cache manager provides the data from the cache to case studies sun network file system the sun network file system nfs provides sharing of file systems in nodes operating under the sunos operating system which is a version of unix figure shows a schematic diagram of the nfs it uses a twolevel architecture consisting of the virtual file system vfs layer see section and the nfs layer the vfs layer implements the mount protocol and creates a systemwide unique chapter distributed file systems client system calls interface server vfs interface vfs interface nfs layer other unix nfs client nfs server unix file systems file system file system rpcxdr rpcxdr network figure architecture of the sun network file system nfs designator for each file called the vnode if the file on which an operation is to be performed is located in one of the local file systems the vfs invokes that file system otherwise it invokes the nfs layer the nfs layer interacts with the remote node containing the file through the nfs protocol this architecture permits a node to be both a client and a server at the same time mount protocol each node in the system contains an export list that contains pairs of the form directory listofnodes each pair indicates that directory which exists in one of the local file systems can be remotely mounted only in the nodes contained in listofnodes when the superuser of a node makes a request to mount a remote directory the nfs checks the validity of the request mounts the directory and returns a file handle which contains the identifier of the file system that contained the remote directory and the inode of the remote directory in that file system users in the node see a directory hierarchy constructed through such mount commands nfs permits cascaded mounting of file systems ie a file system could be mounted at a mount point in another file system which is itself mounted inside another file system and so on however the nfs design carefully avoids transitivity of the mount mechanism for example consider the following situation the superuser in node n of the system mounts the file system c of node n at mount point y in the local file system b the superuser in node n mounts the file system b of node n at mount point x in the local file system a the nfs does not permit users in node n to access the file system c that was mounted over some part of file system b this way each hosts view of the directory hierarchy is the result of the mounts performed by its own superuser only which enables the file server to operate in a stateless manner if this restriction were part distributed operating systems not imposed each file server would have to know about all mounts performed by all clients over its file system which would require the file server to be stateful nfs protocol the nfs protocol uses the remote service paradigm ie remote file processing see section through a clientserver model employing remote procedure calls rpc a file server is stateless so each rpc has parameters that identify the file the directory containing the file record id and the data to be read or written the nfs provides calls for looking up a file within a directory reading directory entries manipulating links and directories accessing file attributes ie inode information and performing a file readwrite operation since a file server is stateless it performs an implicit open and close for every file operation and does not use the unix buffer cache see section for a description of the unix buffer cache an nfs server does not provide locking of files or records users must use their own means for concurrency control path name resolution let a user u located in node n use a path name xyzw where y is the root directory of a mounted file system to start with host node n creates vnodex the vnode for x the nfs uses the mount table of n while looking up the next component of the path name so it knows that y is a mounted directory it creates vnodey from the information in the mount table let vnodey be for a file in node n so the nfs makes a copy of directory y in node n while looking for z in this copy y the nfs again uses the mount table of n this action would resolve z properly even if z were a file system that was mounted by the superuser of node n over some point in the remote file system y the file server in node n which contains y does not need to have any knowledge of this mounting instead of using this procedure if the path name yzw were to be handed over to the file server in node n it would have to know about all mounts performed by all clients over its file system it would require the file server to be stateful a directory names cache is used in each client node to speed up path name resolution it contains remote directory names and their vnodes new entries are added to the cache when a new path name prefix is resolved and entries are deleted when a lookup fails because of mismatch between attributes returned by the file server and those of the cached vnodes file operations and file sharing semantics the nfs uses two caches to speed up file operations a file attributes cache caches inode information this cache is used because it was found that a large percentage of requests made to a file server concerned file attributes the cached attributes are discarded after seconds for files and after seconds for directories the file blocks cache is the conventional file cache it contains data blocks from the file the file server uses large kbytes data blocks and uses readahead and delayedwrite techniques ie buffering techniques see section for improving file access performance cache validation is performed through timestamps associated with each file and with each cache block contents of a cached block are assumed to be valid for a certain period of time for any access after this time the cached block is used only if its timestamp is larger than the timestamp of the file a modified block is sent to the file server for chapter distributed file systems writing into the file at an unspecified time during processing of a file or when the file is closed this policy is used even if clients concurrently access the same file block in conflicting modes as a result of this policy and the cache validation scheme visibility of a file modification made by one client to concurrent clients is unpredictable and the file sharing semantics are neither unix semantics nor session semantics andrew and coda file systems andrew the distributed computing environment developed at the carnegie mellon university is targeted at gigantic distributed systems containing workstations each workstation has a local disk which is used to organize the local name space this name space contains system programs for booting and operation of the workstation and temporary files which are accommodated there for performance reasons all clients have an identical shared name space which is location transparent in nature it is implemented by dedicated servers which are collectively called vice scalable performance is obtained as follows clusters localize file processing activities as much as possible so that file accesses do not cause traffic on the system backbone network traffic within a cluster is reduced by caching an entire file on the local disk of a workstation when it is opened for processing these two techniques ensure that network traffic in the system does not grow as system size grows shared name space andrew uses the concept of a volume a volume typically contains files of a single user many volumes may exist on a disk andrew treats a volume in much the same way unix treats a disk partition though a volume can be substantially smaller than a disk partition a volume can be mounted this fact provides a much finer granularity for mounting than in unix the file identifier used by vice contains volume number of the volume which contains a file and an index into the array of inodes contained in the volume a volume location database vldb contains information about each volume in the system this database is replicated on every server volumes are migrated from one disk to another in order to balance the utilization of disks in the system the server that previously contained a migrated volume maintains some forwarding information until all servers update their volume location databases this arrangement simplifies volume migration by eliminating the need to update all volume location databases at the same time actual migration of a volume is performed with minimum disruption of file processing activities by the following procedure a copy of a volume is made at the new server while this operation is in progress its original server continues to service requests once the copying is completed the volume is made offline recent updates performed after the copy operation was initiated are made on the copy at the new server and the new copy is made operational file operations and file sharing semantics when a client opens a file andrew caches the file on the local disk of the clients workstation using kb chunks part distributed operating systems however it adapts the chunk size on a perclient basis to suit the clients file access pattern as mentioned earlier in section studies conducted in the mids have reported that chunks of kb were widely used and the hit ratio in the file cache typically exceeded file openclose calls are directed to a userlevel process called venus venus caches a file when a client opens it and updates the servers copy when the client closes the file file read and write operations are performed on the cached copy without involving venus consequently changes made to a file are not immediately reflected on the servers copy and they are not visible to other clients accessing the file these file sharing semantics have some features of session semantics however andrew does not maintain multiple versions of a file the file copy cached by the venus process in a node is considered to be valid unless the venus process is told otherwise this way a cached copy of a file may persist across the close operation on the file and the next open operation on it in the same workstation cache validation is performed in a serverinitiated manner using a mechanism called callback when some file f is cached at client node n because of an open the server notes this fact in its table as long as this entry remains in the table node n is said to have a callback on f when the copy of f in the server is updated because some client closed f the server removes ns entry from its table and notifies the venus process in node n that its callback on f has been broken if some client in n tried to open f in the future venus would know that n does not have a callback on f so it would cache file f once again venus maintains two caches a data cache and a status cache the status cache is used to service system calls that query file status information both caches are managed on an lru basis path name resolution is performed on a componentbycomponent basis venus maintains a mapping cache which contains information concerning volumes which have been accessed recently since volumes may be migrated venus treats this information as a hint and discards it if it turns out to be wrong during path name resolution venus also copies each directory involved in the path name in its cache presence of these cached copies may speed up path name resolution in the future file servers are multithreaded to prevent them from becoming a bottleneck a lightweight process package is used to spawn new lightweight processes to handle file requests clientserver communication is organized by using rpcs features of coda coda which is a successor of the andrew file system version added two complementary features to achieve high availability replication and disconnected operation coda supports replication of volumes the collection of servers that have a copy of a volume is known as the volume storage group vsg coda controls use of replicated files through the read one write all policy only one of the copies needs to be available for reading however all copies must be updated at the same time a multicasting rpc called multirpc is used for this purpose a node enters the disconnected mode of operation when the subset of vsg accessible to it is null andrew already supported wholefile caching in chapter distributed file systems a clients node so a client in the disconnected mode of operation could operate on a file in isolation the file updates made by this client would be reflected in the file when the clients node is able to connect to the server any conflicts with file versions created by other file processing activities in the meanwhile would have to be resolved at this time this step can be automated in an applicationspecific manner however it may require human intervention in some cases having a single file in cache may not be adequate for disconnected operation so coda provides hoarding of files a user can provide a hoarding database which contains path names of important files to coda during a session initiated by the user coda uses a prioritized cache management policy to hold some recently accessed files and files named in the hoarding database in the cache of the users node this set of files is refined by recomputing their priorities periodically this way the cache in the node may contain an adequate set of files when the node becomes disconnected which would enable meaningful disconnected operation gpfs the general parallel file system is a highperformance shareddisk file system for large computing clusters operating under linux gpfs uses data striping see section across all disks available in a cluster thus data of a file is written on several disks which can be read from or written to in parallel a largesize block ie strip is used to minimize seek overhead during a file readwrite however a large disk block may not provide high data transfer rates for small files that would occupy only a small number of strips so a smaller subblock which could be as small as of a block is used for small files locking is used to maintain consistency of file data when processes in several nodes of the cluster access a common file high parallelism in accessing a common file requires finegrained locking whereas low locking overhead requires coarsegrained locking so gpfs uses a composite approach that works as follows the first process that performs a write operation on a file is given a lock whose byte range covers the entire file if no other process accesses the same file this process does not have to set and reset locks while processing the file if another process wishes to write into the same file that process is given a lock with a byte range that covers the bytes it wishes to write and the byte range in the lock already held by the first process is reduced to exclude those bytes this way the lock granularity is as coarse as possible but as fine as necessary subject to the restriction that the byte range in a lock can not be smaller than a data block on a disk whenever the byte range in a lock is narrowed updates made on the bytes that are not covered by the new byte range in the lock are flushed to the file this way a process acquiring a lock for these bytes would see their latest values the locking scheme of gpfs involves a centralized lock manager and a few distributed lock managers and employs the notion of lock tokens to reduce the latency and overhead of locking the first time some process in a node accesses part distributed operating systems a file the centralized lock manager issues a lock token to that node this token authorizes the node to locally issue locks on the file to other processes in that node until the lock token is taken away from it this arrangement avoids repeated traffic between a node and the centralized lock manager for acquiring locks on a file when a process in some other node wishes to access the same file the centralized lock manager takes away the lock token from the first node and gives it to the second node now this node can issue locks on that file locally the data bytes covered by byte ranges in the locks issued by a node can be cached locally at that node no cache coherence traffic would be generated when these bytes are accessed or updated because no process in another node is permitted to access these bytes race conditions may arise over the metadata of a file such as the index blocks in the fmt when several nodes update the metadata concurrently for example when two nodes add a pointer each to the same index block in the fmt one clients update of the block would be lost when another client updates it to prevent inconsistencies due to race conditions one of the nodes is designated as the metanode for the file and all accesses and updates to the files metadata are made only by the metanode other nodes that update the file send their metadata to the metanode and the metanode commits them to the disk the list of free disk space can become a performance bottleneck when file processing activities in many nodes need more disk space the central allocation manager avoids it by partitioning the free space map and giving one partition of the map to each node a node makes all disk space allocations using its partition of the map when the free space in that partition is exhausted it requests the allocation manager for another partition each node writes a separate journal for recovery this journal is located in the file system to which the file being processed belongs when a node fails other nodes can access its journal and carry out the pending updates consistency of the data bytes updated in this manner is implicit because the failed node would have locked the data bytes these locks are released only after the journal of the failed node is processed communication failures may partition the system however file processing activities in individual nodes may not be affected because nodes may be able to access some of the disks such operation of the file system can lead to inconsistencies in the metadata to prevent such inconsistencies only nodes in one partition should continue file processing and all other nodes must cease file processing gpfs achieves it as follows only nodes in the majority partition ie the partition that contains a majority of the nodes are allowed to perform file processing at any time gpfs contains a group services layer that uses heartbeat messages to detect node failures it notifies a node when the node has fallen out of the majority partition or has become a part of the majority partition once again however this notification may itself be delayed indefinitely because of communication failures so gpfs uses features in the io subsystem to prevent those nodes that are not included in the majority partition from accessing any disks gpfs uses a replication policy to protect against disk failures multiprogramming systems multiprogramming operating systems were developed to provide efficient resource utilization in a noninteractive environment a multiprogramming os part overview multiprogramming multiprogramming multiprogramming kernel kernel kernel io program io program cpu program cpu program io program io program program cpu program program a b c figure operation of a multiprogramming system a program is in execution while program is performing an io operation b program initiates an io operation program is scheduled c programs io operation completes and it is scheduled has many user programs in the memory of the computer at any time hence the name multiprogramming it employs the dma mode of io see section so it can perform io operations of some programs while using the cpu to execute some other program this arrangement makes efficient use of both the cpu and io devices the io and computational activities in several programs are in progress at any time so it also leads to high system performance we discuss this aspect in section figure illustrates operation of a multiprogramming os the memory contains three programs an io operation is in progress for program while the cpu is executing program the cpu is switched to program when program initiates an io operation and it is switched to program when programs io operation completes the multiprogramming kernel performs scheduling memory management and io management it uses a simple scheduling policy which we will discuss in section and performs simple partitioned or poolbased allocation of memory and io devices since several programs are in memory at the same time the instructions data and io operations of a program should be protected against interference by other programs we shall shortly see how it is achieved a computer must possess the features summarized in table to support multiprogramming see section the dma makes multiprogramming feasible by permitting concurrent operation of the cpu and io devices memory protection prevents a program from accessing memory locations that lie outside the range of addresses defined by contents of the base register and size register of the cpu the kernel and user modes of the cpu provide an effective method of preventing interference between programs recall from section that the os puts the cpu in the user mode while executing user programs and that instructions that load an address into the base register and a number into the size register of the cpu respectively are privileged instructions if a program tries to undermine memory protection by changing contents of the base and size registers through these instructions a program interrupt would be raised because chapter overview of operating systems table architectural support for multiprogramming feature description dma the cpu initiates an io operation when an io instruction is executed the dma implements the data transfer involved in the io operation without involving the cpu and raises an io interrupt when the data transfer completes memory protection a program can access only the part of memory defined by contents of the base register and size register kernel and user certain instructions called privileged instructions can be modes of cpu performed only when the cpu is in the kernel mode a program interrupt is raised if a program tries to execute a privileged instruction when the cpu is in the user mode the cpu is in the user mode the kernel would abort the program while servicing this interrupt the turnaround time of a program is the appropriate measure of user service in a multiprogramming system it depends on the total number of programs in the system the manner in which the kernel shares the cpu between programs and the programs own execution requirements priority of programs an appropriate measure of performance of a multiprogramming os is throughput which is the ratio of the number of programs processed and the total time taken to process them throughput of a multiprogramming os that processes n programs in the interval between times t and tf is ntf t it may be larger than the throughput of a batch processing system because activities in several programs may take place simultaneously one program may execute instructions on the cpu while some other programs perform io operations however actual throughput depends on the nature of programs being processed ie how much computation and how much io they perform and how well the kernel can overlap their activities in time the os keeps a sufficient number of programs in memory at all times so that the cpu and io devices will have sufficient work to perform this number is called the degree of multiprogramming however merely a high degree of multiprogramming can not guarantee good utilization of both the cpu and io devices because the cpu would be idle if each of the programs performed io operations most of the time or the io devices would be idle if each of the programs performed computations most of the time so the multiprogramming os employs the two techniques described in table to ensure an overlap of cpu and io activities in programs it uses an appropriate program mix which ensures that some of the programs in memory are cpubound programs which are programs that part overview table techniques of multiprogramming technique description appropriate the kernel keeps a mix of cpubound and iobound programs program mix in memory where a cpubound program is a program involving a lot of computation and very little io it uses the cpu in long bursts that is it uses the cpu for a long time before starting an io operation an iobound program involves very little computation and a lot of io it uses the cpu in small bursts prioritybased every program is assigned a priority the cpu is always preemptive allocated to the highestpriority program that wishes to use it scheduling a lowpriority program executing on the cpu is preempted if a higherpriority program wishes to use the cpu involve a lot of computation but few io operations and others are iobound programs which contain very little computation but perform more io operations this way the programs being serviced have the potential to keep the cpu and io devices busy simultaneously the os uses the notion of prioritybased preemptive scheduling to share the cpu among programs in a manner that would ensure good overlap of their cpu and io activities we explain this technique in the following definition priority a tiebreaking criterion under which a scheduler decides which request should be scheduled when many requests await service the kernel assigns numeric priorities to programs we assume that priorities are positive integers and a large value implies a high priority when many programs need the cpu at the same time the kernel gives the cpu to the program with the highest priority it uses priority in a preemptive manner ie it preempts a lowpriority program executing on the cpu if a highpriority program needs the cpu this way the cpu is always executing the highestpriority program that needs it to understand implications of prioritybased preemptive scheduling consider what would happen if a highpriority program is performing an io operation a lowpriority program is executing on the cpu and the io operation of the highpriority program completes the kernel would immediately switch the cpu to the highpriority program assignment of priorities to programs is a crucial decision that can influence system throughput multiprogramming systems use the following priority assignment rule an iobound program should have a higher priority than a cpubound program example illustrates operation of this rule chapter overview of operating systems execution of programs in a multiprogramming system example a multiprogramming system has progiob an iobound program and progcb a cpubound program its operation starts at time in figure the cpu and io activities of these programs are plotted in the form of a timing chart in which the x axis shows time and the y axis shows cpu and io activities of the two programs cumulative cpu and io activities are shown at the bottom of the chart note that the chart is not to scale the cpu activity of progiob has been exaggerated for clarity program progiob is the higher priority program hence it starts executing at time after a short burst of cpu activity it initiates an io operation time instant t the cpu is now switched to progcb execution of progcb is thus concurrent with the io operation of progiob being a cpubound program progcb keeps the cpu busy until progiobs io completes at t at which time progcb is preempted because progiob has a higher priority this sequence of events repeats in the period t deviations from this behavior occur when progcb initiates an io operation now both programs are engaged in io operations which go on simultaneously because the programs use different io devices and the cpu remains idle until one of them completes its io operation this explains the cpuidle periods tt and tt in the cumulative cpu activity ioidle periods occur whenever progiob executes on the cpu and progcb is not performing io see intervals t tt and tt but the cpu and the io subsystem are concurrently busy in the intervals tt tt tt and tt cpu activity progiob io activity cpu activity progcb io activity t t t t t t t t t time cumulative busy busy busy cpu activity cumulative busy busy busy io activity figure timing chart when iobound program has higher priority part overview table effect of increasing the degree of multiprogramming action effect add a cpubound a cpubound program say prog can be program introduced to utilize some of the cpu time that was wasted in example eg the intervals tt and tt prog would have the lowest priority hence its presence would not affect the progress of progcb and progiob add an iobound an iobound program say prog can be program introduced its priority would be between the priorities of progiob and progcb presence of prog would improve io utilization it would not affect the progress of progiob at all since progiob has the highest priority and it would affect the progress of progcb only marginally since prog does not use a significant amount of cpu time we can make a few observations from example the cpu utilization is good the io utilization is also good however io idling would exist if the system contained many devices capable of operating in the dma mode periods of concurrent cpu and io activities are frequent progiob makes very good progress because it is the highestpriority program it makes very light use of the cpu and so progcb also makes very good progress the throughput is thus substantially higher than if the programs were executed one after another as in a batch processing system another important feature of this priority assignment is that system throughput can be improved by adding more programs table describes how addition of a cpubound program can reduce cpu idling without affecting execution of other programs while addition of an iobound program can improve io utilization while marginally affecting execution of cpubound programs the kernel can judiciously add cpubound or iobound programs to ensure efficient use of resources when an appropriate program mix is maintained we can expect that an increase in the degree of multiprogramming would result in an increase in throughput figure shows how the throughput of a system actually varies with the degree of multiprogramming when the degree of multiprogramming is the throughput is dictated by the elapsed time of the lone program in the system when more programs exist in the system lowerpriority programs also contribute to throughput however their contribution is limited by their opportunity to use the cpu throughput stagnates with increasing values of the degree of multiprogramming if lowpriority programs do not get any opportunity to execute summary a distributed file system dfs stores user files the notion of transparency concerns the assoin several nodes of a distributed system hence a ciation between the path name of a file and location process and a file being accessed by it may exist of the file whether a user must know a files in different nodes this situation requires a dislocation in order to access it and whether the systributed file system to use special techniques so tem can change the location without affecting the that a user need not know where a file is files name high transparency provides user conlocated can perform file processing even when venience and also enables a dfs to reduce network link and node failures occur in the system and traffic by moving a file to a node where it is accessed can process files efficiently in this chapter we very frequently file sharing semantics represent discussed how distributed file systems fulfill these another aspect of user convenience they specify requirements whether the file updates made by a process would part distributed operating systems be visible to other processes accessing the file conto find its location the notion of a hint is used currently three popular file sharing semantics are to improve performance of a stateless file server as follows in unix semantics file updates made a hint is simply a part of dfs state however the by a process are visible immediately to all other server is designed in such a manner that it uses a processes using the file in session semantics the hint if one is available but proceeds in a stateless updates made by a process are visible to only some manner if a hint is not available processes in the same node in transaction semanperformance of a dfs is affected by network tics a complete file processing activity is treated latencies when a process and the file processed by as a single atomic transaction so that either all file it exist in different nodes a dfs uses the techupdates made during the activity are reflected in the nique of file caching to improve its performance file or none of them are and the updates made by a it maintains a copy of a files data in the node file processing activity are visible to other processes where the process exists so that accesses to file only after the activity completes data are implemented locally in the node rather high availability of a file system requires that than over the network if processes located in difa file processing activity in a process should not be ferent nodes update the same file concurrently affected by a transient fault in the node holding copies of the file would exist in caches in many the file which is called the server node the dfs nodes so a process may not see the latest value uses a stateless server design to provide high availof the data that was updated by another process ability the stateless server does not maintain any this problem is overcome by using cache coherstate information about an ongoing file processing ence techniques which prevent accesses to stale file activity consequently a crash of the server node data however it causes network traffic for refreshdoes not disrupt the file processing activity it can ing stale copies of a files data in caches which be resumed when the servers operation is restored reduces the benefit of file caching session semanhowever the stateless design of the server implies tics eliminate the cache coherence traffic because that every time a file is accessed the file server updates made by a process are not visible outside would have to access the directory entry of the file its node test your concepts classify each of the following statements as true select the appropriate alternative in each of the or false following questions a location independence in a distributed file a a distributed file system uses file caching to system provides user convenience ensure good file access performance which b the session semantics use multipleimage file sharing semantics cause the least cache mutable files validation overhead c robustness of a file can be achieved through i session semantics disk mirroring ii unix semantics d file caching has exactly the same effect as iii transaction semantics file migration ie movement of files among b file replication improves nodes in the system i robustness of a file system e directory caching improves file access perforii recoverability of a file system mance in a distributed file system iii availability of a file system f faults that occur in a file server during a file iv none of iiii processing activity can be tolerated by using a stateless file server chapter distributed file systems exercises discuss how session semantics can be implestateful file server design b a stateless file server mented design should a dfs maintain file buffers at a server what are the benefits and limitations of spawnnode or at a client node what is the influence of ing multiple threads in a file server to hanthis decision on unix file sharing semantics see dle file processing activities of different clients section and session semantics describe the synchronization requirements of justify the following statement file caching these threads integrates well with session semantics but not discuss important issues to be handled during so with unix semantics recovery of a failed node in a system that uses discuss the various techniques discussed in this file replication to provide availability chapter and in chapters and that can be discuss how locking can be used to reduce cache used to ensure robustness of a file validation overhead and enhance scalability of a discuss how a client should protect itself against distributed file system failures in a distributed file system using a a bibliography svobodova and levy and silberschatz files schmuck and haskin discusses use of shared are survey papers on distributed file systems comer disks in a parallel file system and describes distributed and peterson discusses concepts in naming and synchronization and fault tolerance techniques discusses name resolution mechanisms in many systems lampson and terry discuss use of braam p j and p a nelson removing hints to improve performance of a distributed file system bottlenecks in distributed file systems coda and makaroff and eager discusses effect of cache sizes intermezzo as examples proceedings of linux on file system performance expo brownbridge et al discusses the unix united brownbridge d r l f marshall and system which is an early network file system sandberg b randell the newcastle connection and callaghan discuss the sun nfs or unixes of the world unite satyanarayanan discusses the andrew distributed software practice and experience file system while kistler and satyanarayanan describes the coda file system braam and nelson callaghan b nfs illustrated discusses the performance bottlenecks in coda addisonwesley reading mass and intermezzo which is a sequel to coda that incor carns p h w b ligon iii r b ross and porates journaling russinovich and solomon r thakur pvfs a parallel file system discusses data replication and data distribution features for linux clusters extreme linux of the windows file system workshop application processes running in different nodes comer d and l l peterson a model of a cluster of computer systems may make parallel of name resolution in distributed mechanisms accesses to files thekkath et al discusses a scalproceedings of the th international conference on able distributed file system for clusters of computer distributed computing systems systems preslan et al describes fault tolerance ghemawat s h gobioff and s t leung in a cluster file system through journaling carns et al the google file system proceedings of discusses a parallel file system that provides high the th acm symposium on operating system bandwidth for parallel file accesses to data in shared principles part distributed operating systems gray c g and d r cheriton leases journaling in a linux shared disk file system an efficient faulttolerant mechanism for proceedings of the th ieee symposium on mass distributed file cache consistency proceedings of storage systems the th acm symposium on operating systems russinovich m e and d a solomon principles microsoft windows internals th ed microsoft kistler j j and m satyanarayanan press redmond wash disconnected operation in the coda file sandberg r the sun network file system acm transactions on computer system design implementation and experience systems sun microsystems mountain view calif lampson b w hints for computer satyanarayanan m scalable secure system designers proceedings of the th and highly available distributed file access symposium of operating systems principles computer schmuck f and r haskin gpfs a levy e and a silberschatz distributed shareddisk file system for large computing file systems concepts and examples clusters proceedings of the first usenix computing surveys conference on file and storage technologies melamed a s performance analysis of unixbased network file systems ieee micro svobodova l file servers for networkbased distributed systems computing makaroff d j and d l eager disk surveys cache performance for distributed systems terry d b caching hints in distributed proceedings of the th international conference systems ieee transactions on software on distributed computing systems engineering preslan k w a p barry j brassow thekkath c a t mann and e k lee r cattelan a manthei e nygaard s v oort frangipani a scalable dfs proceedings of the d teigland m tilstra m okeefe g erickson th acm symposium on operating system and m agarwal implementing principles issues in distributed system security we term the nodes that are directly under control of the distributed os as secure nodes they contain resources and offer services to users and their processes as shown in figure a user process accesses a remote resource through a message sent to the resource coordinator process such a message may travel over public networks and pass through computer systems called communication processors which operate under local operating systems communication processors employ a storeandforward model to route a message to its destination thus messages between processes are exposed to observation and interference part distributed operating systems n n remote resource user cp coordinator process pi pj process secure node secure node figure security threats in a network by external entities this situation raises new security threats that do not arise in a conventional system security threats in distributed operating systems are of four kinds leakage release of message contents to unauthorized users tampering modification of message contents stealing use of system resources without authorization denial of service to authorized users this threat can be in the form of willful destruction of system resources not amounting to stealing ie destruction without any gain to the perpetrator or disruption of access to resources leakage and tampering are termed threats to message security tampering may be employed to modify the text of a message which is a threat to its integrity or modify the identity of its sender which is a threat to its authenticity an intruder can perform stealing by masquerading through tampering denial of service can be achieved by tampering with the text of a message or ids of its source and destination processes or by masquerading these security threats are addressed through two means message security techniques special techniques are employed to thwart attacks on messages authentication of remote users trusted means are provided to authenticate remote users attacks on integrity and authenticity are addressed through a combination of these two means security mechanisms and policies figure shows an arrangement of security mechanisms and policies authentication in conventional systems has been described earlier in chapter authentication in a distributed system has two new facets the authentication service must be trustworthy and available to all nodes in a system encryption is used to ensure secrecy and integrity of the authentication and authorization databases it is also used to implement message security by encoding the text of messages processes need to know what encryption keys to use while communicating with other processes the lowerlevel mechanism called key distribution generates and chapter distributed system security policies encryption authentikey cation distribution figure mechanisms and policies for distributed system security table classes of security attacks in distributed systems attack description eavesdropping an intruder listens to interprocess messages over the network to obtain information concerning message content or statistical features of messages message tampering an intruder intercepts messages alters their contents and reinserts them into the communication stream message replay an intruder makes copies of messages exchanged by communicating processes and inserts the copies into the communication stream at a later time as if they were genuine messages being sent at that time masquerading an intruder is able to pass off as an authorized user of the system while consuming resources and while sending and receiving messages distributes encryption keys for use by communicating processes it is discussed in section security attacks in distributed systems security attacks in distributed systems which are typically launched through messages can be classified into the four classes summarized in table eavesdropping can take various forms like obtaining the content of a message or collecting information about messages exchanged by specific nodes or passing over specific links in a police or military information system the latter analyses can be used to reveal or guess identities of communicating entities message tampering can be used to mislead the recipient of a message this attack is feasible in a storeandforward network message replay can be used to achieve a variety of nefarious ends the recipient of a replayed message may be misled into thinking that messages are being exchanged in real time if the recipient is a user process it might be fooled into taking actions that are unnecessary absurd or wasteful in terms of resources it may also be misled into revealing confidential information if the recipient is a server process a replayed message may lead to wrong authentication leading to opportunities for masquerading or stealing of resources message security approaches to message security can be classified into linkoriented approaches and endtoend approaches in a linkoriented approach security measures are applied at every link of a communication path this approach tends to be expensive since its cost depends on the number of links over which a message travels for example if a message between process pi located at node n and process pj located at node n passes along the path nnn it has to incur security overhead for links nn and nn in the endtoend approach security measures can be employed selectively by nodes or processes in the system this feature permits users to employ security measures with varying degrees of cost and sophistication in the following discussion we will assume that endtoend measures are used we describe three approaches to message security they involve encryption using public keys private keys and session keys respectively table summarizes their features private key encryption private key encryption also called secret key encryption is the classical approach based on symmetric keys each process pi has a private key vi known to itself and to a few other processes in the system a process sending a message to pi must encrypt it by using vi on receiving a message pi decrypts it by using vi the main advantage of private key encryption is that the number of encryption keys in the system is limited to n where n is the number of communicating entities in the system since all messages intended for process pi are encrypted with the same key pi need not know the identity of the sender of a message in order to read the message private key encryption suffers from a few drawbacks each sender process needs to know the private key of pi thus many processes know the private key of a process and an intruder may discover it as a result of somebodys negligence the private key is exposed to intruder attacks over a long period of time so chances of a successful attack on the private key increase with time however it is not possible to change the private key of a process because it is known to many other processes in the system chapter distributed system security table encryption techniques used for message security technique description private key employs symmetric encryption a process pi has a unique encryption encryption key vi called the private key all messages sent to pi must be encrypted by using vi process pi decrypts them by using vi the private key of a process is exposed to intruder attacks over the entire lifetime of a process public key encryption employs asymmetric encryption a process pi has a pair of unique keys ui vi ui is the public key which can be made known to all processes in the system whereas vi is the private key which is kept secret messages to pi are encrypted by using ui but pi decrypts them by using vi the rivestshamiradelman rsa algorithm is widely used to generate the pair of keys for a process the private key of a process is not exposed to intruder attacks session key a pair of communicating processes pi pj is assigned a encryption session key sk ij when they begin a communication session the session key is used for symmetric encryption of all messages exchanged during the session the session key has a smaller lifetime than a private or public key so it suffers less exposure to intruder attacks user processes do not know each others private keys so private key encryption is not useful for security of interprocess messages in general os processes know private keys of user processes so they use private key encryption while communicating with user processes as discussed in section this feature is used in the implementation of key distribution centers user processes need to use some other encryption scheme while communicating with one another public key encryption each process pi has a pair of keys ui vi ui is the public key of pi which can be made known to all processes in the system vi is the private key known only to process pi ui and vi are chosen such that vi can not be guessed from ui and for any message m dvi eui pm pm i where pm is the plaintext form of message m and e d are the encryption and decryption functions respectively see section when pj wishes to send a message to pi it obtains pis public key from the os transmission of the message takes place as follows process pj encrypts the message with the public key of the destination process pi ie with ui the encrypted message ie eui pm is transmitted over the network and is received by process pi part distributed operating systems process pi decrypts the received message with its own private key ie with vi thus it performs dvi eui pm which yields pm the rivestshamiradelman rsa encryption algorithm is used to generate pairs of keys ui vi that satisfy eq let u v be such a pair of keys given two numbers x and y both smaller than a chosen integer number n encryption and decryption using u and v respectively are performed as follows eux xu mod n dvy yv mod n to encrypt and decrypt a message m the rsa algorithm is used as a block cipher with a block size s which is chosen such that s n the chosen number x is now the number formed by the bit string found in a block of pm the plaintext form of message m and y is the number formed by the bit string in the corresponding block of cm the ciphertext form of message m this way x s and y s so each of them is smaller than n as required the rsa algorithm chooses n as the product of two large prime numbers p and q typically p and q are digits each which makes n a digit number assuming u and v to be the public and private keys to satisfy eq v should be relatively prime to p q ie v and p q should not have any common factors except and u should satisfy the relation u v mod p q choice of u and v as the public and private keys implies that a standard value of n is used in the system alternatively the pair u n can be used as the public key and the pair v n can be used as the private key of a process it will permit different values of n to be used for different pairs of processes an attack on the rsa cipher can succeed if n can be factored into p and q however it is estimated that factorization of a digit number which would be needed to break the cipher would need billion years on a computer that can perform million operations per second public key encryption suffers from some drawbacks when compared with private key encryption keys used in public key encryption are approximately an order of magnitude larger in size than private keys this is unavoidable since public keys have to be large to make factorization prohibitively expensive the encryption and decryption operations are also very expensive when compared with symmetric encryption in many situations these operations are up to times slower therefore it is not practical to use public key encryption for interprocess messages instead it is used to securely communicate a session key to a pair of processes that intend to start a communication session this aspect is discussed in the next section session keys processes pi and pj obtain a session key also called a conversation key for one session of communication this key is used for symmetric encryption during the session and is discarded at the end of the session if the processes chapter distributed system security wish to enter into another session sometime in the future they obtain a fresh session key this approach limits exposure of an encryption key to an intruder thereby reducing the risk of a successful attack on the cryptographic system distribution of encryption keys a process needs to know what encryption key to use while communicating with another process the os contains an interactive service called a key distribution center kdc to provide this information figure shows a schematic of a key distribution center a process pi makes a request to the kdc for an encryption key to communicate with a process pj the kdc returns a key k pi uses this key to encrypt a message m to be sent to process pj if processes use public keys to communicate with one another the kdc maintains a directory containing public keys of all entities in the system if processes use session keys the kdc does not posses a directory it generates a new session key on demand an important issue in the kdc schematic is the protocol used for securely passing the keys when a public key is requested it needs to be passed only to the requester when a session key is requested by a process pi to communicate with a process pj the key has to be passed to both pi and pj however pj is unaware that pi is interested in setting up a session with it so the kdc does not send the session key directly to pj instead pi can send the session key to pj along with its first message these key transmission protocols are described in the following distribution of public keys the following messages are exchanged between pi the requesting process and the kdc pi kdc eukdc pi pj kdc pi eui pj uj pi sends its own id and pj the id of the intended destination process to the kdc this message is encrypted with ukdc the public key of kdc the kdc replies by sending uj the public key of pj encrypted with the public key of pi here the encryption is not used to protect confidentiality of pjs key because an intruder can legitimately obtain this key by itself by making a request to the kdc the purpose of encryption is to prevent an intruder from tampering with messages kdc request key pi pj key k pi ek m pj figure key distribution center kdc in a distributed os part distributed operating systems between pi and the kdc to perpetrate a denialofservice attack in the absence of encryption an intruder could have tampered with pis message to the kdc and changed pj to some pk so that pi would not obtain pj s key or the intruder could have tampered with the kdcs message to pi and changed pjs key that is being passed to pi when encryption is used both the kdc and pi would recognize tampered messages and discard them distribution of session keys when pi wishes to obtain a session key to communicate with pj the session key should be made known to both pi and pj figure illustrates how it is achieved in three steps in the first step pi sends a request message containing its own id and the id of pj to the kdc the kdc allocates a session key sk ij for the session between pi and pj and sends it to pj its reply to pj also contains an encrypted unit containing sk ij which can be decrypted only by pj pi passes this encrypted unit to pj in its first message or in a special message intended for this purpose pj obtains sk ij by decrypting this unit and keeps it for use during the session with pi in a private key system this exchange can be implemented as follows pi kdc pi pj kdc pi evi pj sk ij evj pi sk ij pi pj evj pi sk ij eskij message in the second step the kdc sends a reply to pi which is encrypted with pis private key the reply contains the session key sk ij and evj pi sk ij which is the session key encrypted by using pjs private key pi decrypts the kdcs message with its own private key to obtain the session key sk ij decryption also yields evj pi sk ij pi copies this unit in the first message it sends to pj when pj decrypts this unit it obtains sk ij which it uses to decrypt all messages from pi in a public key system session keys need not be distributed by the kdc a sender process can itself choose a session key it merely has to communicate the session key securely to the destination process which can be achieved through encryption by using the public key of the destination process thus a process pi kdc request session key send pi session key pj pass session key figure distribution of session keys chapter distributed system security can employ the following protocol to communicate a session key to process pj pi kdc eukdc pi pj kdc pi eui pj uj pi pj euj pi sk ij eskij message the first two steps of this protocol are identical with the first two steps of protocol they provide pi with the public key of pj now pi itself generates a session key sk ij and passes the session key and its first message to pj in step preventing message replay attacks in a message replay attack an intruder simply copies messages passing over the network and plays them back in the future a replayed message may mislead its recipient into taking wrong or duplicate actions which may affect data consistency or reveal confidential information for example in a system using session keys an intruder could replay the message of step in protocol or protocol when pj receives the replayed message it would be tricked into thinking that pi is communicating with it using the session key sk ij when process pj responds to this message the intruder would replay the next copied message in this manner it could replay an entire session the recipient of a message can employ the challengeresponse protocol to check whether the message exchange is taking place in real time steps of the challengeresponse protocol are as follows challenge when a process pj receives a message originated by a process pi it throws a challenge to pi to prove that it is engaged in a message exchange with it in real time the challenge is in the form of a message containing a challenge string which is encrypted in such a manner that only process pi can decrypt it response on receiving the challenge message process pi is expected to decrypt it obtain the challenge string transform it in a manner expected by pj encrypt the result so that only pj can decrypt it and send it back to pj detect on receiving a reply message process pj decrypts the message and checks whether the decrypted contents match its expectations a mismatch indicates that it is subject to a replay attack a challenger could send a number as the challenge string and expect a reply that is the result of some simple tranformation of that number like adding to it however the challenger should use a different number in every challenge so that a replay of an old conversation would not provide the expected reply two choices of the challenge string are a random number or the current time of the day the actual value of a challenge string is immaterial so it is called a nonce part distributed operating systems the challengeresponse protocol should be used in every situation where a message replay attack would be meaningful as an example consider the distribution of session keys through protocol an intruder could save the message of step and replay it sometime in the future to trick process pj into starting a conversation with it using sk ij so before using the session key obtained in step process pj would use the challengeresponse protocol to ensure that the conversation is taking place in real time pj pi esk ij n pi pj eskij n here n is a nonce pi is expected to obtain n through decryption using the session key sk ij add to it encrypt the result by using sk ij and send it back to pj an intruder would not be able to perform these actions correctly since it does not know sk ij in fact pis ability to extract n from pj s message implicitly verifies its identity this property is useful in mutual authentication discussed in the next session mutual authentication to defeat masquerading attacks processes involved in a communication session should validate each others identity at the start of the session recall from the previous section that the challengeresponse protocol implicitly verifies the identity of the process that responds to a chellenge so it can be employed for this purpose consider protocol which is used to select session keys in a public key system in step pi sends the session key to pj in a message that is encrypted by using the public key of pj in principle any process could fabricate such a message and trick process pj into thinking that it is engaging in a session with process pi so pj must authenticate pi before it engages in a session with it pj can achieve it as in the following protocol whose first three steps are identical with protocol pi kdc eukdc pi pj kdc pi eui pj uj pi pj euj pi sk ij pj pi eui pj n pi pj euj n pi pj eskij message in step pj sends a nonce n encrypted with the public key of pi the identity of pi is verified by its ability to decrypt this message extract the nonce and transform it in the expected manner note that in step pj must not encrypt its message by using the session key sk ij as the intruder would be able to decrypt such a message if he had fabricated the message in step authentication of data and messages authenticity of data requires that a process should be capable of verifying that data was originated or sent by a claimed person or process and that it has not been tampered with by an intruder the latter aspect implies integrity of data integrity of data is ensured as follows when data d is originated or is to be transmitted a special oneway hash function h is used to compute a hash value v this hash value also called a message digest has a fixed length irrespective of the size of data apart from the properties of oneway functions described earlier in section this special oneway hash function has the property that a birthday attack is infeasible ie given the hash value v of data d it is impractical to construct another data d whose hash value would also be v the data and the hash value are stored and transmitted as a pair d v to check the authenticity of d its hash value is computed afresh by using h and it is compared with v following from the special property of h mentioned above data d is considered to be in its original form if the two match otherwise d has been tampered with for this scheme to work the value v should itself be protected against tampering or substitution by an intruder otherwise an intruder could substitute a pair d v by another pair d v and mislead other processes into thinking that data d is genuine accordingly the person or process originating or transmitting d encrypts v or the pair d v using its own encryption key so that tampering or substitution of v can be detected note that it is less expensive to encrypt v rather than d v authenticity requires one more check verify whether v or d v was encrypted by the claimed person or process this check is made by using a certification authority which provides information concerning encryption keys used by persons or processes in a secure manner details of this check are described in the following certification authorities and digital certificates a certification authority ca assigns public and private keys to an entity whether a person or a process after ascertaining its identity by using some means of physical verification the keys are valid for a specific period of time the certification authority also acts like a key distribution center discussed in section it keeps a record of all keys assigned by it and when a process requests it for the public key of some person or process it issues a public key certificate which includes the following information serial number of the certificate owners distinguished name dn which consists of the dns name of the owner and the owners name unit locality state and country in a textual form identifying information of owner such as address owners public key part distributed operating systems date of issue and date of expiry and the issuers distinguished name digital signature on the above information by the certification authority a number of certification authorities could operate in parallel a server would obtain a certificate from one of these if a client knows which certification authority a server is registered with it can request the certification authority for the servers public key certificate alternatively if it knows the ip address of the server it can request the server to forward its own public key certificate the purpose of asking for the certificate of an entity is to obtain its public key for communicating with it however before the receiver of the certificate uses the key to communicate with the entity it has to ensure that the certificate is genuine and belongs to the entity with which it wishes to communicate ie it is not subject to a security attack called the maninthemiddle attack in this attack an intruder masquerades as a server when a client requests the server for the servers digital certificate the intruder intercepts the message and sends a forged certificate containing its own public key to the client now if it can intercept subsequent messages from the client to the server it can read those messages by using its own private key if it so desires it can initiate a conversation with the genuine server this time masquerading as the client and pass on the clients messages to the server after reading them neither the client nor the server would be able to discover that they are subject to a successful maninthemiddle attack the public key certificate contains many items of information that are used to prevent such attacks the certificate is digitally signed by the certification authority the client can use this digital signature to ensure that the certificate has not been tampered with or forged we discuss details of digital certificates in section for this it requires the public key of the certification authority that issued the certificate if it does not already know this key it can request a higherorder certification authority for a certificate of this certification authority once genuineness of the certificate has been established it can check whether the certificate is valid by checking whether the current date falls within the validity period of the certificate if it knows the ip address of the server it can check that against the ip address information mentioned in the certificate it begins exchanging messages with the server only if all these checks succeed message authentication codes and digital signatures a message authentication code mac is used to check the integrity of data a process that originates or transmits data d obtains macd the message authentication code of d as follows it generates a message digest v for d through a oneway hashing function it encrypts v by using an encryption key that is known only to itself and to the intended recipient of d the result is macd it now stores or transmits the pair d macd only the intended recipient of d can check and verify the integrity of d thirdparty authentication an open system uses standard wellspecified interfaces with other systems a process in any node with matching interfaces can request access to resources and services of an open system this fact gives rise to an obvious problem in authentication how does a server know whether a process wishing to act as its client was created by an authorized user one solution is to require each server to authenticate every user through a password this approach is inconvenient since each server would have to possess a systemwide authentication database and each user would be authenticated several times while using the system an alternative is to use a thirdparty authenticator and a secure arrangement by which the authenticator can introduce an authorized user to a server this way each server does not have to authenticate each user we discuss two protocols for thirdparty authentication in a distributed system the kerberos protocol employs an authentication database whereas the secure sockets layer ssl protocol performs authentication in a decentralized manner kerberos kerberos is a thirdparty authenticator developed in project athena at mit for use in an open system environment it enables a user to prove his identity to the part distributed operating systems message message text text oneway oneway hash hash message message digest digest encryption digital digital decryption n not signature signature authentic sent received y message message authentic private key public key message of sender of sender senderside actions receiverside actions step action description message digest of a oneway hash function is applied to the message text message text to produce a message digest which is a bit string of a standard length create digital signature the message digest and a timestamp are encrypted by using the private key of the sender the result of encryption is the digital signature append signature the digital signature is added at the end of the message text transmission the message consisting of the message text and the digital signature is transmitted to the destination message digest of the same oneway hash function as used in the sender is received text applied to the message text to produce a message digest decryption of digital the digital signature at the end of the message is signature extracted and decrypted by using the public key of the sender authenticity check the message digest produced in step and the result of decryption in step are compared the message is authentic if the two are identical figure message authenticity through digital signature servers in an open system without being subject to repeated authentication a user is authenticated at log in time using a password the authentication service issues tickets to an authenticated user each ticket is like a capability it grants a privilege to access one server the user presents a ticket to a server whenever it wishes to use its service the server provides the service if the ticket is valid private keys are assigned to users and servers a users key is used to encrypt messages from kerberos to the users processes while a servers key is used to encrypt the tickets for the server session keys are used to ensure message security they are chapter distributed system security generated by using a schematic similar to figure to limit exposure of a session key to intruders it is valid for only a limited amount of time timestamps are used to implement this aspect and to foil message replay attacks hence nodes in the system must contain loosely synchronized clocks a client is a process that operates on a users computer and requests remote services on behalf of the user when a client c wishes to use the services of a server sj it creates a new authenticator and presents a ticket for sj and the authenticator to sj the ticket is used to communicate the session key to the server in a secure manner while the authenticator is used to prevent message replay attacks the ticket contains the client and server ids ie c and sj the session key assigned to the communication session between c and sj a timestamp indicating when the ticket was created and the lifetime of the ticket ie its expiry time it is valid only during the time period starting at timestamp and ending at lifetime typically this period is about hours this arrangement limits exposure of the session key to intruder attacks the authenticator presented by c contains cs id and address and a timestamp encrypted by using the session key the server decrypts the ticket by using its own key it checks the timestamp and lifetime of the ticket to ensure that the ticket is valid it now extracts the session key and uses it to decrypt the authenticator it checks the timestamp in the authenticator to ensure that the request has originated in real time and within the validity period of the ticket it performs the service requested by the client only if all these checks succeed thus an intruder can not replay authenticators and tickets to obtain a service working of kerberos the kerberos system has two main components kerberos authentication server kas and ticket granting server tgs kas authenticates a user at log in time using an authentication database and provides him with a ticket to tgs tgs enables a client to obtain tickets to other servers in the system a user achieves use of servers through a threestage protocol figure illustrates various steps in the protocol n n and n are nonces initial authentication the user is authenticated at log in time as follows user c u password c kas u tgs n kas c evu n sk u tgs ttgs the user submits his id and password to the client in step in step the client forwards the user id to kas it also encloses a nonce n to authenticate kas this message is a request for a ticket to tgs note that the users password is not passed to kas this fact avoids its exposure over the network it also implies that authentication is not performed by kas it is actually performed by c in an interesting manner described later in step kas uses the user id u to retrieve vu the private key of u from the authentication database and uses it to encrypt its reply to c sk utgs is a session key for the session between the user and tgs and ttgs is a ticket for tgs encrypted with the key of tgs ttgs is also called a ticket granting ticket tgt part distributed operating systems authentication database kerberos kas tgs client server user figure kerberos c has to decrypt the reply from kas by using the key vu to obtain sk utgs and ttgs this step authenticates the user as follows vu the private key of the user satisfies the relation vu f password where f is a oneway function known to c c obtains vu by applying f to password it now decrypts the reply received from kas by using this key decryption would be unsuccessful if the password supplied by the user is invalid in this case c can not extract ttgs from the reply sent by kas so the user can not use any services or resources in the system obtaining a ticket for a server when a user wishes to use a server c obtains a ticket for the server using the following protocol c tgs serverid ttgs au n tgs c esk utgs n tserverid sk u serverid serverid where serverid is the name of the server that c wishes to use au is an authenticator sk userverid is a session key for the session between the client and the desired server and tserverid is the ticket for the desired server encrypted by using the key of the server before replying to the client tgs verifies that the ticket presented by the client is valid and that the request has originated in real time and within the validity period of the ticket obtaining a service when user u makes a service request c generates an authenticator and a nonce and exchanges the following messages with the server c server tserverid au eskuserverid service request n server c eskuserverid n chapter distributed system security the server performs the service if it finds that the ticket is valid and the request originated in real time and within the validity period of the ticket it returns the nonce n to the client so that the client can authenticate it if it so desires secure sockets layer ssl ssl is a message security protocol providing authentication and communication privacy it works on top of a reliable transport protocol such as the tcpip its successor the transport layer security tls protocol is based on ssl we discuss features that are common to both when a client wishes to communicate with a server the ssl handshake protocol is used before message exchange can start it uses rsa public key encryption to authenticate the server and optionally authenticate the client and generates symmetric session keys for message exchange between the client and the server actual message exchange is performed through the ssl record protocol which performs symmetric encryption of messages and transmits them over the network thus message communication between the client and the server is reliable because of the transport protocol secure because of authentication performed by the handshake protocol and private because of encryption performed by the record protocol authenticity of data is ensured through a digital signature on a message if mere integrity checking is desired it is provided through a message authentication code mac higherlevel application protocols such as http and ftp can be implemented on top of the ssl the ssl handshake protocol performs the following functions it performs authentication of the server it allows the client and the server to select the cryptographic algorithms to be used during the session from among rc rc des tripledes and a few other algorithms and digital signature and hash algorithms from among dsa md and sha it optionally performs authentication of the client it enables the client and the server to generate a shared secret which would be used to generate the session keys a simplified overview of the ssl handshake protocol is as follows the client sends a clienthello message to the server this message contains a specification of the cryptographic and compression options and a byte random number that we will call nclient the server responds with a serverhello message which contains another random number nserver immediately following the serverhello message the server sends its certificate ssl has a list of certificate authorities cas on the client side using which it ensures that the servers certificate is from one of the listed cas and verifies the servers authenticity by using public key cryptography the server if it so wishes asks for the clients certificate and verifies the clients identity in a similar manner following this the client sends part distributed operating systems the encrypted premaster secret message which contains a byte premaster secret string encrypted by the public key of the server both client and server now generate a byte master secret string from nclient nserver and premaster secret using a standard oneway function use of nclient and nserver which are randomly chosen values ensures that the master secret would be different for different sessions between the same clientserver pair the master secret string is used to obtain four symmetric session keys using a standard algorithm these keys are used as follows keys kccrypst and kscrypct are used for encryption and decryption of messages sent by the client to the server and by the server to the client respectively and keys kcmacs and ksmacc are used to generate message authentication codes for messages sent by the client and by the server respectively following key generation both client and server send finished messages to one another at this time the ssl handshake protocol is complete exchange of messages is performed by the ssl record protocol using the session keys generated during the handshake the steps in sending a message m from the client to the server are as follows the client generates macm which is a message authentication code for message m using the key kcmacs the pair m macm is encrypted by using the key kccrypst and the encrypted string is sent to the server the server decrypts the string by using the key kccrypst to obtain the pair m macm it accepts m if its mac computed using the key kcmacs matches mac m the ssl protocol could be subverted by a maninthemiddle attack where an intruder intercepts a clients messages to a server in the ssl handshake protocol and masquerades as the server in all subsequent message exchanges it may analogously masquerade as the client and set up a secured ssl connection with the server the client and server processes must take precautions to defeat the maninthemiddle attack during the initial handshake when the server provides its certificate to the client in the ssl handshake protocol the client must verify that the distinguished name and ip address mentioned in the servers certificate match those of the server with which it is attempting to set up the ssl connection a mismatch would indicate that it is subject to a maninthemiddle attack so it should abort the handshake protocol if this is the case the server does not know the ip address where a client resides so it has to use a different approach to authenticate the client if the server requires client authentication in the ssl handshake protocol the client is required to provide a certificate and also a piece of random data known to the server which it digitally signs using its private key to authenticate itself the server obtains the public key of the client from the client certificate and validates the clients digital signature a failure in this step would indicate that it is subject to a maninthemiddle attack so it aborts the handshake protocol this step is analogous to that in the challengeresponse protocol described earlier in section summary interprocess messages in a distributed system may distribution center kdc is used to provide public pass through links and nodes that are not under keys of processes or to generate session keys on control of the distributed os it provides an oppordemand by communicating processes tunity for an intruder to launch a variety of attacks an intruder can launch a message replay such as unauthorized reading of messages tamperattack to masquerade as another user in this ing with messages masquerading as a registered attack the intruder records messages to or from a user or interfering with use of resources or serprocess and plays them back at a later time to vices by users which is called denial of service in fool the os or the kdc a challengeresponse this chapter we discussed how a distributed os protocol is used to thwart such attacks by authendeals with these threats ticating the sender of a message this protocol is the threats of leakage or tampering are called included in the protocol for communicating with threats to message security they are countered the kdc processes can also use it for mutual through encryption in private key encryption mesauthentication however mutual authentication in sages sent to a process must be encrypted with this manner is cumbersome and expensive hence the key assigned to the process this arrangement thirdparty authenticators such as kerberos and is convenient for communication between system ssl are employed in practice entities and user processes because the system entiwhen processes exchange data it is important ties can readily find a users key however it is to know that the data is authentic that is it was not suitable for communication between user prooriginated or sent by the claimed process and it has cesses in public key encryption each process pi has not been tampered with by anyone a digital signaa pair of keys ui vi where ui is the public key ture is used to verify authenticity of data it consists which is made known to all processes in the sysof a hash code generated from the data which is tem and vi is a private key known only to pi these encrypted using the private key of the originator keys have the property that a message encrypted or sender of the data authenticity of the data is by using ui can be decrypted by using vi and vice verified as follows a public key certificate of the versa the rivestshamiradelman rsa algooriginator or sender of the data is obtained from rithm is used to generate the pairs of keys for a certification authority the digital signature of processes public key encryption has the disadthe data is decrypted by using the public key of the vantage that the keys are an order of magnitude originator or sender found in the certificate a suclarger than the keys used in private key encryption cessful decryption establishes that the originator so encryption is an expensive operation because or sender had indeed originated or sent the data of these drawbacks processes are assigned session the data is genuine if a hash code generated from it keys for use during a communication session a key matches the decrypted form of its digital signature test your concepts classify each of the following statements as true c in a distributed system using public key or false encryption a message being sent by process a message replay is an active security pi to process pj should be encrypted with the attack private key of process pi b encryption prevents eavesdropping but cand public key encryption incurs higher overhead not prevent tampering with messages than private key encryption part distributed operating systems e session keys are used to limit exposure of b in a public key system a key distribution encryption keys to intruder attacks center is used f a challengeresponse protocol can be used i to ensure confidentiality of the private to prevent a masquerading attack key of a process g a key distribution center is used to distribute ii to distribute information about private private keys of processes keys of processes select the appropriate alternative in each of the iii to ensure confidentiality of the public following questions key of a process a a message contains the id of its sender iv to distribute information about public process the id of the receiver process and keys of processes a ciphertext form of the message text an c a digital signature intruder can eavesdrop on the message i is a string that uniquely identifies the readily person who sent a message i if the message text is encrypted by using ii consists of the text of a message and the a session key name or id of its sender ii if the message text is encrypted by using iii consists of the encrypted form of a the public key of the receiver process message and the name or id of its iii if the message text is encrypted by using sender the private key of the sender process iv none of iiii iv none of iiii timesharing systems in an interactive computing environment a user submits a computational requirement a subrequest to a process and examines its response on the monitor screen a timesharing operating system is designed to provide a quick response to subrequests made by users it achieves this goal by sharing the cpu time among processes in such a way that each process to which a subrequest has been made would get a turn on the cpu without much delay the scheduling technique used by a timesharing kernel is called roundrobin scheduling with timeslicing it works as follows see figure the kernel maintains a scheduling queue of processes that wish to use the cpu it always schedules the process at the head of the queue when a scheduled process completes servicing of a subrequest or starts an io operation the kernel removes it from the queue and schedules another process such a process would be added at the end of the queue when it receives a new subrequest or when its io operation completes this arrangement ensures that all processes would suffer comparable part overview delays before getting to use the cpu however response times of processes would degrade if a process consumes too much cpu time in servicing its subrequest the kernel uses the notion of a time slice to avoid this situation we use the notation for the time slice definition time slice the largest amount of cpu time any timeshared process can consume when scheduled to execute on the cpu if the time slice elapses before the process completes servicing of a subrequest the kernel preempts the process moves it to the end of the scheduling queue and schedules another process the preempted process would be rescheduled when it reaches the head of the queue once again thus a process may have to be scheduled several times before it completes servicing of a subrequest the kernel employs a timer interrupt to implement timeslicing see section and table the appropriate measure of user service in a timesharing system is the time taken to service a subrequest ie the response time rt it can be estimated in the following manner let the number of users using the system at any time be n let the complete servicing of each user subrequest require exactly cpu seconds and let be the scheduling overhead ie the cpu time consumed by the kernel to perform scheduling if we assume that an io operation completes instantaneously and a user submits the next subrequest immediately after receiving a response to the previous subrequest the response time rt and the cpu efficiency are given by rt n the actual response time may be different from the value of rt predicted by eq for two reasons first all users may not have made subrequests to their processes hence rt would not be influenced by n the total number of users in the system it would be actually influenced by the number of active users second user subrequests do not require exactly cpu seconds to produce a response hence the relationship of rt and with is more complex than shown in eqs and example illustrates roundrobin scheduling with timeslicing and how it results in interleaved operation of processes example operation of processes in a timesharing system processes p and p follow a cyclic behavior pattern each cycle contains a burst of cpu activity to service a subrequest and a burst of io activity to report its result followed by a wait until the next subrequest is submitted to it the cpu bursts of processes p and p are and ms respectively while the io bursts are and ms respectively chapter overview of operating systems figure shows operation of the processes in a timesharing system using a time slice of ms the table in the top half of figure shows the scheduling list and scheduling decisions of the kernel assuming scheduling overhead to be negligible while the timing chart shows the cpu and io activities of the processes both processes have to be scheduled a few times before they can complete the cpu bursts of their execution cycle and start io process p uses the cpu from time to ms and p uses the cpu from to ms without completing the cpu bursts of their execution cycles p is scheduled once again at ms and starts an io operation at ms now p gets two consecutive time slices however these time slices are separated by the scheduling overhead because the os preempts process p at ms and schedules it again since no other process in the system needs the cpu ps io operation completes at ms p starts an io operation at ms which completes at ms thus the response times are ms and ms respectively swapping of programs throughput of subrequests is the appropriate measure of performance of a timesharing operating system the timesharing os of example completes two subrequests in ms hence its throughput is subrequests per second over the period to ms however the throughput would drop after ms if users do not make the next subrequests to these processes immediately the cpu is scheduling scheduled time list program remarks p p p p is preempted at ms p p p p is preempted at ms p p p p starts io at ms p p p is preempted at ms p p p starts io at ms cpu is idle cpu activity p p io activity p p time figure operation of processes p and p in a timesharing system realtime operating systems in a class of applications called realtime applications users need the computer to perform some actions in a timely manner to control the activities in an external system or to participate in them the timeliness of actions is determined by chapter overview of operating systems the time constraints of the external system accordingly we define a realtime application as follows definition realtime application a program that responds to activities in an external system within a maximum time determined by the external system if the application takes too long to respond to an activity a failure can occur in the external system we use the term response requirement of a system to indicate the largest value of response time for which the system can function perfectly a timely response is one whose response time is not larger than the response requirement of the system consider a system that logs data received from a satellite remote sensor the satellite sends digitized samples to the earth station at the rate of samples per second the application process is required to simply store these samples in a file since a new sample arrives every two thousandth of a second ie every ms the computer must respond to every store the sample request in less than ms or the arrival of a new sample would wipe out the previous sample in the computers memory this system is a realtime application because a sample must be stored in less than ms to prevent a failure its response requirement is ms the deadline of an action in a realtime application is the time by which the action should be performed in the current example if a new sample is received from the satellite at time t the deadline for storing it on disk is t ms examples of realtime applications can be found in missile guidance command and control applications like process control and air traffic control data sampling and data acquisition systems like display systems in automobiles multimedia systems and applications like reservation and banking systems that employ large databases the response requirements of these systems vary from a few microseconds or milliseconds for guidance and control systems to a few seconds for reservation and banking systems hard and soft realtime systems to take advantage of the features of realtime systems while achieving maximum costeffectiveness two kinds of realtime systems have evolved a hard realtime system is typically dedicated to processing realtime applications and provably meets the response requirement of an application under all conditions a soft realtime system makes the best effort to meet the response requirement of a realtime application but can not guarantee that it will be able to meet it under all conditions typically it meets the response requirements in some probabilistic manner say percent of the time guidance and control applications fail if they can not meet the response requirement hence they are serviced by hard realtime systems applications that aim at providing good quality of service eg multimedia applications and applications like reservation and banking do not have a notion of failure so they may be serviced by soft realtime systems the picture quality provided by a videoondemand system may deteriorate occasionally but one can still watch the video part overview features of a realtime operating system a realtime os provides the features summarized in table the first three features help an application in meeting the response requirement of a system as follows a realtime application can be coded such that the os can execute its parts concurrently ie as separate processes when these parts are assigned priorities and prioritybased scheduling is used we have a situation analogous to multiprogramming within the application if one part of the application initiates an io operation the os would schedule another part of the application thus cpu and io activities of the application can be overlapped with one another which helps in reducing the duration of an application ie its running time deadlineaware scheduling is a technique used in the kernel that schedules processes in such a manner that they may meet their deadlines ability to specify domainspecific events and event handling actions enables a realtime application to respond to special conditions in the external system promptly predictability of policies and overhead of the os enables an application developer to calculate the worstcase running time of the application and decide whether the response requirement of the external system can be met the predictability requirement forces a hard realtime os to shun features such as virtual memory whose performance can not be predicted precisely see chapter the os would also avoid shared use of resources by processes because it can lead to delays that are hard to predict and unbounded ie arbitrarily large a realtime os employs two techniques to ensure continuity of operation when faults occur fault tolerance and graceful degradation a faulttolerant computer system uses redundancy of resources to ensure that the system will keep functioning even if a fault occurs eg it may have two disks even though the application actually needs only one disk graceful degradation is the ability of a system to fall back to a reduced level of service when a fault occurs and to revert to normal operations when the fault is rectified the programmer can table essential features of a realtime operating system feature explanation concurrency a programmer can indicate that some parts of an application within an should be executed concurrently with one another the os application considers execution of each such part as a process process priorities a programmer can assign priorities to processes scheduling the os uses prioritybased or deadlineaware scheduling domainspecific a programmer can define special situations within the external events interrupts system as events associate interrupts with them and specify event handling actions for them predictability policies and overhead of the os should be predictable reliability the os ensures that an application can continue to function even when faults occur in the computer distributed operating systems a distributed computer system consists of several individual computer systems connected through a network each computer system could be a pc a multiprocessor system see chapter or a cluster which is itself a group of computers that work together in an integrated manner see section thus many resources of a kind eg many memories cpus and io devices exist in the distributed system a distributed operating system exploits the multiplicity of resources and the presence of a network to provide the benefits summarized in table however the possibility of network faults or faults in individual computer systems complicates functioning of the operating system and necessitates use of special techniques in its design users also need to use special techniques to access resources over the network we discuss these aspects in section resource sharing has been the traditional motivation for distributed operating systems a user of a pc or workstation can use resources such as printers over a local area network lan and access specialized hardware or software resources of a geographically distant computer system over a wide area network wan a distributed operating system provides reliability through redundancy of computer systems resources and communication paths if a computer system or a resource used in an application fails the os can switch the application to another computer system or resource and if a path to a resource fails it can utilize another path to the resource reliability can be used to offer high availability of resources and services which is defined as the fraction of time a resource or service is operable high availability of a data resource eg a file can be provided by keeping copies of the file in various parts of the system computation speedup implies a reduction in the duration of an application ie in its running time it is achieved by dispersing processes of an application table benefits of distributed operating systems benefit description resource sharing resources can be utilized across boundaries of individual computer systems reliability the os continues to function even when computer systems or resources in it fail computation speedup processes of an application can be executed in different computer systems to speed up its completion communication users can communicate among themselves irrespective of their locations in the system part overview to different computers in the distributed system so that they can execute at the same time and finish earlier than if they were to be executed in a conventional os users of a distributed operating system have user ids and passwords that are valid throughout the system this feature greatly facilitates communication between users in two ways first communication through user ids automatically invokes the security mechanisms of the os and thus ensures authenticity of communication second users can be mobile within the distributed system and still be able to communicate with other users through the system special techniques of distributed operating systems a distributed system is more than a mere collection of computers connected to a network functioning of individual computers must be integrated to achieve the benefits summarized in table it is achieved through participation of all computers in the control functions of the operating system accordingly we define a distributed system as follows definition distributed system a system consisting of two or more nodes where each node is a computer system with its own clock and memory some networking hardware and a capability of performing some of the control functions of an os table summarizes three key concepts and techniques used in a distributed os distributed control is the opposite of centralized control it implies that the control functions of the distributed system are performed by several computers in the system in the manner of definition instead of being performed by a single computer distributed control is essential for ensuring that failure of a single computer or a group of computers does not halt operation of the entire system transparency of a resource or service implies that a user should be able to access it without having to know which node in the distributed system contains it this feature enables the os to change the position of a software resource or service to optimize its use by applications for example in a system providing table key concepts and techniques used in a distributed os concepttechnique description distributed control a control function is performed through participation of several nodes possibly all nodes in a distributed system transparency a resource or service can be accessed without having to know its location in the distributed system remote procedure a process calls a procedure that is located in a different call rpc computer system the rpc is analogous to a procedure or function call in a programming language except that the os passes parameters to the remote procedure over the network and returns its results over the network modern operating systems users engage in diverse activities in a modern computing environment hence a modern operating system can not use a uniform strategy for all processes it must use a strategy that is appropriate for each individual process for example as mentioned in section a user may open a mail handler edit a few files execute some programs including some programs in the background mode and watch a video at the same time here operation of some of the programs may be interactive or may involve activities in other nodes of a distributed computer system whereas rendering of a video is a soft realtime activity hence the os must use roundrobin scheduling for program executions use prioritybased scheduling for processes of the video application and implement remote procedure calls rpc to support activities in another node thus a modern os uses most concepts and techniques that we discussed in connection with the batch processing multiprogramming timesharing realtime and distributed operating systems table shows typical examples of how the earlier concepts are drawn upon to handle diverse activities effectively the os employs strategies that adapt to the situations encountered during their operation some examples of such strategies are the kernel employs prioritybased scheduling however instead of assigning fixed priorities to all processes as in a multiprogramming system it assigns fixed high priorities only to processes with realtime constraints and changes current priorities of other processes to suit their recent behavior increases the priority of a process if it has been engaged in an interaction or an io operation recently and reduces its priority if it has not been a modern os typically uses the feature called virtual memory whereby only some of the parts of a process are held in memory at any time and other parts are loaded when needed the kernel considers the recent behavior of a process to decide how much memory it should allocate to the process it allocates less memory if the process had used only a few of its parts recently and allocates more memory if the process had used several of its parts the kernel provides a plugandplay capability whereby io devices could be connected to the computer at any time during its operation and the kernel would select appropriate methods of handling them we will see several instances of adaptive strategies in the following chapters summary a computing environment consists of a computer execute instructions while an io operation was system its interfaces with other systems and in progress operating systems exploited this feathe services provided by its operating system to ture to service several programs simultaneously by its users and their programs computing envioverlapping an io operation within one program ronments evolved with advances in computer with execution of instructions in another program technology and computer applications each envia multiprogramming operating system assigned ronment desired a different combination of effihigh priorities to iobound programs and percient use and user service so it was serviced by a formed prioritybased scheduling to achieve good separate class of operating systems that employed system performance its own concepts and techniques in this chapter user convenience became important when the we discussed the concepts and techniques used in cost of computing hardware declined accordthe fundamental classes of operating systems ingly the timesharing operating systems focused the batch processing operating systems on providing fast response to user programs it focused on automating processing of a collection of was achieved through roundrobin scheduling with programs which reduced cpu idle times between timeslicing which serviced all programs by turn programs development of the direct memory and limited the amount of cpu time a program access dma technology enabled the cpu to could use when it was its turn to use the cpu chapter overview of operating systems a realtime computer application has to performs its control functions in several of these satisfy time constraints specified by an external computers it achieves efficient use of resources of system hard realtime systems such as mission all computers by letting programs share them over control systems require their time constraints to the network speeds up execution of a program by be satisfied in a guaranteed manner whereas running its parts in different computers at the same soft realtime systems such as multimedia systems time and provides reliability through redundancy can tolerate occasional failure to meet their time of resources and services constraints realtime operating systems support a modern operating system controls a diverse concurrency within an application program and computing environment that has elements of all employ techniques such as prioritybased schedulthe classic computing environments so it has to ing and deadlineaware scheduling to help meet the use different techniques for different applications time constraints it employs an adaptive strategy that selects the a distributed operating system controls a most appropriate techniques for each application group of computer systems that are networked it according to its nature test your concepts programs a b c and d have similar b throughput increases almost linearly with structure each of them consists of a single loop the degree of multiprogramming that contains n statements that perform some c cpu efficiency changes only marginally with processing on each element of a single dimenthe degree of multiprogramming sioned array z other features of these programs d cpu efficiency increases linearly with the are as follows degree of multiprogramming classify each of the following statements as true program a n and z is a huge array or false program b n and z is a huge array a because of presence of the cache memory program c n and z is a small array a program requires more cpu time to exeprogram d n and z is a small cute in a multiprogramming or timesharing array system than it would require if it were to be executed in a batch processing system these programs are executed in a batch processb to achieve high throughput a multiproing system list these programs in the descendgramming os assigns a higher priority to ing order by cache hit ratio cpubound programs a multiprogramming system is used to execute c if a multiprogramming kernel finds that the a collection of programs c the system has cpu efficiency is low it should remove an enough memory to accommodate a large numiobound program from memory ber of programs the programs in c are executed d if the time slice in a timesharing system is several times each time with a different degree of too large processes will complete their opermultiprogramming and throughput of the sysation in the same order in which they were tem and cpu efficiency are plotted against the initiated degree of multiprogramming in each of the fole two persons using the same timesharing syslowing cases what inference can you draw about tem at the same time might receive widely the nature of programs in c different response times a throughput changes only marginally with f it is incorrect to use masking of interrupts in the degree of multiprogramming a realtime operating system part overview exercises a system is described as overloaded if more work operation that lasts for ms the program is is directed at it than its capacity to perform executed in a multiprogramming os with negliwork it is considered underloaded if some of gible overhead prepare a timing chart showing its capacity is going to waste the following polthe cpu and io activities of the program and icy is proposed to improve the throughput of a compute its elapsed time in the following cases batch processing system classify jobs into small a the program has the highest priority in the jobs and long jobs depending on their cpu time system requirements form separate batches of short b the program is multiprogrammed with n and long jobs execute a batch of long jobs only other programs with identical characteristics if no batches of short jobs exist does this policy and has the lowest priority consider cases improve the throughput of a batch processing i n ii n and iii n system that is a underloaded b overloaded a multiprogramming operating system has a the kernel of a multiprogramming system classinegligible overhead it services programs that fies a program as cpubound or iobound and are identical in size each program contains a assigns an appropriate priority to it what would loop that has n iterations where each iterabe the consequence of a wrong classification of tion contains computations that consume tc ms programs for throughput and turnaround times of cpu time followed by io operations that in a multiprogramming system what would be require tio ms the programs are of two classes the effect of a wrong classification on the plot of values of n tc and tio for these two classes are throughput versus degree of multiprogramming class n tc tio of figure a the cpu of a multiprogramming system is exeb cuting a highpriority program when an interrupt signaling completion of an io operation the system has sufficient memory to accommooccurs show all actions and activities in the os date only two programs ten programs arrive in following the interrupt if the system at time five each of classes a and b a the io operation was started by a lowerdraw a timing chart showing operation of propriority program grams in the system until two programs complete b the io operation was started by a highertheir operation find their turnaround times priority program a program is said to make progress if either illustrate each case with the help of a timing the cpu is executing its instructions or its io chart operation is in progress the progress coefficient a multiprogramming os has programs progiob of a program is the fraction of its lifetime in the and progcb in memory with progcb having a system during which it makes progress comhigher priority draw a timing chart for the syspute progress coefficients of the programs in tem analogous to figure and show that exercise b the throughput is less than for the system of comment on the validity of the following statefigure ment a cpubound program always has a very draw a timing chart for a system containing two low progress coefficient in a multiprogramming cpubound programs and two iobound prosystem grams when a cpubound programs have a a multiprogramming system uses a degree of higher priority b iobound programs have a multiprogramming m it is proposed to higher priority double the throughput of the system by augmen a program consists of a single loop that executes tationreplacement of its hardware components times the loop contains a computation that would any of the following three proposals consumes ms of cpu time followed by an io achieve the desired result chapter overview of operating systems a replace the cpu by a cpu with twice the a computer is operated under a timesharing speed os it is proposed to add a second cpu to b expand the memory to twice its present size the computer to improve its throughput under c replace the cpu by a cpu with twice the what conditions would addition of the secspeed and expand the memory to twice its ond cpu improve throughput only if mempresent size ory is increased under what conditions would programs being serviced in a multiprogramit improve throughput even if memory is not ming system are named p pm where m increased is the degree of multiprogramming such that a timesharing system uses swapping as the funpriority of program pi priority of program damental memory management technique it pi all programs are cyclic in nature with uses the following lists to govern its actions each cycle containing a burst of cpu activity a scheduling list a swappedout list containand a burst of io activity let bicpu and biio be ing processes that are swapped out a beingthe cpu and io bursts of program pi comswappedout list containing processes to be ment on the validity of each of the following swapped out and a beingswappedin list constatements taining processes to be swapped in explain a cpu idling occurs if biho jhbcjpu where when and why the timesharing kernel should ph is the highestpriority program put processes in the beingswappedout and b program pm is guaranteed to receive beingswappedin lists cpu time if biio bicpu biio and biio a timesharing system uses a time slice of ms jimbcj pu for all values of i each process has a cyclic behavior pattern in m each cycle it requires an average of ms of a program is said to starve if it does not receive cpu time to compute the result of a subrequest any cpu time which of the following condiand an average of ms to print it on the users tions implies starvation of the lowestpriority screen a process receives a new subrequest secprogram in a multiprogramming system the ond after it has finished printing results of the notation is the same as in exercise previous subrequest the operating system can a for some program pi biio j i m bcjpu accommodate processes in memory at any time however it has enough io devices for b for some program pi biio j i m bcjpu processes the swapin and swapout times of and bci pu bijo for all j i each process are ts ms each calculate the aver a timesharing system contains n identical proage throughput of the system over a second cesses each executing a loop that contains a period in each of the following cases computation requiring tp cpu seconds and an a the operating system contains processes io operation requiring tio seconds draw a b the operating system contains processes graph depicting variation of response time with and ts is ms values of the time slice hint consider cases c the operating system contains processes for tp tp and tp and ts is ms comment on the validity of the following state a realtime application requires a response time ment operation of a timesharing system is of seconds discuss the feasibility of using a identical with operation of a multiprogramming timesharing system for the realtime application system executing the same programs if exceeds if the average response time in the timesharing the cpu burst of every program system is a seconds b seconds or c answer the following with full justifications seconds a does swapping improve or degrade the effi a timesharing system services n processes it ciency of system utilization uses a time slice of cpu seconds and requires b can swapping be used in a multiprogramts cpu seconds to switch between processes a ming system realtime application requires tc seconds of cpu part overview time followed by an io operation that lasts a is this a realtime application justify your for tio seconds and has to produce a response answer within td seconds what is the largest value of b would creation of multiple processes reduce for which the timesharing system can satthe response time of the application if so isfy the response requirements of the real time what should be the processes in it what application should be their priorities an application program is being developed for c is it necessary to define any domainspecific a microprocessorbased controller for an autoevents and interrupts if so specify their mobile the application is required to perform priorities the following functions if two independent events e and e have the i monitor and display the speed of the autoprobabilities of occurrence pr and pr where mobile both pr and pr the probability that ii monitor the fuel level and raise an alarm if both events occur at the same time is pr pr necessary a distributed system contains two disks the iii display the fuel efficiency ie milesgallon probability that both disks fail is required to at current speed be what should be the probability of iv monitor the engine condition and raise an failure of a disk alarm if an unusual condition arises to obtain computation speedup in a distributed v periodically record some auxiliary informasystem an application is coded as three parts to tion like speed and fuel level ie implement be executed on three computer systems under a black box as in an airliner control of a distributed operating system howanswer the following questions concerning the ever the speedup obtained is list all possible application reasons for the poor speedup bibliography literature on batch processing multiprogramming and unix operating system beck et al bovet and timesharing systems dates back to the s zhao cesati and love discuss the linux oper and liu are good sources for realtime ating system while stevens and rago describes systems most operating systems texts cover the classes unix linux and bsd operating systems mauro of operating systems described in this chapter some and mcdougall discusses solaris russinovich recent os texts are tanenbaum bic and shaw and solomon describes the windows operating nutt silberschatz et al and systems stallings several comprehensive bibliographies on operating systems are available on the internet bach m j the design of the unix tanenbaum and renesse is a good startoperating system prentice hall englewood ing point for a study of distributed operating syscliffs nj tems it discusses the major design issues in distributed beck m h bohme m dziadzka u kunitz operating systems and contains a survey of some disr magnus c schroter and d verworner tributed operating systems tanenbaum discusses linux kernel programming rd ed some wellknown distributed operating systems in detail pearson education new york coulouris et al discusses the concepts and design bic l and a c shaw operating systems of distributed systems principles prentice hall englewood cliffs nj several books describe specific modern operating bovet d p and m cesati understanding systems bach and vahalia describe the the linux kernel rd ed oreilly sebastopol chapter overview of operating systems coulouris g j dollimore and t kindberg sinha p k distributed operating distributed systems concepts and systems ieee press new york design rd ed addisonwesley new york smith a j multiprogramming and crowley c operating systems a design memory contention software practice and oriented approach mcgrawhill new york experience denning p j third generation stallings w operating systems operating systems computing surveys internals and design principles th ed pearson education new york fortier p j design of distributed stevens w r and s a rago advanced operating systems mcgrawhill new york programming in the unix environment nd ed goscinski a distributed operating addisonwesley professional systems the logical design addisonwesley tanenbaum a s computer networks new york th ed prentice hall englewood cliffs nj liu j w s realtime systems pearson tanenbaum a s modern operating education new york systems nd ed prentice hall englewood love r linux kernel development cliffs nj nd ed novell press tanenbaum a s and r van renesse mauro j and r mcdougall solaris distributed operating systems computing internals nd ed prentice hall surveys nutt g operating systems a modern tanenbaum a s distributed operating perspective rd ed addisonwesley reading systems prentice hall englewood cliffs nj mass vahalia u unix internals the new russinovich m e and d a solomon frontiers prentice hall englewood microsoft windows internals th ed microsoft cliffs nj press redmond wash wirth n on multiprogramming silberschatz a p b galvin and g gagne machine coding and computer organization operating system principles th ed communications of the acm john wiley new york singhal m and n g shivaratri zhao w special issue on realtime advanced concepts in operating systems operating systems operating system review mcgrawhill new york