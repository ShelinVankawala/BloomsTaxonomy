introduction two idea lie gleaming on the jeweler the first is the calculus the second the the calculus and the rich body of mathematical analysis to which it gave rise made modern science possible but it ha been the algorithm that ha made possible the modern david berlinski the advent of the algorithm why do you need to study if you are going to be a computer professional there are both practical and theoretical reason to study algo from a practical standpoint you have to know a standard set of important algorithm from different area of computing in addition you should be able to design new algorithm and analyze their from the theoretical stand point the study of algorithm sometimes called algorithmics ha come to be recognized a the cornerstone of computer david harel in his delightful book pointedly titled algorithmics the spirit of computing put it a follows algorithmics is more than a branch of computer it is the core of computer science and in all fairness can be said to be relevant to most of science business and har but even if you are not a student in a computing related program there are compelling reason to study to put it bluntly computer program would not exist without and with computer application becoming indispensable in almost all aspect of our professional and personal life studying algorithm becomes a necessity for more and more another reason for studying algorithm is their usefulness in developing an alytical after all algorithm can be seen a special kind of solution to problem not just answer but precisely defined procedure for getting consequently specific algorithm design technique can be interpreted a problem solving strategy that can be useful regardless of whether a computer is of course the precision inherently imposed by algorithmic thinking limit the kind of problem that can be solved with an you will not find for example an algorithm for living a happy life or becoming rich and on the other hand this required precision ha an important educational donald knuth one of the most prominent computer scientist in the history of algorithmics put it a follows a person well trained in computer science know how to deal with algorithm how to construct them manipulate them understand them analyze this knowledge is preparation for much more than writing good computer program it is a general purpose mental tool that will be a definite aid to the understanding of other subject whether they be chemistry linguistics or music the reason for this may be understood in the following way it ha often been said that a person doe not really understand something until after teaching it to someone actually a person doe not really understand something until after teaching it to a computer expressing it a an algorithm an attempt to formalize thing a algorithm lead to a much deeper understanding than if we simply try to comprehend thing in the traditional knu we take up the notion of algorithm in section a example we use three algorithm for the same problem computing the greatest common there are several reason for this first it deal with a problem familiar to ev erybody from their middle school second it make the important point that the same problem can often be solved by several quite typically these algorithm differ in their idea level of sophistication and third one of these algorithm deserves to be introduced first both because of it age it ap peared in euclid famous treatise more than two thousand year ago and it enduring power and finally investigation of these three algorithm lead to some general observation about several important property of algo rithms in section deal with algorithmic problem there we discus several important issue related to the design and analysis of the different aspect of algorithmic problem solving range from analysis of the problem and the mean of expressing an algorithm to establishing it correctness and analyzing it the section doe not contain a magic recipe for designing an algorithm for an arbitrary it is a well established fact that such a recipe doe not still the material of section should be useful for organizing your work on designing and analyzing section is devoted to a few problem type that have proven to be partic ularly important to the study of algorithm and their in fact there are textbook sed organized around such problem i hold the view shared by many others that an organization based on algorithm design technique is in any case it is very important to be aware of the princi pal problem not only are they the most commonly encountered problem type in real life application they are used throughout the book to demonstrate particular algorithm design section contains a review of fundamental data it is meant to serve a a reference rather than a deliberate discussion of this if you need a more detailed exposition there is a wealth of good book on the subject most of them tailored to a particular programming sequential search and brute force string matching we saw in the previous section two application of the brute force approach to the sorting here we discus two application of this strategy to the problem of the first deal with the canonical problem of searching for an item of a given value in a given the second is different in that it deal with the string matching sequential search we have already encountered a brute force algorithm for the general searching problem it is called sequential search see section to repeat the algorithm simply compare successive element of a given list with a given search key until either a match is encountered successful search or the list is exhausted without finding a match unsuccessful a simple extra trick is often employed in implementing sequential search if we append the search key to the end of the list the search for the key will have to be successful and therefore we can eliminate the end of list check here is pseudocode of this enhanced algorithm k implement sequential search with a search key a a sentinel input an array a of n element and a search key k output the index of the first element in whose value is equal to k or if no such element is found an k i while ai k do ii if i n return i else return another straightforward improvement can be incorporated in sequential search if a given list is known to be sorted searching in such a list can be stopped a soon a an element greater than or equal to the search key is sequential search provides an excellent illustration of the brute force ap proach with it characteristic strength simplicity and weakness inferior effi the efficiency result obtained in section for the standard version of sequential search change for the enhanced version only very slightly so that the algorithm remains linear in both the worst and average we discus later in the book several searching algorithm with a better time brute force string matching recall the string matching problem introduced in section given a string of n character called the text and a string of m character m n called the pattern find a substring of the text that match the to put it more precisely we want to find i the index of the leftmost character of the first matching substring in the text such that ti p tij pj tim pm t ti tij tim tn text t p pj pm pattern p if match other than the first one need to be found a string matching algorithm can simply continue working until the entire text is a brute force algorithm for the string matching problem is quite obvious align the pattern against the first m character of the text and start matching the corresponding pair of character from left to right until either all the m pair of the character match then the algorithm can stop or a mismatching pair is in the latter case shift the pattern one position to the right and resume the character comparison starting again with the first character of the pattern and it counterpart in the note that the last position in the text that can still be a beginning of a matching substring is n m provided the text position are indexed from to n beyond that position there are not enough character to match the entire pattern hence the algorithm need not make any comparison algorithm bruteforcestringmatcht p implement brute force string matching input an array t of n character representing a text and an array p of m character representing a pattern output the index of the first character in the text that start a matching substring or if the search is unsuccessful for i to n m do j while j m and p j t i j do j j if j m return i return an operation of the algorithm is illustrated in figure note that for this example the algorithm shift the pattern almost always after a single character the worst case is much worse the algorithm may have to make all m comparison before shifting the pattern and this can happen for each of the n m problem in this section exercise asks you to give a specific example of such a thus in the worst case the algorithm make n o b o d y n o t i c e d h i m n o t n o t n o t n o t n o t n o t n o t n o t figure example of brute force string the pattern character that are compared with their text counterpart are in bold mn m character comparison which put it in the onm for a typical word search in a natural language text however we should expect that most shift would happen after very few comparison check the example therefore the average case efficiency should be considerably better than the worst case indeed it is for searching in random text it ha been shown to be linear there are several more sophisticated and more efficient algorithm for string the most widely known of them by boyer and moore is outlined in section along with it simplification suggested by exercise find the number of comparison made by the sentinel version of sequential search in the worst in the average case if the probability of a successful search is p p a shown in section the average number of key comparison made by sequential search without a sentinel under standard assumption about it input is given by the formula cavgn pn n p where p is the probability of a successful determine for a fixed n the value of p p for which this formula yield the maximum value of cavgn and the minimum value of gadget testing a firm want to determine the highest floor of it n story headquarters from which a gadget can fall without the firm ha two identical gadget to experiment if one of them get broken it cannot be repaired and the experiment will have to be completed with the remaining design an algorithm in the best efficiency class you can to solve this determine the number of character comparison made by the brute force algorithm in searching for the pattern gandhi in the text thereismoretolifethanincreasingitsspeed assume that the length of the text it is character long is known before the search how many comparison both successful and unsuccessful will be made by the brute force algorithm in searching for each of the following pattern in the binary text of one thousand give an example of a text of length n and a pattern of length m that constitutes a worst case input for the brute force string matching exactly how many character comparison will be made for such in solving the string matching problem would there be any advantage in comparing pattern and text character right to left instead of left to consider the problem of counting in a given text the number of substring that start with an a and end with a for example there are four such substring in design a brute force algorithm for this problem and determine it effi ciency design a more efficient algorithm for this gin write a visualization program for the brute force string matching word find a popular diversion in the united state word find or word search puzzle ask the player to find each of a given set of word in a square table filled with single a word can read horizontally left or right vertically up or down or along a degree diagonal in any of the four direction formed by consecutively adjacent cell of the table it may wrap around the table boundary but it must read in the same direction with no the same cell of the table may be used in different word but in a given word the same cell may be used no more than write a computer program for solving this battleship game write a program based on a version of brute force pattern matching for playing the game battleship on the the rule of the game are a there are two opponent in the game in this case a human player and the the game is played on two identical board table of square on which each opponent place his or her ship not seen by the each player ha five ship each of which occupies a certain number of square on the board a destroyer two square a submarine three square a cruiser three square a battleship four square and an aircraft carrier five each ship is placed either horizontally or vertically with no two ship touching each the game is played by the opponent taking turn shooting at each others the result of every shot is displayed a either a hit or a in case of a hit the player get to go again and keep playing until the goal is to sink all the opponent ship before the opponent succeeds in doing it to sink a ship all square occupied by the ship must be closest pair and convex hull problem by brute force in this section we consider a straightforward approach to two well known prob lem dealing with a finite set of point in the these problem aside from their theoretical interest arise in two important applied area computational ge ometry and operation closest pair problem the closest pair problem call for finding the two closest point in a set of n it is the simplest of a variety of problem in computational geometry that deal with proximity of point in the plane or higher dimensional point in question can represent such physical object a airplane or post office a well a database record statistical sample dna sequence and so an air traffic controller might be interested in two closest plane a the most probable collision a regional postal service manager might need a solution to the closest pair problem to find candidate post office location to be one of the important application of the closest pair problem is cluster analy si in based on n data point hierarchical cluster analysis seek to orga nize them in a hierarchy of cluster based on some similarity for numerical data this metric is usually the euclidean distance for text and other nonnumerical data metric such a the hamming distance see problem in this section ex ercises are a bottom up algorithm begin with each element a a separate cluster and merges them into successively larger cluster by combining the closest pair of for simplicity we consider the two dimensional case of the closest pair prob we assume that the point in question are specified in a standard fashion by their x y cartesian coordinate and that the distance between two point pixi yi and pj xj yj is the standard euclidean distance dpi pj xi xj yi yj the brute force approach to solving this problem lead to the following ob vious algorithm compute the distance between each pair of distinct point and find a pair with the smallest of course we do not want to compute the distance between the same pair of point to avoid doing so we consider only the pair of point pi pj for which i j pseudocode below computes the distance between the two closest point getting the closest point themselves requires just a trivial algorithm bruteforceclosestpairp find distance between two closest point in the plane by brute force input a list p of n n point px y pnxn yn output the distance between the closest pair of point d for i to n do for j i to n do d mind sqrtxi xj yi yj sqrt is square root return d the basic operation of the algorithm is computing the square in the age of electronic calculator with a square root button one might be led to believe that computing the square root is a simple an operation a say addition or of course it is for starter even for most integer square root are irrational number that therefore can be found only moreover computing such approximation is not a trivial but in fact computing square root in the loop can be can you think the trick is to realize that we can simply ignore the square root function and compare the value xi xj yi yj we can do this because the smaller a number of which we take the square root the smaller it square root or a mathematician say the square root function is strictly then the basic operation of the algorithm will be squaring a the number of time it will be executed can be computed a follows n n n cn n i i j i i n n n n of course speeding up the innermost loop of the algorithm could only decrease the algorithm running time by a constant factor see problem in this section exercise but it cannot improve it asymptotic efficiency in chapter we discus a linearithmic algorithm for this problem which is based on a more sophisticated design convex hull problem on to the other problem that of computing the convex finding the convex hull for a given set of point in the plane or a higher dimensional space is one of the most important some people believe the most important problem in computational this prominence is due to a variety of application in which this problem need to be solved either by itself or a a part of a larger sev eral such application are based on the fact that convex hull provide convenient approximation of object shape and data set for example in computer an imation replacing object by their convex hull speed up collision detection the same idea is used in path planning for mar mission convex hull are used in computing accessibility map produced from satellite image by geographic information they are also used for detecting outlier by some statisti cal an efficient algorithm for computing a diameter of a set of point which is the largest distance between two of the point need the set convex hull to find the largest distance between two of it extreme point see finally convex hull are important for solving many optimization problem because their extreme point provide a limited set of solution we start with a definition of a convex definition a set of point finite or infinite in the plane is called convex if for any two point p and q in the set the entire line segment with the endpoint at p and q belongs to the all the set depicted in figure are convex and so are a straight line a triangle a rectangle and more generally any convex polygon a circle and the entire on the other hand the set depicted in figure any finite set of two or more distinct point the boundary of any convex polygon and a circumference are example of set that are not now we are ready for the notion of the convex intuitively the convex hull of a set of n point in the plane is the smallest convex polygon that contains all of them either inside or on it if this formulation doe not fire up your enthusiasm consider the problem a one of barricading n sleeping tiger by a fence of the shortest this interpretation is due to harel har it is somewhat lively however because the fenceposts have to be erected right at the spot where some of the tiger there is another much tamer interpretation of this imagine that the point in question are represented by nail driven into a large sheet of plywood representing the take a rubber band and stretch it to include all the nail then let it snap into the convex hull is the area bounded by the snapped rubber band figure a formal definition of the convex hull that is applicable to arbitrary set including set of point that happen to lie on the same line definition the convex hull of a set s of point is the smallest convex set containing the smallest requirement mean that the convex hull of s must be a subset of any convex set containing if s is convex it convex hull is obviously s if s is a set of two point it convex hull is the line segment connecting these if s is a set of three by a triangle a rectangle and more generally any convex polygon we mean here a region the set of point both inside and on the boundary of the shape in a b figure a convex b set that are not figure rubber band interpretation of the convex point not on the same line it convex hull is the triangle with the vertex at the three point given if the three point do lie on the same line the convex hull is the line segment with it endpoint at the two point that are farthest for an example of the convex hull for a larger set see figure a study of the example make the following theorem an expected theorem the convex hull of any set s of n point not all on the same line is a convex polygon with the vertex at some of the point of if all the point do lie on the same line the polygon degenerate to a line segment but still with the endpoint at two point of p p p p p p p p figure the convex hull for this set of eight point is the convex polygon with vertex at p p p p and the convex hull problem is the problem of constructing the convex hull for a given set s of n to solve it we need to find the point that will serve a the vertex of the polygon in mathematician call the vertex of such a polygon extreme by definition an extreme point of a convex set is a point of this set that is not a middle point of any line segment with endpoint in the for example the extreme point of a triangle are it three vertex the extreme point of a circle are all the point of it circumference and the extreme point of the convex hull of the set of eight point in figure are p p p p and extreme point have several special property other point of a convex set do not one of them is exploited by the simplex method a very important algorithm discussed in section this algorithm solves linear programming problem which are problem of finding a minimum or a maximum of a linear function of n variable subject to linear constraint see problem in this section exercise for an example and section and for a general here however we are interested in extreme point because their identification solves the convex hull actually to solve this problem completely we need to know a bit more than just which of n point of a given set are extreme point of the set convex hull we need to know which pair of point need to be connected to form the boundary of the convex note that this issue can also be addressed by listing the extreme point in a clockwise or a counterclockwise so how can we solve the convex hull problem in a brute force if you do not see an immediate plan for a frontal attack do not be dismayed the convex hull problem is one with no obvious algorithmic nevertheless there is a simple but inefficient algorithm that is based on the following observation about line segment making up the boundary of a convex hull a line segment connecting two point pi and pj of a set of n point is a part of the convex hull boundary if and only if all the other point of the set lie on the same side of the straight line through these two verify this property for the set in figure repeating this test for every pair of point yield a list of line segment that make up the convex hull a few elementary fact from analytical geometry are needed to implement this first the straight line through two point x y x y in the coordinate plane can be defined by the equation ax by c where a y y b x x c xy second such a line divide the plane into two half plane for all the point in one of them ax by c while for all the point in the other ax by for the point on the line itself of course ax by thus to check whether certain point lie on the same side of the line we can simply check whether the expression ax by c ha the same sign for each of these we leave the implementation detail a an what is the time efficiency of this it is in on for each of nn pair of distinct point we may need to find the sign of ax by c for each of the other n there are much more efficient algorithm for this important problem and we discus one of them later in the exercise assuming that sqrt take about time longer than each of the other oper ations in the innermost loop of bruteforceclosestpoints which are assumed to take the same amount of time estimate how much faster the algorithm will run after the improvement discussed in section can you design a more efficient algorithm than the one based on the brute force strategy to solve the closest pair problem for n point x x xn on the real let x x xn be real number representing coordinate of n village located along a straight a post office need to be built in one of these design an efficient algorithm to find the post office location minimizing the average distance between the village and the post design an efficient algorithm to find the post office location minimizing the maximum distance from a village to the post for the sake of simplicity we assume here that no three point of a given set lie on the same a modification needed for the general case is left for the there are several alternative way to define a distance between two point px y and px y in the cartesian in particular the manhat tan distance is defined a dmp p x x y prove that dm satisfies the following axiom which every distance function must satisfy dmp p for any two point p and p and dmp p if and only if p p dmp p dmp p dmp p dmp p dmp p for any p p and p sketch all the point in the cartesian plane whose manhattan distance to the origin is equal to do the same for the euclidean true or false a solution to the closest pair problem doe not depend on which of the two metric de euclidean or dm manhattan is the hamming distance between two string of equal length is defined a the number of position at which the corresponding symbol are it is named after richard hamming a prominent american scientist and engineer who introduced it in his seminal paper on error detecting and error correcting doe the hamming distance satisfy the three axiom of a distance metric listed in problem what is the time efficiency class of the brute force algorithm for the closest pair problem if the point in question are string of m symbol long and the distance between two of them is measured by the hamming odd pie fight there are n people positioned on a field euclidean plane so that each ha a unique nearest each person ha a cream at a signal everybody hurl his or her pie at the nearest assuming that n is odd and that nobody can miss his or her target true or false there always remains at least one person not hit by a car the closest pair problem can be posed in the k dimensional space in which the euclidean distance between two point p x xk and p x xk is defined a dp p k sxs x what is the time efficiency class of the brute force algorithm for the k dimensional closest pair find the convex hull of the following set and identify their extreme point if they have any a line segment a square the boundary of a square a straight line design a linear time algorithm to determine two extreme point of the convex hull of a given set of n point in the what modification need to be made in the brute force algorithm for the convex hull problem to handle more than two point on the same straight write a program implementing the brute force algorithm for the convex hull consider the following small instance of the linear programming problem maximize x y subject to x y x y x y sketch in the cartesian plane the problem feasible region defined a the set of point satisfying all the problem identify the region extreme solve this optimization problem by using the following theorem a linear programming problem with a nonempty bounded feasible region always ha a solution which can be found at one of the extreme point of it feasible exhaustive search many important problem require finding an element with a special property in a domain that grows exponentially or faster with an instance typically such problem arise in situation that involve explicitly or implicitly combinatorial object such a permutation combination and subset of a given many such problem are optimization problem they ask to find an element that maximizes or minimizes some desired characteristic such a a path length or an assignment exhaustive search is simply a brute force approach to combinatorial prob it suggests generating each and every element of the problem domain se lecting those of them that satisfy all the constraint and then finding a desired element the one that optimizes some objective note that although the idea of exhaustive search is quite straightforward it implementation typically requires an algorithm for generating certain combinatorial we delay a dis cussion of such algorithm until the next chapter and assume here that they we illustrate exhaustive search by applying it to three important problem the traveling salesman problem the knapsack problem and the assignment traveling salesman problem the traveling salesman problem tsp ha been intriguing researcher for the last year by it seemingly simple formulation important application and interesting connection to other combinatorial in layman term the problem asks to find the shortest tour through a given set of n city that visit each city exactly once before returning to the city where it the problem can be conveniently modeled by a weighted graph with the graph vertex representing the city and the edge weight specifying the then the problem can be stated a the problem of finding the shortest hamiltonian circuit of the a hamiltonian circuit is defined a a cycle that pass through all the vertex of the graph exactly it is named after the irish mathematician sir william rowan hamilton who became interested in such cycle a an application of his algebraic it is easy to see that a hamiltonian circuit can also be defined a a sequence of n adjacent vertex vi vi vin vi where the first vertex of the sequence is the same a the last one and all the other n vertex are further we can assume with no loss of generality that all circuit start and end at one particular vertex they are cycle after all are they thus we can get all the tour by generating all the permutation of n intermediate city compute the tour length and find the shortest among figure present a small instance of the problem and it solution by this an inspection of figure reveals three pair of tour that differ only by their hence we could cut the number of vertex permutation by we could for example choose any two intermediate vertex say b and c and then consider only permutation in which b precedes this trick implicitly defines a tour this improvement cannot brighten the efficiency picture much the total number of permutation needed is still n which make the exhaustive search approach impractical for all but very small value of on the other hand if you always see your glass a half full you can claim that cutting the work by half is nothing to sneeze at even if you solve a small instance of the problem especially by also note that had we not limited our investigation to the circuit starting at the same vertex the number of permutation would have been even larger by a factor of knapsack problem here is another well known problem in given n item of known weight w w wn and value v v vn and a knapsack of capacity w find the most valuable subset of the item that fit into the if you do not like the idea of putting yourself in the shoe of a thief who want to steal the most a b c d to ur l e ng th a b c d a i a b d c a i optimal a c b d a i a c d b a i optimal a d b c a i a d c b a i figure solution to a small instance of the traveling salesman problem by exhaustive valuable loot that fit into his knapsack think about a transport plane that ha to deliver the most valuable set of item to a remote location without exceeding the plane figure present a small instance of the knapsack the exhaustive search approach to this problem lead to generating all the subset of the set of n item given computing the total weight of each subset in order to identify feasible subset the one with the total weight not exceeding the knapsack capacity and finding a subset of the largest value among a an example the solution to the instance of figure is given in figure since the number of subset of an n element set is n the exhaustive search lead to a n algorithm no matter how efficiently individual subset are thus for both the traveling salesman and knapsack problem considered above exhaustive search lead to algorithm that are extremely inefficient on every in fact these two problem are the best known example of socalled np hard no polynomial time algorithm is known for any nphard moreover most computer scientist believe that such algorithm do not exist although this very important conjecture ha never been more sophisticated approach backtracking and branch and bound see section and enable u to solve some but not all instance of these and w w w w v v v v knapsack item item item item a subset total weight total value not feasible not feasible not feasible not feasible not feasible not feasible not feasible b figure a instance of the knapsack b it solution by exhaustive the information about the optimal selection is in similar problem in le than exponential alternatively we can use one of many approximation algorithm such a those described in section assignment problem in our third example of a problem that can be solved by exhaustive search there are n people who need to be assigned to execute n job one person per that is each person is assigned to exactly one job and each job is assigned to exactly one the cost that would accrue if the ith person is assigned to the j th job is a known quantity ci j for each pair i j the problem is to find an assignment with the minimum total a small instance of this problem follows with the table entry representing the assignment cost ci j job job job job person person person person it is easy to see that an instance of the assignment problem is completely specified by it cost matrix in term of this matrix the problem is to select one element in each row of the matrix so that all selected element are in different column and the total sum of the selected element is the smallest note that no obvious strategy for finding a solution work for example we cannot select the smallest element in each row because the smallest element may happen to be in the same in fact the smallest element in the entire matrix need not be a component of an optimal thus opting for the exhaustive search may appear a an unavoidable we can describe feasible solution to the assignment problem a n tuples j jn in which the ith component i n indicates the column of the element selected in the ith row the job number assigned to the ith for example for the cost matrix above indicates the assignment of person to job person to job person to job and person to job the requirement of the assignment problem imply that there is a one to one correspondence between feasible assignment and permutation of the first n therefore the exhaustive search approach to the assignment problem would require generating all the permutation of integer n computing the total cost of each assignment by summing up the corresponding element of the cost matrix and finally selecting the one with the smallest a few first iteration of applying this algorithm to the instance given above are shown in figure you are asked to complete it in the cost cost c cost cost cost cost figure first few iteration of solving a small instance of the assignment problem by exhaustive since the number of permutation to be considered for the general case of the assignment problem is exhaustive search is impractical for all but very small instance of the fortunately there is a much more efficient algorithm for this problem called the hungarian method after the hungarian mathematician ko nig and egerva ry whose work underlies the method see this is good news the fact that a problem domain grows exponentially or faster doe not necessarily imply that there can be no efficient algorithm for solving in fact we present several other example of such problem later in the however such example are more of an exception to the more often than not there are no known polynomial time algorithm for problem whose domain grows exponentially with instance size provided we want to solve them and a we mentioned above such algorithm quite possibly do not exercise assuming that each tour can be generated in constant time what will be the efficiency class of the exhaustive search algorithm outlined in the text for the traveling salesman if this algorithm is programmed on a computer that make ten billion addition per second estimate the maximum number of city for which the problem can be solved in outline an exhaustive search algorithm for the hamiltonian circuit outline an algorithm to determine whether a connected graph represented by it adjacency matrix ha an eulerian what is the efficiency class of your complete the application of exhaustive search to the instance of the assign ment problem started in the give an example of the assignment problem whose optimal solution doe not include the smallest element of it cost consider the partition problem given n positive integer partition them into two disjoint subset with the same sum of their of course the prob lem doe not always have a design an exhaustive search algorithm for this try to minimize the number of subset the algorithm need to consider the clique problem given a graph g and a positive integer k deter mine whether the graph contains a clique of size k a complete subgraph of k design an exhaustive search algorithm for this explain how exhaustive search can be applied to the sorting problem and determine the efficiency class of such an eight queen problem consider the classic puzzle of placing eight queen on an chessboard so that no two queen are in the same row or in the same column or on the same how many different position are there so that no two queen are on the same no two queen are in the same no two queen are in the same row or in the same also estimate how long it would take to find all the solution to the problem by exhaustive search based on each of these approach on a computer capable of checking billion position per magic square a magic square of order n is an arrangement of the integer from to n in an n n matrix with each number occurring exactly once so that each row each column and each main diagonal ha the same prove that if a magic square of order n exists the sum in question must be equal to nn design an exhaustive search algorithm for generating all magic square of order go to the internet or your library and find a better algorithm for generating magic implement the two algorithm the exhaustive search and the one you have found and run an experiment to determine the largest value of n for which each of the algorithm is able to find a magic square of order n in le than minute on your famous alphametic a puzzle in which the digit in a correct mathematical expression such a a sum are replaced by letter is called cryptarithm if in addition the puzzle word make sense it is said to be an the most well known alphametic wa published by the renowned british puzzlist henry dudeney s end more money two condition are assumed first the correspondence between letter and decimal digit is one to one each letter represents one digit only and dif ferent letter represent different second the digit zero doe not appear a the left most digit in any of the to solve an alphametic mean to find which digit each letter note that a solution uniqueness cannot be assumed and ha to be verified by the write a program for solving cryptarithms by exhaustive assume that a given cryptarithm is a sum of two solve dudeneys puzzle the way it wa expected to be solved when it wa first published in depth first search and breadth first search the term exhaustive search can also be applied to two very important algorithm that systematically process all vertex and edge of a these two traversal algorithm are depth first search dfs and breadth first search these algorithm have proved to be very useful for many application involving graph in artificial intelligence and operation in addition they are indispensable for efficient investigation of fundamental property of graph such a connectivity and cycle depth first search depth first search start a graph traversal at an arbitrary vertex by marking it a on each iteration the algorithm proceeds to an unvisited vertex that is adjacent to the one it is currently if there are several such vertex a tie can be resolved a a practical matter which of the adjacent unvisited candidate is chosen is dictated by the data structure representing the in our example we always break tie by the alphabetical order of the this process continues until a dead end a vertex with no adjacent unvisited vertex is at a dead end the algorithm back up one edge to the vertex it came from and try to continue visiting unvisited vertex from the algorithm eventually halt after backing up to the starting vertex with the latter being a dead by then all the vertex in the same connected component a the starting vertex have been if unvisited vertex still remain the depth first search must be restarted at any one of it is convenient to use a stack to trace the operation of depth first we push a vertex onto the stack when the vertex is reached for the first time the a g g h e c h a e b j c f d f i d f i c h d b a g j b j i e a b c figure example of a dfs a b traversal stack the first subscript number indicates the order in which a vertex is visited pushed onto the stack the second one indicates the order in which it becomes a dead end popped off the c dfs forest with the tree and back edge shown with solid and dashed line visit of the vertex start and we pop a vertex off the stack when it becomes a dead end the visit of the vertex it is also very useful to accompany a depth first search traversal by constructing the so called depth first search the starting vertex of the traversal serf a the root of the first tree in such a whenever a new unvisited vertex is reached for the first time it is attached a a child to the vertex from which it is being such an edge is called a tree edge because the set of all such edge form a the algorithm may also encounter an edge leading to a previously visited vertex other than it immediate predecessor it parent in the such an edge is called a back edge because it connects a vertex to it ancestor other than the parent in the depth first search figure provides an example of a depth first search traversal with the traversal stack and corresponding depth first search forest shown a here is pseudocode of the depth first algorithm dfsg implement a depth first search traversal of a given graph input graph g v e output graph g with it vertex marked with consecutive integer in the order they are first encountered by the dfs traversal mark each vertex in v with a a mark of being unvisited count for each vertex v in v do if v is marked with dfsv dfsv visit recursively all the unvisited vertex connected to vertex v by a path and number them in the order they are encountered via global variable count count count mark v with count for each vertex w in v adjacent to v do if w is marked with dfsw the brevity of the dfs pseudocode and the ease with which it can be per formed by hand may create a wrong impression about the level of sophistication of this to appreciate it true power and depth you should trace the algorithm action by looking not at a graph diagram but at it adjacency matrix or adjacency try it for the graph in figure or a smaller how efficient is depth first it is not difficult to see that this algorithm is in fact quite efficient since it take just the time proportional to the size of the data structure used for representing the graph in thus for the adjacency matrix representation the traversal time is in v and for the adjacency list representation it is in v e where v and e are the number of the graph vertex and edge a dfs forest which is obtained a a by product of a dfs traversal deserves a few comment to begin with it is not actually a rather we can look at it a the given graph with it edge classified by the dfs traversal into two disjoint class tree edge and back no other type are possible for a dfs forest of an undirected again tree edge are edge used by the dfs traversal to reach previously unvisited if we consider only the edge in this class we will indeed get a back edge connect vertex to previously visited vertex other than their immediate predecessor in the they connect vertex to their ancestor in the forest other than their a dfs traversal itself and the forest like representation of the graph it pro vides have proved to be extremely helpful for the development of efficient al gorithms for checking many important property of note that the dfs yield two ordering of vertex the order in which the vertex are reached for the first time pushed onto the stack and the order in which the vertex become dead end popped off the these order are qualitatively different and various application can take advantage of either of important elementary application of dfs include checking connectivity and checking acyclicity of a since dfs halt after visiting all the vertex con the discovery of several such application wa an important breakthrough achieved by the two american computer scientist john hopcroft and robert tarjan in the for this and other contribution they were given the turing award the most prestigious prize in the computing field hop nected by a path to the starting vertex checking a graph connectivity can be done a start a dfs traversal at an arbitrary vertex and check after the algorithm halt whether all the vertex of the graph will have been if they have the graph is connected otherwise it is not more generally we can use dfs for identifying connected component of a graph a for checking for a cycle presence in a graph we can take advantage of the graph representation in the form of a dfs if the latter doe not have back edge the graph is clearly if there is a back edge from some vertex u to it ancestor v the back edge from d to a in figure the graph ha a cycle that comprises the path from v to u via a sequence of tree edge in the dfs forest followed by the back edge from u to you will find a few other application of dfs later in the book although more sophisticated application such a finding articulation point of a graph are not a vertex of a connected graph is said to be it articulation point if it removal with all edge incident to it break the graph into disjoint breadth first search if depth first search is a traversal for the brave the algorithm go a far from home a it can breadth first search is a traversal for the it proceeds in a concentric manner by visiting first all the vertex that are adjacent to a starting vertex then all unvisited vertex two edge apart from it and so on until all the vertex in the same connected component a the starting vertex are if there still remain unvisited vertex the algorithm ha to be restarted at an arbitrary vertex of another connected component of the it is convenient to use a queue note the difference from depth first to trace the operation of breadth first the queue is initialized with the traversal starting vertex which is marked a on each iteration the algorithm identifies all unvisited vertex that are adjacent to the front vertex mark them a visited and add them to the queue after that the front vertex is removed from the similarly to a dfs traversal it is useful to accompany a bfs traversal by constructing the so called breadth first search the traversal starting vertex serf a the root of the first tree in such a whenever a new unvisited vertex is reached for the first time the vertex is attached a a child to the vertex it is being reached from with an edge called a tree if an edge leading to a previously visited vertex other than it immediate predecessor it parent in the tree is encountered the edge is noted a a cross figure provides an example of a breadth first search traversal with the traversal queue and corresponding breadth first search forest g h a g a e c f a c d e f b c d e h j g h j i d b f b i j i a b c figure example of a bfs a b traversal queue with the number indicating the order in which the vertex are visited added to and removed from the c bfs forest with the tree and cross edge shown with solid and dotted line here is pseudocode of the breadth first algorithm bfsg implement a breadth first search traversal of a given graph input graph g v e output graph g with it vertex marked with consecutive integer in the order they are visited by the bfs traversal mark each vertex in v with a a mark of being unvisited count for each vertex v in v do if v is marked with bfsv bfsv visit all the unvisited vertex connected to vertex v by a path and number them in the order they are visited via global variable count count count mark v with count and initialize a queue with v while the queue is not empty do for each vertex w in v adjacent to the front vertex do if w is marked with count count mark w with count add w to the queue remove the front vertex from the queue a a b c d b e e f g h c f d g a b figure illustration of the bfs based algorithm for finding a minimum edge a b part of it bfs tree that identifies the minimum edge path from a to breadth first search ha the same efficiency a depth first search it is in v for the adjacency matrix representation and in v e for the adjacency list unlike depth first search it yield a single ordering of vertex because the queue is a fifo first in first out structure and hence the order in which vertex are added to the queue is the same order in which they are removed from a to the structure of a bfs forest of an undirected graph it can also have two kind of edge tree edge and cross tree edge are the one used to reach previously unvisited cross edge connect vertex to those visited before but unlike back edge in a dfs tree they connect vertex either on the same or adjacent level of a bfs bfs can be used to check connectivity and acyclicity of a graph essentially in the same manner a dfs it is not applicable however for several le straightforward application such a finding articulation on the other hand it can be helpful in some situation where dfs for example bfs can be used for finding a path with the fewest number of edge between two given to do this we start a bfs traversal at one of the two vertex and stop it a soon a the other vertex is the simple path from the root of the bfs tree to the second vertex is the path for example path a b c g in the graph in figure ha the fewest number of edge among all the path between vertex a and although the correctness of this application appears to stem immediately from the way bfs operates a mathematical proof of it validity is not quite elementary see cor section table summarizes the main fact about depth first search and breadth first table main fact about depth first search dfs and breadth first search bfs dfs bfs data structure a stack a queue number of vertex ordering two ordering one ordering edge type undirected graph tree and back edge tree and cross edge application connectivity connectivity acyclicity acyclicity articulation point minimum edge path efficiency for adjacency matrix v v efficiency for adjacency list v e v e exercise consider the following f b c g d a e write down the adjacency matrix and adjacency list specifying this assume that the matrix row and column and vertex in the adjacency list follow in the alphabetical order of the vertex starting at vertex a and resolving tie by the vertex alphabetical order traverse the graph by depth first search and construct the corresponding depth first search give the order in which the vertex were reached for the first time pushed onto the traversal stack and the order in which the vertex became dead end popped off the if we define sparse graph a graph for which e ov which implemen tation of dfs will have a better time efficiency for such graph the one that us the adjacency matrix or the one that us the adjacency let g be a graph with n vertex and m true or false all it dfs forest for traversal starting at different ver tices will have the same number of true or false all it dfs forest will have the same number of tree edge and the same number of back traverse the graph of problem by breadth first search and construct the corresponding breadth first search start the traversal at vertex a and resolve tie by the vertex alphabetical prove that a cross edge in a bfs tree of an undirected graph can connect vertex only on either the same level or on two adjacent level of a bfs explain how one can check a graph acyclicity by using breadth first doe either of the two traversal dfs or bfs always find a cycle faster than the if you answer yes indicate which of them is better and explain why it is the case if you answer no give two example supporting your explain how one can identify connected component of a graph by using a depth first a breadth first a graph is said to be bipartite if all it vertex can be partitioned into two disjoint subset x and y so that every edge connects a vertex in x with a vertex in y one can also say that a graph is bipartite if it vertex can be colored in two color so that every edge ha it vertex colored in different color such graph are also called for example graph i is bipartite while graph ii is x y x a b y x y c d i ii design a dfs based algorithm for checking whether a graph is design a bfs based algorithm for checking whether a graph is write a program that for a given graph output vertex of each connected component it cycle or a message that the graph is acyclic one can model a maze by having a vertex for a starting point a finishing point dead end and all the point in the maze where more than one path can be taken and then connecting the vertex according to the path in the construct such a graph for the following which traversal dfs or bfs would you use if you found yourself in a maze and three jug sime on denis poisson a famous french mathemati cian and physicist is said to have become interested in mathematics after encountering some version of the following old given an pint jug full of water and two empty jug of and pint capacity get exactly pint of water in one of the jug by completely filling up andor emptying jug into solve this puzzle by using breadth first summary brute force is a straightforward approach to solving a problem usually directly based on the problem statement and definition of the concept the principal strength of the brute force approach are wide applicability and simplicity it principal weakness is the subpar efficiency of most brute force a first application of the brute force approach often result in an algorithm that can be improved with a modest amount of the following noted algorithm can be considered a example of the brute force approach definition based algorithm for matrix multiplication selection sort sequential search straightforward string matching algorithm exhaustive search is a brute force approach to combinatorial it suggests generating each and every combinatorial object of the problem selecting those of them that satisfy all the constraint and then finding a desired the traveling salesman problem the knapsack problem and the assignment problem are typical example of problem that can be solved at least theoretically by exhaustive search exhaustive search is impractical for all but very small instance of problem it can be applied depth first search dfs and breadth first search bfs are two principal graph traversal by representing a graph in a form of a depth first or breadth first search forest they help in the investigation of many important property of the both algorithm have the same time efficiency v for the adjacency matrix representation and v e for the adjacency list decrease and conquer plutarch say that sertorius in order to teach his soldier that perseverance and wit are better than brute force had two horse brought before them and set two men to pull out their one of the men wa a burly hercules who tugged and tugged but all to no purpose the other wa a sharp weasel faced tailor who plucked one hair at a time amidst roar of laughter and soon left the tail quite cobham brewer dictionary of phrase and fable the decrease and conquer technique is based on exploiting the relationship between a solution to a given instance of a problem and a solution to it smaller once such a relationship is established it can be exploited either top down or bottom the former lead naturally to a recursive implementa tion although a one can see from several example in this chapter an ultimate implementation may well be the bottom up variation is usually implemented iteratively starting with a solution to the smallest instance of the problem it is called sometimes the incremental there are three major variation of decrease and conquer decrease by a constant decrease by a constant factor variable size decrease in the decrease by a constant variation the size of an instance is reduced by the same constant on each iteration of the typically this constant is equal to one figure although other constant size reduction do happen consider a an example the exponentiation problem of computing an where a and n is a nonnegative the relationship between a solution to an instance of size n and an instance of size n is obtained by the obvious formula an an so the function f n an can be computed either top down by using it recursive definition problem of size n subproblem of size n solution to the subproblem solution to the original problem figure decrease by one and conquer f n f n a if n if n or bottom up by multiplying by a n yes it is the same a the brute force algorithm but we have come to it by a different thought more interesting example of decrease by one algorithm appear in section the decrease by a constant factor technique suggests reducing a problem instance by the same constant factor on each iteration of the in most application this constant factor is equal to can you give an example of such an the decrease by half idea is illustrated in figure for an example let u revisit the exponentiation if the instance of size n is to compute an the instance of half it size is to compute an with the obvious relationship between the two an but since we consider here instance with integer exponent only the former doe not work for odd if n is odd we have to compute an by using the rule for even valued exponent and then multiply the result by to summarize we have the following formula problem of size n subproblem of size n solution to the subproblem solution to the original problem figure decrease by half and conquer an an an a if n is even and positive if n is odd if n if we compute an recursively according to formula and measure the algorithm efficiency by the number of multiplication we should expect the algorithm to be in log n because on each iteration the size is reduced by about a half at the expense of one or two a few other example of decrease by a constant factor algorithm are given in section and it such algorithm are so efficient however that there are few example of this finally in the variable size decrease variety of decrease and conquer the size reduction pattern varies from one iteration of an algorithm to euclid algorithm for computing the greatest common divisor provides a good example of such a recall that this algorithm is based on the formula gcdm n gcdn m mod though the value of the second argument is always smaller on the right hand side than on the left hand side it decrease neither by a constant nor by a constant a few other example of such algorithm appear in section insertion sort in this section we consider an application of the decrease by one technique to sorting an array following the technique idea we assume that the smaller problem of sorting the array ha already been solved to give u a sorted array of size n a an how can we take advantage of this solution to the smaller problem to get a solution to the original problem by taking into account the element an obviously all we need is to find an appropriate position for an among the sorted element and insert it this is usually done by scanning the sorted subarray from right to left until the first element smaller than or equal to an is encountered to insert an right after that the resulting algorithm is called straight insertion sort or simply insertion though insertion sort is clearly based on a recursive idea it is more efficient to implement this algorithm bottom up a shown in figure starting with a and ending with an ai is inserted in it appropriate place among the first i element of the array that have been already sorted but unlike selection sort are generally not in their final here is pseudocode of this algorithm sort a given array by insertion sort input an array of n orderable element output array sorted in nondecreasing order for i to n do v ai j i while j and aj v do aj aj j j aj v a a j a j ai ai an smaller than or equal to ai greater than ai figure iteration of insertion sort ai is inserted in it proper position among the preceding element previously figure example of sorting with insertion a vertical bar separate the sorted part of the array from the remaining element the element being inserted is in the operation of the algorithm is illustrated in figure the basic operation of the algorithm is the key comparison aj why not j because it is almost certainly faster than the former in an actual computer moreover it is not germane to the algorithm a better implementation with a sentinel see problem in this section exercise eliminates it the number of key comparison in this algorithm obviously depends on the nature of the in the worst case aj v is executed the largest number of time for every j i since v ai it happens if and only if aj ai for j i note that we are using the fact that on the ith iteration of insertion sort all the element preceding ai are the first i element in the input albeit in the sorted thus for the worst case input we get a a for i a a for i an an for i n in other word the worst case input is an array of strictly decreasing the number of key comparison for such an input is n i n n n cworst n i i j i thus in the worst case insertion sort make exactly the same number of comparison a selection sort see section in the best case the comparison aj v is executed only once on every iteration of the outer it happens if and only if ai ai for every i n if the input array is already sorted in nondecreasing though it make sense that the best case of an algorithm happens when the problem is already solved it is not always the case a you are going to see in our discussion of quicksort in chapter thus for sorted array the number of key comparison is n cbest n n i this very good performance in the best case of sorted array is not very useful by itself because we cannot expect such convenient however almost sorted file do arise in a variety of application and insertion sort preserve it excellent performance on such a rigorous analysis of the algorithm average case efficiency is based on investigating the number of element pair that are out of order see problem in this section it show that on randomly ordered array insertion sort make on average half a many comparison a on decreasing array cavgn n this twice a fast average case performance coupled with an excellent efficiency on almost sorted array make insertion sort stand out among it principal com petitors among elementary sorting algorithm selection sort and bubble in addition it extension named shellsort after it inventor shell she give u an even better algorithm for sorting moderately large file see problem in this section exercise ferrying soldier a detachment of n soldier must cross a wide and deep river with no bridge in they notice two year old boy playing in a rowboat by the the boat is so tiny however that it can only hold two boy or one how can the soldier get across the river and leave the boy in joint possession of the how many time need the boat pas from shore to alternating glass there are n glass standing next to each other in a row the first n of them filled with a soda drink and the remaining n glass make the glass alternate in a filled empty filled empty pattern in the minimum number of glass gar solve the same problem if n glass n with a drink and n empty are initially in a random marking cell design an algorithm for the following for any even n mark n cell on an infinite sheet of graph paper so that each marked cell ha an odd number of marked two cell are considered neighbor if they are next to each other either horizontally or vertically but not the marked cell must form a contiguous region a region in which there is a path between any pair of marked cell that go through a sequence of marked kor design a decrease by one algorithm for generating the power set of a set of n the power set of a set s is the set of all the subset of s including the empty set and s consider the following algorithm to check connectivity of a graph defined by it adjacency algorithm input adjacency matrix of an undirected graph g output true if g is connected and false if it is not if n return one vertex graph is connected by definition else if not return else for j to n do if an j return return doe this algorithm work correctly for every undirected graph with n if you answer yes indicate the algorithm efficiency class in the worst case if you answer no explain team ordering you have the result of a completed round robin tournament in which n team played each other each game ended either with a victory for one of the team or with a design an algorithm that list the team in a sequence so that every team did not lose the game with the team listed immediately after what is the time efficiency class of your apply insertion sort to sort the list e x a m p l e in alphabetical what sentinel should be put before the first element of an array being sorted in order to avoid checking the in bound condition j on each iteration of the inner loop of insertion is the sentinel version in the same efficiency class a the original is it possible to implement insertion sort for sorting linked will it have the same on time efficiency a the array compare the text implementation of insertion sort with the following ver algorithm for i to n do j i while j and aj aj do swapaj aj j j what is the time efficiency of this how is it compared to that of the version given in section let be an array of n sortable for simplicity you may assume that all the element are a pair ai aj is called an inversion if i j and ai aj what array of size n have the largest number of inversion and what is this answer the same question for the smallest number of show that the average case number of key comparison in insertion sort is given by the formula cavgn n shellsort more accurately shell sort is an important sorting algorithm that work by applying insertion sort to each of several interleaving sublists of a given on each pas through the list the sublists in question are formed by stepping through the list with an increment hi taken from some predefined decreasing sequence of step size h hi which must end with the algorithm work for any such sequence though some sequence are known to yield a better efficiency than for example the sequence used of course in reverse is known to be among the best for this apply shellsort to the list s h e l l s o r t i s u s e f u l is shellsort a stable sorting implement shellsort straight insertion sort selection sort and bubble sort in the language of your choice and compare their performance on random array of size n for n and a well a on increasing and decreasing array of these topological sorting in this section we discus an important problem for directed graph with a variety of application involving prerequisite restricted before we pose this problem though let u review a few basic fact about directed graph a directed graph or digraph for short is a graph with direction specified for all it edge figure is an the adjacency matrix and adjacency list are still two principal mean of representing a there are only two notable difference between undirected and directed graph in representing them the adjacency matrix of a directed graph doe not have to be symmetric an edge in a directed graph ha just one not two corresponding node in the digraph adjacency a b a d c b e d c e a b figure a b dfs forest of the digraph for the dfs traversal started at depth first search and breadth first search are principal traversal algorithm for traversing digraph a well but the structure of corresponding forest can be more complex than for undirected thus even for the simple example of figure the depth first search forest figure exhibit all four type of edge possible in a dfs forest of a directed graph tree edge ab bc de back edge ba from vertex to their ancestor forward edge ac from vertex to their descendant in the tree other than their child and cross edge dc which are none of the aforementioned note that a back edge in a dfs forest of a directed graph can connect a vertex to it whether or not it is the case the presence of a back edge indicates that the digraph ha a directed a directed cycle in a digraph is a sequence of three or more of it vertex that start and end with the same vertex and in which every vertex is connected to it immediate predecessor by an edge directed from the predecessor to the for example a b a is a directed cycle in the digraph in figure conversely if a dfs forest of a digraph ha no back edge the digraph is a dag an acronym for directed acyclic edge direction lead to new question about digraph that are either meaningless or trivial for undirected in this section we discus one such a a motivating example consider a set of five required course c c c c c a part time student ha to take in some degree the course can be taken in any order a long a the following course prerequisite are met c and c have no prerequisite c requires c and c c requires c and c requires c and the student can take only one course per in which order should the student take the the situation can be modeled by a digraph in which vertex represent course and directed edge indicate prerequisite requirement figure in term of this digraph the question is whether we can list it vertex in such an order that for every edge in the graph the vertex where the edge start is listed before the vertex where the edge can you find such an ordering of this digraph this problem is called topological it can be posed for an c c c c c figure digraph representing the prerequisite structure of five c c c the popping off order c c c c c c c c the topologically sorted list c c c c c c c c c a b c figure a digraph for which the topological sorting problem need to be b dfs traversal stack with the subscript number indicating the popping off c solution to the arbitrary digraph but it is easy to see that the problem cannot have a solution if a digraph ha a directed thus for topological sorting to be possible a digraph in question must be a it turn out that being a dag is not only necessary but also sufficient for topological sorting to be possible if a digraph ha no directed cycle the topological sorting problem for it ha a moreover there are two efficient algorithm that both verify whether a digraph is a dag and if it is produce an ordering of vertex that solves the topological sorting the first algorithm is a simple application of depth first search perform a dfs traversal and note the order in which vertex become dead end popped off the traversal reversing this order yield a solution to the topological sorting problem provided of course no back edge ha been encountered during the if a back edge ha been encountered the digraph is not a dag and topological sorting of it vertex is why doe the algorithm when a vertex v is popped off a dfs stack no vertex u with an edge from u to v can be among the vertex popped off before otherwise u v would have been a back hence any such vertex u will be listed after v in the popped off order list and before v in the reversed figure illustrates an application of this algorithm to the digraph in fig ure note that in figure we have drawn the edge of the digraph and they all point from left to right a the problem statement it is a con venient way to check visually the correctness of a solution to an instance of the topological sorting c c c c c delete c c delete c c c c c c c delete c c delete c delete c c c the solution obtained is c c c c c figure illustration of the source removal algorithm for the topological sorting on each iteration a vertex with no incoming edge is deleted from the the second algorithm is based on a direct implementation of the decrease by one and conquer technique repeatedly identify in a remaining digraph a source which is a vertex with no incoming edge and delete it along with all the edge outgoing from if there are several source break the tie if there are none stop because the problem cannot be solved see problem a in this section the order in which the vertex are deleted yield a solution to the topological sorting the application of this algorithm to the same digraph representing the five course is given in figure note that the solution obtained by the source removal algorithm is different from the one obtained by the dfs based both of them are correct of course the topological sorting problem may have several alternative the tiny size of the example we used might create a wrong impression about the topological sorting but imagine a large project in construction research or software development that involves a multitude of interrelated task with known the first thing to do in such a situation is to make sure that the set of given prerequisite is not the convenient way of doing this is to solve the topological sorting problem for the project only then can one start thinking about scheduling task to say minimize the total completion time of the this would require of course other algorithm that you can find in general book on operation research or in special one on cpm critical path method and pert program evaluation and review technique a to application of topological sorting in computer science they include instruction scheduling in program compilation cell evaluation ordering in spreadsheet formula and resolving symbol dependency in exercise apply the dfs based algorithm to solve the topological sorting problem for the following digraph a b a b c d c d e g e f g f a b prove that the topological sorting problem ha a solution if and only if it is a for a digraph with n vertex what is the largest number of distinct solution the topological sorting problem can what is the time efficiency of the dfs based algorithm for topological how can one modify the dfs based algorithm to avoid reversing the vertex ordering generated by can one use the order in which vertex are pushed onto the dfs stack instead of the order they are popped off it to solve the topological sorting apply the source removal algorithm to the digraph of problem prove that a nonempty dag must have at least one how would you find a source or determine that such a vertex doe not exist in a digraph represented by it adjacency what is the time efficiency of this how would you find a source or determine that such a vertex doe not exist in a digraph represented by it adjacency what is the time efficiency of this can you implement the source removal algorithm for a digraph represented by it adjacency list so that it running time is in ov implement the two topological sorting algorithm in the language of your run an experiment to compare their running a digraph is called strongly connected if for any pair of two distinct vertex u and v there exists a directed path from u to v and a directed path from v to in general a digraph vertex can be partitioned into disjoint maximal subset of vertex that are mutually accessible via directed path these subset are called strongly connected component of the there are two dfs based algorithm for identifying strongly connected here is the simpler but somewhat le efficient one of the two step perform a dfs traversal of the digraph given and number it vertex in the order they become dead step reverse the direction of all the edge of the step perform a dfs traversal of the new digraph by starting and if necessary restarting the traversal at the highest numbered vertex among still unvisited the strongly connected component are exactly the vertex of the dfs tree obtained during the last apply this algorithm to the following digraph to determine it strongly connected component a b c d e f g h what is the time efficiency class of this give separate answer for the adjacency matrix representation and adjacency list representation of an input how many strongly connected component doe a dag spider web a spider sits at the bottom point s of it web and a fly sits at the top how many different way can the spider reach the fly by moving along the web line in the direction indicated by the kor f s algorithm for generating combinatorial object in this section we keep our promise to discus algorithm for generating combi natorial the most important type of combinatorial object are permuta tions combination and subset of a given they typically arise in problem that require a consideration of different we already encountered them in chapter when we discussed exhaustive combinatorial object are stud ied in a branch of discrete mathematics called mathematician of course are primarily interested in different counting formula we should be grate ful for such formula because they tell u how many item need to be in particular they warn u that the number of combinatorial object typically grows exponentially or even faster a a function of the problem but our primary interest here lie in algorithm for generating combinatorial object not just in counting generating permutation we start with for simplicity we assume that the underlying set whose element need to be permuted is simply the set of integer from to n more generally they can be interpreted a index of element in an n element set a what would the decrease by one technique suggest for the problem of generating all permutation of the smaller by one problem is to generate all n assuming that the smaller problem is solved we can get a solution to the larger one by inserting n in each of the n possible position among element of every permutation of n all the permu tations obtained in this fashion will be distinct and their total number will be nn hence we will obtain all the permutation of we can insert n in the previously generated permutation either left to right or right to it turn out that it is beneficial to start with inserting n into n by moving right to left and then switch direction every time a new permutation of n need to be an example of applying this approach bottom up for n is given in figure the advantage of this order of generating permutation stem from the fact that it satisfies the minimal change requirement each permutation can be ob tained from it immediate predecessor by exchanging just two element in for the method being discussed these two element are always adjacent to each start insert into right to left insert into right to left insert into left to right figure generating permutation bottom check this for the permutation generated in figure the minimal change requirement is beneficial both for the algorithm speed and for application using the for example in section we needed permutation of city to solve the traveling salesman problem by exhaustive if such permutation are generated by a minimal change algorithm we can compute the length of a new tour from the length of it predecessor in constant rather than linear time it is possible to get the same ordering of permutation of n element without explicitly generating permutation for smaller value of it can be done by associating a direction with each element k in a we indicate such a direction by a small arrow written above the element in question the element k is said to be mobile in such an arrow marked permutation if it arrow point to a smaller number adjacent to for example for the permutation and are mobile while and are using the notion of a mobile element we can give the following description of the johnson trotter algorithm for generating algorithm johnsontrottern implement johnson trotter algorithm for generating permutation input a positive integer n output a list of all permutation of n initialize the first permutation with n while the last permutation ha a mobile element do find it largest mobile element k swap k with the adjacent element k arrow point to reverse the direction of all the element that are larger than k add the new permutation to the list here is an application of this algorithm for n with the largest mobile element shown in bold this algorithm is one of the most efficient for generating permutation it can be implemented to run in time proportional to the number of permutation in of course it is horribly slow for all but very small value of n however this is not the algorithm fault but rather the fault of the problem it simply asks to generate too many one can argue that the permutation ordering generated by the johnsontrotter algorithm is not quite natural for example the natural place for permutation nn seems to be the last one on the this would be the case if permutation were listed in increasing order also called the lexicographic or der which is the order in which they would be listed in a dictionary if the number were interpreted a letter of an for example for n so how can we generate the permutation following aa an an in lexi cographic if an an which is the case for exactly one half of all the permutation we can simply transpose these last two for example is followed by if an an we find the permutation longest decreasing suffix ai ai an but ai ai increase ai by exchanging it with the smallest element of the suffix that is greater than ai and reverse the new suffix to put it in increasing for example is followed by here is pseudocode of this simple algorithm whose origin go a far back a th century algorithm lexicographicpermuten generates permutation in lexicographic order input a positive integer n output a list of all permutation of n in lexicographic order initialize the first permutation with n while last permutation ha two consecutive element in increasing order do let i be it largest index such that ai ai ai ai an find the largest index j such that ai aj j i since ai ai swap ai with aj aiai an will remain in decreasing order reverse the order of the element from ai to an inclusive add the new permutation to the list generating subset recall that in section we examined the knapsack problem which asks to find the most valuable subset of item that fit a knapsack of a given the exhaustive search approach to solving this problem discussed there wa based on generating all subset of a given set of in this section we discus algorithm for generating all n subset of an abstract set a a mathematician call the set of all subset of a set it power the decrease by one idea is immediately applicable to this problem all subset of a a an can be divided into two group those that do not contain an and those that the former group is nothing but all the subset of a an while each and every element of the latter can be obtained by adding an to a subset of a an thus once we have a list of all subset of a an we can get all the subset of a an by adding to the list all it element with an put into each of an application of this algorithm to generate all subset of a a a is illustrated in figure similarly to generating permutation we do not have to generate power set of smaller a convenient way of solving the problem directly is based on a one to one correspondence between all n subset of an n element set a a an n subset a a a a a a a a a a a a a a a a a figure generating subset bottom and all n bit string b bn of length the easiest way to establish such a correspondence is to assign to a subset the bit string in which bi if ai belongs to the subset and bi if ai doe not belong to we mentioned this idea of bit vector in section for example the bit string will correspond to the empty subset of a three element set will correspond to the set itself a a a and will represent a with this correspondence in place we can generate all the bit string of length n by generating successive binary number from to n padded when necessary with an appropriate number of leading for example for the case of n we obtain bit string subset a a a a a a a a a a a a note that although the bit string are generated by this algorithm in lexicographic order in the two symbol alphabet of and the order of the subset look anything but for example we might want to have the so called squashed order in which any subset involving aj can be listed only after all the subset involving a aj a wa the case for the list of the three element set in figure it is easy to adjust the bit stringbased algorithm above to yield a squashed ordering of the subset involved see problem in this section a more challenging question is whether there exists a minimal change algorithm for generating bit string so that every one of them differs from it immediate predecessor by only a single in the language of subset we want every subset to differ from it immediate predecessor by either an addition or a deletion but not both of a single the answer to this question is for example for n we can get such a sequence of bit string is called the binary reflected gray frank gray a researcher at att bell laboratory reinvented it in the s to minimize the effect of error in transmitting digital signal see ro seventy year earlier the french engineer e mile baudot used such code in here is pseudocode that generates the binary reflected gray code algorithm brgcn generates recursively the binary reflected gray code of order n input a positive integer n output a list of all bit string of length n composing the gray code if n make list l containing bit string and in this order else generate list l of bit string of size n by calling brgcn copy list l to list l in reversed order add in front of each bit string in list l add in front of each bit string in list l append l to l to get list l return l the correctness of the algorithm stem from the fact that it generates n bit string and all of them are both these assertion are easy to check by mathematical note that the binary reflected gray code is cyclic it last bit string differs from the first one by a single for a nonrecursive algorithm for generating the binary reflected gray code see problem in this section exercise is it realistic to implement an algorithm that requires generating all permu tations of a element set on your what about all the subset of such a generate all permutation of by the bottom up minimal change the johnson trotter the lexicographic order apply lexicographicpermute to multiset doe it generate correctly all the permutation in lexicographic consider the following implementation of the algorithm for generating per mutation discovered by heap algorithm heappermuten implement heap algorithm for generating permutation input a positive integer n and a global array output all permutation of element of a if n write a else for i to n do heappermuten if n is odd swap a and an else swap ai and an trace the algorithm by hand for n and prove the correctness of heap what is the time efficiency of generate all the subset of a four element set a a a a a by each of the two algorithm outlined in this what simple trick would make the bit stringbased algorithm generate subset in squashed write pseudocode for a recursive algorithm for generating all n bit string of length write a nonrecursive algorithm for generating n bit string of length n that implement bit string a array and doe not use binary generate the binary reflexive gray code of order trace the following nonrecursive algorithm to generate the binary re flexive gray code of order start with the n bit string of all for i n generate the ith bit string by flipping bit b in the previ ous bit string where b is the position of the least significant in the binary representation of design a decrease and conquer algorithm for generating all combination of k item chosen from n all k element subset of a given n element is your algorithm a minimal change gray code and the tower of hanoi show that the disk move made in the classic recursive algorithm for the tower of hanoi puzzle can be used for generating the binary reflected gray show how the binary reflected gray code can be used for solving the tower of hanoi fair attraction in olden day one could encounter the following attraction at a a light bulb wa connected to several switch in such a way that it lighted up only when all the switch were each switch wa controlled by a push button pressing the button toggled the switch but there wa no way to know the state of the the object wa to turn the light bulb design an algorithm to turn on the light bulb with the minimum number of button push needed in the worst case for n decrease by a constant factor algorithm you may recall from the introduction to this chapter that decrease by a constant factor is the second major variety of decrease and a an example of an algorithm based on this technique we mentioned there exponentiation by squar ing defined by formula in this section you will find a few other example of such the most important and well known of them is binary decrease by a constant factor algorithm usually run in logarithmic time and be ing very efficient do not happen often a reduction by a factor other than two is especially binary search binary search is a remarkably efficient algorithm for searching in a sorted it work by comparing a search key k with the array middle element if they match the algorithm stop otherwise the same operation is repeated recursively for the first half of the array if k am and for the second half if k am k a am am am an search here if search here if k am k am a an example let u apply binary search to searching for k in the array the iteration of the algorithm are given in the following table index value iteration l m r iteration l m r iteration lm r though binary search is clearly based on a recursive idea it can be easily implemented a a nonrecursive algorithm here is pseudocode of this nonre cursive algorithm k implement nonrecursive binary search input an array sorted in ascending order and a search key k output an index of the array element that is equal to k or if there is no such element l r n while l r do m l r if k am return m else if k am r m else l m return the standard way to analyze the efficiency of binary search is to count the number of time the search key is compared with an element of the moreover for the sake of simplicity we will count the so called three way this assumes that after one comparison of k with am the algorithm can determine whether k is smaller equal to or larger than how many such comparison doe the algorithm make on an array of n the answer obviously depends not only on n but also on the specific of a particular instance of the let u find the number of key comparison in the worst case the worst case input include all array that do not contain a given search key a well a some successful since after one comparison the algorithm face the same situation but for an array half the size we get the following recurrence relation for cworstn cworst n cworst n for n cworst stop and convince yourself that n must be indeed rounded down and that the initial condition must be written a we already encountered recurrence with a different initial condition in section see recurrence and it solution there for n for the initial condition cworst we obtain cworst k k log n further similarly to the case of recurrence problem in exercise the solution given by formula for n k can be tweaked to get a solution valid for an arbitrary positive integer n cworst n log n logn formula deserves first it implies that the worst case time efficiency of binary search is in log second it is the answer we should have fully expected since the algorithm simply reduces the size of the remaining array by about half on each iteration the number of such iteration needed to reduce the initial size n to the final size ha to be about log third to reiterate the point made in section the logarithmic function grows so slowly that it value remain small even for very large value of in particular according to formula it will take no more than log three way comparison to find an element of a given value or establish that there is no such element in any sorted array of one thousand element and it will take no more than log comparison to do it for any sorted array of size one what can we say about the average case efficiency of binary a so phisticated analysis show that the average number of key comparison made by binary search is only slightly smaller than that in the worst case cavgn log more accurate formula for the average number of comparison in a successful and an unsuccessful search are cayvegs n log n and canvogn logn though binary search is an optimal searching algorithm if we restrict our op erations only to comparison between key see section there are searching algorithm see interpolation search in section and hashing in section with a better average case time efficiency and one of them hashing doe not even re quire the array to be these algorithm do require some special calculation in addition to key comparison finally the idea behind binary search ha several application beyond searching see in addition it can be applied to solving nonlinear equation in one unknown we discus this continuous analogue of binary search called the method of bisection in section fake coin problem of several version of the fake coin identification problem we consider here the one that best illustrates the decrease by a constant factor among n identical looking coin one is with a balance scale we can compare any two set of that is by tipping to the left to the right or staying even the balance scale will tell whether the set weigh the same or which of the set is heavier than the other but not by how the problem is to design an efficient algorithm for detecting the fake an easier version of the problem the one we discus here assumes that the fake coin is known to be say lighter than the genuine the most natural idea for solving this problem is to divide n coin into two pile of n coin each leaving one extra coin aside if n is odd and put the two a much more challenging version assumes no additional information about the relative weight of the fake and genuine coin or even the presence of the fake coin among n given we pursue this more difficult version in the exercise for section pile on the if the pile weigh the same the coin put aside must be fake otherwise we can proceed in the same manner with the lighter pile which must be the one with the fake we can easily set up a recurrence relation for the number of weighing w n needed by this algorithm in the worst case w n w n for n w this recurrence should look familiar to indeed it is almost identical to the one for the worst case number of comparison in binary the difference is in the initial this similarity is not really surprising since both algorithm are based on the same technique of halving an instance the solution to the recurrence for the number of weighing is also very similar to the one we had for binary search w n log n this stuff should look elementary by now if not outright but wait the interesting point here is the fact that the above algorithm is not the most efficient it would be more efficient to divide the coin not into two but into three pile of about n coin detail of a precise formulation are developed in this section do not miss if your instructor forgets demand the instructor to assign problem after weighing two of the pile we can reduce the instance size by a factor of accordingly we should expect the number of weighing to be about log n which is smaller than log russian peasant multiplication now we consider a nonorthodox algorithm for multiplying two positive integer called multiplication a la russe or the russian peasant let n and m be positive integer whose product we want to compute and let u measure the instance size by the value of now if n is even an instance of half the size ha to deal with n and we have an obvious formula relating the solution to the problem larger instance to the solution to the smaller one n m n if n is odd we need only a slight adjustment of this formula n m n m using these formula and the trivial case of m m to stop we can compute product n m either recursively or an example of computing with this algorithm is given in figure note that all the extra addend shown in parenthesis in figure are in the row that have odd value in the first therefore we can find the product by simply adding all the element in the m column that have an odd number in the n column figure also note that the algorithm involves just the simple operation of halving doubling and adding a feature that might be attractive for example to those n m n m a b figure computing by the russian peasant who do not want to memorize the table of it is this feature of the algorithm that most probably made it attractive to russian peasant who accord ing to western visitor used it widely in the nineteenth century and for whom the method is in fact the method wa known to egyptian mathematician a early a cha it also lead to very fast hardware implementa tion since doubling and halving of binary number can be performed using shift which are among the most basic operation at the machine josephus problem our last example is the josephus problem named for flavius josephus a famous jewish historian who participated in and chronicled the jewish revolt of against the josephus a a general managed to hold the fortress of jotapata for day but after the fall of the city he took refuge with diehard in a nearby there the rebel voted to perish rather than josephus proposed that each man in turn should dispatch his neighbor the order to be determined by casting josephus contrived to draw the last lot and a one of the two surviving men in the cave he prevailed upon his intended victim to surrender to the so let n people numbered to n stand in a starting the grim count with person number we eliminate every second person until only one survivor is the problem is to determine the survivor number j for example figure if n is people in position and will be eliminated on the first pas through the circle and people in initial position and will be eliminated on the second pas leaving a sole survivor in initial position thus j to give another example if n is people in position and will be eliminated on the first pas it is more convenient to include in the first pas and people in position and for convenience on the second thus j a b figure instance of the josephus problem for a n and b n subscript number indicate the pas on which the person in that position is the solution are j and j it is convenient to consider the case of even and odd n if n is even n k the first pas through the circle yield an instance of exactly the same problem but half it initial the only difference is in position numbering for example a person in initial position will be in position for the second pas a person in initial position will be in position and so on check figure it is easy to see that to get the initial position of a person we simply need to multiply his new position by and subtract this relationship will hold in particular for the survivor j k j k let u now consider the case of an odd n n n k the first pas eliminates people in all even if we add to this the elimination of the person in position right after that we are left with an instance of size here to get the initial position that corresponds to the new position numbering we have to multiply the new position number by and add check figure thus for odd value of n we get j k j k can we get a closed form solution to the two case recurrence subject to the initial condition j the answer is yes though getting it requires more ingenuity than just applying backward in fact one way to find a solution is to apply forward substitution to get say the first value of j n discern a pattern and then prove it general validity by mathematical we leave the execution of this plan to the exercise alternatively you can look it up in gkp whose exposition of the josephus problem we have been interestingly the most elegant form of the closed form answer involves the binary representation of size n j n can be obtained by a bit cyclic shift left of n for example j j and j j exercise cutting a stick a stick n inch long need to be cut into n inch outline an algorithm that performs this task with the minimum number of cut if several piece of the stick can be cut at the same also give a formula for the minimum number of design a decrease by half algorithm for computing log n and determine it time what is the largest number of key comparison made by binary search in searching for a key in the following list all the key of this array that will require the largest number of key comparison when searched for by binary find the average number of key comparison made by binary search in a successful search in this assume that each key is searched for with the same find the average number of key comparison made by binary search in an unsuccessful search in this assume that search for key in each of the interval formed by the array element are equally estimate how many time faster an average successful search will be in a sorted array of one million element if it is done by binary search versus sequential the time efficiency of sequential search doe not depend on whether a list is implemented a an array or a a linked is it also true for searching a sorted list by binary design a version of binary search that us only two way comparison such a and implement your algorithm in the language of your choice and carefully debug it such program are notorious for being prone to analyze the time efficiency of the two way comparison version designed in part picture guessing a version of the popular problem solving task involves pre senting people with an array of picture seven row of six picture each and asking them to identify the target picture by asking question that can be answered yes or further people are then required to identify the picture with a few question a suggest the most efficient algorithm for this problem and indicate the largest number of question that may be consider ternary search the following algorithm for searching in a sorted array if n simply compare the search key k with the single element of the array otherwise search recursively by comparing k with a n and if k is larger compare it with a n to determine in which third of the array to continue the what design technique is this algorithm based set up a recurrence for the number of key comparison in the worst you may assume that n solve the recurrence for n compare this algorithm efficiency with that of binary an array contains n integer from to n in increasing thus one integer in this range is design the most efficient algorithm you can to find the missing integer and indicate it time write pseudocode for the divide into three algorithm for the fake coin make sure that your algorithm handle properly all value of n not only those that are multiple of set up a recurrence relation for the number of weighing in the divide into three algorithm for the fake coin problem and solve it for n for large value of n about how many time faster is this algorithm than the one based on dividing coin into two your answer should not depend on apply the russian peasant algorithm to compute from the standpoint of time efficiency doe it matter whether we multiply n by m or m by n by the russian peasant write pseudocode for the russian peasant multiplication what is the time efficiency class of russian peasant find j the solution to the josephus problem for n prove that the solution to the josephus problem is for every n that is a power of for the josephus problem compute j n for n discern a pattern in the solution for the first fifteen value of n and prove it general prove the validity of getting j n by a bit cyclic shift left of the binary representation of variable size decrease algorithm in the third principal variety of decrease and conquer the size reduction pattern varies from one iteration of the algorithm to euclid algorithm for computing the greatest common divisor section provides a good example of this kind of in this section we encounter a few more example of this computing a median and the selection problem the selection problem is the problem of finding the kth smallest element in a list of n this number is called the kth order of course for k or k n we can simply scan the list in question to find the smallest or largest element a more interesting case of this problem is for k n which asks to find an element that is not larger than one half of the list element and not smaller than the other this middle value is called the median and it is one of the most important notion in mathematical obviously we can find the kth smallest element in a list by sorting the list first and then selecting the kth element in the output of a sorting the time of such an algorithm is determined by the efficiency of the sorting algorithm thus with a fast sorting algorithm such a mergesort discussed in the next chapter the algorithm efficiency is in on log you should immediately suspect however that sorting the entire list is most likely overkill since the problem asks not to order the entire list but just to find it kth smallest indeed we can take advantage of the idea of partitioning a given list around some value p of say it first in general this is a rearrangement of the list element so that the left part contains all the element smaller than or equal to p followed by the pivot p itself followed by all the element greater than or equal to p all are p p all are p of the two principal algorithmic alternative to partition an array here we discus the lomuto partitioning ben we introduce the better known hoares algorithm in the next to get the idea behind the lomuto parti tioning it is helpful to think of an array or more generally a subarray l r n under consideration a composed of three contiguous seg listed in the order they follow pivot p they are a follows a segment with element known to be smaller than p the segment of element known to be greater than or equal to p and the segment of element yet to be compared to p see fig ure note that the segment can be empty for example it is always the case for the first two segment before the algorithm starting with i l the algorithm scan the subarray left to right maintaining this structure until a partition is on each iteration it com pares the first element in the unknown segment pointed to by the scanning index i in figure with the pivot if ai p i is simply incremented to expand the segment of the element greater than or equal to p while shrinking the un processed if ai p it is the segment of the element smaller than p that need to be this is done by incrementing s the index of the last l s i r p p p a l s r p p p b l s r p p p c figure illustration of the lomuto element in the first segment swapping ai and a and then incrementing i to point to the new first element of the shrunk unprocessed after no unprocessed element remain figure the algorithm swap the pivot with a to achieve a partition being sought figure here is pseudocode implementing this partitioning algorithm lomutopartitional partition subarray by lomutos algorithm using first element a pivot input a subarray of array defined by it left and right index l and r l r output partition of and the new position of the pivot p al sl for i l to r do if ai p s s swapas ai swapal a return s how can we take advantage of a list partition to find the kth smallest element in let u assume that the list is implemented a an array whose element are indexed starting with a and let s be the partition split position the index of the array element occupied by the pivot after if s k pivot p itself is obviously the kth smallest element which solves the if s k the kth smallest element in the entire array can be found a the kth smallest element in the left part of the partitioned and if s k it can be found a the k sth smallest element in it right thus if we do not solve the problem outright we reduce it instance to a smaller one which can be solved by the same approach this algorithm is called to find the kth smallest element in array by this algorithm call k where algorithm k solves the selection problem by recursive partition based algorithm input subarray of array of orderable element and integer k k r l output the value of the kth smallest element in s or another partition algorithm if s k return a else if s l k k else quickselectas k s in fact the same idea can be implemented without recursion a for the nonrecursive version we need not even adjust the value of k but just continue until s k example apply the partition based algorithm to find the median of the fol lowing list of nine number here k and our task is to find the th smallest element in the we use the above version of array partitioning showing the pivot in s i s i s i s i s i since s is smaller than k we proceed with the right part of the array s i s i s i now s k and hence we can stop the found median is which is greater than and but smaller than and how efficient is partitioning an n element array always requires n key if it produce the split that solves the selection problem without requiring more iteration then for this best case we obtain cbestn n unfortunately the algorithm can produce an extremely unbalanced partition of a given array with one part being empty and the other containing n in the worst case this can happen on each of the n for a specific example of the worst case input consider say the case of k n and a strictly increasing this implies that cworstn n n n n n which compare poorly with the straightforward sorting based approach mentioned in the beginning of our selection problem thus the usefulness of the partition based algorithm depends on the algorithm efficiency in the average fortunately a careful mathematical analysis ha shown that the average case efficiency is in fact computer scientist have discovered a more sophisticated way of choosing a pivot in quickselect that guarantee linear time even in the worst case blo but it is too complicated to be recommended for practical it is also worth noting that the partition based algorithm solves a somewhat more general problem of identifying the k smallest and n k largest element of a given list not just the value of it kth smallest interpolation search a the next example of a variable size decrease algorithm we consider an algorithm for searching in a sorted array called interpolation unlike binary search which always compare a search key with the middle value of a given sorted array and hence reduces the problem instance size by half interpolation search take into account the value of the search key in order to find the array element to be compared with the search in a sense the algorithm mimic the way we value ar v al l x r index figure index computation in interpolation search for a name in a telephone book if we are searching for someone named brown we open the book not in the middle but very close to the beginning unlike our action when searching for someone named say more precisely on the iteration dealing with the array portion between the leftmost element al and the rightmost element ar the algorithm assumes that the array value increase linearly along the straight line through the point l al and r the accuracy of this assumption can influence the algorithm efficiency but not it accordingly the search key value v is compared with the element whose index is computed a the round off of the x coordinate of the point on the straight line through the point l al and r ar whose y coordinate is equal to the search value v figure writing down a standard equation for the straight line passing through the point l al and r ar substituting v for y and solving it for x lead to the following formula xl v alr l ar al the logic behind this approach is quite we know that the array value are increasing more accurately not decreasing from al to ar but we do not know how they do had these value increased linearly which is the simplest manner possible the index computed by formula would be the expected location of the array element with the value equal to of course if v is not between al and ar formula need not be applied after comparing v with ax the algorithm either stop if they are equal or proceeds by searching in the same manner among the element indexed either between l and x or between x and r depending on whether ax is smaller or larger than thus the size of the problem instance is reduced but we cannot tell a priori by how the analysis of the algorithm efficiency show that interpolation search us fewer than log log n key comparison on the average when searching in a list of n random this function grows so slowly that the number of comparison is a very small constant for all practically feasible input see problem in this section but in the worst case interpolation search is only linear which must be considered a bad performance assessing the worthiness of interpolation search versus that of binary search robert sedgewick wrote in the second edition of his algorithm that binary search is probably better for smaller file but interpolation search is worth considering for large file and for application where comparison are particularly expensive or access cost are very note that in section we discus a continuous counterpart of interpolation search which can be seen a one more example of a variable size decrease searching and insertion in a binary search tree let u revisit the binary search recall that this is a binary tree whose node contain element of a set of orderable item one element per node so that for every node all element in the left subtree are smaller and all the element in the right subtree are greater than the element in the subtrees when we need to search for an element of a given value v in such a tree we do it recursively in the following if the tree is empty the search end in if the tree is not empty we compare v with the tree root if they match a desired element is found and the search can be stopped if they do not match we continue with the search in the left subtree of the root if v kr and in the right subtree if v thus on each iteration of the algorithm the problem of searching in a binary search tree is reduced to searching in a smaller binary search the most sensible measure of the size of a search tree is it height obviously the decrease in a tree height normally change from one iteration to another of the binary tree search thus giving u an excellent example of a variable size decrease in the worst case of the binary tree search the tree is severely this happens in particular if a tree is constructed by successive insertion of an increasing or decreasing sequence of key figure obviously the search for an in such a tree requires n comparison making the worst case efficiency of the search operation fall into fortunately the average case efficiency turn out to be in log more precisely the number of key comparison needed for a search in a binary search tree built from n random key is about ln n log since insertion of a new key into a binary search tree is almost identical to that of searching there it also exemplifies the variablesize decrease technique and ha the same efficiency characteristic a the search a a a a an an an an a b figure binary search tree for a an increasing sequence of key and b a decreasing sequence of the game of nim there are several well known game that share the following there are two player who move in no randomness or hidden information is permitted all player know all information about a game is impartial each player ha the same move available from the same game each of a finite number of available move lead to a smaller instance of the same the game end with a win by one of the player there are no the winner is the last player who is able to a prototypical example of such game is generally the game is played with several pile of chip but we consider the one pile version thus there is a single pile of n two player take turn by removing from the pile at least one and at most m chip the number of chip taken may vary from one move to another but both the lower and upper limit stay the who win the game by taking the last chip the player moving first or second if both player make the best move let u call an instance of the game a winning position for the player to move next if that player ha a winning strategy a sequence of move that result in a victory no matter what move the opponent let u call an instance of the game a losing position for the player to move next if every move available for that player lead to a winning position for the the standard approach to determining which position are winning and which are losing is to investigate small value of n it is logical to consider the instance of n a a losing one for the player to move next because this player is the first one who cannot make a any instance with n m chip is obviously a winning position for the player to move next the instance with n m chip is a losing one because taking any allowed number of chip put the opponent in a winning see an illustration for m in figure any instance with m n m chip is a winning position for the player to move next because there is a move that leaf the opponent with m chip which is a losing figure illustration of one pile nim with the maximum number of chip that may be taken on each move m the number indicate n the number of chip in the the losing position for the player to move are only winning move from the winning position are shown in m m chip is the next losing position and so it is not difficult to see the pattern that can be formally proved by mathematical induction an instance with n chip is a winning position for the player to move next if and only if n is not a multiple of m the winning strategy is to take n modm chip on every move any deviation from this strategy put the opponent in a winning one pile nim ha been known for a very long it appeared in particular a the summation game in the first published book on recreational mathematics authored by claude gaspar bachet a french aristocrat and mathematician in a player pick a positive integer le than say and then his opponent and he take turn adding any integer le than the first player to reach exactly is the winner in general nim is played with i pile of chip of size n n ni on each move a player can take any available number of chip including all of them from any single the goal is the same to be the last player able to make a note that for i it is easy to figure out who win this game and here is a hint the answer for the game instance with n n differs from the answer for those with n a solution to the general case of nim is quite unexpected because it is based on the binary representation of the pile let b b bi be the pile size in compute their binary digital sum also known a the nim sum defined a the sum of binary digit discarding any in other word a binary digit si in the sum is if the number of s in the ith position in the addend is even and it is if the number of s is it turn out that an instance of nim is a winning one for the player to move next if and only if it nim sum contains at least one consequently nim instance is a losing instance if and only if it nim sum contains only for example for the commonly played instance with n n n the nim sum is since this sum contains a the instance is a winning one for the player moving to find a winning move from this position the player need to change one of the three bit string so that the new nim sum contains only it is not difficult to see that the only way to accomplish this is to remove two chip from the first this ingenious solution to the game of nim wa discovered by harvard math ematics professor bouton more than year since then mathemati cians have developed a much more general theory of such an excellent account of this theory with application to many specific game is given in the monograph by berlekamp conway and guy exercise if we measure an instance size of computing the greatest common divisor of m and n by the size of the second number n by how much can the size decrease after one iteration of euclid prove that an instance size will always decrease at least by a factor of two after two successive iteration of euclid apply quickselect to find the median of the list of number write pseudocode for a nonrecursive implementation of derive the formula underlying interpolation give an example of the worst case input for interpolation search and show that the algorithm is linear in the worst find the smallest value of n for which log log n is greater than determine which if any of the following assertion are true log log n olog n log log n log n log log n log n outline an algorithm for finding the largest key in a binary search would you classify your algorithm a a variable size decrease what is the time efficiency class of your algorithm in the worst outline an algorithm for deleting a key from a binary search would you classify this algorithm a a variable size decrease what is the time efficiency class of your algorithm in the worst outline a variable size decrease algorithm for constructing an eulerian circuit in a connected graph with all vertex of even misere one pile nim consider the so called misere version of the one pile nim in which the player taking the last chip loses the all the other condition of the game remain the same the pile contains n chip and on each move a player take at least one but no more than m identify the winning and losing position for the player to move next in this moldy chocolate two player take turn by breaking an m n chocolate bar which ha one spoiled each break must be a single straight line cutting all the way across the bar along the boundary between the after each break the player who broke the bar last eats the piece that doe not contain the spoiled the player left with the spoiled square loses the is it better to go first or second in this write an interactive program to play this game with the your program should make a winning move in a winning position and a random legitimate move in a losing flipping pancake there are n pancake all of different size that are stacked on top of each you are allowed to slip a flipper under one of the pancake and flip over the whole stack above the the purpose is to arrange pancake according to their size with the biggest at the you can see a visualization of this puzzle on the interactive mathematics miscellany and puzzle site design an algorithm for solving this you need to search for a given number in an n n matrix in which every row and every column is sorted in increasing can you design a on algorithm for this laa summary decrease and conquer is a general algorithm design technique based on exploiting a relationship between a solution to a given instance of a problem and a solution to a smaller instance of the same once such a relationship is established it can be exploited either top down usually recursively or bottom there are three major variation of decrease and conquer decrease by a constant most often by one insertion sort decrease by a constant factor most often by the factor of two binary search variable size decrease euclid algorithm insertion sort is a direct application of the decrease by one and conquer technique to the sorting it is a n algorithm both in the worst and average case but it is about twice a fast on average than in the worst the algorithm notable advantage is a good performance on almost sorted a digraph is a graph with direction on it the topological sorting problem asks to list vertex of a digraph in an order such that for every edge of the digraph the vertex it start at is listed before the vertex it point this problem ha a solution if and only if a digraph is a dag directed acyclic graph it ha no directed there are two algorithm for solving the topological sorting the first one is based on depth first search the second is based on a direct application of the decrease by one the decrease by one technique is a natural approach to developing algo rithms for generating elementary combinatorial the most efficient class of such algorithm are minimal change however the num ber of combinatorial object grows so fast that even the best algorithm are of practical interest only for very small instance of such binary search is a very efficient algorithm for searching in a sorted it is a principal example of a decrease by a constant factor other example include exponentiation by squaring identifying a fake coin with a balance scale russian peasant multiplication and the josephus for some decrease and conquer algorithm the size reduction varies from one iteration of the algorithm to example of such variable size decrease algorithm include euclid algorithm the partition based algorithm for the selection problem interpolation search and searching and insertion in a binary search nim exemplifies game that proceed through a series of diminishing instance of the same divide and conquer whatever man prays for he prays for a every prayer reduces itself to this great god grant that twice two be not ivan turgenev russian novelist and short story writer divide and conquer is probably the best known general algorithm design though it fame may have something to do with it catchy name it is well deserved quite a few very efficient algorithm are specific implementation of this general divide and conquer algorithm work according to the following general plan a problem is divided into several subproblems of the same type ideally of about equal the subproblems are solved typically recursively though sometimes a dif ferent algorithm is employed especially when subproblems become small if necessary the solution to the subproblems are combined to get a solution to the original the divide and conquer technique is diagrammed in figure which depicts the case of dividing a problem into two smaller subproblems by far the most widely occurring case at least for divide and conquer algorithm designed to be executed on a single processor a an example let u consider the problem of computing the sum of n number a an if n we can divide the problem into two instance of the same problem to compute the sum of the first n number and to compute the sum of the remaining n of course if n we simply return a a the once each of these two sum is computed by applying the same method recursively we can add their value to get the sum in question a an a a n a n an is this an efficient way to compute the sum of n a moment of reflection why could it be more efficient than the brute force a problem of size n subproblem subproblem of size n of size n solution to solution to subproblem subproblem solution to the original problem figure divide and conquer technique typical small example of summing say four number by this algorithm a formal analysis which follows and common sense we do not normally compute sum this way do all lead to a negative answer to this thus not every divide and conquer algorithm is necessarily more efficient than even a brute force but often our prayer to the goddess of algorithmics see the chapter epigraph are answered and the time spent on executing the divide and conquer plan turn out to be significantly smaller than solving a problem by a different in fact the divide and conquer approach yield some of the most important and efficient algorithm in computer we discus a few classic example of such algorithm in this though we consider only sequential algorithm here it is worth keeping in mind that the divide and conquer technique is ideally suited for parallel computation in which each subproblem can be solved simultaneously by it own actually the divide and conquer algorithm called the pairwise summation may substantially reduce the accumulated round off error of the sum of number that can be represented only approximately in a digital computer a mentioned above in the most typical case of divide and conquer a problem instance of size n is divided into two instance of size more generally an instance of size n can be divided into b instance of size nb with a of them needing to be here a and b are constant a and b assuming that size n is a power of b to simplify our analysis we get the following recurrence for the running time t n t n at nb f n where f n is a function that account for the time spent on dividing an instance of size n into instance of size nb and combining their for the sum example above a b and f n recurrence is called the general divide and conquer obviously the order of growth of it solution t n depends on the value of the constant a and b and the order of growth of the function f the efficiency analysis of many divide and conquer algorithm is greatly simplified by the following theorem see appendix master theorem if f n nd where d in recurrence then nd if a bd t n nd log n if a bd nlogb a if a analogous result hold for the o and notation for example the recurrence for the number of addition an made by the divide and conquer sum computation algorithm see above on input of size n k is an an thus for this example a b and d hence since a bd an nlogb a nlog note that we were able to find the solution efficiency class without going through the drudgery of solving the but of course this approach can only establish a solution order of growth to within an unknown multiplicative constant whereas solving a recurrence equation with a specific initial condition yield an exact answer at least for n that are power of it is also worth pointing out that if a recurrence cover decreaseby a constant factor algorithm discussed in the previous in fact some people consider such algorithm a binary search degenerate case of divide andconquer where just one of two subproblems of half the size need to be it is better not to do this and consider decrease by a constant factor and divideand conquer a different design mergesort mergesort is a perfect example of a successful application of the divide and conquer it sort a given array by dividing it into two half n and a n sorting each of them recursively and then merging the two smaller sorted array into a single sorted algorithm sort array by recursive mergesort input an array of orderable element output array sorted in nondecreasing order if n copy n to n copy a n to n n n mergeb c a see below the merging of two sorted array can be done a two pointer array index are initialized to point to the first element of the array being the element pointed to are compared and the smaller of them is added to a new array being constructed after that the index of the smaller element is incremented to point to it immediate successor in the array it wa copied this operation is repeated until one of the two given array is exhausted and then the remaining element of the other array are copied to the end of the new algorithm q merges two sorted array into one sorted array input array and both sorted output sorted array q of the element of b and c i j k while i p and j q do if bi cj ak bi i i else ak cj j j kk if i p copy to q else copy to q the operation of the algorithm on the list is illustrated in figure figure example of mergesort how efficient is assuming for simplicity that n is a power of the recurrence relation for the number of key comparison cn is cn cn cmergen for n c let u analyze cmergen the number of key comparison performed during the merging at each step exactly one comparison is made after which the total number of element in the two array still needing to be processed is reduced by in the worst case neither of the two array becomes empty before the other one contains just one element smaller element may come from the alternating therefore for the worst case cmergen n and we have the recurrence cworst n cworst n n for n cworst hence according to the master theorem cworstn n log n in fact it is easy to find the exact solution to the worst case recurrence for n k cworstn n log n n the number of key comparison made by mergesort in the worst case come very close to the theoretical minimum that any general comparison based sorting algorithm can for large n the number of comparison made by this algo rithm in the average case turn out to be about le see gon and hence is also in n log a noteworthy advantage of mergesort over quick sort and heapsort the two important advanced sorting algorithm to be discussed later is it stability see problem in this section the principal short coming of mergesort is the linear amount of extra storage the algorithm though merging can be done in place the resulting algorithm is quite complicated and of theoretical interest there are two main idea leading to several variation of first the algorithm can be implemented bottom up by merging pair of the array element then merging the sorted pair and so if n is not a power of only slight bookkeeping complication this avoids the time and space overhead of using a stack to handle recursive second we can divide a list to be sorted in more than two part sort each recursively and then merge them this scheme which is particularly useful for sorting file residing on secondary memory device is called multiway exercise write pseudocode for a divide and conquer algorithm for finding the po sition of the largest element in an array of n what will be your algorithm output for array with several element of the largest set up and solve a recurrence relation for the number of key comparison made by your how doe this algorithm compare with the brute force algorithm for this write pseudocode for a divide and conquer algorithm for finding value of both the largest and smallest element in an array of n set up and solve for n k a recurrence relation for the number of key comparison made by your how doe this algorithm compare with the brute force algorithm for this write pseudocode for a divide and conquer algorithm for the exponenti ation problem of computing an where n is a positive set up and solve a recurrence relation for the number of multiplication made by this a we shall see in section this theoretical minimum is log n log n how doe this algorithm compare with the brute force algorithm for this a mentioned in chapter logarithm base are irrelevant in most context arising in analyzing an algorithm efficiency is this true for both asser tions of the master theorem that include find the order of growth for solution of the following t n t n n t t n t n n t t n t n n t apply mergesort to sort the list e x a m p l e in alphabetical is mergesort a stable sorting solve the recurrence relation for the number of key comparison made by mergesort in the worst you may assume that n set up a recurrence relation for the number of key comparison made by mergesort on best case input and solve it for n set up a recurrence relation for the number of key move made by the version of mergesort given in section doe taking the number of key move into account change the algorithm efficiency let be an array of n real a pair ai aj is said to be an inversion if these number are out of order i j but ai aj design an on log n algorithm for counting the number of implement the bottom up version of mergesort in the language of your tromino puzzle a tromino more accurately a right tromino is an l shaped tile formed by three the problem is to cover any n n chess board with a missing square with trominoes can be oriented in an arbitrary way but they should cover all the square of the board except the missing one exactly and with no gol design a divide and conquer algorithm for this quicksort quicksort is the other important sorting algorithm that is based on the divide and conquer unlike mergesort which divide it input element according to their position in the array quicksort divide them according to their we already encountered this idea of an array partition in section where we discussed the selection a partition is an arrangement of the array element so that all the element to the left of some element a are le than or equal to a and all the element to the right of a are greater than or equal to it a a a a an all are a all are a obviously after a partition is achieved a will be in it final position in the sorted array and we can continue sorting the two subarrays to the left and to the right of a independently by the same note the difference with mergesort there the division of the problem into two subproblems is immediate and the entire work happens in combining their solution here the entire work happens in the division stage with no work required to combine the solution to the here is pseudocode of quicksort call where algorithm quicksortal sort a subarray by quicksort input subarray of array defined by it left and right index l and r output subarray sorted in nondecreasing order if l r s s is a split position quicksortas a a partition algorithm we can certainly use the lomuto partition discussed in section alternatively we can partition and more generally it subarray l r n by the more sophisticated method suggested by hoare the prominent british computer scientist who invented hoare at age invented his algorithm in while trying to sort word for a machine translation project from russian to say hoare my first thought on how to do this wa bubblesort and by an amazing stroke of luck my second thought wa it is hard to disagree with his overall assessment i have been very what a wonderful way to start a career in computing by discovering a new sorting twenty year later he received the turing award for fundamental contribution to the definition and design of programming language in he wa also knighted for service to education and computer a before we start by selecting a pivot an element with respect to whose value we are going to divide the there are several different strategy for selecting a pivot we will return to this issue when we analyze the algorithm for now we use the simplest strategy of selecting the subarrays first element p unlike the lomuto algorithm we will now scan the subarray from both end comparing the subarrays element to the the left to right scan denoted below by index pointer i start with the second since we want element smaller than the pivot to be in the left part of the subarray this scan skip over element that are smaller than the pivot and stop upon encountering the first element greater than or equal to the the right to left scan denoted below by index pointer j start with the last element of the since we want element larger than the pivot to be in the right part of the subarray this scan skip over element that are larger than the pivot and stop on encountering the first element smaller than or equal to the why is it worth stopping the scan after encountering an element equal to the because doing this tends to yield more even split for array with a lot of duplicate which make the algorithm run for example if we did otherwise for an array of n equal element we would have gotten a split into subarrays of size n and reducing the problem size just by after scanning the entire after both scan stop three situation may arise depending on whether or not the scanning index have if scanning index i and j have not crossed i j we simply exchange ai and aj and resume the scan by incrementing i and decrementing j respectively i j p all are p p p all are p if the scanning index have crossed over i j we will have partitioned the subarray after exchanging the pivot with aj j i p all are p p p all are p finally if the scanning index stop while pointing to the same element i j the value they are pointing to must be equal to p thus we have the subarray partitioned with the split position s i j ji p all are p p all are p we can combine the last case with the case of crossed over index i j by exchanging the pivot with aj whenever i j here is pseudocode implementing this partitioning algorithm hoarepartitional partition a subarray by hoares algorithm using the first element a a pivot input subarray of array defined by it left and right index l and r l r output partition of with the split position returned a this function value p al i l j r repeat repeat i i until ai p repeat j j until aj p swapai aj until i j swapai aj undo last swap when i j swapal aj return j note that index i can go out of the subarrays bound in this rather than checking for this possibility every time index i is incremented we can append to array a sentinel that would prevent index i from advancing beyond position note that the more sophisticated method of pivot selection mentioned at the end of the section make such a sentinel an example of sorting an array by quicksort is given in figure we start our discussion of quicksorts efficiency by noting that the number of key comparison made before a partition is achieved is n if the scanning index cross over and n if they coincide if all the split happen in the middle of corresponding subarrays we will have the best the number of key comparison in the best case satisfies the recurrence cbest n cbest n n for n cbest according to the master theorem cbestn n log n solving it exactly for n k yield cbestn n log in the worst case all the split will be skewed to the extreme one of the two subarrays will be empty and the size of the other will be just le than the size of the subarray being this unfortunate situation will happen in particular for increasing array for input for which the problem is already indeed if is a strictly increasing array and we use a a the pivot the left to right scan will stop on a while the right to left scan will go all the way to reach a indicating the split at position i j i j i j i j i j j i i j i r s i j i j i r i r s s j i i r i r i r i r ij s j i i r i r i j b i j j i a figure example of quicksort a array transformation with pivot shown in b tree of recursive call to quicksort with input value l and r of subarray bound and split position s of a partition j i a a an so after making n comparison to get to this partition and exchanging the pivot a with itself the algorithm will be left with the strictly increasing array to this sorting of strictly increasing array of diminishing size will continue until the last one an ha been the total number of key comparison made will be equal to cworst n n n n n thus the question about the utility of quicksort come down to it average case let cavgn be the average number of key comparison made by quicksort on a randomly ordered array of size a partition can happen in any position s s n after n comparison are made to achieve the after the partition the left and right subarrays will have s and n s element assuming that the partition split can happen in each position s with the same probability n we get the following recurrence relation n cavgn n n cavgs cavgn s for n s cavg cavg it solution which is much trickier than the worst and best case analysis turn out to be cavgn n ln n log thus on the average quicksort make only more comparison than in the best moreover it innermost loop is so efficient that it usually run faster than mergesort and heapsort another n log n algorithm that we discus in chapter on randomly ordered array of nontrivial this certainly justifies the name given to the algorithm by it because of quicksorts importance there have been persistent effort over the year to refine the basic among several improvement discovered by researcher are better pivot selection method such a randomized quicksort that us a random element or the median of three method that us the median of the leftmost rightmost and the middle element of the array switching to insertion sort on very small subarrays between and element for most computer system or not sorting small subarrays at all and finishing the algorithm with insertion sort applied to the entire nearly sorted array modification of the partitioning algorithm such a the three way partition into segment smaller than equal to and larger than the pivot see problem in this section exercise according to robert sedgewick sed the world leading expert on quicksort such improvement in combination can cut the running time of the algorithm by like any sorting algorithm quicksort ha it is not it requires a stack to store parameter of subarrays that are yet to be while the size of this stack can be made to be in olog n by always sorting first the smaller of two subarrays obtained by partitioning it is worse than the o space efficiency of although more sophisticated way of choosing a pivot make the quadratic running time of the worst case very unlikely they do not eliminate it and even the performance on randomly ordered array is known to be sensitive not only to implementation detail of the algorithm but also to both computer architecture and data still the januaryfebruary issue of computing in science engineering a joint publication of the american institute of physic and the ieee computer society selected quicksort a one of the algorithm with the greatest influence on the development and practice of science and engineering in the th exercise apply quicksort to sort the list e x a m p l e in alphabetical draw the tree of the recursive call for the partitioning procedure outlined in this section prove that if the scanning index stop while pointing to the same element i j the value they are pointing to must be equal to prove that when the scanning index stop j cannot point to an element more than one position to the left of the one pointed to by give an example showing that quicksort is not a stable sorting give an example of an array of n element for which the sentinel mentioned in the text is actually what should be it also explain why a single sentinel suffices for any for the version of quicksort given in this section are array made up of all equal element the worst case input the best case input or are strictly decreasing array the worst case input the best case input or for quicksort with the median of three pivot selection are strictly increas ing array the worst case input the best case input or answer the same question for strictly decreasing estimate how many time faster quicksort will sort an array of one million random number than insertion true or false for every n there are n element array that are sorted faster by insertion sort than by design an algorithm to rearrange element of a given array of n real num bers so that all it negative element precede all it positive your algorithm should be both time efficient and space the dutch national flag problem is to rearrange an array of character r w and b red white and blue are the color of the dutch national flag so that all the r come first the w come next and the b come dij design a linear in place algorithm for this explain how a solution to the dutch national flag problem can be used in implement quicksort in the language of your run your program on a sample of input to verify the theoretical assertion about the algorithm nut and bolt you are given a collection of n bolt of different width and n corresponding you are allowed to try a nut and bolt together from which you can determine whether the nut is larger than the bolt smaller than the bolt or match the bolt however there is no way to compare two nut together or two bolt the problem is to match each bolt to it design an algorithm for this problem with average case efficiency in n log raw important problem type in the limitless sea of problem one encounter in computing there are a few area that have attracted particular attention from by and large their interest ha been driven either by the problem practical importance or by some specific characteristic making the problem an interesting research subject fortunately these two motivating force reinforce each other in most in this section we are going to introduce the most important problem type sorting searching string processing graph problem combinatorial problem geometric problem numerical problem these problem are used in subsequent chapter of the book to illustrate different algorithm design technique and method of algorithm sorting the sorting problem is to rearrange the item of a given list in nondecreasing of course for this problem to be meaningful the nature of the list item must allow such an mathematician would say that there must exist a relation of total a a practical matter we usually need to sort list of number character from an alphabet character string and most important record similar to those maintained by school about their student library about their holding and company about their in the case of record we need to choose a piece of information to guide for example we can choose to sort student record in alphabetical order of name or by student number or by student grade point such a specially chosen piece of information is called a computer scientist often talk about sorting a list of key even when the list item are not record but say just why would we want a sorted to begin with a sorted list can be a required output of a task such a ranking internet search result or ranking student by their gpa further sorting make many question about the list easier to the most important of them is searching it is why dictionary telephone book class list and so on are you will see other example of the usefulness of list presorting in section in a similar vein sorting is used a an auxiliary step in several important algorithm in other area geometric algorithm and data the greedy approach an important algorithm design technique discussed later in the book requires a sorted by now computer scientist have discovered dozen of different sorting in fact inventing a new sorting algorithm ha been likened to designing the proverbial and i am happy to report that the hunt for a better sorting mousetrap this perseverance is admirable in view of the following on the one hand there are a few good sorting algorithm that sort an arbitrary array of size n using about n log n on the other hand no algorithm that sort by key comparison a opposed to say comparing small piece of key can do substantially better than there is a reason for this embarrassment of algorithmic rich in the land of although some algorithm are indeed better than others there is no algorithm that would be the best solution in all some of the algorithm are simple but relatively slow while others are faster but more complex some work better on randomly ordered input while others do better on almost sorted list some are suitable only for list residing in the fast memory while others can be adapted for sorting large file stored on a disk and so two property of sorting algorithm deserve special a sorting algorithm is called stable if it preserve the relative order of any two equal element in it in other word if an input list contains two equal element in position i and j where i j then in the sorted list they have to be in position i and j respectively such that i j this property can be desirable if for example we have a list of student sorted alphabetically and we want to sort it according to student gpa a stable algorithm will yield a list in which student with the same gpa will still be sorted generally speaking algorithm that can exchange key located far apart are not stable but they usually work faster you will see how this general comment applies to important sorting algorithm later in the the second notable feature of a sorting algorithm is the amount of extra memory the algorithm an algorithm is said to be in place if it doe not require extra memory except possibly for a few memory there are important sorting algorithm that are in place and those that are searching the searching problem deal with finding a given value called a search key in a given set or a multiset which permit several element to have the same there are plenty of searching algorithm to choose they range from the straightforward sequential search to a spectacularly efficient but limited binary search and algorithm based on representing the underlying set in a different form more conducive to the latter algorithm are of particular importance for real world application because they are indispensable for storing and retriev ing information from large for searching too there is no single algorithm that fit all situation some algorithm work faster than others but require more memory some are very fast but applicable only to sorted array and so unlike with sorting algorithm there is no stability problem but different issue specifically in application where the underlying data may change frequently relative to the number of search searching ha to be considered in conjunction with two other operation an addition to and deletion from the data set of an in such situation data structure and algorithm should be chosen to strike a balance among the requirement of each also organizing very large data set for efficient searching pose special challenge with important implication for real world string processing in recent decade the rapid proliferation of application dealing with nonnumer ical data ha intensified the interest of researcher and computing practitioner in string handling a string is a sequence of character from an string of particular interest are text string which comprise letter number and special character bit string which comprise zero and one and gene sequence which can be modeled by string of character from the four character alphabet a c g it should be pointed out however that string processing algorithm have been important for computer science for a long time in conjunction with computer language and compiling one particular problem that of searching for a given word in a text ha attracted special attention from they call it string several algorithm that exploit the special nature of this type of searching have been we introduce one very simple algorithm in chapter and discus two algorithm based on a remarkable idea by boyer and moore in chapter graph problem one of the oldest and most interesting area in algorithmics is graph informally a graph can be thought of a a collection of point called vertex some of which are connected by line segment called a more formal definition is given in the next graph are an interesting subject to study for both theoretical and practical graph can be used for modeling a wide variety of application including transportation communication social and economic network project scheduling and studying different technical and social aspect of the internet in particular is one of the active area of current research involving computer scientist economist and social scientist see basic graph algorithm include graph traversal algorithm how can one reach all the point in a shortest path algorithm what is the best route between two and topological sorting for graph with directed edge is a set of course with their prerequisite consistent or self fortunately these algorithm can be considered illustration of general design technique accordingly you will find them in corresponding chapter of the some graph problem are computationally very hard the most well known example are the traveling salesman problem and the graph coloring the traveling salesman problem tsp is the problem of finding the shortest tour through n city that visit every city exactly in addition to obvious application involving route planning it arises in such modern application a circuit board and vlsi chip fabrication x ray crystallography and genetic the graph coloring problem seek to assign the smallest number of color to the vertex of a graph so that no two adjacent vertex are the same this problem arises in several application such a event scheduling if the event are represented by vertex that are connected by an edge if and only if the corresponding event cannot be scheduled at the same time a solution to the graph coloring problem yield an optimal combinatorial problem from a more abstract perspective the traveling salesman problem and the graphcoloring problem are example of combinatorial these are problem that ask explicitly or implicitly to find a combinatorial object such a a permutation a combination or a subset that satisfies certain a desired combinatorial object may also be required to have some additional property such a a maximum value or a minimum generally speaking combinatorial problem are the most difficult problem in computing from both a theoretical and practical their difficulty stem from the following first the number of combinatorial object typically grows extremely fast with a problem size reaching unimaginable magnitude even for moderate sized second there are no known algorithm for solving most such problem exactly in an acceptable amount of moreover most computer scientist believe that such algorithm do not this conjecture ha been neither proved nor disproved and it remains the most important unresolved issue in theoretical computer we discus this topic in more detail in section some combinatorial problem can be solved by efficient algorithm but they should be considered fortunate exception to the the shortest path problem mentioned earlier is among such geometric problem geometric algorithm deal with geometric object such a point line and poly the ancient greek were very much interested in developing procedure they did not call them algorithm of course for solving a variety of geometric problem including problem of constructing simple geometric shape triangle circle and so on with an unmarked ruler and a then for about year intense interest in geometric algorithm disappeared to be resurrected in the age of computer no more ruler and compass just bit byte and good old human of course today people are interested in geometric algorithm with quite different application in mind such a computer graphic robotics and we will discus algorithm for only two classic problem of computational geometry the closest pair problem and the convex hull the closest pair problem is self explanatory given n point in the plane find the closest pair among the convex hull problem asks to find the smallest convex polygon that would include all the point of a given if you are interested in other geometric algorithm you will find a wealth of material in such specialized monograph a deb oro and numerical problem numerical problem another large special area of application are problem that involve mathematical object of continuous nature solving equation and system of equation computing definite integral evaluating function and so the majority of such mathematical problem can be solved only another principal difficulty stem from the fact that such problem typically require manipulating real number which can be represented in a computer only moreover a large number of arithmetic operation performed on approximately represented number can lead to an accumulation of the round off error to a point where it can drastically distort an output produced by a seemingly sound many sophisticated algorithm have been developed over the year in this area and they continue to play a critical role in many scientific and engineering but in the last year or so the computing industry ha shifted it focus to business these new application require primarily algorithm for information storage retrieval transportation through network and presentation to a a result of this revolutionary change numerical analysis ha lost it formerly dominating position in both industry and computer science still it is important for any computer literate person to have at least a rudimentary idea about numerical we discus several classical numerical algorithm in section and exercise consider the algorithm for the sorting problem that sort an array by counting for each of it element the number of smaller element and then us this information to put the element in it appropriate position in the sorted array algorithm sort an array by comparison counting input array of orderable value output array of a element sorted in nondecreasing order for i to n do counti for i to n do for j i to n do if ai aj countj countj else counti counti for i to n do scounti ai return s apply this algorithm to sorting the list is this algorithm is it in name the algorithm for the searching problem that you already give a good succinct description of each algorithm in if you know no such algorithm use this opportunity to design design a simple algorithm for the string matching ko nigsberg bridge the ko nigsberg bridge puzzle is universally accepted a the problem that gave birth to graph it wa solved by the great swiss born mathematician leonhard euler the problem asked whether one could in a single stroll cross all seven bridge of the city of ko nigsberg exactly once and return to a starting following is a sketch of the river with it two island and seven bridge state the problem a a graph doe this problem have a if you believe it doe draw such a stroll if you believe it doe not explain why and indicate the smallest number of new bridge that would be required to make such a stroll icosian game a century after euler discovery see problem another famous puzzle this one invented by the renowned irish mathematician sir william hamilton wa presented to the world under the name of the icosian the game board wa a circular wooden board on which the following graph wa carved find a hamiltonian circuit a path that visit all the graph vertex exactly once before returning to the starting vertex for this consider the following problem design an algorithm to determine the best route for a subway passenger to take from one designated station to another in a well developed subway system similar to those in such city a washington and london the problem statement is somewhat vague which is typical of real life in particular what reasonable criterion can be used for defining the best how would you model this problem by a rephrase the traveling salesman problem in combinatorial object rephrase the graph coloring problem in combinatorial object consider the following map b a d c e f explain how we can use the graph coloring problem to color the map so that no two neighboring region are colored the use your answer to part a to color the map with the smallest number of design an algorithm for the following problem given a set of n point in the cartesian plane determine whether all of them lie on the same write a program that read a it input the x y coordinate of the endpoint of two line segment pq and pq and determines whether the segment have a common binary tree traversal and related property in this section we see how the divide and conquer technique can be applied to binary a binary tree t is defined a a finite set of node that is either empty or consists of a root and two disjoint binary tree tl and tr called respectively the left and right subtree of the we usually think of a binary tree a a special case of an ordered tree figure this standard interpretation wa an alternative definition of a binary tree in section since the definition itself divide a binary tree into two smaller structure of the same type the left subtree and the right subtree many problem about binary tree can be solved by applying the divide and conquer a an example let u consider a recursive algorithm for computing the height of a binary recall that the height is defined a the length of the longest path from the root to a hence it can be computed a the maximum of the height of the root left tleft tright figure standard representation of a binary and right subtrees plus we have to add to account for the extra level of the also note that it is convenient to define the height of the empty tree a thus we have the following recursive algorithm heightt computes recursively the height of a binary tree input a binary tree t output the height of t if t return else return maxheighttlef t heighttright we measure the problem instance size by the number of node nt in a given binary tree t obviously the number of comparison made to compute the maximum of two number and the number of addition ant made by the algorithm are the we have the following recurrence relation for ant ant antlef t antright for nt a before we solve this recurrence can you tell what it solution let u note that addition is not the most frequently executed operation of this what checking and this is very typical for binary tree algorithm that the tree is not for example for the empty tree the comparison t is executed once but there are no addition and for a single node tree the comparison and addition number are and it help in the analysis of tree algorithm to draw the tree extension by replacing the empty subtrees by special the extra node shown by little square in figure are called external the original node shown by little circle are called by definition the extension of the empty binary tree is a single external it is easy to see that the height algorithm make exactly one addition for every internal node of the extended tree and it make one comparison to check whether a b figure binary tree on the left and it extension on the internal node are shown a circle external node are shown a the tree is empty for every internal and external therefore to ascertain the algorithm efficiency we need to know how many external node an extended binary tree with n internal node can after checking figure and a few similar example it is easy to hypothesize that the number of external node x is always more than the number of internal node n x n to prove this equality consider the total number of node both internal and since every node except the root is one of the two child of an internal node we have the equation n x n which immediately implies equality note that equality also applies to any nonempty full binary tree in which by definition every node ha either zero or two child for a full binary tree n and x denote the number of parental node and leaf returning to algorithm height the number of comparison to check whether the tree is empty is cn n x n and the number of addition is an the most important divide and conquer algorithm for binary tree are the three classic traversal preorder inorder and all three traversal visit node of a binary tree recursively by visiting the tree root and it left and right they differ only by the timing of the root visit in the preorder traversal the root is visited before the left and right subtrees are visited in that in the inorder traversal the root is visited after visiting it left subtree but before visiting the right in the postorder traversal the root is visited after visiting the left and right subtrees in that these traversal are illustrated in figure their pseudocodes are quite straightforward repeating the description given these traversal are also a standard feature of data structure a to their efficiency analysis it is identical to the above analysis of the height algorithm because a recursive call is made for each node of an extended binary finally we should note that obviously not all question about binary tree require traversal of both left and right for example the search and insert operation for a binary search tree require processing only one of the two accordingly we considered them in section not a application of divide and conquer but rather a example of the variable size decrease a b c preorder a b d g e c f inorder d g b e a f c d e f postorder g d e b f c a g figure binary tree and it exercise design a divide and conquer algorithm for computing the number of level in a binary in particular the algorithm must return and for the empty and single node tree what is the time efficiency class of your the following algorithm seek to compute the number of leaf in a binary algorithm leafcountert computes recursively the number of leaf in a binary tree input a binary tree t output the number of leaf in t if t return else return leafcountertlef t leafcountertright is this algorithm if it is prove it if it is not make an appropriate can you compute the height of a binary tree with the same asymptotic ef ficiency a the section divide and conquer algorithm but without using a stack explicitly or of course you may use a different algorithm prove equality by mathematical traverse the following binary tree in in in a b c d e f write pseudocode for one of the classic traversal algorithm preorder in order and postorder for binary assuming that your algorithm is recur sive find the number of recursive call which of the three classic traversal algorithm yield a sorted list if applied to a binary search prove this draw a binary tree with node labeled in such a way that the inorder and postorder traversal of the tree yield the following list inorder and give an example of two permutation of the same n label n that cannot be inorder and postorder traversal list of the same binary design an algorithm that construct a binary tree for which two given list of n label n are generated by the inorder and postorder traversal of the your algorithm should also identify input for which the problem ha no the internal path length i of an extended binary tree is defined a the sum of the length of the path taken over all internal node from the root to each internal similarly the external path length e of an extended binary tree is defined a the sum of the length of the path taken over all external node from the root to each external prove that e i n where n is the number of internal node in the write a program for computing the internal path length of an extended binary use it to investigate empirically the average number of key comparison for searching in a randomly generated binary search chocolate bar puzzle given an n m chocolate bar you need to break it into nm you can break a bar only in a straight line and only one bar can be broken at a design an algorithm that solves the problem with the minimum number of bar what is this minimum justify your answer by using property of a binary multiplication of large integer and strassens matrix multiplication in this section we examine two surprising algorithm for seemingly straightfor ward task multiplying two integer and multiplying two square both achieve a better asymptotic efficiency by ingenious application of the divide and conquer multiplication of large integer some application notably modern cryptography require manipulation of inte gers that are over decimal digit since such integer are too long to fit in a single word of a modern computer they require special this practi cal need support investigation of algorithm for efficient manipulation of large in this section we outline an interesting algorithm for multiplying such obviously if we use the conventional pen and pencil algorithm for mul tiplying two n digit integer each of the n digit of the first number is multiplied by each of the n digit of the second number for the total of n digit if one of the number ha fewer digit than the other we can pad the shorter number with leading zero to equalize their though it might appear that it would be impossible to design an algorithm with fewer than n digit multiplica tions this turn out not to be the the miracle of divide and conquer come to the rescue to accomplish this to demonstrate the basic idea of the algorithm let u start with a case of two digit integer say and these number can be represented a follows and now let u multiply them the last formula yield the correct answer of of course but it us the same four digit multiplication a the pen and pencil fortunately we can compute the middle term with just one digit multiplication by taking advantage of the product and that need to be computed anyway of course there is nothing special about the number we just for any pair of two digit number a aa and b bb their product c can be computed by the formula c a b c c c where c a b is the product of their first digit c a b is the product of their second digit c a a b b c c is the product of the sum of the a digit and the sum of the b digit minus the sum of c and now we apply this trick to multiplying two n digit integer a and b where n is a positive even let u divide both number in the middle after all we promised to take advantage of the divide and conquer we denote the first half of the a digit by a and the second half by a for b the notation are b and b in these notation a aa implies that a an a and b bb implies that b bn therefore taking advantage of the same trick we used for two digit number we get c a b an a bn b a bn a b a bn a b cn cn c where c a b is the product of their first half c a b is the product of their second half c a a b b c c is the product of the sum of the a half and the sum of the b half minus the sum of c and if n is even we can apply the same method for computing the product c c and thus if n is a power of we have a recursive algorithm for computing the product of two n digit in it pure form the recursion is stopped when n becomes it can also be stopped when we deem n small enough to multiply the number of that size how many digit multiplication doe this algorithm since multiplica tion of n digit number requires three multiplication of n digit number the recurrence for the number of multiplication mn is mn mn for n m solving it by backward substitution for n k yield mk mk mk mk imk i kmk k since k log n mn log n nlog on the last step we took advantage of the following property of logarithm alogb c clogb but what about addition and have we not decreased the num ber of multiplication by requiring more of those let an be the number of digit addition and subtraction executed by the above algorithm in multiplying two n digit decimal besides an of these operation needed to compute the three product of n digit number the above formula require five addition and one hence we have the recurrence an an cn for n a applying the master theorem which wa stated in the beginning of the chapter we obtain an nlog which mean that the total number of addition and subtraction have the same asymptotic order of growth a the number of multipli the asymptotic advantage of this algorithm notwithstanding how practical is the answer depends of course on the computer system and program quality implementing the algorithm which might explain the rather wide disparity of reported on some machine the divide and conquer algorithm ha been reported to outperform the conventional method on number only decimal digit long and to run more than twice faster with number over decimal digit long the area of particular importance for modern whatever this outperformance crossover point happens to be on a particular machine it is worth switching to the conventional algorithm after the multiplicand become smaller than the crossover finally if you program in an object oriented language such a java c or smalltalk you should also be aware that these language have special class for dealing with large discovered by year old russian mathematician anatoly karatsuba in the divide and conquer algorithm proved wrong the then prevailing opinion that the time efficiency of any integer multiplication algorithm must be in the discovery encouraged researcher to look for even asymptotically faster algorithm for this and other algebraic we will see such an algorithm in the next strassens matrix multiplication now that we have seen that the divide and conquer approach can reduce the number of one digit multiplication in multiplying two integer we should not be surprised that a similar feat can be accomplished for multiplying such an algorithm wa published by strassen in the principal insight of the algorithm lie in the discovery that we can find the product c of two matrix a and b with just seven multiplication a opposed to the eight required by the brute force algorithm see example in section this is accomplished by using the following formula c c a a b b c c a a b b m m m m m m m m m m m m where m a a b b m a a b m a b b m a b b m a a b m a a b b m a a b thus to multiply two matrix strassens algorithm make seven multipli cation and additionssubtractions whereas the brute force algorithm requires eight multiplication and four these number should not lead u to multiplying matrix by strassens it importance stem from it asymptotic superiority a matrix order n go to let a and b be two n n matrix where n is a power of if n is not a power of matrix can be padded with row and column of we can divide a b and their product c into four n n submatrices each a follows c c a a b b c c a a b b it is not difficult to verify that one can treat these submatrices a number to get the correct for example c can be computed either a a b a b or a m m m m where m m m and m are found by strassens formula with the number replaced by the corresponding if the seven product of n n matrix are computed recursively by the same method we have strassens algorithm for matrix let u evaluate the asymptotic efficiency of this if mn is the number of multiplication made by strassens algorithm in multiplying two n n matrix where n is a power of we get the following recurrence relation for it mn mn for n m since n k mk mk mk mk imk i kmk k since k log n mn log n nlog which is smaller than n required by the brute force since this saving in the number of multiplication wa achieved at the expense of making extra addition we must check the number of addition an made by strassens to multiply two matrix of order n the algorithm need to multiply seven matrix of order n and make additionssubtractions of matrix of size n when n no addition are made since two number are simply these observation yield the following recurrence relation an an n for n a though one can obtain a closed form solution to this recurrence see problem in this section exercise here we simply establish the solution order of according to the master theorem an nlog in other word the number of addition ha the same order of growth a the number of this put strassens algorithm in nlog which is a better efficiency class than n of the brute force since the time of strassens discovery several other algorithm for multiplying two n n matrix of real number in on time with progressively smaller constant have been the fastest algorithm so far is that of coopersmith and winograd coo with it efficiency in the decreasing value of the exponent have been obtained at the expense of the increasing complexity of these because of large multiplicative constant none of them is of practical however they are interesting from a theoretical point of on one hand they get closer and closer to the best theoretical lower bound known for matrix multiplication which is n multiplication though the gap between this bound and the best available algorithm remains on the other hand matrix multiplication is known to be computationally equivalent to some other important problem such a solving system of linear equation discussed in the next exercise what are the smallest and largest number of digit the product of two decimal n digit integer can compute by applying the divide and conquer algorithm outlined in the prove the equality alogb c clogb a which wa used in section why is nlog better than log n a a closed form formula for why did we not include multiplication by n in the multiplication count mn of the large integer multiplication in addition to assuming that n is a power of we made for the sake of simplicity another more subtle assumption in setting up the recurrence for mn and an which is not always true it doe not change the final answer what is this how many one digit addition are made by the pen and pencil algorithm in multiplying two n digit you may disregard potential verify the formula underlying strassens algorithm for multiplying apply strassens algorithm to compute exiting the recursion when n computing the product of matrix by the brute force solve the recurrence for the number of addition required by strassens algo assume that n is a power of pan pan ha discovered a divide and conquer matrix multiplication algorithm that is based on multiplying two matrix using find the asymptotic efficiency of pan algorithm you may ignore addition and compare it with that of strassens practical implementation of strassens algorithm usually switch to the brute force method after matrix size become smaller than some crossover run an experiment to determine such a crossover point on your computer the closest pair and convex hull problem by divide and conquer in section we discussed the brute force approach to solving two classic prob lem of computational geometry the closest pair problem and the convex hull we saw that the two dimensional version of these problem can be solved by brute force algorithm in n and on time in this sec tion we discus more sophisticated and asymptotically more efficient algorithm for these problem which are based on the divide and conquer the closest pair problem let p be a set of n point in the cartesian for the sake of simplicity we assume that the point are we can also assume that the point are ordered in nondecreasing order of their x if they were not we could sort them first by an efficeint sorting algorithm such a it will also be convenient to have the point sorted in a separate list in nondecreasing order of the y coordinate we will denote such a list if n the problem can be solved by the obvious brute force if n we can divide the point into two subset pl and pr of n and n point respectively by drawing a vertical line through the median m of their x coordinate so that n point lie to the left of or on the line itself and n point lie to the right of or on the then we can solve the closest pair problem xm dl dr xm d d d min p d d a b figure a idea of the divide and conquer algorithm for the closest pair b rectangle that may contain point closer than dmin to point recursively for subset pl and let dl and dr be the smallest distance between pair of point in pl and pr respectively and let d mindl note that d is not necessarily the smallest distance between all the point pair because point of a closer pair can lie on the opposite side of the separating therefore a a step combining the solution to the smaller subproblems we need to examine such obviously we can limit our attention to the point inside the symmetric vertical strip of width d around the separating line since the distance between any other pair of point is at least d figure let s be the list of point inside the strip of width d around the separating line obtained from q and hence ordered in nondecreasing order of their y coor we will scan this list updating the information about dmin the minimum distance seen so far if we encounter a closer pair of initially dmin d and subsequently dmin let px y be a point on this for a point p x y to have a chance to be closer to p than dmin the point must follow p on list s and the difference between their y coordinate must be le than dmin geometri cally this mean that p must belong to the rectangle shown in figure the principal insight exploited by the algorithm is the observation that the rectangle can contain just a few such point because the point in each half left and right of the rectangle must be at least distance d it is easy to prove that the total number of such point in the rectangle including p doe not exceed eight prob lem in this section exercise a more careful analysis reduces this number to six see joh thus the algorithm can consider no more than five next point following p on the list s before moving up to the next here is pseudocode of the we follow the advice given in section to avoid computing square root inside the innermost loop of the algorithm efficientclosestpairp q solves the closest pair problem by divide and conquer input an array p of n point in the cartesian plane sorted in nondecreasing order of their x coordinate and an array q of the same point sorted in nondecreasing order of the y coordinate output euclidean distance between the closest pair of point if n return the minimal distance found by the brute force algorithm else copy the first n point of p to array pl copy the same n point from q to array ql copy the remaining n point of p to array pr copy the same n point from q to array qr dl efficientclosestpairpl ql dr efficientclosestpairpr qr d mindl dr m p n copy all the point of q for which x m d into array dminsq d for i to num do ki while k num and dminsq dminsq dminsq kk return sqrtdminsq the algorithm spends linear time both for dividing the problem into two problem half the size and combining the obtained therefore assuming a usual that n is a power of we have the following recurrence for the running time of the algorithm t n t n f n where f n applying the master theorem with a b and d we get t n n log the necessity to presort input point doe not change the overall efficiency class if sorting is done by a on log n algorithm such a in fact this is the best efficiency class one can achieve because it ha been proved that any algorithm for this problem must be in n log n under some natural assumption about operation an algorithm can perform see pre convex hull problem let u revisit the convex hull problem introduced in section find the smallest convex polygon that contains n given point in the we consider here a divide and conquer algorithm called quickhull because of it resemblance to let s be a set of n point px y pnxn yn in the cartesian we assume that the point are sorted in nondecreasing order of their x coordinate with tie resolved by increasing order of the y coordinate of the point it is not difficult to prove the geometrically obvious fact that the leftmost point p and the rightmost point pn are two distinct extreme point of the set convex hull figure let p pn be the straight line through point p and pn directed from p to this line separate the point of s into two set s is the set of point to the left of this line and s is the set of point to the right of this we say that point q is to the left of the line q q directed from point q to point q if qqq form a counterclockwise later we cite an analytical way to check this condition based on checking the sign of a determinant formed by the coordinate of the three the point of s on the line p pn other than p and pn cannot be extreme point of the convex hull and hence are excluded from further the boundary of the convex hull of s is made up of two polygonal chain an upper boundary and a lower the upper boundary called the upper hull is a sequence of line segment with vertex at p some of the point in s if s is not empty and the lower boundary called the lower hull is a sequence of line segment with vertex at p some of the point in s if s is not empty and the fact that the convex hull of the entire set s is composed of the upper and lower hull which can be constructed independently and in a similar fashion is a very useful observation exploited by several algorithm for this for concreteness let u discus how quickhull proceeds to construct the upper hull the lower hull can be constructed in the same if s is empty the pn p figure upper and lower hull of a set of pmax pn p figure the idea of upper hull is simply the line segment with the endpoint at p and if s is not empty the algorithm identifies point pmax in s which is the farthest from the line p pn figure if there is a tie the point that maximizes the angle pmaxppn can be note that point pmax maximizes the area of the triangle with two vertex at p and pn and the third one at some other point of then the algorithm identifies all the point of set s that are to the left of the line p pmax these are the point that will make up the set the point of s to the left of the line p m a x pn will make up the set it is not difficult to prove the following pmax is a vertex of the upper the point inside ppmaxpn cannot be vertex of the upper hull and hence can be eliminated from further there are no point to the left of both line p pmax and p m a x therefore the algorithm can continue constructing the upper hull of p s pmax and pmax s pn recursively and then simply concatenate them to get the upper hull of the entire set p s now we have to figure out how the algorithm geometric operation can be actually fortunately we can take advantage of the following very useful fact from analytical geometry if qx y qx y and qx y are three arbitrary point in the cartesian plane then the area of the triangle qqq is equal to one half of the magnitude of the determinant x y x y xy xy xy xy xy xy x y while the sign of this expression is positive if and only if the point q x y is to the left of the line q using this formula we can check in constant time whether a point lie to the left of the line determined by two other point a well a find the distance from the point to the quickhull ha the same n worst case efficiency a quicksort problem in this section in the average case however we should expect a much better first the algorithm should benefit from the quicksort like saving from the on average balanced split of the problem into two smaller second a significant fraction of the point namely those inside ppmaxpn see figure are eliminated from further under a natural assumption that point given are chosen randomly from a uniform dis tribution over some convex region a circle or a rectangle the average case efficiency of quickhull turn out to be linear exercise for the one dimensional version of the closest pair problem for the problem of finding two closest number among a given set of n real num bers design an algorithm that is directly based on the divide and conquer technique and determine it efficiency is it a good algorithm for this prove that the divide and conquer algorithm for the closest pair problem examines for every point p in the vertical strip see figure and no more than seven other point that can be closer to p than dmin the minimum distance between two point encountered by the algorithm up to that consider the version of the divide and conquer two dimensional closest pair algorithm in which instead of presorting input set p we simply sort each of the two set pl and pr in nondecreasing order of their y coordinate on each recursive assuming that sorting is done by mergesort set up a recurrence relation for the running time in the worst case and solve it for n implement the divide and conquer closest pair algorithm outlined in this section in the language of your find on the web a visualization of an algorithm for the closest pair what algorithm doe this visualization the voronoi polygon for a point p of a set s of point in the plane is defined to be the perimeter of the set of all point in the plane closer to p than to any other point in the union of all the voronoi polygon of the point in s is called the voronoi diagram of what is the voronoi diagram for a set of three find a visualization of an algorithm for generating the voronoi diagram on the web and study a few example of such based on your observation can you tell how the solution to the previous question is generalized to the general explain how one can find point pmax in the quickhull algorithm what is the best case efficiency of give a specific example of input that make quickhull run in quadratic implement quickhull in the language of your creating decagon there are point in the plane no three of them on the same devise an algorithm to construct decagon with their vertex at these the decagon need not be convex but each of them ha to be simple it boundary should not cross itself and no two decagon may have a common shortest path around there is a fenced area in the two dimensional eu clidean plane in the shape of a convex polygon with vertex at point px y px y pnxn yn not necessarily in this there are two more point axa ya and bxb yb such that xa minx x xn and xb maxx x design a reasonably efficient algorithm for comput ing the length of the shortest path between a and oro summary divide and conquer is a general algorithm design technique that solves a problem by dividing it into several smaller subproblems of the same type ideally of about equal size solving each of them recursively and then combining their solution to get a solution to the original many efficient algorithm are based on this technique although it can be both inapplicable and inferior to simpler algorithmic running time t n of many divide and conquer algorithm satisfies the recurrence t n at nb f the master theorem establishes the order of growth of it mergesort is a divide and conquer sorting it work by dividing an input array into two half sorting them recursively and then merging the two sorted half to get the original array the algorithm time efficiency is in n log n in all case with the number of key comparison being very close to the theoretical it principal drawback is a significant extra storage quicksort is a divide and conquer sorting algorithm that work by partitioning it input element according to their value relative to some preselected quicksort is noted for it superior efficiency among n log n algorithm for sorting randomly ordered array but also for the quadratic worst case the classic traversal of a binary tree preorder inorder and postorder and similar algorithm that require recursive processing of both left and right subtrees can be considered example of the divide and conquer their analysis is helped by replacing all the empty subtrees of a given tree by special external there is a divide and conquer algorithm for multiplying two n digit integer that requires about one digit strassens algorithm need only seven multiplication to multiply two by exploiting the divide and conquer technique this algorithm can multiply two n n matrix with about the divide and conquer technique can be successfully applied to two important problem of computational geometry the closest pair problem and the convex hull transform and conquer thats the secret to life replace one worry with charles schulz american cartoonist the creator of peanut this chapter deal with a group of design method that are based on the idea of we call this general technique transform and conquer because these method work a two stage first in the transformation stage the problem instance is modified to be for one reason or another more amenable to then in the second or conquering stage it is there are three major variation of this idea that differ by what we transform a given instance to figure transformation to a simpler or more convenient instance of the same problem we call it instance transformation to a different representation of the same instance we call it representation transformation to an instance of a different problem for which an algorithm is already available we call it problem in the first three section of this chapter we encounter example of the instance simplification section deal with the simple but fruitful idea of many algorithmic problem are easier to solve if their input is of course the benefit of sorting should more than compensate for the simpler instance or problem another representation solution instance or another problem instance figure transform and conquer time spent on it otherwise we would be better off dealing with an unsorted input section introduces one of the most important algorithm in applied mathematics gaussian this algorithm solves a system of linear equation by first transforming it to another system with a special property that make finding a solution quite in section the idea of instance simplification and representation change are applied to search the result are avl tree and multiway balanced search tree of the latter we consider the simplest case section present heap and even if you are already familiar with this important data structure and it application to sorting you can still benefit from looking at them in this new light of transform and conquer in section we discus horners rule a remarkable algorithm for evaluating if there were an algorithm hall of fame horners rule would be a serious candidate for induction based on the algorithm elegance and we also consider there two interesting algorithm for the exponentiation problem both based on the representation change the chapter concludes with a review of several application of the third variety of transform and conquer problem this variety should be considered the most radical of the three one problem is reduced to another transformed into an entirely different this is a very powerful idea and it is extensively used in the complexity theory chapter it application to designing practical algorithm is not trivial first we need to identify a new problem into which the given problem should be then we must make sure that the transformation algorithm followed by the algorithm for solving the new prob lem is time efficient compared to other algorithmic among several example we discus an important special case of mathematical modeling or expressing a problem in term of purely mathematical object such a variable function and presorting presorting is an old idea in computer in fact interest in sorting algorithm is due to a significant degree to the fact that many question about a list are easier to answer if the list is obviously the time efficiency of algorithm that involve sorting may depend on the efficiency of the sorting algorithm being for the sake of simplicity we assume throughout this section that list are implemented a array because some sorting algorithm are easier to implement for the array so far we have discussed three elementary sorting algorithm selection sort bubble sort and insertion sort that are quadratic in the worst and average case and two advanced algorithm mergesort which is always in n log n and quicksort whose efficiency is also n log n in the average case but is quadratic in the worst are there faster sorting a we have already stated in section see also section no general comparison based sorting algorithm can have a better efficiency than n log n in the worst case and the same result hold for the average case following are three example that illustrate the idea of more example can be found in this section example checking element uniqueness in an array if this element unique ness problem look familiar to you it should we considered a brute force algo rithm for the problem in section see example the brute force algorithm compared pair of the array element until either two equal element were found or no more pair were it worst case efficiency wa in alternatively we can sort the array first and then check only it consecutive element if the array ha equal element a pair of them must be next to each other and vice algorithm solves the element uniqueness problem by sorting the array first input an array of orderable element output return true if a ha no equal element false otherwise sort the array a for i to n do if ai ai return false return true the running time of this algorithm is the sum of the time spent on sorting and the time spent on checking consecutive since the former requires at least n log n comparison and the latter need no more than n comparison it is the sorting part that will determine the overall efficiency of the so if we use a quadratic sorting algorithm here the entire algorithm will not be more efficient than the brute force but if we use a good sorting algorithm such a mergesort with worst case efficiency in n log n the worst case efficiency of the entire presorting based algorithm will be also in n log n t n tsort n tscann n log n n n log example computing a mode a mode is a value that occurs most often in a given list of for example for the mode is if several different value occur most often any of them can be considered a the brute force approach to computing a mode would scan the list and compute the frequency of all it distinct value then find the value with the largest sorting algorithm called radix sort are linear but in term of the total number of input these algorithm work by comparing individual bit or piece of key rather than key in their although the running time of these algorithm is proportional to the number of input bit they are still essentially n log n algorithm because the number of bit per key must be at least log n in order to accommodate n distinct key of in order to implement this idea we can store the value already encountered along with their frequency in a separate on each iteration the ith element of the original list is compared with the value already encountered by traversing this auxiliary if a matching value is found it frequency is incremented otherwise the current element is added to the list of distinct value seen so far with a frequency of it is not difficult to see that the worst case input for this algorithm is a list with no equal for such a list it ith element is compared with i element of the auxiliary list of distinct value seen so far before being added to the list with a frequency of a a result the worst case number of comparison made by this algorithm in creating the frequency list is n i n n n cn i the additional n comparison needed to find the largest frequency in the aux iliary list do not change the quadratic worst case efficiency class of the a an alternative let u first sort the then all equal value will be adjacent to each to compute the mode all we need to do is to find the longest run of adjacent equal value in the sorted algorithm computes the mode of an array by sorting it first input an array of orderable element output the array mode sort the array a i current run begin at position i modef requency highest frequency seen so far while i n do runlength runvalue ai while i runlength n and ai runlength runvalue runlength runlength if runlength modef requency modef requency runlength modevalue runvalue i i runlength return modevalue the analysis here is similar to the analysis of example the running time of the algorithm will be dominated by the time spent on sorting since the remainder of the algorithm take linear time consequently with an n log n sort this method worst case efficiency will be in a better asymptotic class than the worst case efficiency of the brute force example searching problem consider the problem of searching for a given value v in a given array of n sortable the brute force solution here is sequential search section which need n comparison in the worst if the array is sorted first we can then apply binary search which requires only log n comparison in the worst assuming the most efficient n log n sort the total running time of such a searching algorithm in the worst case will be t n tsort n tsearchn n log n log n n log n which is inferior to sequential the same will also be true for the averagecase of course if we are to search in the same list more than once the time spent on sorting might well be problem in this section exercise asks to estimate the minimum number of search needed to justify before we finish our discussion of presorting we should mention that many if not most geometric algorithm dealing with set of point use presorting in one way or point can be sorted by one of their coordinate or by their distance from a particular line or by some angle and so for example presorting wa used in the divide and conquer algorithm for the closest pair problem and for the convex hull problem which were discussed in section further some problem for directed acyclic graph can be solved more easily after topologically sorting the digraph in the problem of finding the longest and shortest path in such digraph see the exercise for section and illustrate this finally most algorithm based on the greedy technique which is the subject of chapter require presorting of their input a an intrinsic part of their exercise consider the problem of finding the distance between the two closest number in an array of n the distance between two number x and y is computed a x design a presorting based algorithm for solving this problem and deter mine it efficiency compare the efficiency of this algorithm with that of the brute force algo rithm see problem in exercise let a a an and b b bm be two set of consider the problem of finding their intersection the set c of all the number that are in both a and design a brute force algorithm for solving this problem and determine it efficiency design a presorting based algorithm for solving this problem and deter mine it efficiency consider the problem of finding the smallest and largest element in an array of n design a presorting based algorithm for solving this problem and deter mine it efficiency compare the efficiency of the three algorithm i the brute force algo rithm ii this presorting based algorithm and iii the divide and conquer algorithm see problem in exercise estimate how many search will be needed to justify time spent on presorting an array of element if sorting is done by mergesort and searching is done by binary you may assume that all search are for element known to be in the what about an array of to sort or not to design a reasonably efficient algorithm for solving each of the following problem and determine it efficiency you are given n telephone bill and m check sent to pay the bill n assuming that telephone number are written on the check find out who failed to for simplicity you may also assume that only one check is written for a particular bill and that it cover the bill in you have a file of n student record indicating each student number name home address and date of find out the number of student from each of the given a set of n point in the cartesian plane connect them in a simple polygon a closed path through all the point so that it line segment the polygon edge do not intersect except for neighboring edge at their common for example p p p p p p p p p p p p doe the problem always have a doe it always have a unique design a reasonably efficient algorithm for solving this problem and indi cate it efficiency you have an array of n real number and another integer find out whether the array contains two element whose sum is for example for the array and s the answer is yes but for the same array and s the answer is design an algorithm for this problem with a better than quadratic time you have a list of n open interval a b a b an bn on the real an open interval a b comprises all the point strictly between it endpoint a and b a b x a x find the maximum number of these interval that have a common for example for the interval this maximum number is design an algorithm for this problem with a better than quadratic time number placement given a list of n distinct integer and a sequence of n box with pre set inequality sign inserted between them design an algo rithm that place the number into the box to satisfy those for example the number can be placed in the five box a shown below maximum search a point xi yi in the cartesian plane is said to be dominated by point xj yj if xi xj and yi yj with at least one of the two inequality being given a set of n point one of them is said to be a maximum of the set if it is not dominated by any other point in the for example in the figure below all the maximum point of the set of point are y x design an efficient algorithm for finding all the maximum point of a given set of n point in the cartesian what is the time efficiency class of your give a few real world application of this anagram detection design an efficient algorithm for finding all set of anagram in a large file such a a dictionary of english word for example eat ate and tea belong to one such write a program implementing the gaussian elimination you are certainly familiar with system of two linear equation in two unknown ax ay b ax ay recall that unless the coefficient of one equation are proportional to the coef ficients of the other the system ha a unique the standard method for finding this solution is to use either equation to express one of the variable a a function of the other and then substitute the result into the other equation yield ing a linear equation whose solution is then used to find the value of the second in many application we need to solve a system of n equation in n unknown ax ax anxn b ax ax anxn b anx anx annxn bn where n is a large theoretically we can solve such a system by general izing the substitution method for solving system of two linear equation what general design technique would such a method be based however the resulting algorithm would be extremely fortunately there is a much more elegant algorithm for solving system of linear equation called gaussian the idea of gaussian elimination is to transform a system of n linear equation in n unknown to an equivalent system a system with the same solution a the original one with an upper triangular coefficient matrix a matrix with all zero below it main diagonal the method is named after carl friedrich gauss who like other giant in the history of mathematics such a isaac newton and leonhard euler made numerous fundamental contribution to both theoretical and computational the method wa known to the chinese year before the european rediscovered ax ax anxn b ax ax anxn b ax anxn b ax anxn anx anx annxn bn annxn in matrix notation we can write this a ax b axb where a a an b a a an b a a an b a a an b an an ann bn ann bn we added prime to the matrix element and right hand side of the new system to stress the point that their value differ from their counterpart in the original why is the system with the upper triangular coefficient matrix better than a system with an arbitrary coefficient because we can easily solve the system with an upper triangular coefficient matrix by back substitution a first we can immediately find the value of xn from the last equation then we can substitute this value into the next to last equation to get xn and so on until we substitute the known value of the last n variable into the first equation from which we find the value of so how can we get from a system with an arbitrary coefficient matrix a to an equivalent system with an upper triangular coefficient matrix a we can do that through a series of the so called elementary operation exchanging two equation of the system replacing an equation with it nonzero multiple replacing an equation with a sum or difference of this equation and some multiple of another equation since no elementary operation can change a solution to a system any system that is obtained through a series of such operation will have the same solution a the original let u see how we can get to a system with an upper triangular first we use a a a pivot to make all x coefficient zero in the equation below the first specifically we replace the second equation with the difference between it and the first equation multiplied by aa to get an equation with a zero coefficient for doing the same for the third fourth and finally nth equation with the multiple aa aa ana of the first equation respectively make all the coefficient of x below the first equation then we get rid of all the coefficient of x by subtracting an appropriate multiple of the second equation from each of the equation below the second repeating this elimination for each of the first n variable ultimately yield a system with an upper triangular coefficient before we look at an example of gaussian elimination let u note that we can operate with just a system coefficient matrix augmented a it n st column with the equation right hand side in other word we need to write explicitly neither the variable name nor the plus and equality example solve the system by gaussian x x x x x x x x x row row row row row row now we can obtain the solution by back substitution x x x and x x x here is pseudocode of the first stage called forward elimination of the algorithm applies gaussian elimination to matrix a of a system coefficient augmented with vector b of the system right hand side value input matrix and column vector output an equivalent upper triangular matrix in place of a with the corresponding right hand side value in the n st column for i to n do ai n bi augments the matrix for i to n do for j i to n do for k i to n do aj k aj k ai k aj i ai i there are two important observation to make about this first it is not always correct if ai i we cannot divide by it and hence cannot use the ith row a a pivot for the ith iteration of the in such a case we should take advantage of the first elementary operation and exchange the ith row with some row below it that ha a nonzero coefficient in the ith if the system ha a unique solution which is the normal case for system under consideration such a row must since we have to be prepared for the possibility of row exchange anyway we can take care of another potential difficulty the possibility that ai i is so small and consequently the scaling factor aj iai i so large that the new value of aj k might become distorted by a round off error caused by a subtraction of two number of greatly different to avoid this problem we can always look for a row with the largest absolute value of the coefficient in the ith column exchange it with the ith row and then use the new ai i a the ith iteration this modification called partial pivoting guarantee that the magnitude of the scaling factor will never exceed the second observation is the fact that the innermost loop is written with a glaring can you find it before checking the following pseudocode which both incorporates partial pivoting and eliminates this algorithm implement gaussian elimination with partial pivoting input matrix and column vector output an equivalent upper triangular matrix in place of a and the corresponding right hand side value in place of the n st column for i to n do ai n bi appends b to a a the last column for i to n do pivotrow i for j i to n do if aj i apivotrow i pivotrow j for k i to n do swapai k apivotrow k for j i to n do temp aj i ai i for k i to n do aj k aj k ai k temp let u find the time efficiency of this it innermost loop consists of a single line aj k aj k ai k temp we discus round off error in more detail in section which contains one multiplication and one on most computer multi plication is unquestionably more expensive than additionsubtraction and hence it is multiplication that is usually quoted a the algorithm basic the standard summation formula and rule reviewed in section see also appen dix a are very helpful in the following derivation n n n n n n n cn n i n i i j i ki i j i i j i n n n in i n in i i i n n nn n n n n nn n n j j j j j j j nn n n since the second back substitution stage of gaussian elimination is in n a you are asked to show in the exercise the running time is dominated by the cubic elimination stage making the entire algorithm cubic a theoretically gaussian elimination always either yield an exact solution to a system of linear equation when the system ha a unique solution or discovers that no such solution in the latter case the system will have either no solution or infinitely many of in practice solving system of significant size on a computer by this method is not nearly so straightforward a the method would lead u to the principal difficulty lie in preventing an accumulation of round off error see section consult textbook on numerical analysis that analyze this and other implementation issue in great lu decomposition gaussian elimination ha an interesting and very useful byproduct called lu de composition of the coefficient in fact modern commercial implementa tions of gaussian elimination are based on such a decomposition rather than on the basic algorithm outlined example let u return to the example in the beginning of this section where we applied gaussian elimination to the matrix a we mentioned in section on some computer multiplication is not necessarily more expensive than for this algorithm this point is moot since we can simply count the number of time the innermost loop is executed which is of course exactly the same number a the number of multiplication and the number of subtraction a consider the lower triangular matrix l made up of s on it main diagonal and the row multiple used in the forward elimination process l and the upper triangular matrix u that wa the result of this elimination u it turn out that the product lu of these matrix is equal to matrix for this particular pair of l and u you can verify this fact by direct multiplication but a a general proposition it need of course a proof which we omit therefore solving the system ax b is equivalent to solving the system lu x the latter system can be solved a denote y u x then ly solve the system ly b first which is easy to do because l is a lower triangular matrix then solve the system u x y with the upper triangular matrix u to find thus for the system at the beginning of this section we first solve ly b y y y it solution is y y y y y y solving u x y mean solving x x x and the solution is x x x x x x note that once we have the lu decomposition of matrix a we can solve system ax b with a many right hand side vector b a we want to one at a this is a distinct advantage over the classic gaussian elimination discussed also note that the lu decomposition doe not actually require extra memory because we can store the nonzero part of u in the upper triangular part of a including the main diagonal and store the nontrivial part of l below the main diagonal of computing a matrix inverse gaussian elimination is a very useful algorithm that tackle one of the most important problem of applied mathematics solving system of linear in fact gaussian elimination can also be applied to several other problem of linear algebra such a computing a matrix the inverse of an n n matrix a is an n n matrix denoted a such that aa i where i is the n n identity matrix the matrix with all zero element except the main diagonal element which are all not every square matrix ha an inverse but when it exists the inverse is if a matrix a doe not have an inverse it is called one can prove that a matrix is singular if and only if one of it row is a linear combination a sum of some multiple of the other a convenient way to check whether a matrix is nonsingular is to apply gaussian elimination if it yield an upper triangular matrix with no zero on the main diagonal the matrix is nonsingular otherwise it is so being singular is a very special situation and most square matrix do have their theoretically inverse matrix are very important because they play the role of reciprocal in matrix algebra overcoming the absence of the explicit division operation for for example in a complete analogy with a linear equation in one unknown ax b whose solution can be written a x a b if a is not zero we can express a solution to a system of n equation in n unknown ax b a x a b if a is nonsingular where b is of course a vector not a according to the definition of the inverse matrix for a nonsingular n n matrix a to compute it we need to find n number xij i j n such that a a an x x xn a an x xn an an ann xn xn xnn we can find the unknown by solving n system of linear equation that have the same coefficient matrix a the vector of unknown xj is the j th column of the inverse and the right hand side vector ej is the j th column of the identity matrix j n axj ej we can solve these system by applying gaussian elimination to matrix a aug mented by the n n identity better yet we can use forward elimina tion to find the lu decomposition of a and then solve the system lu xj ej j n a explained computing a determinant another problem that can be solved by gaussian elimination is computing a the determinant of an n n matrix a denoted det a or a is a number whose value can be defined recursively a if n if a consists of a single element a det a is equal to a for n det a is computed by the recursive formula n det a sj aj det aj j where sj is if j is odd and if j is even aj is the element in row and column j and aj is the n n matrix obtained from matrix a by deleting it row and column j in particular for a matrix the definition implies a formula that is easy to remember det a a a det a a det a aa a a in other word the determinant of a matrix is simply equal to the difference between the product of it diagonal for a matrix we get a a a det a a a a a a a det a a a det a a a det a a a a a a a a aaa aaa aaa aaa aaa incidentally this formula is very handy in a variety of in particular we used it twice already in section a a part of the quickhull but what if we need to compute a determinant of a large although this is a task that is rarely needed in practice it is worth discussing using the recursive definition can be of little help because it implies computing the sum of here gaussian elimination come to the rescue the central point is the fact that the determinant of an upper triangular matrix is equal to the product of element on it main diagonal and it is easy to see how elementary operation employed by the elimination algorithm influence the determinant basically it either remains unchanged or change a sign or is multiplied by the constant used by the elimination a a result we can compute the determinant of an n n matrix in cubic determinant play an important role in the theory of system of linear specifically a system of n linear equation in n unknown ax b ha a unique solution if and only if the determinant of it coefficient matrix det a is not equal to moreover this solution can be found by the formula called cramers rule x det a xj det aj xn det an det a det a det a where det aj is the determinant of the matrix obtained by replacing the j th column of a by the column you are asked to investigate in the exercise whether using cramers rule is a good algorithm for solving system of linear exercise solve the following system by gaussian elimination x x x x x x x x x solve the system of the previous question by the lu decomposition from the standpoint of general algorithm design technique how would you classify the lu decomposition solve the system of problem by computing the inverse of it coefficient matrix and then multiplying it by the vector on the right hand would it be correct to get the efficiency class of the forward elimination stage of gaussian elimination a n n n n cn n in i i j i ki i n n n in i i n n n n n n i i i i since sn ni n n n sn in n i n and sn n i n sn sn sn i write pseudocode for the back substitution stage of gaussian elimination and show that it running time is in assuming that division of two number take three time longer than their multiplication estimate how much faster betterforwardelimination is than of course you should also assume that a compiler is not going to eliminate the inefficiency in give an example of a system of two linear equation in two unknown that ha a unique solution and solve it by gaussian give an example of a system of two linear equation in two unknown that ha no solution and apply gaussian elimination to give an example of a system of two linear equation in two unknown that ha infinitely many solution and apply gaussian elimination to the gauss jordan elimination method differs from gaussian elimination in that the element above the main diagonal of the coefficient matrix are made zero at the same time and by the same use of a pivot row a the element below the main apply the gauss jordan method to the system of problem of these what general design strategy is this algorithm based in general how many multiplication are made by this method in solving a system of n equation in n how doe this compare with the number of multiplication made by the gaussian elimination method in both it elimination and back substitution a system ax b of n linear equation in n unknown ha a unique solution if and only if det a is it a good idea to check this condition before applying gaussian elimination to the apply cramers rule to solve the system of problem of these estimate how many time longer it will take to solve a system of n linear equation in n unknown by cramers rule than by gaussian assume that all the determinant in cramers rule formula are computed independently by gaussian light out this one person game is played on an n n board composed of light each panel ha a switch that can be turned on and off thereby toggling the onoff state of this and four vertically and horizontally adjacent of course toggling a corner square affect a total of three panel and toggling a noncorner panel on the board border affect a total of four given an initial subset of lighted square the goal is to turn all the light show that an answer can be found by solving a system of linear equation with coefficient and right hand side using the modulo use gaussian elimination to solve the all one instance of this problem where all the panel of the board are initially use gaussian elimination to solve the all one instance of this problem where all the panel of the board are initially balanced search tree in section and we discussed the binary search tree one of the prin cipal data structure for implementing it is a binary tree whose node contain element of a set of orderable item one element per node so that all ele ments in the left subtree are smaller than the element in the subtrees root and all the element in the right subtree are greater than note that this transformation from a set to a binary search tree is an example of the representation change tech what do we gain by such transformation compared to the straightforward implementation of a dictionary by say an we gain in the time efficiency of searching insertion and deletion which are all in log n but only in the av erage in the worst case these operation are in n because the tree can degenerate into a severely unbalanced one with it height equal to n computer scientist have expended a lot of effort in trying to find a structure that preserve the good property of the classical binary search tree principally the logarithmic efficiency of the dictionary operation and having the set ele ments sorted but avoids it worst case they have come up with two the first approach is of the instance simplification variety an unbalanced binary search tree is transformed into a balanced because of this such tree are called self specific implementation of this idea differ by their definition of an avl tree requires the difference between the height of the left and right subtrees of every node never exceed a red black tree tolerates the height of one subtree being twice a large a the other subtree of the same if an insertion or deletion of a new node creates a tree with a violated balance requirement the tree is restructured by one of a family of special transformation called rotation that restore the balance in this section we will discus only avl information about other type of binary search tree that utilize the idea of rebalancing via rotation including red black tree and splay tree can be found in the reference cor sed and the second approach is of the representation change variety allow more than one element in a node of a search specific case of such tree are tree tree and more general and important b they differ in the number of element admissible in a single node of a search tree but all are perfectly we discus the simplest case of such tree the tree in this section leaving the discussion of b tree for chapter avl tree avl tree were invented in by two russian scientist adelson velsky and landis ade after whom this data structure is a b figure a avl b binary search tree that is not an avl the number above the node indicate the node balance definition an avl tree is a binary search tree in which the balance factor of every node which is defined a the difference between the height of the node left and right subtrees is either or or the height of the empty tree is defined a of course the balance factor can also be computed a the difference between the number of level rather than the height difference of the node left and right for example the binary search tree in figure is an avl tree but the one in figure is if an insertion of a new node make an avl tree unbalanced we transform the tree by a a rotation in an avl tree is a local transformation of it subtree rooted at a node whose balance ha become either or if there are several such node we rotate the tree rooted at the unbalanced node that is the closest to the newly inserted there are only four type of rotation in fact two of them are mirror image of the other in their simplest form the four rotation are shown in figure the first rotation type is called the single right rotation or r imagine rotating the edge connecting the root and it left child in the binary tree in figure to the figure present the single r rotation in it most general note that this rotation is performed after a new key is inserted into the left subtree of the left child of a tree whose root had the balance of before the the symmetric single left rotation or l rotation is the mirror image of the single r it is performed after a new key is inserted into the right subtree of the right child of a tree whose root had the balance of before the you are asked to draw a diagram of the general case of the single l rotation in the r a l b lr c rl d figure four rotation type for avl tree with three a single r b single l c double lr d double rl the second rotation type is called the double left right rotation lr it is in fact a combination of two rotation we perform the l rotation of the left subtree of root r followed by the r rotation of the new tree rooted at r figure it is performed after a new key is inserted into the right subtree of the left child of a tree whose root had the balance of before the single r rotation r c c r t t t t t t figure general form of the r rotation in the avl a shaded node is the last one double lr rotation r g c c r g t t t t t t t t or or figure general form of the double lr rotation in the avl a shaded node is the last one it can be either in the left subtree or in the right subtree of the root the double right left rotation rl rotation is the mirror image of the double lr rotation and is left for the note that the rotation are not trivial transformation though fortunately they can be done in constant not only should they guarantee that a resulting tree is balanced but they should also preserve the basic requirement of a binary search for example in the initial tree of figure all the key of subtree t are smaller than c which is smaller than all the key of subtree t which are smaller than r which is smaller than all the key of subtree and the same relationship among the key value hold a they must for the balanced tree after the l r lr rl figure construction of an avl tree for the list by successive the parenthesized number of a rotation abbreviation indicates the root of the tree being an example of constructing an avl tree for a given list of number is shown in figure a you trace the algorithm operation keep in mind that if there are several node with the balance the rotation is done for the tree rooted at the unbalanced node that is the closest to the newly inserted how efficient are avl a with any search tree the critical charac teristic is the tree it turn out that it is bounded both above and below by logarithmic specifically the height h of any avl tree with n node satisfies the inequality log n h logn these weird looking constant are round offs of some irrational number related to fibonacci number and the golden ratio see section the inequality immediately imply that the operation of searching and insertion are log n in the worst getting an exact formula for the average height of an avl tree constructed for random list of key ha proved to be difficult but it is known from extensive experiment that it is about n except when n is small knuiii thus searching in an avl tree requires on average almost the same number of comparison a searching in a sorted array by binary the operation of key deletion in an avl tree is considerably more difficult than insertion but fortunately it turn out to be in the same efficiency class a insertion these impressive efficiency characteristic come at a price the drawback of avl tree are frequent rotation and the need to maintain balance for it these drawback have prevented avl tree from becoming the standard structure for implementing at the same time their underlying idea that of rebalancing a binary search tree via rotation ha proved to be very fruitful and ha led to discovery of other interesting variation of the classical binary search tree a mentioned at the beginning of this section the second idea of balancing a search tree is to allow more than one key in the same node of such a the simplest implementation of this idea is tree introduced by the computer scientist john hopcroft in a tree is a tree that can have node of two kind node and a node contains a single key k and ha two child the left child serf a the root of a subtree whose key are le than k and the right child serf a the root of a subtree whose key are greater than in other word a node is the same kind of node we have in the classical binary search a node contains two ordered key k and k k k and ha three the leftmost child serf a the root of a subtree with key le than k the middle child serf a the root of a subtree with key between k and k and the rightmost child serf a the root of a subtree with key greater than k figure the last requirement of the tree is that all it leaf must be on the same in other word a tree is always perfectly height balanced the length of a path from the root to a leaf is the same for every it is this property that we buy by allowing more than one key in the same node of a search searching for a given key k in a tree is quite we start at the if the root is a node we act a if it were a binary search tree we either stop if k is equal to the root key or continue the search in the left or right node node k k k k k k k k k figure two kind of node of a subtree if k is respectively smaller or larger than the root if the root is a node we know after no more than two key comparison whether the search can be stopped if k is equal to one of the root key or in which of the root three subtrees it need to be inserting a new key in a tree is done a first of all we always insert a new key k in a leaf except for the empty the appropriate leaf is found by performing a search for if the leaf in question is a node we insert k there a either the first or the second key depending on whether k is smaller or larger than the node old if the leaf is a node we split the leaf in two the smallest of the three key two old one and the new key is put in the first leaf the largest key is put in the second leaf and the middle key is promoted to the old leaf if the leaf happens to be the tree root a new root is created to accept the middle note that promotion of a middle key to it parent can cause the parent overflow if it wa a node and hence can lead to several node split along the chain of the leaf an example of a tree construction is given in figure a for any search tree the efficiency of the dictionary operation depends on the tree so let u first find an upper bound for a tree of height h with the smallest number of key is a full tree of node such a the final tree in figure for h therefore for any tree of height h with n node we get the inequality n h h and hence h logn on the other hand a tree of height h with the largest number of key is a full tree of node each with two key and three therefore for any tree with n node n h h h figure construction of a tree for the list and hence h logn these lower and upper bound on height h logn h logn imply that the time efficiency of searching insertion and deletion are all in log n in both the worst and average we consider a very important generalization of tree called b tree in section exercise which of the following binary tree are avl a b c for n and draw all the binary tree with n node that satisfy the balance requirement of avl draw a binary tree of height that can be an avl tree and ha the smallest number of node among all such draw diagram of the single l rotation and of the double rl rotation in their general for each of the following list construct an avl tree by inserting their ele ments successively starting with the empty for an avl tree containing real number design an algorithm for comput ing the range the difference between the largest and smallest number in the tree and determine it worst case true or false the smallest and the largest key in an avl tree can always be found on either the last level or the next to last write a program for constructing an avl tree for a given list of n distinct construct a tree for the list c o m p u t i n use the alphabetical order of the letter and insert them successively starting with the empty assuming that the probability of searching for each of the key the letter are the same find the largest number and the average number of key comparison for successful search in this let tb and t be respectively a classical binary search tree and a tree constructed for the same list of key inserted in the corresponding tree in the same true or false searching for the same key in t always take fewer or the same number of key comparison a searching in for a tree containing real number design an algorithm for computing the range the difference between the largest and smallest number in the tree and determine it worst case write a program for constructing a tree for a given list of n heap and heapsort the data structure called the heap is definitely not a disordered pile of item a the word definition in a standard dictionary might rather it is a clever partially ordered data structure that is especially suitable for implementing priority recall that a priority queue is a multiset of item with an orderable characteristic called an item priority with the following operation figure illustration of the definition of heap only the leftmost tree is a finding an item with the highest largest priority deleting an item with the highest priority adding a new item to the multiset it is primarily an efficient implementation of these operation that make the heap both interesting and priority queue arise naturally in such ap plication a scheduling job execution by computer operating system and traf fic management by communication they also arise in several impor tant algorithm prims algorithm section dijkstras algorithm sec tion huffman encoding section and branch and bound application section the heap is also the data structure that serf a a cornerstone of a theoretically important sorting algorithm called we discus this algo rithm after we define the heap and investigate it basic notion of the heap definition a heap can be defined a a binary tree with key assigned to it node one key per node provided the following two condition are met the shape property the binary tree is essentially complete or simply com plete all it level are full except possibly the last level where only some rightmost leaf may be the parental dominance or heap property the key in each node is greater than or equal to the key in it this condition is considered auto matically satisfied for all for example consider the tree of figure the first tree is a the second one is not a heap because the tree shape property is and the third one is not a heap because the parental dominance fails for the node with key note that key value in a heap are ordered top down a sequence of value on any path from the root to a leaf is decreasing nonincreasing if equal key are however there is no left to right order in key value there is no some author require the key at each node to be le than or equal to the key at it we call this variation a min the array representation index value parent leaf figure heap and it array relationship among key value for node either on the same level of the tree or more generally in the left and right subtrees of the same here is a list of important property of heap which are not difficult to prove check these property for the heap of figure a an there exists exactly one essentially complete binary tree with n it height is equal to log n the root of a heap always contains it largest a node of a heap considered with all it descendant is also a a heap can be implemented a an array by recording it element in the top down left to right it is convenient to store the heap element in position through n of such an array leaving h either unused or putting there a sentinel whose value is greater than every element in the in such a representation the parental node key will be in the first n position of the array while the leaf key will occupy the last n position the child of a key in the array parental position i i n will be in position i and i and correspondingly the parent of a key in position i i n will be in position i thus we could also define a heap a an array h in which every element in position i in the first half of the array is greater than or equal to the element in position i and i h i maxh i h i for i n of course if i n just h i h i need to be while the idea behind the majority of algorithm dealing with heap are easier to understand if we think of heap a binary tree their actual implementation are usually much simpler and more efficient with how can we construct a heap for a given list of there are two principal alternative for doing the first is the bottom up heap construction algorithm illustrated in figure it initializes the essentially complete binary tree with n node by placing key in the order given and then heapifies the tree a starting with the last parental node the algorithm check whether the parental figure bottom up construction of a heap for the list the double headed arrow show key comparison verifying the parental dominance hold for the key in this if it doe not the algorithm exchange the node key k with the larger key of it child and check whether the parental dominance hold for k in it new this process continues until the parental dominance for k is eventually it ha to because it hold automatically for any key in a after completing the heapification of the subtree rooted at the current parental node the algorithm proceeds to do the same for the node immediate the algorithm stop after this is done for the root of the algorithm heapbottomuph construct a heap from element of a given array by the bottom up algorithm input an array h of orderable item output a heap h for i n downto do k i v h k heap false while not heap and k n do j k if j n there are two child if h j h j j j if v h j heap true else h k h j kj h k v how efficient is this algorithm in the worst assume for simplicity that n k so that a heap tree is full the largest possible number of node occurs on each let h be the height of the according to the first property of heap in the list at the beginning of the section h log n or just log n k for the specific value of n we are each key on level i of the tree will travel to the leaf level h in the worst case of the heap construction since moving to the next level down requires two comparison one to find the larger child and the other to determine whether the exchange is required the total number of key comparison involving a key on level i will be h therefore the total number of key comparison in the worst case will be h h cworst n h i h ii n logn i level i key i where the validity of the last equality can be proved either by using the closed form formula for the sum h ii see appendix a or by mathematical induction on i thus with this bottom up algorithm a heap of size n can be constructed with fewer than n the alternative and le efficient algorithm construct a heap by successive insertion of a new key into a previously constructed heap some people call it the top down heap construction so how can we insert a new key k into a first attach a new node with key k in it after the last leaf of the existing then sift k up to it appropriate place in the new heap a compare k with it parent key if the latter is greater than or equal to k stop the structure is a heap otherwise swap these two key and compare k with it new this swapping continues until k is not greater than it last parent or it reach the root illustrated in figure obviously this insertion operation cannot require more key comparison than the heap since the height of a heap with n node is about log n the time efficiency of insertion is in olog how can we delete an item from a we consider here only the most important case of deleting the root key leaving the question about deleting an arbitrary key in a heap for the author of textbook like to do such thing to their reader do they deleting the root key from a heap can be done with the following algorithm illustrated in figure figure inserting a key into the heap constructed in figure the new key is sifted up via a swap with it parent until it is not larger than it parent or is in the step step step figure deleting the root key from a the key to be deleted is swapped with the last key after which the smaller tree is heapified by exchanging the new key in it root with the larger key in it child until the parental dominance requirement is maximum key deletion from a heap step exchange the root key with the last key k of the step decrease the heap size by step heapify the smaller tree by sifting k down the tree exactly in the same way we did it in the bottom up heap construction that is verify the parental dominance for k if it hold we are done if not swap k with the larger of it child and repeat this operation until the parental dominance condition hold for k in it new the efficiency of deletion is determined by the number of key comparison needed to heapify the tree after the swap ha been made and the size of the tree is decreased by since this cannot require more key comparison than twice the heap height the time efficiency of deletion is in olog n a heapsort now we can describe heapsort an interesting sorting algorithm discovered by williams this is a two stage algorithm that work a stage heap construction construct a heap for a given stage maximum deletion apply the root deletion operation n time to the remaining a a result the array element are eliminated in decreasing but since under the array implementation of heap an element being deleted is placed last the resulting array will be exactly the original array sorted in increasing heapsort is traced on a specific input in figure the same input a the one stage heap construction stage maximum deletion figure sorting the array by in figure is intentionally used so that you can compare the tree and array implementation of the bottom up heap construction since we already know that the heap construction stage of the algorithm is in on we have to investigate just the time efficiency of the second for the number of key comparison cn needed for eliminating the root key from the heap of diminishing size from n to we get the following inequality n cn logn logn log log i i n logn n logn n log i this mean that cn on log n for the second stage of for both stage we get on on log n on log a more detailed analysis show that the time efficiency of heapsort is in fact in n log n in both the worst and average thus heapsorts time efficiency fall in the same class a that of unlike the latter heapsort is in place it doe not require any extra timing experiment on random file show that heapsort run more slowly than quicksort but can be competitive with exercise construct a heap for the list by the bottom up construct a heap for the list by successive key insertion top down is it always true that the bottom up and top down algorithm yield the same heap for the same outline an algorithm for checking whether an array h is a heap and determine it time find the smallest and the largest number of key that a heap of height h can prove that the height of a heap with n node is equal to log n prove the following equality used in section h h ii n logn where n h i design an efficient algorithm for finding and deleting an element of the smallest value in a heap and determine it time design an efficient algorithm for finding and deleting an element of a given value v in a heap h and determine it time indicate the time efficiency class of the three main operation of the priority queue implemented a an unsorted a sorted a binary search an avl a sort the following list by heapsort by using the array representation of in increasing order in increasing order s o r t i n g in alphabetical order is heapsort a stable sorting what variety of the transform and conquer technique doe heapsort repre which sorting algorithm other than heapsort us a priority implement three advanced sorting algorithm mergesort quicksort and heapsort in the language of your choice and investigate their performance on array of size n and for each of these size consider randomly generated file of integer in the range increasing file of integer decreasing file of integer n n spaghetti sort imagine a handful of uncooked spaghetti individual rod whose length represent number that need to be outline a spaghetti sort a sorting algorithm that take advantage of this unorthodox what doe this example of computer science folklore see dew have to do with the topic of this chapter in general and heapsort in horners rule and binary exponentiation in this section we discus the problem of computing the value of a polynomial px anxn an xn ax a at a given point x and it important special case of computing polynomial constitute the most important class of function because they posse a wealth of good property on the one hand and can be used for approximating other type of function on the the problem of manipulating polynomial efficiently ha been important for several century new discovery were still being made the last by far the most important of them wa the fast fourier transform the practical importance of this remarkable algorithm which is based on representing a polynomial by it value at specially chosen point wa such that some people consider it one of the most important algorithmic discovery of all because of it relative complexity we do not discus the fft algorithm in this an interested reader will find a wealth of literature on the subject including reasonably accessible treatment in such textbook a kle and horners rule horners rule is an old but very elegant and efficient algorithm for evaluating a it is named after the british mathematician horner who pub lished it in the early th but according to knuth knuii the method wa used by isaac newton year before you will appreciate this method much more if you first design an algorithm for the polynomial evalu ation problem by yourself and investigate it efficiency see problem and in this section horners rule is a good example of the representation change technique since it is based on representing px by a formula different from this new formula is obtained from by successively taking x a a common factor in the remaining polynomial of diminishing degree px anx an x for example for the polynomial px x x x x we get px x x x x xx x x xxx x xxxx it is in formula that we will substitute a value of x at which the polynomial need to be it is hard to believe that this is a way to an efficient algorithm but the unpleasant appearance of formula is just that an a we shall see there is no need to go explicitly through the transformation leading to it all we need is an original list of the polynomial the pen and pencil calculation can be conveniently organized with a tworow the first row contains the polynomial coefficient including all the coefficient equal to zero if any listed from the highest an to the lowest except for it first entry which is an the second row is filled left to right a follows the next entry is computed a the x value time the last entry in the second row plus the next coefficient from the first the final entry computed in this fashion is the value being example evaluate px x x x x at x coefficient x thus p on comparing the table entry with formula you will see that is the value of x at x is the value of xx at x is the value of xxx at x and finally is the value of xxxx px at x pseudocode of this algorithm is the shortest one imaginable for a nontrivial algorithm algorithm hornerp x evaluates a polynomial at a given point by horners rule input an array p of coefficient of a polynomial of degree n stored from the lowest to the highest and a number x output the value of the polynomial at x p p n for i n downto do p x p p i return p the number of multiplication and the number of addition are given by the same sum n mn an i to appreciate how efficient horners rule is consider only the first term of a polynomial of degree n just computing this single term by the brute force algorithm would require n multiplication whereas horners rule computes in addition to this term n other term and it still us the same number of it is not surprising that horners rule is an optimal algorithm for polynomial evaluation without preprocessing the polynomial but it took scientist year after horners publication to come to the realization that such a question wa worth horners rule also ha some useful the intermediate number generated by the algorithm in the process of evaluating px at some point x turn out to be the coefficient of the quotient of the division of px by x x and the final result in addition to being px is equal to the remainder of this thus according to example the quotient and the remainder of the division of x x x x by x are x x x and this division algorithm known a synthetic division is more convenient than so called long binary exponentiation the amazing efficiency of horners rule fade if the method is applied to comput ing an which is the value of xn at x in fact it degenerate to the brute force multiplication of a by itself with wasteful addition of zero in since computing an actually an mod m is an essential operation in several important primality testing and encryption method we consider now two algorithm for computing an that are based on the representation change they both exploit the binary representation of exponent n but one of them process this binary string left to right whereas the second doe it right to let n bi bi b be the bit string representing a positive integer n in the binary number this mean that the value of n can be computed a the value of the polynomial px bi xi bixi b at x for example if n it binary representation is and let u now compute the value of this polynomial by applying horners rule and see what the method operation imply for computing the power an ap abi i horners rule for the binary polynomial p implication for an ap p the leading digit is always for n ap a for i i downto do for i i downto do p p bi ap apbi but apbi ap abi ap abi ap if bi ap a if bi thus after initializing the accumulator value to a we can scan the bit string representing the exponent n to always square the last value of the accumulator and if the current binary digit is also to multiply it by these observation lead to the following left to right binary exponentiation method of computing algorithm leftrightbinaryexponentiationa bn computes an by the left to right binary exponentiation algorithm input a number a and a list bn of binary digit bi b in the binary expansion of a positive integer n output the value of an product a for i i downto do product product product if bi product product a return product example compute a by the left to right binary exponentiation here n so we have binary digit of n product accumulator a a a a a a a a a since the algorithm make one or two multiplication on each repetition of it only loop the total number of multiplication mn made by it in computing an is b mn b where b is the length of the bit string representing the exponent taking into account that b log n we can conclude that the efficiency of the leftto right binary exponentiation is thus this algorithm is in a better efficiency class than the brute force exponentiation which always requires n the right to left binary exponentiation us the same binary polynomial p see yielding the value of but rather than applying horners rule to it a the previous method did this one exploit it differently an abi i abi i abii thus an can be computed a the product of the term abii ai if bi if bi the product of consecutive term ai skipping those for which the binary digit bi is in addition we can compute ai by simply squaring the same term we computed for the previous value of i since ai ai so we compute all such power of a from the smallest to the largest from right to left but we include in the product accumulator only those whose corresponding binary digit is here is pseudocode of this algorithm rightleftbinaryexponentiationa bn computes an by the right to left binary exponentiation algorithm input a number a and a list bn of binary digit bi b in the binary expansion of a nonnegative integer n output the value of an term a initializes ai if b product a else product for i to i do term term term if bi product product term return product example compute a by the right to left binary exponentiation here n so we have the following table filled in from right to left binary digit of n a a a a term ai a a a a a a a product accumulator obviously the algorithm efficiency is also logarithmic for the same reason the left to right binary multiplication the usefulness of both binary exponentia tion algorithm is reduced somewhat by their reliance on availability of the explicit binary expansion of exponent problem in this section exercise asks you to design an algorithm that doe not have this exercise consider the following brute force algorithm for evaluating a algorithm bruteforcepolynomialevaluationp x computes the value of polynomial p at a given point x by the highest to lowest term brute force algorithm input an array p of the coefficient of a polynomial of degree n stored from the lowest to the highest and a number x output the value of the polynomial at the point x p for i n downto do power for j to i do power power x p p p i power return p find the total number of multiplication and the total number of addition made by this write pseudocode for the brute force polynomial evaluation that stem from substituting a given value of the variable into the polynomial formula and evaluating it from the lowest term to the highest determine the number of multiplication and the number of addition made by this estimate how much faster horners rule is compared to the lowest to highest term brute force algorithm of problem if i the time of one multiplication is significantly larger than the time of one addition ii the time of one multiplication is about the same a the time of one is horners rule more time efficient at the expense of being le space efficient than the brute force apply horners rule to evaluate the polynomial px x x x at x use the result of the above application of horners rule to find the quo tient and remainder of the division of px by x apply horners rule to convert from binary to compare the number of multiplication and additionssubtractions needed by the long division of a polynomial px anxn an xn a by x c where c is some constant with the number of these operation in the synthetic apply the left to right binary exponentiation algorithm to compute is it possible to extend the left to right binary exponentiation algorithm to work for every nonnegative integer apply the right to left binary exponentiation algorithm to compute design a nonrecursive algorithm for computing an that mimic the right to left binary exponentiation but doe not explicitly use the binary representation of is it a good idea to use a general purpose polynomial evaluation algorithm such a horners rule to evaluate the polynomial px xn xn x according to the corollary of the fundamental theorem of algebra every polynomial px anxn an xn a can be represented in the form px anx xx x x xn where x x xn are the root of the polynomial generally complex and not necessarily discus which of the two representation is more convenient for each of the following operation polynomial evaluation at a given point addition of two polynomial multiplication of two polynomial polynomial interpolation given a set of n data point xi yi where no two xi are the same find a polynomial px of degree at most n such that pxi yi for every i problem reduction here is my version of a well known joke about professor x a noted mathematician noticed that when his wife wanted to boil water for their tea she took their kettle from their cupboard filled it with water and put it on the once when his wife wa away if you have to know she wa signing her best seller in a local bookstore the professor had to boil water by he saw that the kettle wa sitting on the kitchen what did professor x he put the kettle in the cupboard first and then proceeded to follow his wife reduction a problem problem solution to be solved solvable by a to problem figure problem reduction the way professor x approached his task is an example of an important problem solving strategy called problem if you need to solve a problem reduce it to another problem that you know how to solve figure the joke about the professor notwithstanding the idea of problem reduction play a central role in theoretical computer science where it is used to classify problem according to their we will touch on this classification in chapter but the strategy can be used for actual problem solving the practical difficulty in applying it lie of course in finding a problem to which the problem at hand should be moreover if we want our effort to be of practical value we need our reduction based algorithm to be more efficient than solving the original problem note that we have already encountered this technique earlier in the in section for example we mentioned the so called synthetic division done by applying horners rule for polynomial in section we used the following fact from analytical geometry if px y px y and px y are three arbitrary point in the plane then the determinant x y x y xy xy xy xy xy xy x y is positive if and only if the point p is to the left of the directed line p p through point p and in other word we reduced a geometric question about the relative location of three point to a question about the sign of a in fact the entire idea of analytical geometry is based on reducing geometric problem to algebraic and the vast majority of geometric algorithm take advantage of this historic insight by rene descartes in this section we give a few more example of algorithm based on the strategy of problem computing the least common multiple recall that the least common multiple of two positive integer m and n denoted lcmm n is defined a the smallest integer that is divisible by both m and for example lcm and lcm the least common multiple is one of the most important notion in elementary arithmetic and perhaps you remember the following middle school method for computing it given the prime factorization of m and n compute the product of all the common prime factor of m and n all the prime factor of m that are not in n and all the prime factor of n that are not in for example lcm a a computational procedure this algorithm ha the same drawback a the middle school algorithm for computing the greatest common divisor discussed in section it is inefficient and requires a list of consecutive a much more efficient algorithm for computing the least common multiple can be devised by using problem after all there is a very efficient algorithm euclid algorithm for finding the greatest common divisor which is a product of all the common prime factor of m and can we find a formula relating lcmm n and gcdm it is not difficult to see that the product of lcmm n and gcdm n includes every factor of m and n exactly once and hence is simply equal to the product of m and this observation lead to the formula lcmm n gcdm n where gcdm n can be computed very efficiently by euclid counting path in a graph a our next example we consider the problem of counting path between two vertex in a it is not difficult to prove by mathematical induction that the number of different path of length k from the ith vertex to the j th vertex of a graph undirected or directed equal the i j th element of ak where a is the adjacency matrix of the therefore the problem of counting a graph path can be solved with an algorithm for computing an appropriate power of it adjacency note that the exponentiation algorithm we discussed before for computing power of number are applicable to matrix a a a specific example consider the graph of figure it adjacency matrix a and it square a indicate the number of path of length and respectively between the corresponding vertex of the in particular there are three a b a b c d a b c d a a a b a b c c c d d d figure a graph it adjacency matrix a and it square the element of a and a indicate the number of path of length and path of length that start and end at vertex a a b a a c a and a d a but there is only one path of length from a to c a d reduction of optimization problem our next example deal with solving optimization if a problem asks to find a maximum of some function it is said to be a maximization problem if it asks to find a function minimum it is called a minimization suppose now that you need to find a minimum of some function f x and you have an algorithm for function how can you take advantage of the the answer lie in the simple formula min f x max f in other word to minimize a function we can maximize it negative instead and to get a correct minimal value of the function itself change the sign of the this property is illustrated for a function of one real variable in figure of course the formula max f x min f x is valid a well it show how a maximization problem can be reduced to an equivalent minimization this relationship between minimization and maximization problem is very general it hold for function defined on any domain in particular we can y f x f x x x f x f x figure relationship between minimization and maximization problem min f x max f apply it to function of several variable subject to additional a very important class of such problem is introduced below in this now that we are on the topic of function optimization it is worth pointing out that the standard calculus procedure for finding extremum point of a function is in fact also based on problem indeed it suggests finding the function derivative f x and then solving the equation f x to find the function critical in other word the optimization problem is reduced to the problem of solving an equation a the principal part of finding extremum note that we are not calling the calculus procedure an algorithm since it is not clearly in fact there is no general method for solving a little secret of calculus textbook is that problem are carefully selected so that critical point can always be found without this make the life of both student and instructor easier but in the process may unintentionally create a wrong impression in student linear programming many problem of optimal decision making can be reduced to an instance of the linear programming problem a problem of optimizing a linear function of several variable subject to constraint in the form of linear equation and linear example consider a university endowment that need to invest this sum ha to be split between three type of investment stock bond and the endowment manager expect an annual return of and for their stock bond and cash investment since stock are more risky than bond the endowment rule require the amount invested in stock to be no more than one third of the money invested in in addition at least of the total amount invested in stock and bond must be invested in how should the manager invest the money to maximize the let u create a mathematical model of this let x y and z be the amount in million of dollar invested in stock bond and cash by using these variable we can pose the following optimization problem maximize subject to x y z x y z y x y z although this example is both small and simple it doe show how a problem of optimal decision making can be reduced to an instance of the general linear programming problem maximize or minimize cx cnxn subject to aix ainxn or or bi for i m x xn the last group of constraint called the nonnegativity constraint are strictly speaking unnecessary because they are special case of more general constraint aix ainxn bi but it is convenient to treat them linear programming ha proved to be flexible enough to model a wide variety of important application such a airline crew scheduling transportation and communication network planning oil exploration and refining and industrial production in fact linear programming is considered by many a one of the most important achievement in the history of applied the classic algorithm for this problem is called the simplex method section it wa discovered by the mathematician george dantzig in the s although the worst case efficiency of this algorithm is known to be exponential it performs very well on typical moreover a more recent algorithm by narendra karmarkar kar not only ha a proven polynomial worstcase efficiency but ha also performed competitively with the simplex method in empirical it is important to stress however that the simplex method and karmarkars algorithm can successfully handle only linear programming problem that do not limit it variable to integer when variable of a linear programming problem are required to be integer the linear programming problem is said to be an integer linear programming except for some special case the assignment problem and the problem discussed in section integer linear programming problem are much more there is no known polynomial time algorithm for solving an arbitrary instance of the general integer linear programming problem and a we see in chapter such an algorithm quite possibly doe not other approach such a the branch and bound technique discussed in section are typically used for solving integer linear programming example let u see how the knapsack problem can be reduced to a linear programming recall from section that the knapsack problem can be posed a given a knapsack of capacity w and n item of weight w wn and value v vn find the most valuable subset of the item that fit into the we consider first the continuous or fractional version of the problem in which any fraction of any item given can be taken into the let xj j n be a variable representing a fraction of item j taken into the obviously xj must satisfy the inequality xj then the total weight of the selected item can be expressed by the sum n wj xj and their n j total value by the sum j vj xj thus the continuous version of the knapsack problem can be posed a the following linear programming problem n maximize vj xj j n subject to wj xj w j xj for j there is no need to apply a general method for solving linear programming problem here this particular problem can be solved by a simple special algorithm that is introduced in section but why try to discover it on your own this reduction of the knapsack problem to an instance of the linear programming problem is still useful though to prove the correctness of the algorithm in in the discrete or version of the knapsack problem we are only allowed either to take a whole item or not to take it at hence we have the following integer linear programming problem for this version n maximize vj xj j n subject to wj xj w j xj for j this seemingly minor modification make a drastic difference for the com plexity of this and similar problem constrained to take only discrete value in their potential despite the fact that the version might seem to be ea ier because it can ignore any subset of the continuous version that ha a fractional value of an item the version is in fact much more complicated than it con tinuous the reader interested in specific algorithm for solving this problem will find a wealth of literature on the subject including the monograph mar and reduction to graph problem a we pointed out in section many problem can be solved by a reduction to one of the standard graph this is true in particular for a variety of puzzle and in these application vertex of a graph typically represent possible state of the problem in question and edge indicate permitted transi tions among such one of the graph vertex represents an initial state and another represents a goal state of the there might be several vertex of the latter such a graph is called a state space thus the transfor mation just described reduces the problem to the question about a path from the initial state vertex to a goal state pwgc pg wc pg p pwc g pw c pwg pc pg pgc w w pgc pc pg pwg c pw g pwc p pg wc pg pwgc figure state space graph for the peasant wolf goat and cabbage example let u revisit the classic river crossing puzzle that wa included in the exercise for section a peasant find himself on a river bank with a wolf a goat and a head of he need to transport all three to the other side of the river in his however the boat ha room only for the peasant himself and one other item either the wolf the goat or the in his absence the wolf would eat the goat and the goat would eat the find a way for the peasant to solve his problem or prove that it ha no the state space graph for this problem is given in figure it vertex are labeled to indicate the state they represent p w g c stand for the peasant the wolf the goat and the cabbage respectively the two bar denote the river for convenience we also label the edge by indicating the boat occupant for each in term of this graph we are interested in finding a path from the initial state vertex labeled pwgc to the final state vertex labeled it is easy to see that there exist two distinct simple path from the initialstate vertex to the final state vertex what are if we find them by applying breadth first search we get a formal proof that these path have the smallest number of edge hence this puzzle ha two solution requiring seven river crossing which is the minimum number of crossing our success in solving this simple puzzle should not lead you to believe that generating and investigating state space graph is always a straightforward to get a better appreciation of them consult book on artificial intelligence ai the branch of computer science in which state space graph are a principal in this book we deal with an important special case of state space graph in section and exercise prove the equality lcmm n gcdm n that underlies the algorithm for computing lcmm euclid algorithm is known to be in olog if it is the algorithm that is used for computing gcdm n what is the efficiency of the algorithm for computing lcmm you are given a list of number for which you need to construct a min a min heap is a complete binary tree in which every key is le than or equal to the key in it how would you use an algorithm for constructing a max heap a heap a defined in section to construct a min prove that the number of different path of length k from the ith vertex to the j th vertex in a graph undirected or directed equal the i j th element of ak where a is the adjacency matrix of the design an algorithm with a time efficiency better than cubic for checking whether a graph with n vertex contains a cycle of length consider the following algorithm for the same starting at an arbi trary vertex traverse the graph by depth first search and check whether it depth first search forest ha a vertex with a back edge leading to it grand if it doe the graph contains a triangle if it doe not the graph doe not contain a triangle a it is this algorithm given n point p x y pn xn yn in the coordinate plane design an algorithm to check whether all the point lie within a triangle with it vertex at three of the point you can either design an algorithm from scratch or reduce the problem to another one with a known consider the problem of finding for a given positive integer n the pair of integer whose sum is n and whose product is a large a design an efficient algorithm for this problem and indicate it efficiency the assignment problem introduced in section can be stated a follows there are n people who need to be assigned to execute n job one person per that is each person is assigned to exactly one job and each job is assigned to exactly one the cost that would accrue if the ith person is assigned to the j th job is a known quantity ci j for each pair i j the problem is to assign the people to the job to minimize the total cost of the express the assignment problem a a linear programming solve the instance of the linear programming problem given in section maximize subject to x y z x y z y x y z the graph coloring problem is usually stated a the vertex coloring prob lem assign the smallest number of color to vertex of a given graph so that no two adjacent vertex are the same consider the edge coloring problem assign the smallest number of color possible to edge of a given graph so that no two edge with the same endpoint are the same ex plain how the edge coloring problem can be reduced to a vertex coloring consider the two dimensional post office location problem given n point x y xn yn in the cartesian plane find a location x y for a post nixi x yi y the average manhattan dis office that minimizes n tance from the post office to these explain how this problem can be efficiently solved by the problem reduction technique provided the post office doe not have to be located at one of the input jealous husband there are n married couple who need to cross a they have a boat that can hold no more than two people at a to complicate matter all the husband are jealous and will not agree on any crossing procedure that would put a wife on the same bank of the river with another woman husband without the wife husband being there too even if there are other people on the same can they cross the river under such solve the problem for n solve the problem for n which is the classical version of this doe the problem have a solution for n if it doe indicate how many river crossing it will take if it doe not explain double n domino domino are small rectangular tile with dot called spot or pip embossed at both half of the a standard double six domino set ha tile one for each unordered pair of integer from to in general a double n domino set would consist of domino tile for each unordered pair of integer from to n determine all value of n for which one construct a ring made up of all the tile in a double n domino summary transform and conquer is the fourth general algorithm design and problem solving strategy discussed in the it is in fact a group of technique based on the idea of transformation to a problem that is easier to there are three principal variety of the transform and conquer strategy instance simplification representation change and problem instance simplification is transforming an instance of a problem to an instance of the same problem with some special property that make the problem easier to list presorting gaussian elimination and rotation in avl tree are good example of this representation change implies changing one representation of a problem instance to another representation of the same example discussed in this chapter include representation of a set by a tree heap and heapsort horners rule for polynomial evaluation and two binary exponentiation problem reduction call for transforming a given problem to another problem that can be solved by a known among example of applying this idea to algorithmic problem solving see section reduction to linear programming and reduction to graph problem are especially some example used to illustrate transform and conquer happen to be very important data structure and they are heap and heapsort avl and tree gaussian elimination and horners a heap is an essentially complete binary tree with key one per node satisfying the parental dominance though defined a binary tree heap are normally implemented a heap are most important for the efficient implementation of priority queue they also underlie heapsort is a theoretically important sorting algorithm based on arranging element of an array in a heap and then successively removing the largest element from a remaining the algorithm running time is in n log n both in the worst case and in the average case in addition it is in avl tree are binary search tree that are always balanced to the extent possible for a binary the balance is maintained by transformation of four type called all basic operation on avl tree are in olog n it eliminates the bad worst case efficiency of classic binary search tree achieve a perfect balance in a search tree by allowing a node to contain up to two ordered key and have up to three this idea can be generalized to yield very important b tree discussed later in the gaussian elimination an algorithm for solving system of linear equation is a principal algorithm in linear it solves a system by transforming it to an equivalent system with an upper triangular coefficient matrix which is easy to solve by back gaussian elimination requires about n horners rule is an optimal algorithm for polynomial evaluation without coefficient it requires only n multiplication and n addition to evaluate an n degree polynomial at a given horners rule also ha a few useful byproduct such a the synthetic division two binary exponentiation algorithm for computing an are introduced in section both of them exploit the binary representation of the exponent n but they process it in the opposite direction left to right and right to linear programming concern optimizing a linear function of several variable subject to constraint in the form of linear equation and linear there are efficient algorithm capable of solving very large instance of this problem with many thousand of variable and constraint provided the variable are not required to be the latter called integer linear programming constitute a much more difficult class of fundamental data structure since the vast majority of algorithm of interest operate on data particular way of organizing data play a critical role in the design and analysis of a data structure can be defined a a particular scheme of organizing related data the nature of the data item is dictated by the problem at hand they can range from elementary data type integer or character to data structure a one dimensional array of one dimensional array is often used for implementing there are a few data structure that have proved to be particularly important for computer since you are undoubtedly familiar with most if not all of them just a quick review is provided linear data structure the two most important elementary data structure are the array and the linked a one dimensional array is a sequence of n item of the same data type that are stored contiguously in computer memory and made accessible by specifying a value of the array index figure in the majority of case the index is an integer either between and n a shown in figure or between and some computer language allow an array index to range between any two integer bound low and high and some even permit nonnumerical index to specify for example data item corresponding to the month of the year by the month each and every element of an array can be accessed in the same constant amount of time regardless of where in the array the element in question is this feature positively distinguishes array from linked list discussed array are used for implementing a variety of other data promi nent among them is the string a sequence of character from an alphabet termi nated by a special character indicating the string string composed of zero and one are called binary string or bit string are indispensable for pro cessing textual data defining computer language and compiling program written in them and studying abstract computational operation we usually per form on string differ from those we typically perform on other array say array of they include computing the string length comparing two string to determine which one precedes the other in lexicographic alphabetical or der and concatenating two string forming one string from two given string by appending the second to the end of the a linked list is a sequence of zero or more element called node each containing two kind of information some data and one or more link called pointer to other node of the linked a special pointer called null is used to indicate the absence of a node in a singly linked list each node except the last one contains a single pointer to the next element figure to access a particular node of a linked list one start with the list first node and traverse the pointer chain until the particular node is thus the time needed to access an element of a singly linked list unlike that of an array depends on where in the list the element is on the positive side linked list do item item item n figure array of n item item item n null figure singly linked list of n null item item item n null figure doubly linked list of n not require any preliminary reservation of the computer memory and insertion and deletion can be made quite efficiently in a linked list by reconnecting a few appropriate we can exploit flexibility of the linked list structure in a variety of for example it is often convenient to start a linked list with a special node called the this node may contain information about the linked list itself such a it current length it may also contain in addition to a pointer to the first element a pointer to the linked list last another extension is the structure called the doubly linked list in which every node except the first and the last contains pointer to both it successor and it predecessor figure the array and linked list are two principal choice in representing a more abstract data structure called a linear list or simply a a list is a finite sequence of data item a collection of data item arranged in a certain linear the basic operation performed on this data structure are searching for inserting and deleting an two special type of list stack and queue are particularly a stack is a list in which insertion and deletion can be done only at the this end is called the top because a stack is usually visualized not horizontally but vertically akin to a stack of plate whose operation it mimic very a a result when element are added to pushed onto a stack and deleted from popped off it the structure operates in a last infirst out lifo fashion exactly like a stack of plate if we can add or remove a plate only from the stack have a multitude of application in particular they are indispensable for implementing recursive a queue on the other hand is a list from which element are deleted from one end of the structure called the front this operation is called dequeue and new element are added to the other end called the rear this operation is called consequently a queue operates in a first infirst out fifo fashion akin to a queue of customer served by a single teller in a queue also have many important application including several algorithm for graph many important application require selection of an item of the highest priority among a dynamically changing set of a data structure that seek to satisfy the need of such application is called a priority a priority queue is a collection of data item from a totally ordered universe most often integer or real the principal operation on a priority queue are find ing it largest element deleting it largest element and adding a new of course a priority queue must be implemented so that the last two operation yield another priority straightforward implementation of this data struc ture can be based on either an array or a sorted array but neither of these option yield the most efficient solution a better implementation of a priority queue is based on an ingenious data structure called the we discus heap and an important sorting algorithm based on them in section graph a we mentioned in the previous section a graph is informally thought of a a collection of point in the plane called vertex or node some of them connected by line segment called edge or formally a graph g v e is defined by a pair of two set a finite nonempty set v of item called vertex and a set e of pair of these item called if these pair of vertex are unordered a pair of vertex u v is the same a the pair v u we say that the vertex u and v are adjacent to each other and that they are connected by the undirected edge u we call the vertex u and v endpoint of the edge u v and say that u and v are incident to this edge we also say that the edge u v is incident to it endpoint u and a graph g is called undirected if every edge in it is if a pair of vertex u v is not the same a the pair v u we say that the edge u v is directed from the vertex u called the edge tail to the vertex v called the edge we also say that the edge u v leaf u and enters a graph whose every edge is directed is called directed graph are also called it is normally convenient to label vertex of a graph or a digraph with letter integer number or if an application call for it character string figure the graph depicted in figure ha six vertex and seven undirected edge v a b c d e f e a c a d b c b f c e d e e f the digraph depicted in figure ha six vertex and eight directed edge v a b c d e f e a c b c b f c e d a d e e c e f a c b a c b d e f d e f a b figure a undirected b our definition of a graph doe not forbid loop or edge connecting vertex to unless explicitly stated otherwise we will consider graph without since our definition disallows multiple edge between the same vertex of an undirected graph we have the following inequality for the number of edge e possible in an undirected graph with v vertex and no loop e v v we get the largest number of edge in a graph if there is an edge connecting each of it v vertex with all v other we have to divide product v v by however because it includes every edge a graph with every pair of it vertex connected by an edge is called a standard notation for the complete graph with v vertex is kv a graph with relatively few possible edge missing is called dense a graph with few edge relative to the number of it vertex is called whether we are dealing with a dense or sparse graph may influence how we choose to represent the graph and consequently the running time of an algorithm being designed or graph representation graph for computer algorithm are usually represented in one of two way the adjacency matrix and adjacency the adjacency matrix of a graph with n vertex is an n n boolean matrix with one row and one column for each of the graph vertex in which the element in the ith row and the j th column is equal to if there is an edge from the ith vertex to the j th vertex and equal to if there is no such for example the adjacency matrix for the graph of figure is given in figure note that the adjacency matrix of an undirected graph is always symmetric ai j aj i for every i j n the adjacency list of a graph or a digraph is a collection of linked list one for each vertex that contain all the vertex adjacent to the list vertex all the vertex connected to it by an usually such list start with a header identifying a vertex for which the list is for example figure represents the graph in figure via it adjacency to put it another way a b c d e f a a c d b b c f c c a b e d d a e e e c d f f f b e a b figure a adjacency matrix and b adjacency list of the graph in figure adjacency list indicate column of the adjacency matrix that for a given vertex contain if a graph is sparse the adjacency list representation may use le space than the corresponding adjacency matrix despite the extra storage consumed by pointer of the linked list the situation is exactly opposite for dense in general which of the two representation is more convenient depends on the nature of the problem on the algorithm used for solving it and possibly on the type of input graph sparse or weighted graph a weighted graph or weighted digraph is a graph or di graph with number assigned to it these number are called weight or an interest in such graph is motivated by numerous real world applica tions such a finding the shortest path between two point in a transportation or communication network or the traveling salesman problem mentioned both principal representation of a graph can be easily adopted to accommo date weighted if a weighted graph is represented by it adjacency matrix then it element ai j will simply contain the weight of the edge from the ith to the j th vertex if there is such an edge and a special symbol if there is no such such a matrix is called the weight matrix or cost this approach is illustrated in figure for the weighted graph in figure for some ap plication it is more convenient to put s on the main diagonal of the adjacency adjacency list for a weighted graph have to include in their node not only the name of an adjacent vertex but also the weight of the corresponding edge figure path and cycle among the many property of graph two are important for a great number of application connectivity and both are based on the notion of a a path from vertex u to vertex v of a graph g can be defined a a sequence of adjacent connected by an edge vertex that start with u and end with if all vertex of a path are distinct the path is said to be the length of a path is the total number of vertex in the vertex sequence defining the path minus which is the same a the number of edge in the for example a c b f is a simple path of length from a to f in the graph in figure whereas a c e c b f is a path not simple of length from a to a b c d a b a a b c b b a c d c c a b d c d d d b c a b c figure a weighted b it weight c it adjacency a f b c e g h d i figure graph that is not in the case of a directed graph we are usually interested in directed a directed path is a sequence of vertex in which every consecutive pair of the vertex is connected by an edge directed from the vertex listed first to the vertex listed for example a c e f is a directed path from a to f in the graph in figure a graph is said to be connected if for every pair of it vertex u and v there is a path from u to if we make a model of a connected graph by connecting some ball representing the graph vertex with string representing the edge it will be a single if a graph is not connected such a model will consist of several connected piece that are called connected component of the formally a connected component is a maximal not expandable by including another vertex and an edge connected subgraph of a given for example the graph in figure and are connected whereas the graph in figure is not because there is no path for example from a to the graph in figure ha two connected component with vertex a b c d e and f g h i graph with several connected component do happen in real world appli a graph representing the interstate highway system of the united state would be an example it is important to know for many application whether or not a graph under consideration ha a cycle is a path of a positive length that start and end at the same vertex and doe not traverse the same edge more than for example f h i g f is a cycle in the graph in figure a graph with no cycle is said to be we discus acyclic graph in the next tree a tree more accurately a free tree is a connected acyclic graph figure a graph that ha no cycle but is not necessarily connected is called a forest each of it connected component is a tree figure a subgraph of a given graph g v e is a graph g v e such that v v and e a b a b h c d c d e i f g f g j a b figure a b i d a c a e b d e b c g f h g f h i a b figure a free b it transformation into a rooted tree have several important property other graph do not in par ticular the number of edge in a tree is always one le than the number of it vertex e v a the graph in figure demonstrates this property is necessary but not suffi cient for a graph to be a however for connected graph it is sufficient and hence provides a convenient way of checking whether a connected graph ha a rooted tree another very important property of tree is the fact that for every two vertex in a tree there always exists exactly one simple path from one of these vertex to the this property make it possible to select an arbitrary vertex in a free tree and consider it a the root of the so called rooted a rooted tree is usually depicted by placing it root on the top level of the tree the vertex adjacent to the root below it level the vertex two edge apart from the root still below level and so figure present such a transformation from a free tree to a rooted rooted tree play a very important role in computer science a much more important one than free tree do in fact for the sake of brevity they are often referred to a simply an obvious application of tree is for describing hierarchy from file directory to organizational chart of there are many le obvious application such a implementing dictionary see below efficient access to very large data set section and data encoding section a we discus in chapter tree also are helpful in analysis of recursive to finish this far from complete list of tree application we should mention the so called state space tree that underline two important algorithm design technique backtracking and branch and bound section and for any vertex v in a tree t all the vertex on the simple path from the root to that vertex are called ancestor of the vertex itself is usually considered it own ancestor the set of ancestor that excludes the vertex itself is referred to a the set of proper if u v is the last edge of the simple path from the root to vertex v and u v u is said to be the parent of v and v is called a child of u vertex that have the same parent are said to be a vertex with no child is called a leaf a vertex with at least one child is called all the vertex for which a vertex v is an ancestor are said to be descendant of v the proper descendant exclude the vertex v all the descendant of a vertex v with all the edge connecting them form the subtree of t rooted at that thus for the tree in figure the root of the tree is a vertex d g f h and i are leaf and vertex a b e and c are parental the parent of b is a the child of b are c and g the sibling of b are d and e and the vertex of the subtree rooted at b are b c g h the depth of a vertex v is the length of the simple path from the root to the height of a tree is the length of the longest simple path from the root to a for example the depth of vertex c of the tree in figure is and the height of the tree is thus if we count tree level top down starting with for the root level the depth of a vertex is simply it level in the tree and the tree height is the maximum level of it you should be alert to the fact that some author define the height of a tree a the number of level in it this make the height of a tree larger by than the height defined a the length of the longest simple path from the root to a ordered tree an ordered tree is a rooted tree in which all the child of each vertex are it is convenient to assume that in a tree diagram all the child are ordered left to a binary tree can be defined a an ordered tree in which every vertex ha no more than two child and each child is designated a either a left child or a right child of it parent a binary tree may also be an example of a binary tree is given in figure the binary tree with it root at the left right child of a vertex in a binary tree is called the left right subtree of that since left and right subtrees are binary tree a well a binary tree can also be defined this make it possible to solve many problem involving binary tree by recursive a b figure a binary b binary search null null null null null null null null figure standard implementation of the binary search tree in figure in figure some number are assigned to vertex of the binary tree in figure note that a number assigned to each parental vertex is larger than all the number in it left subtree and smaller than all the number in it right such tree are called binary search binary tree and binary search tree have a wide variety of application in computer science you will encounter some of them throughout the in particular binary search tree can be generalized to more general type of search tree called multiway search tree which are indispensable for efficient access to very large data a you will see later in the book the efficiency of most important algorithm for binary search tree and their extension depends on the tree there fore the following inequality for the height h of a binary tree with n node are especially important for analysis of such algorithm log n h n a binary tree is usually implemented for computing purpose by a collection of node corresponding to vertex of the each node contains some informa tion associated with the vertex it name or some value assigned to it and two pointer to the node representing the left child and right child of the vertex re figure illustrates such an implementation for the binary search tree in figure a computer representation of an arbitrary ordered tree can be done by simply providing a parental vertex with the number of pointer equal to the number of it this representation may prove to be inconvenient if the number of child varies widely among the we can avoid this inconvenience by using node with just two pointer a we did for binary here however the left pointer will point to the first child of the vertex and the right pointer will point to it next accordingly this representation is called the first childnext sibling thus all the sibling of a vertex are linked via the node right pointer in a singly linked list with the first element of the list pointed to by the left pointer of their figure illustrates this representation for the tree in figure it is not difficult to see that this representation effectively transforms an ordered tree into a binary tree said to be associated with the ordered we get this representation by rotating the pointer about degree clockwise see figure set and dictionary the notion of a set play a central role in a set can be described a an unordered collection possibly empty of distinct item called element of the a null a b null d e null b c null g null null f null c d null h null i null h g e i f a b figure a first childnext sibling representation of the tree in figure b it binary tree a specific set is defined either by an explicit listing of it element s or by specifying a property that all the set element and only they must satisfy s n n is a prime number smaller than the most important set operation are checking membership of a given item in a given set finding the union of two set which comprises all the element in either or both of them and finding the intersection of two set which comprises all the common element in the set can be implemented in computer application in two the first considers only set that are subset of some large set u called the universal if set u ha n element then any subset s of u can be represented by a bit string of size n called a bit vector in which the ith element is if and only if the ith element of u is included in set thus to continue with our example if u then s is represented by the bit string this way of representing set make it possible to implement the standard set operation very fast but at the expense of potentially using a large amount of the second and more common way to represent a set for computing purpose is to use the list structure to indicate the set of course this option too is feasible only for finite set fortunately unlike mathematics this is the kind of set most computer application note however the two principal point of distinction between set and first a set cannot contain identical element a list this requirement for uniqueness is sometimes circumvented by the introduction of a multiset or bag an unordered collection of item that are not necessarily second a set is an unordered collection of item therefore changing the order of it element doe not change the a list defined a an ordered collection of item is exactly the this is an important theoretical distinction but fortunately it is not important for many it is also worth mentioning that if a set is represented by a list depending on the application at hand it might be worth maintaining the list in a sorted in computing the operation we need to perform for a set or a multiset most often are searching for a given item adding a new item and deleting an item from the a data structure that implement these three operation is called the note the relationship between this data structure and the problem of searching mentioned in section obviously we are dealing here with searching in a dynamic consequently an efficient implementation of a dictionary ha to strike a compromise between the efficiency of searching and the efficiency of the other two there are quite a few way a dictionary can be they range from an unsophisticated use of array sorted or not to much more sophisticated technique such a hashing and balanced search tree which we discus later in the a number of application in computing require a dynamic partition of some n element set into a collection of disjoint after being initialized a a collection of n one element subset the collection is subjected to a sequence of intermixed union and search this problem is called the set union we discus efficient algorithmic solution to this problem in section in conjunction with one of it important you may have noticed that in our review of basic data structure we almost always mentioned specific operation that are typically performed for the structure in this intimate relationship between the data and operation ha been recognized by computer scientist for a long it ha led them in particular to the idea of an abstract data type adt a set of abstract object representing data item with a collection of operation that can be performed on a illustration of this notion reread say our definition of the priority queue and although abstract data type could be implemented in older procedural language such a pascal see aho it is much more convenient to do this in object oriented language such a c and java which support abstract data type by mean of exercise describe how one can implement each of the following operation on an array so that the time it take doe not depend on the array size delete the ith element of an array i delete the ith element of a sorted array the remaining array ha to stay sorted of if you have to solve the searching problem for a list of n number how can you take advantage of the fact that the list is known to be give separate answer for list represented a list represented a linked show the stack after each operation of the following sequence that start with the empty stack pusha pushb pop pushc pushd pop show the queue after each operation of the following sequence that start with the empty queue enqueuea enqueueb dequeue enqueuec enqueued dequeue let a be the adjacency matrix of an undirected explain what prop erty of the matrix indicates that the graph is the graph ha a loop an edge connecting a vertex to the graph ha an isolated vertex a vertex with no edge incident to answer the same question for the adjacency list give a detailed description of an algorithm for transforming a free tree into a tree rooted at a given vertex of the free prove the inequality that bracket the height of a binary tree with n vertex log n h n indicate how the adt priority queue can be implemented a an unsorted a sorted a binary search how would you implement a dictionary of a reasonably small size n if you knew that all it element are distinct name of the state of the united specify an implementation of each dictionary for each of the following application indicate the most appropriate data structure answering telephone call in the order of their known priority sending backlog order to customer in the order they have been received implementing a calculator for computing simple arithmetical expression anagram checking design an algorithm for checking whether two given word are anagram whether one word can be obtained by permuting the letter of the for example the word tea and eat are summary an algorithm is a sequence of nonambiguous instruction for solving a problem in a finite amount of an input to an algorithm specifies an instance of the problem the algorithm algorithm can be specified in a natural language or pseudocode they can also be implemented a computer among several way to classify algorithm the two principal alternative are to group algorithm according to type of problem they solve to group algorithm according to underlying design technique they are based upon the important problem type are sorting searching string processing graph problem combinatorial problem geometric problem and numerical algorithm design technique or strategy or paradigm are general approach to solving problem algorithmically applicable to a variety of problem from different area of although designing an algorithm is undoubtedly a creative activity one can identify a sequence of interrelated action involved in such a they are summarized in figure a good algorithm is usually the result of repeated effort and the same problem can often be solved by several for example three algorithm were given for computing the greatest common divisor of two integer euclid algorithm the consecutive integer checking algorithm and the middle school method enhanced by the sieve of eratosthenes for generating a list of algorithm operate on this make the issue of data structuring critical for efficient algorithmic problem the most important elementary data structure are the array and the linked they are used for representing more abstract data structure such a the list the stack the queue the graph via it adjacency matrix or adjacency list the binary tree and the an abstract collection of object with several operation that can be performed on them is called an abstract data type the list the stack the queue the priority queue and the dictionary are important example of abstract data modern object oriented language support implementation of adts by mean of space and time trade offs thing which matter most must never be at the mercy of thing which matter johann wolfgang von go ethe space and time trade offs in algorithm design are a well known issue for both theoretician and practitioner of consider a an example the problem of computing value of a function at many point in it if it is time that is at a premium we can precompute the function value and store them in a this is exactly what human computer had to do before the advent of electronic computer in the process burdening library with thick volume of mathematical though such table have lost much of their appeal with the widespread use of electronic computer the underlying idea ha proven to be quite useful in the development of several important algorithm for other in somewhat more general term the idea is to preprocess the problem input in whole or in part and store the additional information obtained to accelerate solving the problem we call this approach input enhancement and discus the following algorithm based on it counting method for sorting section boyer moore algorithm for string matching and it simplified version sug gested by horspool section the other type of technique that exploit space for time trade offs simply us extra space to facilitate faster andor more flexible access to the we call this approach this name highlight two facet of this variation of the space for time trade off some processing is done before a problem in question the standard term used synonymously for this technique are preprocessing and confusingly these term can also be applied to method that use the idea of preprocessing but do not use extra space see chapter thus in order to avoid confusion we use input enhancement a a special name for the space for time trade off technique being discussed is actually solved but unlike the input enhancement variety it deal with access we illustrate this approach by hashing section indexing with b tree section there is one more algorithm design technique related to the space for time trade off idea dynamic this strategy is based on recording solu tions to overlapping subproblems of a given problem in a table from which a solu tion to the problem in question is then we discus this well developed technique separately in the next chapter of the two final comment about the interplay between time and space in algo rithm design need to be first the two resource time and space do not have to compete with each other in all design in fact they can align to bring an algorithmic solution that minimizes both the running time and the space such a situation arises in particular when an algorithm us a space efficient data structure to represent a problem input which lead in turn to a faster consider a an example the problem of traversing re call that the time efficiency of the two principal traversal algorithm depth first search and breadth first search depends on the data structure used for repre senting graph it is n for the adjacency matrix representation and n m for the adjacency list representation where n and m are the number of vertex and edge if input graph are sparse have few edge relative to the number of vertex say m on the adjacency list representation may well be more efficient from both the space and the running time point of the same situation arises in the manipulation of sparse matrix and sparse polynomi al if the percentage of zero in such object is sufficiently high we can save both space and time by ignoring zero in the object representation and second one cannot discus space time trade offs without mentioning the hugely important area of data note however that in data compres sion size reduction is the goal rather than a technique for solving another we discus just one data compression algorithm in the next the reader interested in this topic will find a wealth of algorithm in such book a sorting by counting a a first example of applying the input enhancement technique we discus it application to the sorting one rather obvious idea is to count for each element of a list to be sorted the total number of element smaller than this element and record the result in a these number will indicate the position of the element in the sorted list if the count is for some element it should be in the th position with index if we start counting with in the sorted thus we will be able to sort the list by simply copying it element to their appropriate position in a new sorted this algorithm is called comparison counting sort figure array initially count after pas i count after pas i count after pas i count after pas i count after pas i count final state count array figure example of sorting by comparison algorithm sort an array by comparison counting input an array of orderable element output array of a element sorted in nondecreasing order for i to n do counti for i to n do for j i to n do if ai aj countj countj else counti counti for i to n do scounti ai return s what is the time efficiency of this it should be quadratic because the algorithm considers all the different pair of an n element more formally the number of time it basic operation the comparison ai aj is executed is equal to the sum we have encountered several time already n n n n nn cn n i n i i j i i i thus the algorithm make the same number of key comparison a selection sort and in addition us a linear amount of extra on the positive side the algorithm make the minimum number of key move possible placing each of them directly in their final position in a sorted the counting idea doe work productively in a situation in which element to be sorted belong to a known small set of assume for example that we have to sort a list whose value can be either or rather than applying a general sorting algorithm we should be able to take advantage of this additional information about value to be indeed we can scan the list to compute the number of s and the number of s in it and then on the second pas simply make the appropriate number of the first element equal to and the remaining element equal to more generally if element value are integer between some lower bound l and upper bound u we can compute the frequency of each of those value and store them in array f then the first f position in the sorted list must be filled with l the next f position with l and so all this can be done of course only if we can overwrite the given let u consider a more realistic situation of sorting a list of item with some other information associated with their key so that we cannot overwrite the list then we can copy element into a new array to hold the sorted list a the element of a whose value are equal to the lowest possible value l are copied into the first f element of s position through f the element of value l are copied to position from f to f f and so since such accumulated sum of frequency are called a distribution in statistic the method itself is known a distribution example consider sorting the array whose value are known to come from the set and should not be overwritten in the process of the frequency and distribution array are a follows array value frequency distribution value note that the distribution value indicate the proper position for the last occur rences of their element in the final sorted if we index array position from to n the distribution value must be reduced by to get corresponding element it is more convenient to process the input array right to for the example the last element is and since it distribution value is we place this in position of the array s that will hold the sorted then we decrease the s distribution value by and proceed to the next from the right element in the given the entire processing of this example is depicted in figure d s a a a a a a figure example of sorting by distribution the distribution value being decremented are shown in here is pseudocode of this algorithm l u sort an array of integer from a limited range by distribution counting input an array of integer between l and u l u output array of a element sorted in nondecreasing order for j to u l do dj initialize frequency for i to n do dai l dai l compute frequency for j to u l do dj dj dj reuse for distribution for i n downto do j ai l sdj ai dj dj return s assuming that the range of array value is fixed this is obviously a linear algorithm because it make just two consecutive pass through it input array this is a better time efficiency class than that of the most efficient sorting algorithm mergesort quicksort and heapsort we have it is important to remember however that this efficiency is obtained by exploiting the specific nature of input for which sorting by distribution counting work in addition to trading space for exercise is it possible to exchange numeric value of two variable say u and v without using any extra will the comparison counting algorithm work correctly for array with equal assuming that the set of possible list value is a b c d sort the following list in alphabetical order by the distribution counting algorithm b c d c b a a is the distribution counting algorithm design a one line algorithm for sorting any array of size n whose value are n distinct integer from to the ancestry problem asks to determine whether a vertex u is an ancestor of vertex v in a given binary or more generally rooted ordered tree of n design a on input enhancement algorithm that provides sufficient information to solve this problem for any pair of the tree vertex in constant the following technique known a virtual initialization provides a time efficient way to initialize just some element of a given array so that for each of it element we can say in constant time whether it ha been initialized and if it ha been with which this is done by utilizing a variable counter for the number of initialized element in a and two auxiliary array of the same size say and defined a b bcounter contain the index of the element of a that were initialized b contains the index of the element initialized first b contains the index of the element initialized second furthermore if ai wa the kth element k counter to be initialized ci contains sketch the state of array and after the three a signments a x a z a in general how can we check with this scheme whether ai ha been initialized and if it ha been with which least distance sorting there are egyptian stone statue standing in a row in an art gallery a new curator want to move them so that the statue are ordered by their how should this be done to minimize the total distance that the statue are you may assume for simplicity that all the statue have different azi write a program for multiplying two sparse matrix a p q matrix a and a q r matrix write a program for multiplying two sparse polynomial px and qx of degree m and n is it a good idea to write a program that play the classic game of tic tac toe with the human user by storing all possible position on the game board along with the best move for each of input enhancement in string matching in this section we see how the technique of input enhancement can be applied to the problem of string recall that the problem of string matching requires finding an occurrence of a given string of m character called the pattern in a longer string of n character called the we discussed the brute force algorithm for this problem in section it simply match corresponding pair of character in the pattern and the text left to right and if a mismatch occurs shift the pattern one position to the right for the next since the maximum number of such trial is n m and in the worst case m comparison need to be made on each of them the worst case efficiency of the brute force algorithm is in the onm on average however we should expect just a few comparison before a pattern shift and for random natural language text the average case efficiency indeed turn out to be in on several faster algorithm have been most of them exploit the input enhancement idea preprocess the pattern to get some information about it store this information in a table and then use this information during an actual search for the pattern in a given this is exactly the idea behind the two bestknown algorithm of this type the knuth morris pratt algorithm knu and the boyer moore algorithm the principal difference between these two algorithm lie in the way they compare character of a pattern with their counterpart in a text the knuthmorris pratt algorithm doe it left to right whereas the boyer moore algorithm doe it right to since the latter idea lead to simpler algorithm it is the only one that we will pursue note that the boyer moore algorithm start by aligning the pattern against the beginning character of the text if the first trial fails it shift the pattern to the it is comparison within a trial that the algorithm doe right to left starting with the last character in the although the underlying idea of the boyer moore algorithm is simple it actual implementation in a working method is le therefore we start our discussion with a simplified version of the boyer moore algorithm suggested by horspool in addition to being simpler horspools algorithm is not necessarily le efficient than the boyer moore algorithm on random horspools algorithm consider a an example searching for the pattern barber in some text s c sn b a r b e r starting with the last r of the pattern and moving right to left we compare the corresponding pair of character in the pattern and the if all the pattern character match successfully a matching substring is then the search can be either stopped altogether or continued if another occurrence of the same pattern is if a mismatch occurs we need to shift the pattern to the clearly we would like to make a large a shift a possible without risking the possibility of missing a matching substring in the horspools algorithm determines the size of such a shift by looking at the character c of the text that is aligned against the last character of the this is the case even if character c itself match it counterpart in the in general the following four possibility can case if there are no c in the pattern c is letter s in our example we can safely shift the pattern by it entire length if we shift le some character of the pattern would be aligned against the text character c that is known not to be in the pattern s s sn b a r b e r b a r b e r case if there are occurrence of character c in the pattern but it is not the last one there c is letter b in our example the shift should align the rightmost occurrence of c in the pattern with the c in the text s b sn b a r b e r b a r b e r case if c happens to be the last character in the pattern but there are no c among it other m character c is letter r in our example the situation is similar to that of case and the pattern should be shifted by the entire pattern length m s m e r sn l e a d e r l e a d e r case finally if c happens to be the last character in the pattern and there are other c among it first m character c is letter r in our example the situation is similar to that of case and the rightmost occurrence of c among the first m character in the pattern should be aligned with the text c s a r sn r e o r d e r r e o r d e r these example clearly demonstrate that right to left character comparison can lead to farther shift of the pattern than the shift by only one position always made by the brute force however if such an algorithm had to check all the character of the pattern on every trial it would lose much of this fortunately the idea of input enhancement make repetitive comparison we can precompute shift size and store them in a the table will be indexed by all possible character that can be encountered in a text including for natural language text the space punctuation symbol and other special note that no other information about the text in which eventual searching will be done is the table entry will indicate the shift size computed by the formula the pattern length m if c is not among the first m character of the pattern t c the distance from the rightmost c among the first m character of the pattern to it last character for example for the pattern barber all the table entry will be equal to except for the entry for e b r and a which will be and here is a simple algorithm for computing the shift table initialize all the entry to the pattern length m and scan the pattern left to right repeating the following step m time for the j th character of the pattern j m overwrite it entry in the table with m j which is the character distance to the last character of the note that since the algorithm scan the pattern from left to right the last overwrite will happen for the character rightmost occurrence exactly a we would like it to algorithm shifttablep fill the shift table used by horspools and boyer moore algorithm input pattern p and an alphabet of possible character output indexed by the alphabet character and filled with shift size computed by formula for i to size do tablei m for j to m do tablep j m j return table now we can summarize the algorithm a follows horspools algorithm step for a given pattern of length m and the alphabet used in both the pattern and text construct the shift table a described step align the pattern against the beginning of the step repeat the following until either a matching substring is found or the pattern reach beyond the last character of the starting with the last character in the pattern compare the corresponding character in the pattern and text until either all m character are matched then stop or a mismatching pair is in the latter case retrieve the entry t c from the c column of the shift table where c is the text character currently aligned against the last character of the pattern and shift the pattern by t c character to the right along the here is pseudocode of horspools algorithm horspoolmatchingp t implement horspools algorithm for string matching input pattern p and text t output the index of the left end of the first matching substring or if there are no match shifttablep generate table of shift i m position of the pattern right end while i n do k number of matched character while k m and p m k t i k do kk if k m return i m else i i tablet i return example a an example of a complete application of horspools algorithm consider searching for the pattern barber in a text that comprises english letter and space denoted by the shift table a we mentioned is filled a follows character c a b c d e f r z shift t c the actual search in a particular text proceeds a follows j i m s a w m e i n a b a r b e r s h o p b a r b e r b a r b e r b a r b e r b a r b e r b a r b e r b a r b e r a simple example can demonstrate that the worst case efficiency of hor spool algorithm is in onm problem in this section but for random text it is in n and although in the same efficiency class horspools algorithm is obviously faster on average than the brute force in fact a mentioned it is often at least a efficient a it more sophisticated predecessor discovered by boyer and boyer moore algorithm now we outline the boyer moore algorithm if the first comparison of the rightmost character in the pattern with the corresponding character c in the text fails the algorithm doe exactly the same thing a horspools namely it shift the pattern to the right by the number of character retrieved from the table precomputed a explained the two algorithm act differently however after some positive number k k m of the pattern character are matched successfully before a mismatch is encountered s c si k si sn text p pm k pm k pm pattern in this situation the boyer moore algorithm determines the shift size by considering two the first one is guided by the text character c that caused a mismatch with it counterpart in the accordingly it is called the badsymbol the reasoning behind this shift is the reasoning we used in horspools if c is not in the pattern we shift the pattern to just pas this c in the conveniently the size of this shift can be computed by the formula tc k where tc is the entry in the precomputed table used by horspools algorithm see above and k is the number of matched character s c si k si sn text p pm k pm k pm pattern p pm for example if we search for the pattern barber in some text and match the last two character before failing on letter s in the text we can shift the pattern by t position s s e r sn b a r b e r b a r b e r the same formula can also be used when the mismatching character c of the text occurs in the pattern provided tc k for example if we search for the pattern barber in some text and match the last two character before failing on letter a we can shift the pattern by ta position s a e r sn b a r b e r b a r b e r if tc k we obviously do not want to shift the pattern by or a negative number of rather we can fall back on the brute force thinking and simply shift the pattern by one position to the to summarize the bad symbol shift d is computed by the boyer moore algorithm either a tc k if this quantity is positive and a if it is negative or this can be expressed by the following compact formula d maxtc k the second type of shift is guided by a successful match of the last k character of the we refer to the ending portion of the pattern a it suffix of size k and denote it suff accordingly we call this type of shift the good suffix we now apply the reasoning that guided u in filling the bad symbol shift table which wa based on a single alphabet character c to the pattern suffix of size m to fill in the good suffix shift let u first consider the case when there is another occurrence of suff k in the pattern or to be more accurate there is another occurrence of suff k not preceded by the same character a in it rightmost it would be useless to shift the pattern to match another occurrence of suff k preceded by the same character because this would simply repeat a failed in this case we can shift the pattern by the distance d between such a second rightmost occurrence not preceded by the same character a in the rightmost occurrence of suff k and it rightmost for example for the pattern abcbab these distance for k and will be and respectively k pattern d abcbab abcbab what is to be done if there is no other occurrence of suff k not preceded by the same character a in it rightmost in most case we can shift the pattern by it entire length for example for the pattern dbcbab and k we can shift the pattern by it entire length of character s c b a b sn d b c b a b d b c b a b unfortunately shifting the pattern by it entire length when there is no other occurrence of suff k not preceded by the same character a in it rightmost occurrence is not always for example for the pattern abcbab and k shifting by could miss a matching substring that start with the text ab aligned with the last two character of the pattern s c b a b c b a b sn a b c b a b a b c b a b note that the shift by is correct for the pattern dbcbab but not for abcbab because the latter pattern ha the same substring ab a it prefix beginning part of the pattern and a it suffix ending part of the to avoid such an erroneous shift based on a suffix of size k for which there is no other occurrence in the pattern not preceded by the same character a in it rightmost occurrence we need to find the longest prefix of size l k that match the suffix of the same size if such a prefix exists the shift size d is computed a the distance between this prefix and the corresponding suffix otherwise d is set to the pattern length a an example here is the complete list of the d value the good suffix table of the boyer moore algorithm for the pattern abcbab k pattern d abcbab abcbab abcbab abcbab abcbab now we are prepared to summarize the boyer moore algorithm in it the boyer moore algorithm step for a given pattern and the alphabet used in both the pattern and the text construct the bad symbol shift table a described step using the pattern construct the good suffix shift table a described step align the pattern against the beginning of the step repeat the following step until either a matching substring is found or the pattern reach beyond the last character of the starting with the last character in the pattern compare the corresponding character in the pattern and the text until either all m character pair are matched then stop or a mismatching pair is encountered after k character pair are matched in the latter case retrieve the entry tc from the c column of the bad symbol table where c is the text mismatched if k also retrieve the corresponding d entry from the good suffix shift the pattern to the right by the number of position computed by the formula d d if k maxd d if k where d maxtc k shifting by the maximum of the two available shift when k is quite log the two shift are based on the observation the first one about a text mismatched character and the second one about a matched group of the pattern rightmost character that imply that shifting by le than d and d character re spectively cannot lead to aligning the pattern with a matching substring in the since we are interested in shifting the pattern a far a possible without missing a possible matching substring we take the maximum of these two example a a complete example let u consider searching for the pattern baobab in a text made of english letter and the bad symbol table look a follows c a b c d o z tc the good suffix table is filled a follows k pattern d baobab baobab baobab baobab baobab the actual search for this pattern in the text given in figure proceeds a after the last b of the pattern fails to match it counterpart k in the text the algorithm retrieves tk from the bad symbol table and shift the pat tern by d maxtk position to the the new try successfully match two pair of after the failure of the third comparison on the space character in the text the algorithm retrieves t from the bad symbol table and d from the good suffix table to shift the pattern by maxd d max note that on this iteration it is the good suffix rule that lead to a farther shift of the the next try successfully match just one pair of after the failure of the next comparison on the space character in the text the algorithm retrieves t from the bad symbol table and d from the good suffix table to shift b e s s k n e w a b o u t b a o b a b s b a o b a b d tk b a o b a b d t b a o b a b d d t d max d d max b a o b a b figure example of string matching with the boyer moore the pattern by maxdd max note that on this iteration it is the bad symbol rule that lead to a farther shift of the the next try find a matching substring in the text after successfully matching all six character of the pattern with their counterpart in the when searching for the first occurrence of the pattern the worst case effi ciency of the boyer moore algorithm is known to be though this algorithm run very fast especially on large alphabet relative to the length of the pattern many people prefer it simplified version such a horspools algorithm when dealing with natural languagelike exercise apply horspools algorithm to search for the pattern baobab in the text be knew about baobab consider the problem of searching for gene in dna sequence using hor spool a dna sequence is represented by a text on the alphabet a c g t and the gene or gene segment is the construct the shift table for the following gene segment of your chromo some tcctattctt apply horspools algorithm to locate the above pattern in the following dna sequence ttatagatctcgtattcttttatagatctcctattctt how many character comparison will be made by horspools algorithm in searching for each of the following pattern in the binary text of for searching in a text of length n for a pattern of length m n m with horspools algorithm give an example of worst case best case is it possible for horspools algorithm to make more character comparison than the brute force algorithm would make in searching for the same pattern in the same if horspools algorithm discovers a matching substring how large a shift should it make to search for a next possible how many character comparison will the boyer moore algorithm make in searching for each of the following pattern in the binary text of would the boyer moore algorithm work correctly with just the bad symbol table to guide pattern would the boyer moore algorithm work correctly with just the good suffix table to guide pattern if the last character of a pattern and it counterpart in the text do match doe horspools algorithm have to check other character right to left or can it check them left to right answer the same question for the boyer moore implement horspools algorithm the boyer moore algorithm and the brute force algorithm of section in the language of your choice and run an experiment to compare their efficiency for matching random binary pattern in random binary random natural language pattern in natural language you are given two string s and t each n character you have to establish whether one of them is a right cyclic shift of the for example plea is a right cyclic shift of leap and vice formally t is a right cyclic shift of s if t can be obtained by concatenating the n i character suffix of s and the i character prefix of s for some i design a space efficient algorithm for the indicate the space and time efficiency of your design a time efficient algorithm for the indicate the time and space efficiency of your hashing in this section we consider a very efficient way to implement recall that a dictionary is an abstract data type namely a set with the operation of searching lookup insertion and deletion defined on it the element of this set can be of an arbitrary nature number character of some alphabet character string and so in practice the most important case is that of record student record in a school citizen record in a governmental office book record in a typically record comprise several field each responsible for keeping a particular type of information about an entity the record for example a student record may contain field for the student id name date of birth sex home address major and so among record field there is usually at least one called a key that is used for identifying entity represented by the record the student in the discussion below we assume that we have to implement a dictionary of n record with key k k hashing is based on the idea of distributing key among a one dimensional array h called a hash the distribution is done by computing for each of the key the value of some predefined function h called the hash this function assigns an integer between and m called the hash address to a for example if key are nonnegative integer a hash function can be of the form hk k mod m obviously the remainder of division by m is always between and m if key are letter of some alphabet we can first assign a letter it position in the alphabet denoted here ordk and then apply the same kind of a function used for finally if k is a character string cc c we s can use a a very unsophisticated option or d ci mod a better option i is to compute hk a follows h for i to s do h h c ordci mod m where c is a constant larger than every in general a hash function need to satisfy somewhat conflicting require ments a hash table size should not be excessively large compared to the number of key but it should be sufficient to not jeopardize the implementation time efficiency see a hash function need to distribute key among the cell of the hash table a evenly a this requirement make it desirable for most application to have a hash function dependent on all bit of a key not just some of a hash function ha to be easy to this can be obtained by treating ordci a digit of a number in the c based system computing it decimal value by horners rule and finding the remainder of the number after dividing it by ki kj b m figure collision of two key in hashing hki hkj obviously if we choose a hash table size m to be smaller than the number of key n we will get collision a phenomenon of two or more key being hashed into the same cell of the hash table figure but collision should be expected even if m is considerably larger than n see problem in this section in fact in the worst case all the key could be hashed to the same cell of the hash fortunately with an appropriately chosen hash table size and a good hash function this situation happens very still every hashing scheme must have a collision resolution this mechanism is different in the two principal version of hashing open hashing also called separate chaining and closed hashing also called open open hashing separate chaining in open hashing key are stored in linked list attached to cell of a hash each list contains all the key hashed to it consider a an example the following list of word a fool and his money are soon a a hash function we will use the simple function for string mentioned above we will add the position of a word letter in the alphabet and compute the sum remainder after division by we start with the empty the first key is the word a it hash value is ha mod the second key the word fool is installed in the ninth cell since mod and so the final result of this process is given in figure note a collision of the key are and soon because hare mod and hsoon mod how do we search in a dictionary implemented a such a table of linked we do this by simply applying to a search key the same procedure that wa used for creating the to illustrate if we want to search for the key kid in the hash table of figure we first compute the value of the same hash function for the key hkid since the list attached to cell is not empty it linked list may contain the search but because of possible collision we cannot tell whether this is the case until we traverse this linked after comparing the string kid first with the string are and then with the string soon we end up with an unsuccessful in general the efficiency of searching depends on the length of the linked list which in turn depend on the dictionary and table size a well a the quality key a fool and his money are soon parted hash address a and money fool his are parted soon figure example of a hash table construction with separate of the hash if the hash function distributes n key among m cell of the hash table about evenly each list will be about nm key the ratio nm called the load factor of the hash table play a crucial role in the efficiency of in particular the average number of pointer chain link inspected in successful search s and unsuccessful search u turn out to be s and u respectively under the standard assumption of searching for a randomly selected element and a hash function distributing key uniformly among the table these result are quite indeed they are almost identical to searching sequentially in a linked list what we have gained by hashing is a reduction in average list size by a factor of m the size of the hash normally we want the load factor to be not far from having it too small would imply a lot of empty list and hence inefficient use of space having it too large would mean longer linked list and hence longer search but if we do have the load factor around we have an amazingly efficient scheme that make it possible to search for a given key for on average the price of one or two true in addition to comparison we need to spend time on computing the value of the hash function for a search key but it is a constant time operation independent from n and note that we are getting this remarkable efficiency not only a a result of the method ingenuity but also at the expense of extra the two other dictionary operation insertion and deletion are almost identical to insertion are normally done at the end of a list but see problem in this section exercise for a possible modification of this deletion is performed by searching for a key to be deleted and then removing it from it hence the efficiency of these operation is identical to that of searching and they are all in the average case if the number of key n is about equal to the hash table size closed hashing open addressing in closed hashing all key are stored in the hash table itself without the use of linked of course this implies that the table size m must be at least a large a the number of key different strategy can be employed for collision the simplest one called linear probing check the cell following the one where the collision if that cell is empty the new key is installed there if the next cell is already occupied the availability of that cell immediate successor is checked and so note that if the end of the hash table is reached the search is wrapped to the beginning of the table it is treated a a circular this method is illustrated in figure with the same word list and hash function used above to illustrate separate to search for a given key k we start by computing hk where h is the hash function used in the table if the cell hk is empty the search is if the cell is not empty we must compare k with the cell occupant if they are equal we have found a matching key if they are not we compare k with a key in the next cell and continue in this manner until we encounter either a matching key a successful search or an empty cell unsuccessful for example if we search for the word lit in the table of figure we will get hlit mod and since cell is empty we can stop however if we search for kid with hkid mod we will have to compare kid with are soon parted and a before we can declare the search although the search and insertion operation are straightforward for this version of hashing deletion is for example if we simply delete the key are from the last state of the hash table in figure we will be unable to find the key soon indeed after computing hsoon the algorithm would find this location empty and report the unsuccessful search a simple solution key a fool and his money are soon parted hash address a a fool a and fool a and fool his a and money fool his a and money fool his are a and money fool his are soon parted a and money fool his are soon figure example of a hash table construction with linear is to use lazy deletion to mark previously occupied location by a special symbol to distinguish them from location that have not been the mathematical analysis of linear probing is a much more difficult problem than that of separate the simplified version of these result state that the average number of time the algorithm must access the hash table with the load factor in successful and unsuccessful search is respectively s and u and the accuracy of these approximation increase with larger size of the hash these number are surprisingly small even for densely populated table for large percentage value of still a the hash table get closer to being full the performance of linear prob ing deteriorates because of a phenomenon called a cluster in linear probing is a sequence of contiguously occupied cell with a possible for example the final state of the hash table of figure ha two clus ters are bad news in hashing because they make the dictionary operation le a cluster become larger the probability that a new element will be attached to a cluster increase in addition large cluster increase the probabil ity that two cluster will coalesce after a new key insertion causing even more several other collision resolution strategy have been suggested to alleviate this one of the most important is double under this scheme we use another hash function sk to determine a fixed increment for the probing sequence to be used after a collision at location l hk l sk mod m l sk mod m to guarantee that every location in the table is probed by sequence the incre ment sk and the table size m must be relatively prime their only common divisor must be this condition is satisfied automatically if m itself is some function recommended in the literature are sk m k mod m and sk k mod for small table and sk k mod for larger this problem wa solved in by a young graduate student in mathematics named donald knuth went on to become one of the most important computer scientist of our his multivolume treatise the art of computer programming knui knuii knuiii knuiv remains the most comprehensive and influential book on algorithmics ever mathematical analysis of double hashing ha proved to be quite some partial result and considerable practical experience with the method suggest that with good hashing function both primary and secondary double hashing is su perior to linear but it performance also deteriorates when the table get close to being a natural solution in such a situation is rehashing the current table is scanned and all it key are relocated into a larger it is worthwhile to compare the main property of hashing with balanced search tree it principal competitor for implementing asymptotic time efficiency with hashing searching insertion and deletion can be implemented to take time on the average but n time in the very unlikely worst for balanced search tree the average time efficiency are log n for both the average and worst ordering preservation unlike balanced search tree hashing doe not assume existence of key ordering and usually doe not preserve this make hashing le suitable for application that need to iterate over the key in or der or require range query such a counting the number of key between some lower and upper since it discovery in the s by ibm researcher hashing ha found many important in particular it ha become a standard technique for stor ing a symbol table a table of a computer program symbol generated during hashing is quite handy for such ai application a checking whether position generated by a chess playing computer program have already been con with some modification it ha also proved to be useful for storing very large dictionary on disk this variation of hashing is called extendible since disk access is expensive compared with probe performed in the main mem ory it is preferable to make many more probe than disk accordingly a location computed by a hash function in extendible hashing indicates a disk ad dress of a bucket that can hold up to b when a key bucket is identified all it key are read into main memory and then searched for the key in in the next section we discus b tree a principal alternative for storing large exercise for the input and hash function hk k mod construct the open hash find the largest number of key comparison in a successful search in this find the average number of key comparison in a successful search in this for the input and hash function hk k mod construct the closed hash find the largest number of key comparison in a successful search in this find the average number of key comparison in a successful search in this why is it not a good idea for a hash function to depend on just one letter say the first one of a natural language find the probability of all n key being hashed to the same cell of a hash table of size m if the hash function distributes key evenly among all the cell of the birthday paradox the birthday paradox asks how many people should be in a room so that the chance are better than even that two of them will have the same birthday month and find the quite unexpected answer to this what implication for hashing doe this result answer the following question for the separate chaining version of where would you insert key if you knew that all the key in the dictionary are which dictionary operation if any would benefit from this we could keep key of the same linked list which of the dictio nary operation would benefit from this how could we take advantage of this if all the key stored in the entire table need to be explain how to use hashing to check whether all element of a list are what is the time efficiency of this compare it efficiency with that of the brute force algorithm section and of the presorting based algorithm section fill in the following table with the average case a the first entry and worst case a the second entry efficiency class for the five implementation of the adt dictionary unordered ordered binary balanced array array search tree search tree hashing search insertion deletion we have discussed hashing in the context of technique based on spacetime trade but it also take advantage of another general which write a computer program that us hashing for the following given a natural language text generate a list of distinct word with the number of occurrence of each word in the insert appropriate counter in the pro gram to compare the empirical efficiency of hashing with the corresponding theoretical p k p pi ki pi pn kn pn t t ti ti tn tn figure parental node of a b b tree the idea of using extra space to facilitate faster access to a given data set is partic ularly important if the data set in question contains a very large number of record that need to be stored on a a principal device in organizing such data set is an index which provides some information about the location of record with indicated key for data set of structured record a opposed to unstruc tured data such a text image sound and video the most important index organization is the b tree introduced by bayer and mcgreight it extends the idea of the tree see section by permitting more than a single key in the same node of a search in the b tree version we consider here all data record or record key are stored at the leaf in increasing order of the the parental node are used for specifically each parental node contains n ordered key k kn assumed for the sake of simplicity to be the key are interposed with n pointer to the node child so that all the key in subtree t are smaller than k all the key in subtree t are greater than or equal to k and smaller than k with k being equal to the smallest key in t and so on through the last subtree tn whose key are greater than or equal to kn with kn being equal to the smallest key in tn see figure in addition a b tree of order m must satisfy the following structural property the root is either a leaf or ha between and m each node except for the root and the leaf ha between m and m child and hence between m and m the tree is perfectly balanced all it leaf are at the same the node depicted in figure is called the n thus all the node in a classic binary search tree are node a tree introduced in section comprises node and figure example of a b tree of order an example of a b tree of order is given in figure searching in a b tree is very similar to searching in the binary search tree and even more so in the starting with the root we follow a chain of pointer to the leaf that may contain the search then we search for the search key among the key of that note that since key are stored in sorted order at both parental node and leaf we can use binary search if the number of key at a node is large enough to make it it is not the number of key comparison however that we should be concerned about in a typical application of this data when used for storing a large data file on a disk the node of a b tree normally correspond to the disk since the time needed to access a disk page is typically several order of magnitude larger than the time needed to compare key in the fast computer memory it is the number of disk access that becomes the principal indicator of the efficiency of this and similar data how many node of a b tree do we need to access during a search for a record with a given key this number is obviously equal to the height of the tree plus to estimate the height let u find the smallest number of key a b tree of order m and positive height h can the root of the tree will contain at least one level will have at least two node with at least m key in each of them for the total minimum number of key m level will have at least m node the child of the node on level with at least m in each of them for the total minimum number of key m m in general the node of level i i h will contain at least m i m finally level h the leaf level will have at least m h node with at least one key in thus for any b tree of order m with n node and height h we have the following inequality h n m i m m h i after a series of standard simplification see problem in this section exercise this inequality reduces to n m h which in turn yield the following upper bound on the height h of the b tree of order m with n node h log m n inequality immediately implies that searching in a b tree is a olog n but it is important to ascertain here not just the efficiency class but the actual number of disk access implied by this the following table contains the value of the right hand side estimate for a file of million record and a few typical value of the tree order m order m h upper bound keep in mind that the table entry are upper estimate for the number of disk in actual application this number rarely exceeds with the b tree root and sometimes first level node stored in the fast memory to minimize the number of disk the operation of insertion and deletion are le straightforward than search ing but both can also be done in olog n here we outline an insertion algorithm only a deletion algorithm can be found in the reference aho the most straightforward algorithm for inserting a new record into a b tree is quite similar to the algorithm for insertion into a tree outlined in section first we apply the search procedure to the new record key k to find the appropriate leaf for the new if there is room for the record in that leaf we place it there in an appropriate position so that the key remain sorted and we are if there is no room for the record the leaf is split in half by sending the second half of the record to a new after that the smallest key k in the new node and the pointer to it are inserted into the old leaf parent immediately after the key and pointer to the old this recursive procedure may percolate up to the tree if the root is already full too a new root is created with the two half of the old root key split between two child of the new a an example figure show the result of inserting into the b tree in figure under the restriction that the leaf cannot contain more than three you should be aware that there are other algorithm for implementing inser tions into a b for example to avoid the possibility of recursive node split we can split full node encountered in searching for an appropriate leaf for the new another possibility is to avoid some node split by moving a key to the node for example inserting into the b tree in figure can be done by moving the smallest key of the full leaf to it sibling with key and and replacing the key value of their parent by the new smallest value in figure b tree obtained after inserting into the b tree in figure the second this modification tends to save some space at the expense of a slightly more complicated a b tree doe not have to be always associated with the indexing of a large file and it can be considered a one of several search tree a with other type of search tree such a binary search tree avl tree and tree a btree can be constructed by successive insertion of data record into the initially empty the empty tree is considered to be a b tree when all key reside in the leaf and the upper level are organized a a b tree comprising an index the entire structure is usually called in fact a b exercise give example of using an index in real life application that do not involve prove the equality h m i m m h m h i which wa used in the derivation of upper bound for the height of a b complete the derivation of inequality find the minimum order of the b tree that guarantee that the number of disk access in searching in a file of million record doe not exceed assume that the root page is stored in main draw the b tree obtained after inserting and then in the b tree in figure assume that a leaf cannot contain more than three outline an algorithm for finding the largest key in a b a top down tree is a b tree of order with the following modifica tion of the insert operation whenever a search for a leaf for a new key encounter a full node a node with three key the node is split into two node by sending it middle key to the node parent or if the full node happens to be the root the new root for the middle key is construct a top down tree by inserting the following list of key in the initially empty tree what is the principal advantage of this insertion procedure compared with the one used for tree in section what is it write a program implementing a key insertion algorithm in a b write a program for visualization of a key insertion algorithm in a b summary space and time trade offs in algorithm design are a well known issue for both theoretician and practitioner of a an algorithm design technique trading space for time is much more prevalent than trading time for input enhancement is one of the two principal variety of trading space for time in algorithm it idea is to preprocess the problem input in whole or in part and store the additional information obtained in order to accelerate solving the problem sorting by distribution counting and several important algorithm for string matching are example of algorithm based on this distribution counting is a special method for sorting list of element from a small set of possible horspools algorithm for string matching can be considered a simplified version of the boyer moore algorithm are based on the idea of input enhancement and right to left comparison of a pattern both algorithm use the same bad symbol shift table the boyer moore also us a second table called the good suffix shift prestructuring the second type of technique that exploit space for time trade offs us extra space to facilitate a faster andor more flexible access to the hashing and b tree are important example of hashing is a very efficient approach to implementing it is based on the idea of mapping key into a one dimensional the size limitation of such a table make it necessary to employ a collision resolution the two principal variety of hashing are open hashing or separate chaining with key stored in linked list outside of the hash table and closed hashing or open addressing with key stored inside the both enable searching insertion and deletion in time on the b tree is a balanced search tree that generalizes the idea of the tree by allowing multiple key at the same it principal application called the b tree is for keeping index like information about data stored on a by choosing the order of the tree appropriately one can implement the operation of searching insertion and deletion with just a few disk access even for extremely large dynamic programming an idea like a ghost must be spoken to a little before it will explain charles dickens dynamic programming is an algorithm design technique with a rather inter esting it wa invented by a prominent mathematician richard bellman in the s a a general method for optimizing multistage decision pro thus the word programming in the name of this technique stand for planning and doe not refer to computer after proving it worth a an important tool of applied mathematics dynamic programming ha even tually come to be considered at least in computer science circle a a general algorithm design technique that doe not have to be limited to special type of optimization it is from this point of view that we will consider this tech nique dynamic programming is a technique for solving problem with overlapping typically these subproblems arise from a recurrence relating a given problem solution to solution of it smaller rather than solving overlapping subproblems again and again dynamic programming suggests solving each of the smaller subproblems only once and recording the result in a table from which a solution to the original problem can then be this technique can be illustrated by revisiting the fibonacci number dis cussed in section if you have not read that section you will be able to follow the discussion but it is a beautiful topic so if you feel a temptation to read it do succumb to the fibonacci number are the element of the sequence which can be defined by the simple recurrence f n f n f n for n and two initial condition f f if we try to use recurrence directly to compute the nth fibonacci number f n we would have to recompute the same value of this function many time see figure for an note that the problem of computing f n is expressed in term of it smaller and overlapping subproblems of computing f n and f n so we can simply fill element of a one dimensional array with the n consecutive value of f n by starting in view of initial condition with and and using equation a the rule for producing all the other obviously the last element of this array will contain f single loop pseudocode of this very simple algorithm can be found in section note that we can in fact avoid using an extra array to accomplish this task by recording the value of just the last two element of the fibonacci sequence problem in exercise this phenomenon is not unusual and we shall en counter it in a few more example in this thus although a straightforward application of dynamic programming can be interpreted a a special variety of space for time trade off a dynamic programming algorithm can sometimes be re fined to avoid using extra certain algorithm compute the nth fibonacci number without computing all the preceding element of this sequence see section it is typical of an algorithm based on the classic bottom up dynamic programming approach however to solve all smaller subproblems of a given one variation of the dynamic programming approach seek to avoid solving unnecessary this technique illustrated in section exploit so called memory function and can be considered a top down variation of dynamic whether one us the classical bottom up version of dynamic programming or it top down variation the crucial step in designing such an algorithm remains the same deriving a recurrence relating a solution to the problem to solution to it smaller the immediate availability of equation for computing the nth fibonacci number is one of the few exception to this since a majority of dynamic programming application deal with optimiza tion problem we also need to mention a general principle that underline such richard bellman called it the principle of in term some what different from it original formulation it say that an optimal solution to any instance of an optimization problem is composed of optimal solution to it subin the principle of optimality hold much more often than to give a rather rare example it fails for finding the longest simple path in a al though it applicability to a particular problem need to be checked of course such a check is usually not a principal difficulty in developing a dynamic program ming in the section and exercise of this chapter are a few standard example of dynamic programming the algorithm in section were in fact invented independently of the discovery of dynamic programming and only later came to be viewed a example of this technique numerous other application range from the optimal way of breaking text into line baa to image resizing avi to a variety of application to sophisticated engineering problem three basic example the goal of this section is to introduce dynamic programming via three typical example coin row problem there is a row of n coin whose value are some positive integer c c cn not necessarily the goal is to pick up the maximum amount of money subject to the constraint that no two coin adjacent in the initial row can be picked let f n be the maximum amount that can be picked up from the row of n to derive a recurrence for f n we partition all the allowed coin selection into two group those that include the last coin and those without the largest amount we can get from the first group is equal to cn f n the value of the nth coin plus the maximum amount we can pick up from the first n the maximum amount we can get from the second group is equal to f n by the definition of f thus we have the following recurrence subject to the obvious initial condition f n maxcn f n f n for n f f we can compute f n by filling the one row table left to right in the manner similar to the way it wa done for the nth fibonacci number by algorithm fibn in section algorithm coinrowc applies formula bottom up to find the maximum amount of money that can be picked up from a coin row without picking two adjacent coin input array of positive integer indicating the coin value output the maximum amount of money that can be picked up f f c for i to n do f i maxci f i f i return f n the application of the algorithm to the coin row of denomination is shown in figure it yield the maximum amount of it is worth pointing index c f f c f index c f max f index c f max f index c f max f index c f max f index c f max f figure solving the coin row problem by dynamic programming for the coin row out that in fact we also solved the problem for the first i coin in the row given for every i for example for i the maximum amount is f to find the coin with the maximum total value found we need to back trace the computation to see which of the two possibility cn f n or f n produced the maximum in formula in the last application of the formula it wa the sum c f which mean that the coin c is a part of an optimal moving to computing f the maximum wa produced by the sum c f which mean that the coin c is a part of an optimal solution a finally the maximum in computing f wa produced by f implying that the coin c is not the part of an optimal solution and the coin c thus the optimal solution is c c to avoid repeating the same computation during the backtracing the information about which of the two term in wa larger can be recorded in an extra array when the value of f are using the coinrow to find f n the largest amount of money that can be picked up a well a the coin composing an optimal set clearly take n time and n this is by far superior to the alternative the straightforward topdown application of recurrence and solving the problem by exhaustive search problem in this section example change making problem consider the general instance of the following well known give change for amount n using the minimum number of coin of denomination d d for the coin denomination used in the united state a for those used in most if not all other country there is a very simple and efficient algorithm discussed in the next here we consider a dynamic programming algorithm for the general case assuming availability of unlimited quantity of coin for each of the m denomination d d dm where d let f n be the minimum number of coin whose value add up to n it is convenient to define f the amount n can only be obtained by adding one coin of denomination dj to the amount n dj for j m such that n dj therefore we can consider all such denomination and select the one minimizing f n dj since is a constant we can of course find the smallest f n dj first and then add to hence we have the following recurrence for f n f n min f n dj for n j ndj f we can compute f n by filling a one row table left to right in the manner similar to the way it wa done above for the coin row problem but computing a table entry here requires finding the minimum of up to m algorithm n applies dynamic programming to find the minimum number of coin of denomination d d dm where d that add up to a given amount n input positive integer n and array of increasing positive integer indicating the coin denomination where d output the minimum number of coin that add up to n f for i to n do temp j while j m and i dj do temp minf i dj temp j j f i temp return f n the application of the algorithm to amount n and denomination is shown in figure the answer it yield is two the time and space efficiency of the algorithm are obviously onm and n n f f n f minf f n f minf f n f minf f f n f minf f f f n f minf f f f n f minf f f f figure application of algorithm mincoinchange to amount n and coin denomination and to find the coin of an optimal solution we need to backtrace the computa tions to see which of the denomination produced the minimum in formula for the instance considered the last application of the formula for n the minimum wa produced by d the second minimum for n wa also produced for a coin of that thus the minimum coin set for n is two example coin collecting problem several coin are placed in cell of an n m board no more than one coin per a robot located in the upper left cell of the board need to collect a many of the coin a possible and bring them to the bottom right on each step the robot can move either one cell to the right or one cell down from it current when the robot visit a cell with a coin it always pick up that design an algorithm to find the maximum number of coin the robot can collect and a path it need to follow to do let f i j be the largest number of coin the robot can collect and bring to the cell i j in the ith row and j th column of the it can reach this cell either from the adjacent cell i j above it or from the adjacent cell i j to the left of the largest number of coin that can be brought to these cell are f i j and f i j of course there are no adjacent cell above the cell in the first row and there are no adjacent cell to the left of the cell in the first for those cell we assume that f i j and f i j are equal to for their nonexistent therefore the largest number of coin the robot can bring to cell i j is the maximum of these two number plus one possible coin at cell i j in other word we have the following formula for f i j f i j maxf i j f i j cij for i n j m f j for j m and f i for i n where cij if there is a coin in cell i j and cij using these formula we can fill in the n m table of f i j value either row by row or column by column a is typical for dynamic programming algorithm involving two dimensional algorithm applies dynamic programming to compute the largest number of coin a robot can collect on an n m board by starting at and moving right and down from upper left to down right corner input matrix whose element are equal to and for cell with and without a coin respectively output largest number of coin the robot can bring to cell n m f c for j to m do f j f j c j for i to n do f i f i ci for j to m do f i j maxf i j f i j ci j return f n m the algorithm is illustrated in figure for the coin setup in figure since computing the value of f i j by formula for each cell of the table take constant time the time efficiency of the algorithm is it space efficiency is obviously also tracing the computation backward make it possible to get an optimal path if f i j f i j an optimal path to cell i j must come down from the adjacent cell above it if f i j f i j an optimal path to cell i j must come from the adjacent cell on the left and if f i j f i j it can reach cell i j from either this yield two optimal path for the instance in figure which are shown in figure if tie are ignored one optimal path can be obtained in n m a b c figure a coin to b dynamic programming algorithm c two path to collect coin the maximum number of coin exercise what doe dynamic programming have in common with divide and what is a principal difference between solve the instance of the coin row show that the time efficiency of solving the coin row problem by straight forward application of recurrence is show that the time efficiency of solving the coin row problem by exhaustive search is at least apply the dynamic programming algorithm to find all the solution to the change making problem for the denomination and the amount n how would you modify the dynamic programming algorithm for the coin collecting problem if some cell on the board are inaccessible for the apply your algorithm to the board below where the inaccessible cell are shown by how many optimal path are there for this rod cutting problem design a dynamic programming algorithm for the fol lowing find the maximum total sale price that can be obtained by cutting a rod of n unit long into integer length piece if the sale price of a piece i unit long is pi for i what are the time and space efficiency of your shortest path counting a chess rook can move horizontally or vertically to any square in the same row or in the same column of a find the number of shortest path by which a rook can move from one corner of a chessboard to the diagonally opposite the length of a path is measured by the number of square it pass through including the first and the last solve the problem by a dynamic programming by using elementary minimum sum descent some positive integer are arranged in an equilateral triangle with n number in it base like the one shown in the figure below for n the problem is to find the smallest sum in a descent from the triangle apex to it base through a sequence of adjacent number shown in the figure by the design a dynamic programming algorithm for this problem and indicate it time binomial coefficient design an efficient algorithm for computing the bino mial coefficient cn k that us no what are the time and space efficiency of your longest path in a dag design an efficient algorithm for finding the length of the longest path in a this problem is important both a a prototype of many other dynamic programming application and in it own right because it determines the minimal time needed for completing a project comprising precedence constrained show how to reduce the coin row problem discussed in this section to the problem of finding a longest path in a maximum square submatrix given an m n boolean matrix b find it largest square submatrix whose element are all design a dynamic programming algorithm and indicate it time the algorithm may be useful for say finding the largest free square area on a computer screen or for selecting a construction world series odds consider two team a and b playing a series of game until one of the team win n assume that the probability of a winning a game is the same for each game and equal to p and the probability of a losing a game is q hence there are no let p i j be the probability of a winning the series if a need i more game to win the series and b need j more game to win the set up a recurrence relation for p i j that can be used by a dynamic programming find the probability of team a winning a seven game series if the proba bility of it winning a game is write pseudocode of the dynamic programming algorithm for solving this problem and determine it time and space the knapsack problem and memory function we start this section with designing a dynamic programming algorithm for the knapsack problem given n item of known weight w wn and value v vn and a knapsack of capacity w find the most valuable subset of the item that fit into the this problem wa introduced in section where we discussed solving it by exhaustive we assume here that all the weight and the knapsack capacity are positive integer the item value do not have to be to design a dynamic programming algorithm we need to derive a recurrence relation that express a solution to an instance of the knapsack problem in term of solution to it smaller let u consider an instance defined by the first i item i n with weight w wi value v vi and knapsack capacity j j let f i j be the value of an optimal solution to this instance the value of the most valuable subset of the first i item that fit into the knapsack of capacity we can divide all the subset of the first i item that fit the knapsack of capacity j into two category those that do not include the ith item and those that note the following among the subset that do not include the ith item the value of an optimal subset is by definition f i j among the subset that do include the ith item hence j wi an optimal subset is made up of this item and an optimal subset of the first i item that fit into the knapsack of capacity j the value of such an optimal subset is vi f i j thus the value of an optimal solution among all feasible subset of the first i item is the maximum of these two of course if the ith item doe not fit into the knapsack the value of an optimal subset selected from the first i item is the same a the value of an optimal subset selected from the first i these observation lead to the following recurrence f i j maxf i j vi f i j wi if j wi f i j if j wi it is convenient to define the initial condition a follows f j for j and f i for i our goal is to find f n w the maximal value of a subset of the n given item that fit into the knapsack of capacity w and an optimal subset figure illustrates the value involved in equation and for i j to compute the entry in the ith row and the j th column f i j we compute the maximum of the entry in the previous row and the same column and the sum of vi and the entry in the previous row and wi column to the the table can be filled either row by row or column by j wi j w i f i j wi f i j wi vi i f i j n goal figure table for solving the knapsack problem by dynamic capacity j i w v w v w v w v figure example of solving an instance of the knapsack problem by the dynamic programming example let u consider the instance given by the following data item weight value capacity w the dynamic programming table filled by applying formula and is shown in figure thus the maximal value is f we can find the composition of an optimal subset by backtracing the computation of this entry in the since f f item ha to be included in an optimal solution along with an optimal subset for filling remaining unit of the knapsack the value of the latter is f since f f item need not be in an optimal since f f item is a part of an optimal selection which leaf element f to specify it remaining similarly since f f item is the final part of the optimal solution item item item the time efficiency and space efficiency of this algorithm are both in nw the time needed to find the composition of an optimal solution is in you are asked to prove these assertion in the memory function a we discussed at the beginning of this chapter and illustrated in subsequent section dynamic programming deal with problem whose solution satisfy a recurrence relation with overlapping the direct top down approach to finding a solution to such a recurrence lead to an algorithm that solves common subproblems more than once and hence is very inefficient typically exponential or the classic dynamic programming approach on the other hand work bottom up it fill a table with solution to all smaller subproblems but each of them is solved only an unsatisfying aspect of this approach is that solution to some of these smaller subproblems are often not necessary for getting a solution to the problem since this drawback is not present in the top down approach it is natural to try to combine the strength of the top down and bottom up the goal is to get a method that solves only subproblems that are necessary and doe so only such a method exists it is based on using memory this method solves a given problem in the top down manner but in addition maintains a table of the kind that would have been used by a bottom up dynamic programming initially all the table entry are initialized with a special null symbol to indicate that they have not yet been thereafter whenever a new value need to be calculated the method check the corresponding entry in the table first if this entry is not null it is simply retrieved from the table otherwise it is computed by the recursive call whose result is then recorded in the the following algorithm implement this idea for the knapsack after initializing the table the recursive function need to be called with i n the number of item and j w the knapsack algorithm mfknapsacki j implement the memory function method for the knapsack problem input a nonnegative integer i indicating the number of the first item being considered and a nonnegative integer j indicating the knapsack capacity output the value of an optimal feasible subset of the first i item note us a global variable input array w v and table f whose entry are initialized with s except for row and column initialized with s if f i j if j weightsi value mfknapsacki j else value maxmfknapsacki j valuesi mfknapsacki j weightsi f i j value return f i j example let u apply the memory function method to the instance considered in example the table in figure give the only out of nontrivial value not those in row or in column have been capacity j i w v w v w v w v figure example of solving an instance of the knapsack problem by the memory function just one nontrivial entry v is retrieved rather than being for larger instance the proportion of such entry can be significantly in general we cannot expect more than a constant factor gain in using the memory function method for the knapsack problem because it time efficiency class is the same a that of the bottom up algorithm a more significant improvement can be expected for dynamic programming algorithm in which a computation of one value take more than constant you should also keep in mind that a memory function algorithm may be le space efficient than a space efficient version of a bottom up exercise apply the bottom up dynamic programming algorithm to the following instance of the knapsack problem item weight value capacity w how many different optimal subset doe the instance of part a in general how can we use the table generated by the dynamic program ming algorithm to tell whether there is more than one optimal subset for the knapsack problem write pseudocode of the bottom up dynamic programming algorithm for the knapsack write pseudocode of the algorithm that find the composition of an optimal subset from the table generated by the bottom up dynamic programming algorithm for the knapsack for the bottom up dynamic programming algorithm for the knapsack prob lem prove that it time efficiency is nw it space efficiency is nw the time needed to find the composition of an optimal subset from a filled dynamic programming table is true or false a sequence of value in a row of the dynamic programming table for the knapsack problem is always true or false a sequence of value in a column of the dynamic program ming table for the knapsack problem is always design a dynamic programming algorithm for the version of the knapsack problem in which there are unlimited quantity of copy for each of the n item kind indicate the time efficiency of the apply the memory function method to the instance of the knapsack problem given in problem indicate the entry of the dynamic programming table that are i never computed by the memory function method ii retrieved without a prove that the efficiency class of the memory function algorithm for the knap sack problem is the same a that of the bottom up algorithm see problem explain why the memory function approach is unattractive for the problem of computing a binomial coefficient by the formula cn k cn k cn write a research report on one of the following well known application of dynamic programming finding the longest common subsequence in two sequence optimal string editing minimal triangulation of a polygon optimal binary search tree a binary search tree is one of the most important data structure in computer one of it principal application is to implement a dictionary a set of element with the operation of searching insertion and if probability a b b a c c d d figure two out of possible binary search tree with key a b c and of searching for element of a set are known from accumulated data about past search it is natural to pose a question about an optimal binary search tree for which the average number of comparison in a search is the smallest for simplicity we limit our discussion to minimizing the average number of comparison in a successful the method can be extended to include unsuccessful search a a an example consider four key a b c and d to be searched for with probability and figure depicts two out of possible binary search tree containing these the average number of comparison in a successful search in the first of these tree is and for the second one it is neither of these two tree is in fact can you tell which binary tree is for our tiny example we could find the optimal tree by generating all binary search tree with these a a general algorithm this exhaustive search approach is unrealistic the total number of binary search tree with n key is equal to the nth catalan number cn n for n c n n which grows to infinity a fast a see problem in this section so let a an be distinct key ordered from the smallest to the largest and let p pn be the probability of searching for let ci j be the smallest average number of comparison made in a successful search in a binary search tree tij made up of key ai aj where i j are some integer index i j following the classic dynamic programming approach we will find value of ci j for all smaller instance of the problem although we are interested just in c to derive a recurrence underlying a dynamic programming algorithm we will consider all possible way to choose a root ak among the key ai aj for such a binary search tree figure the root contains key ak the left subtree tik contains key ai ak optimally arranged and the right subtree tkj ak optimal optimal bst for bst for ai ak ak aj figure binary search tree bst with root ak and two optimal binary search subtrees tik and contains key ak aj also optimally note how we are taking advantage of the principle of optimality if we count tree level starting with to make the comparison number equal the key level the following recurrence relation is obtained k ci j min pk p level of a in tik ikj si j p level of a in tkj sk k j j min p level of a in tik p level of a in tkj p ikj si sk si j min ci k ck j p ikj si thus we have the recurrence j ci j min ci k ck j p for i j ikj si we assume in formula that ci i for i n which can be interpreted a the number of comparison in the empty note that this formula implies that ci i pi for i n a it should be for a one node binary search tree containing j n p goal p i c ij pn n figure table of the dynamic programming algorithm for constructing an optimal binary search the two dimensional table in figure show the value needed for comput ing ci j by formula they are in row i and the column to the left of column j and in column j and the row below row the arrow point to the pair of en try whose sum are computed in order to find the smallest one to be recorded a the value of ci j this suggests filling the table along it diagonal starting with all zero on the main diagonal and given probability pi i n right above it and moving toward the upper right the algorithm we just sketched computes c n the average number of comparison for successful search in the optimal binary if we also want to get the optimal tree itself we need to maintain another two dimensional table to record the value of k for which the minimum in is the table ha the same shape a the table in figure and is filled in the same manner starting with entry ri i i for i when the table is filled it entry indicate index of the root of the optimal subtrees which make it possible to reconstruct an optimal tree for the entire set example let u illustrate the algorithm by applying it to the four key set we used at the beginning of this section key a b c d probability the initial table look like this main table root table let u compute c k c c p c min s k c c p s thus out of two possible binary tree containing the first two key a and b the root of the optimal tree ha index it contains b and the average number of comparison in a successful search in this tree is we will ask you to finish the computation in the you should arrive at the following final table main table root table thus the average number of key comparison in the optimal tree is equal to since r the root of the optimal tree contains the third key it left subtree is made up of key a and b and it right subtree contains just key d to find the specific structure of these subtrees we find first their root by consulting the root table again a since r the root of the optimal tree containing a and b is b with a being it left child and the root of the onenode tree r since r the root of this one node optimal tree is it only key figure present the optimal tree in it c b d a figure optimal binary search tree for the here is pseudocode of the dynamic programming algorithm optimalbstp find an optimal binary search tree by dynamic programming input an array p of search probability for a sorted list of n key output average number of comparison in successful search in the optimal bst and table r of subtrees root in the optimal bst for i to n do ci i ci i p i ri i i cn n for d to n do diagonal count for i to n d do j id minval for k i to j do if ci k ck j minval minval ci k ck j kmin k ri j kmin sum p i for s i to j do sum sum p s ci j minval sum return c n r the algorithm space efficiency is clearly quadratic the time efficiency of this version of the algorithm is cubic a more careful analysis show that entry in the root table are always nondecreasing along each row and this limit value for ri j to the range ri j ri j and make it possible to reduce the running time of the algorithm to exercise finish the computation started in the section example of constructing an optimal binary search why is the time efficiency of algorithm optimalbst why is the space efficiency of algorithm optimalbst write pseudocode for a linear time algorithm that generates the optimal binary search tree from the root devise a way to compute the sum j p which are used in the dynamic si programming algorithm for constructing an optimal binary search tree in constant time per true or false the root of an optimal binary search tree always contains the key with the highest search how would you construct an optimal binary search tree for a set of n key if all the key are equally likely to be searched what will be the average number of comparison in a successful search in such a tree if n show that the number of distinct binary search tree bn that can be constructed for a set of n orderable key satisfies the recurrence relation n bn bkbn k for n b k it is known that the solution to this recurrence is given by the catalan verify this assertion for n find the order of growth of what implication doe the answer to this question have for the exhaustive search algorithm for constructing an optimal binary search design a n algorithm for finding an optimal binary search generalize the optimal binary search algorithm by taking into account unsuc cessful write pseudocode of a memory function for the optimal binary search tree you may limit your function to finding the smallest number of key comparison in a successful matrix chain multiplication consider the problem of minimizing the total number of multiplication made in computing the product of n matrix a a an whose dimension are d d d d dn dn assume that all intermediate product of two matrix are computed by the brute force definition based give an example of three matrix for which the number of multiplication in a a a and a a a differ at least by a factor of how many different way are there to compute the product of n design a dynamic programming algorithm for finding an optimal order of multiplying n what is an although there is no universally agreed on wording to describe this notion there is general agreement about what the concept mean an algorithm is a sequence of unambiguous instruction for solving a problem for obtaining a required output for any legitimate input in a finite amount of this definition can be illustrated by a simple diagram figure the reference to instruction in the definition implies that there is some thing or someone capable of understanding and following the instruction we call this a computer keeping in mind that before the electronic computer wa invented the word computer meant a human being involved in perform ing numeric nowadays of course computer are those ubiquitous electronic device that have become indispensable in almost everything we note however that although the majority of algorithm are indeed intended for eventual computer implementation the notion of algorithm doe not depend on such an a example illustrating the notion of the algorithm we consider in this section three method for solving the same problem computing the greatest common divisor of two these example will help u to illustrate several important point the nonambiguity requirement for each step of an algorithm cannot be com the range of input for which an algorithm work ha to be specified the same algorithm can be represented in several different there may exist several algorithm for solving the same problem algorithm input computer output figure the notion of the algorithm for the same problem can be based on very different idea and can solve the problem with dramatically different recall that the greatest common divisor of two nonnegative not both zero integer m and n denoted gcdm n is defined a the largest integer that divide both m and n evenly with a remainder of euclid of alexandria third century outlined an algorithm for solving this problem in one of the volume of his element most famous for it systematic exposition of in modern term euclid algorithm is based on applying repeatedly the equality gcdm n gcdn m mod n where m mod n is the remainder of the division of m by n until m mod n is equal to since gcdm m the last value of m is also the greatest common divisor of the initial m and for example gcd can be computed a follows gcd gcd gcd if you are not impressed by this algorithm try finding the greatest common divisor of larger number such a those in problem in this section here is a more structured description of this algorithm euclid algorithm for computing gcdm n step if n return the value of m a the answer and stop otherwise proceed to step step divide m by n and assign the value of the remainder to step assign the value of n to m and the value of r to go to step alternatively we can express the same algorithm in pseudocode algorithm euclidm n computes gcdm n by euclid algorithm input two nonnegative not both zero integer m and n output greatest common divisor of m and n while n do r m mod n mn nr return m how do we know that euclid algorithm eventually come to a this follows from the observation that the second integer of the pair get smaller with each iteration and it cannot become indeed the new value of n on the next iteration is m mod n which is always smaller than n hence the value of the second integer eventually becomes and the algorithm just a with many other problem there are several algorithm for computing the greatest common let u look at the other two method for this the first is simply based on the definition of the greatest common divisor of m and n a the largest integer that divide both number obviously such a common divisor cannot be greater than the smaller of these number which we will denote by t minm so we can start by checking whether t divide both m and n if it doe t is the answer if it doe not we simply decrease t by and try how do we know that the process will eventually for example for number and the algorithm will try first then and so on until it reach where it consecutive integer checking algorithm for computing gcdm n step assign the value of minm n to step divide m by if the remainder of this division is go to step otherwise go to step step divide n by if the remainder of this division is return the value of t a the answer and stop otherwise proceed to step step decrease the value of t by go to step note that unlike euclid algorithm this algorithm in the form presented doe not work correctly when one of it input number is this example illustrates why it is so important to specify the set of an algorithm input explicitly and the third procedure for finding the greatest common divisor should be familiar to you from middle middle school procedure for computing gcdm n step find the prime factor of step find the prime factor of step identify all the common factor in the two prime expansion found in step and step if p is a common factor occurring pm and pn time in m and n respectively it should be repeated minpm pn step compute the product of all the common factor and return it a the greatest common divisor of the number thus for the number and we get gcd nostalgia for the day when we learned this method should not prevent u from noting that the last procedure is much more complex and slower than euclid we will discus method for finding and comparing running time of algorithm in the next in addition to inferior efficiency the middleschool procedure doe not qualify in the form presented a a legitimate because the prime factorization step are not defined unambiguously they require a list of prime number and i strongly suspect that your middle school math teacher did not explain how to obtain such a this is not a matter of unnecessary unless this issue is resolved we cannot say write a program implementing this incidentally step is also not defined clearly it ambiguity is much easier to rectify than that of the factorization step how would you find common element in two sorted so let u introduce a simple algorithm for generating consecutive prime not exceeding any given integer n it wa probably invented in ancient greece and is known a the sieve of eratosthenes the algorithm start by initializing a list of prime candidate with consecutive integer from to then on it first iteration the algorithm eliminates from the list all multiple of and so then it move to the next item on the list which is and eliminates it in this straightforward version there is an overhead because some number such a are eliminated more than no pas for number is needed since itself and all it multiple are also multiple of they were already eliminated on a previous the next remaining number on the list which is used on the third pas is the algorithm continues in this fashion until no more number can be eliminated from the the remaining integer of the list are the prime a an example consider the application of the algorithm to finding the list of prime not exceeding n for this example no more pass are needed because they would eliminate num bers already eliminated on previous iteration of the the remaining number on the list are the consecutive prime le than or equal to what is the largest number p whose multiple can still remain on the list to make further iteration of the algorithm before we answer this question let u first note that if p is a number whose multiple are being eliminated on the current pas then the first multiple we should consider is p p because all it smaller multiple p p p have been eliminated on earlier pass through the this observation help to avoid eliminating the same number more than obviously p p should not be greater than n and therefore p cannot exceed n rounded down denoted n using the so called floor we assume in the following pseudocode that there is a function available for computing n alternatively we could check the inequality p p n a the loop continuation condition algorithm sieven implement the sieve of eratosthenes input a positive integer n output array l of all prime number le than or equal to n for p to ndo ap p for p to n do see note before pseudocode if ap p hasnt been eliminated on previous pass j pp while j n do aj mark element a eliminated j j p copy the remaining element of a to array l of the prime i for p to n do if ap li ap ii return l so now we can incorporate the sieve of eratosthenes into the middle school procedure to get a legitimate algorithm for computing the greatest common divisor of two positive note that special care need to be exercised if one or both input number are equal to because mathematician do not consider to be a prime number strictly speaking the method doe not work for such before we leave this section one more comment is in the example considered in this section notwithstanding the majority of algorithm in use today even those that are implemented a computer program do not deal with mathematical look around for algorithm helping u through our daily routine both professional and may this ubiquity of algorithm in today world strengthen your resolve to learn more about these fascinating engine of the information exercise do some research on al khorezmi also al khwarizmi the man from whose name the word algorithm is in particular you should learn what the origin of the word algorithm and algebra have in given that the official purpose of the patent system is the promotion of the useful art do you think algorithm are patentable in this should they write down driving direction for going from your school to your home with the precision required from an algorithm write down a recipe for cooking your favorite dish with the precision required by an design an algorithm for computing n for any positive integer besides assignment and comparison your algorithm may only use the four basic arithmetical design an algorithm to find all the common element in two sorted list of for example for the list and the output should be what is the maximum number of comparison your algorithm make if the length of the two given list are m and n find gcd by applying euclid estimate how many time faster it will be to find gcd by euclid algorithm compared with the algorithm based on checking con secutive integer from minm n down to gcdm prove the equality gcdm n gcdn m mod n for every pair of positive integer m and what doe euclid algorithm do for a pair of integer in which the first is smaller than the what is the maximum number of time this can happen during the algorithm execution on such an what is the minimum number of division made by euclid algorithm among all input m n what is the maximum number of division made by euclid algorithm among all input m n euclid algorithm a presented in euclid treatise us subtraction rather than integer write pseudocode for this version of euclid euclid game see bog start with two unequal positive integer on the two player move in on each move a player ha to write on the board a positive number equal to the difference of two number already on the board this number must be new different from all the number already on the the player who cannot move loses the should you choose to move first or second in this the extended euclid algorithm determines not only the greatest common divisor d of two positive integer m and n but also integer not necessarily positive x and y such that mx ny look up a description of the extended euclid algorithm see knui and implement it in the language of your modify your program to find integer solution to the diophantine equation ax by c with any set of integer coefficient a b and locker door there are n locker in a hallway numbered sequentially from to initially all the locker door are you make n pass by the locker each time starting with locker on the ith pas i n you toggle the door of every ith locker if the door is closed you open it if it is open you close after the last pas which locker door are open and which are how many of them are warshalls and floyds algorithm in this section we look at two well known algorithm warshalls algorithm for computing the transitive closure of a directed graph and floyds algorithm for the all pair shortest path these algorithm are based on essentially the same idea exploit a relationship between a problem and it simpler rather than smaller warshall and floyd published their algorithm without mention ing dynamic nevertheless the algorithm certainly have a dynamic programming flavor and have come to be considered application of this tech warshalls algorithm recall that the adjacency matrix a aij of a directed graph is the boolean matrix that ha in it ith row and j th column if and only if there is a directed edge from the ith vertex to the j th we may also be interested in a matrix containing the information about the existence of directed path of arbitrary length between vertex of a given such a matrix called the transitive closure of the digraph would allow u to determine in constant time whether the j th vertex is reachable from the ith here are a few application when a value in a spreadsheet cell is changed the spreadsheet software must know all the other cell affected by the if the spreadsheet is modeled by a digraph whose vertex represent the spreadsheet cell and edge indicate cell dependency the transitive closure will provide such in software engineering transitive closure can be used for investigating data flow and control flow dependency a well a for inheritance testing of object oriented in electronic engineering it is used for redundancy identification and test generation for digital definition the transitive closure of a directed graph with n vertex can be defined a the n n boolean matrix t tij in which the element in the ith row and the j th column is if there exists a nontrivial path directed path of a positive length from the ith vertex to the j th vertex otherwise tij is an example of a digraph it adjacency matrix and it transitive closure is given in figure we can generate the transitive closure of a digraph with the help of depth first search or breadth first performing either traversal starting at the ith a b a b c d a b c d a a a b t b c c c d d d a b c figure a b it adjacency c it transitive vertex give the information about the vertex reachable from it and hence the column that contain s in the ith row of the transitive thus doing such a traversal for every vertex a a starting point yield the transitive closure in it since this method traverse the same digraph several time we should hope that a better algorithm can be indeed such an algorithm it is called warshalls algorithm after stephen warshall who discovered it it is convenient to assume that the digraph vertex and hence the row and column of the adjacency matrix are numbered from to warshalls algorithm construct the transitive closure through a series of n n boolean matrix r rk rk each of these matrix provides certain information about directed path in the specifically the element rijk in the ith row and j th column of matrix rk i j n k n is equal to if and only if there exists a directed path of a positive length from the ith vertex to the j th vertex with each intermediate vertex if any numbered not higher than thus the series start with r which doe not allow any intermediate vertex in it path hence r is nothing other than the adjacency matrix of the recall that the adjacency matrix contains the information about one edge path path with no intermediate r contains the information about path that can use the first vertex a intermediate thus with more freedom so to speak it may contain more s than in general each subsequent matrix in series ha one more vertex to use a intermediate for it path than it predecessor and hence may but doe not have to contain more the last matrix in the series rn reflects path that can use all n vertex of the digraph a intermediate and hence is nothing other than the digraph transitive the central point of the algorithm is that we can compute all the element of each matrix rk from it immediate predecessor rk in series let rijk the element in the ith row and j th column of matrix rk be equal to this mean that there exists a path from the ith vertex vi to the j th vertex vj with each intermediate vertex numbered not higher than k vi a list of intermediate vertex each numbered not higher than k vj j k j k r k k r k k i i figure rule for changing zero in warshalls two situation regarding this path are in the first the list of it inter mediate vertex doe not contain the kth then this path from vi to vj ha intermediate vertex numbered not higher than k and therefore rijk is equal to a the second possibility is that path doe contain the kth vertex vk among the intermediate without loss of generality we may assume that vk occurs only once in that if it is not the case we can create a new path from vi to vj with this property by simply eliminating all the vertex between the first and last occurrence of vk in with this caveat path can be rewritten a follows vi vertex numbered k vk vertex numbered k vj the first part of this representation mean that there exists a path from vi to vk with each intermediate vertex numbered not higher than k hence rikk and the second part mean that there exists a path from vk to vj with each intermediate vertex numbered not higher than k hence rkjk what we have just proved is that if rijk then either rijk or both rikk and rkjk it is easy to see that the converse of this assertion is also thus we have the following formula for generating the element of matrix rk from the element of matrix rk rijk rijk or rikk and rkjk formula is at the heart of warshalls this formula implies the following rule for generating element of matrix rk from element of matrix rk which is particularly convenient for applying warshalls algorithm by hand if an element rij is in rk it remains in if an element rij is in rk it ha to be changed to in rk if and only if the element in it row i and column k and the element in it column j and row k are both s in rk this rule is illustrated in figure a an example the application of warshalls algorithm to the digraph in figure is shown in figure a b c d s reflect the existence of path a b a with no intermediate vertex r b r is just the adjacency matrix c boxed row and column are used for getting r c d d a b c d s reflect the existence of path a with intermediate vertex numbered r b not higher than just vertex a c note a new path from d to b d boxed row and column are used for getting r a b c d s reflect the existence of path a with intermediate vertex numbered r b not higher than a and b c note two new path d boxed row and column are used for getting r a b c d s reflect the existence of path a with intermediate vertex numbered r b not higher than a b and c c no new path d boxed row and column are used for getting r a b c d a s reflect the existence of path r b with intermediate vertex numbered c not higher than a b c and d d note five new figure application of warshalls algorithm to the digraph new s are in here is pseudocode of warshalls algorithm implement warshalls algorithm for computing the transitive closure input the adjacency matrix a of a digraph with n vertex output the transitive closure of the digraph r a for k to n do for i to n do for j to n do rki j rk i j or rk i k and rk k j return rn several observation need to be made about warshalls first it is remarkably succinct is it still it time efficiency is only in fact for sparse graph represented by their adjacency list the traversal based algorithm a b a b c d a b c d a a w b d b c c c d d d a b c figure a b it weight c it distance mentioned at the beginning of this section ha a better asymptotic efficiency than warshalls algorithm we can speed up the above implementation of warshalls algorithm for some input by restructuring it innermost loop see problem in this section another way to make the algorithm run faster is to treat matrix row a bit string and employ the bitwise or operation available in most modern computer a to the space efficiency of warshalls algorithm the situation is similar to that of computing a fibonacci number and some other dynamic programming although we used separate matrix for recording intermediate result of the algorithm this is in fact problem in this section exercise asks you to find a way of avoiding this wasteful use of the computer finally we shall see below how the underlying idea of warshalls algorithm can be applied to the more general problem of finding length of shortest path in weighted floyds algorithm for the all pair shortest path problem given a weighted connected graph undirected or directed the all pair shortest path problem asks to find the distance the length of the shortest path from each vertex to all other this is one of several variation of the problem involving shortest path in because of it important application to communication transportation network and operation research it ha been thoroughly studied over the among recent application of the all pair shortest path problem is precomputing distance for motion planning in computer it is convenient to record the length of shortest path in an n n matrix d called the distance matrix the element dij in the ith row and the j th column of this matrix indicates the length of the shortest path from the ith vertex to the j th for an example see figure we can generate the distance matrix with an algorithm that is very similar to warshalls it is called floyds algorithm after it co inventor robert it is applicable to both undirected and directed weighted graph provided floyd explicitly referenced warshalls paper in presenting his algorithm three year earlier bernard roy published essentially the same algorithm in the proceeding of the french academy of science that they do not contain a cycle of a negative the distance between any two vertex in such a cycle can be made arbitrarily small by repeating the cycle enough the algorithm can be enhanced to find not only the length of the shortest path for all vertex pair but also the shortest path themselves problem in this section floyds algorithm computes the distance matrix of a weighted graph with n vertex through a series of n n matrix d dk dk each of these matrix contains the length of shortest path with certain constraint on the path considered for the matrix in specifically the element dijk in the ith row and the j th column of matrix dk i j n k n is equal to the length of the shortest path among all path from the ith vertex to the j th vertex with each intermediate vertex if any numbered not higher than in particular the series start with d which doe not allow any intermediate vertex in it path hence d is simply the weight matrix of the the last matrix in the series dn contains the length of the shortest path among all path that can use all n vertex a intermediate and hence is nothing other than the distance matrix being a in warshalls algorithm we can compute all the element of each matrix dk from it immediate predecessor dk in series let dijk be the element in the ith row and the j th column of matrix this mean that dijk is equal to the length of the shortest path among all path from the ith vertex vi to the j th vertex vj with their intermediate vertex numbered not higher than k vi a list of intermediate vertex each numbered not higher than k vj we can partition all such path into two disjoint subset those that do not use the kth vertex vk a intermediate and those that since the path of the first subset have their intermediate vertex numbered not higher than k the shortest of them is by definition of our matrix of length dijk what is the length of the shortest path in the second if the graph doe not contain a cycle of a negative length we can limit our attention only to the path in the second subset that use vertex vk a their intermediate vertex exactly once because visiting vk more than once can only increase the path all such path have the following form vi vertex numbered k vk vertex numbered k vj in other word each of the path is made up of a path from vi to vk with each intermediate vertex numbered not higher than k and a path from vk to vj with each intermediate vertex numbered not higher than k the situation is depicted symbolically in figure since the length of the shortest path from vi to vk among the path that use intermediate vertex numbered not higher than k is equal to dikk and the length of the shortest path from vk to vj among the path that use intermediate d k ij vi vj dikk dkkj vk figure underlying idea of floyds vertex numbered not higher than k is equal to dkjk the length of the shortest path among the path that use the kth vertex is equal to dikk dkjk taking into account the length of the shortest path in both subset lead to the following recurrence dijk mindijk dikk dkjk for k dij wij to put it another way the element in row i and column j of the current distance matrix dk is replaced by the sum of the element in the same row i and the column k and in the same column j and the row k if and only if the latter sum is smaller than it current the application of floyds algorithm to the graph in figure is illustrated in figure here is pseudocode of floyds it take advantage of the fact that the next matrix in sequence can be written over it algorithm floydw implement floyds algorithm for the all pair shortest path problem input the weight matrix w of a graph with no negative length cycle output the distance matrix of the shortest path length dw is not necessary if w can be overwritten for k to n do for i to n do for j to n do di j mindi j di k dk j return d obviously the time efficiency of floyds algorithm is cubic a is the time efficiency of warshalls in the next chapter we examine dijkstras algorithm another method for finding shortest a b a b c d a length of the shortest path d b with no intermediate vertex c d is simply the weight c d d a b c d a length of the shortest path b with intermediate vertex numbered d c not higher than just a d note two new shortest path from b to c and from d to c a b c d a length of the shortest path d b with intermediate vertex numbered c not higher than a and b d note a new shortest path from c to a b c d a length of the shortest path b with intermediate vertex numbered d c not higher than a b and c d note four new shortest path from a to b from a to d from b to d and from d to a b c d a length of the shortest path b with intermediate vertex numbered d c not higher than a b c and d d note a new shortest path from c to figure application of floyds algorithm to the digraph updated element are shown in exercise apply warshalls algorithm to find the transitive closure of the digraph de fined by the following adjacency matrix prove that the time efficiency of warshalls algorithm is explain why the time efficiency class of warshalls algorithm is inferior to that of the traversal based algorithm for sparse graph represented by their adjacency explain how to implement warshalls algorithm without using extra memory for storing element of the algorithm intermediate explain how to restructure the innermost loop of the algorithm warshall to make it run faster at least on some rewrite pseudocode of warshalls algorithm assuming that the matrix row are represented by bit string on which the bitwise or operation can be per explain how warshalls algorithm can be used to determine whether a given digraph is a dag directed acyclic is it a good algorithm for this is it a good idea to apply warshalls algorithm to find the transitive closure of an undirected solve the all pair shortest path problem for the digraph with the following weight matrix prove that the next matrix in sequence of floyds algorithm can be written over it give an example of a graph or a digraph with negative weight for which floyds algorithm doe not yield the correct enhance floyds algorithm so that shortest path themselves not just their length can be jack straw in the game of jack straw a number of plastic or wooden straw are dumped on the table and player try to remove them one by one without disturbing the other here we are only concerned with whether various pair of straw are connected by a path of touching given a list of the endpoint for n straw a if they were dumped on a large piece of graph paper determine all the pair of straw that are note that touching is connecting but also that two straw can be connected indirectly via other connected east central regionals of the acm international collegiate programming contest summary dynamic programming is a technique for solving problem with overlapping typically these subproblems arise from a recurrence relating a solution to a given problem with solution to it smaller subproblems of the same dynamic programming suggests solving each smaller subproblem once and recording the result in a table from which a solution to the original problem can be then applicability of dynamic programming to an optimization problem requires the problem to satisfy the principle of optimality an optimal solution to any of it instance must be made up of optimal solution to it among many other problem the change making problem with arbitrary coin denomination can be solved by dynamic solving a knapsack problem by a dynamic programming algorithm exemplifies an application of this technique to difficult problem of combinatorial the memory function technique seek to combine the strength of the topdown and bottom up approach to solving problem with overlapping it doe this by solving in the top down fashion but only once just the necessary subproblems of a given problem and recording their solution in a dynamic programming can be used for constructing an optimal binary search tree for a given set of key and known probability of searching for warshalls algorithm for finding the transitive closure and floyds algorithm for the all pair shortest path problem are based on the idea that can be interpreted a an application of the dynamic programming greedy technique greed for lack of a better word is greed is greed michael douglas u actor in the role of gordon gecko in the film wall street let u revisit the change making problem faced at least subconsciously by million of cashier all over the world give change for a specific amount n with the least number of coin of the denomination d d dm used in that here unlike section we assume that the denomination are ordered in decreasing for example the widely used coin denomination in the united state are d quarter d dime d nickel and d how would you give change with coin of these denomination of say if you came up with the answer quarter dime and penny you followed consciously or not a logical strategy of making a sequence of best choice among the currently available indeed in the first step you could have given one coin of any of the four greedy thinking lead to giving one quarter because it reduces the remaining amount the most namely to in the second step you had the same coin at your disposal but you could not give a quarter because it would have violated the problem so your best selection in this step wa one dime reducing the remaining amount to giving one more dime left you with cent to be given with three is this solution to the instance of the change making problem yes it in fact one can prove that the greedy algorithm yield an optimal solution for every positive integer amount with these coin at the same time it is easy to give an example of coin denomination that do not yield an optimal solution for some amount d d d and n the approach applied in the opening paragraph to the change making prob lem is called computer scientist consider it a general design technique despite the fact that it is applicable to optimization problem the greedy approach suggests constructing a solution through a sequence of step each ex panding a partially constructed solution obtained so far until a complete solution to the problem is on each step and this is the central point of this technique the choice made must be feasible it ha to satisfy the problem constraint locally optimal it ha to be the best local choice among all feasible choice available on that step irrevocable once made it cannot be changed on subsequent step of the algorithm these requirement explain the technique name on each step it suggests a greedy grab of the best alternative available in the hope that a sequence of locally optimal choice will yield a globally optimal solution to the entire we refrain from a philosophical discussion of whether greed is good or if you have not seen the movie from which the chapter epigraph is taken it hero did not end up from our algorithmic perspective the question is whether such a greedy strategy work or a we shall see there are problem for which a sequence of locally optimal choice doe yield an optimal solution for every instance of the problem in however there are others for which this is not the case for such problem a greedy algorithm can still be of value if we are interested in or have to be satisfied with an approximate in the first two section of the chapter we discus two classic algorithm for the minimum spanning tree problem prims algorithm and kruskals what is remarkable about these algorithm is the fact that they solve the same problem by applying the greedy approach in two different way and both of them always yield an optimal in section we introduce another classic algorithm dijkstras algorithm for the shortest path problem in a weighted section is devoted to huffman tree and their principal application huffman code an important data compression method that can be interpreted a an application of the greedy finally a few example of approximation algorithm based on the greedy approach are discussed in section a a rule greedy algorithm are both intuitively appealing and given an optimization problem it is usually easy to figure out how to proceed in a greedy manner possibly after considering a few small instance of the what is usually more difficult is to prove that a greedy algorithm yield an optimal solution when it one of the common way to do this is illustrated by the proof given in section using mathematical induction we show that a partially constructed solution obtained by the greedy algorithm on each iteration can be extended to an optimal solution to the the second way to prove optimality of a greedy algorithm is to show that on each step it doe at least a well a any other algorithm could in advancing toward the problem consider a an example the following problem find the minimum number of move needed for a chess knight to go from one corner of a board to the diagonally opposite the knight move are l shaped jump two square horizontally or vertically followed by one square in the perpendicular a greedy solution is clear here jump a close to the goal a possible on each thus if it start and finish square are and respectively a sequence of move such a solves the the number k of two move advance can be obtained from the equation k why is this a minimum move because if we measure the distance to the goal by the manhattan distance which is the sum of the difference between the row number and the difference between the column number of two square in question the greedy algorithm decrease it by on each move the best the knight can the third way is simply to show that the final result obtained by a greedy algorithm is optimal based on the algorithm output rather than the way it a an example consider the problem of placing the maximum number of chip on an board so that no two chip are placed on the same or adjacent vertically horizontally or diagonally to follow the prescription of the greedy strategy we should place each new chip so a to leave a many available square a possible for next for example starting with the upper left corner of the board we will be able to place chip a shown in figure why is this solution to see why partition the board into sixteen square a shown in figure obviously it is impossible to place more than one chip in each of these square which implies that the total number of nonadjacent chip on the board cannot exceed a a final comment we should mention that a rather sophisticated theory ha been developed behind the greedy technique which is based on the abstract combinatorial structure called an interested reader can check such book a cor a well a a variety of internet resource on the figure a placement of chip on non adjacent b partition of the board proving impossibility of placing more than a b a b a b a b c d c d c d c d graph wt wt wt figure graph and it spanning tree with t being the minimum spanning prims algorithm the following problem arises naturally in many practical situation given n point connect them in the cheapest possible way so that there will be a path between ev ery pair of it ha direct application to the design of all kind of network including communication computer transportation and electrical by providing the cheapest way to achieve it identifies cluster of point in data it ha been used for classification purpose in archeology biology sociology and other it is also helpful for constructing approximate solution to more difficult problem such the traveling salesman problem see section we can represent the point given by vertex of a graph possible connection by the graph edge and the connection cost by the edge then the question can be posed a the minimum spanning tree problem defined formally a definition a spanning tree of an undirected connected graph is it connected acyclic subgraph a tree that contains all the vertex of the if such a graph ha weight assigned to it edge a minimum spanning tree is it spanning tree of the smallest weight where the weight of a tree is defined a the sum of the weight on all it the minimum spanning tree problem is the problem of finding a minimum spanning tree for a given weighted connected figure present a simple example illustrating these if we were to try constructing a minimum spanning tree by exhaustive search we would face two serious first the number of spanning tree grows exponentially with the graph size at least for dense second generating all spanning tree for a given graph is not easy in fact it is more difficult than finding a minimum spanning tree for a weighted graph by using one of several efficient algorithm available for this in this section we outline prims algorithm which go back to at least robert prim rediscovered the algorithm published year earlier by the czech mathematician vojte ch jarnik in a czech prims algorithm construct a minimum spanning tree through a sequence of expanding the initial subtree in such a sequence consists of a single vertex selected arbitrarily from the set v of the graph on each iteration the algorithm expands the current tree in the greedy manner by simply attaching to it the nearest vertex not in that by the nearest vertex we mean a vertex not in the tree connected to a vertex in the tree by an edge of the smallest tie can be broken the algorithm stop after all the graph vertex have been included in the tree being since the algorithm expands a tree by exactly one vertex on each of it iteration the total number of such iteration is n where n is the number of vertex in the the tree generated by the algorithm is obtained a the set of edge used for the tree here is pseudocode of this algorithm primg prims algorithm for constructing a minimum spanning tree input a weighted connected graph g v e output et the set of edge composing a minimum spanning tree of g vt v the set of tree vertex can be initialized with any vertex et for i to v do find a minimum weight edge e v u among all the edge v u such that v is in vt and u is in v vt vt vt u et et e return et the nature of prims algorithm make it necessary to provide each vertex not in the current tree with the information about the shortest edge connecting the vertex to a tree we can provide such information by attaching two label to a vertex the name of the nearest tree vertex and the length the weight of the corresponding vertex that are not adjacent to any of the tree vertex can be given the label indicating their infinite distance to the tree vertex and a null label for the name of the nearest tree alternatively we can split the vertex that are not in the tree into two set the fringe and the the fringe contains only the vertex that are not in the tree but are adjacent to at least one tree these are the candidate from which the next tree vertex is the unseen vertex are all the other vertex of the graph called unseen because they are yet to be affected by the with such label finding the next vertex to be added to the current tree t vt et becomes a simple task of finding a vertex with the smallest distance label in the set v vt tie can be broken after we have identified a vertex u to be added to the tree we need to perform two operation move u from the set v vt to the set of tree vertex vt for each remaining vertex u in v vt that is connected to u by a shorter edge than the u current distance label update it label by u and the weight of the edge between u and u figure demonstrates the application of prims algorithm to a specific doe prims algorithm always yield a minimum spanning the answer to this question is let u prove by induction that each of the subtrees ti i n generated by prims algorithm is a part a subgraph of some minimum spanning this immediately implies of course that the last tree in the sequence tn is a minimum spanning tree itself because it contains all n vertex of the the basis of the induction is trivial since t consists of a single vertex and hence must be a part of any minimum spanning for the inductive step let u assume that ti is part of some minimum spanning tree t we need to prove that ti generated from ti by prims algorithm is also a part of a minimum spanning we prove this by contradiction by assuming that no minimum spanning tree of the graph can contain let ei v u be the minimum weight edge from a vertex in ti to a vertex not in ti used by prims algorithm to expand ti to by our assumption ei cannot belong to any minimum spanning tree including t therefore if we add ei to t a cycle must be formed figure in addition to edge ei v u this cycle must contain another edge v u connecting a vertex v ti to a vertex u that is not in ti it is possible that v coincides with v or u coincides with u but not if we now delete the edge v u from this cycle we will obtain another spanning tree of the entire graph whose weight is le than or equal to the weight of t since the weight of ei is le than or equal to the weight of v u hence this spanning tree is a minimum spanning tree which contradicts the assumption that no minimum spanning tree contains this completes the correctness proof of prims how efficient is prims the answer depends on the data structure chosen for the graph itself and for the priority queue of the set v vt whose vertex priority are the distance to the nearest tree you may want to take another look at the example in figure to see that the set v vt indeed operates a a priority in particular if a graph is represented by it weight matrix and the priority queue is implemented a an unordered array the algorithm running time will be in v indeed on each of the v iteration the array implementing the priority queue is traversed to find and delete the minimum and then to update if necessary the priority of the remaining we can also implement the priority queue a a min a min heap is a mirror image of the heap structure discussed in section in fact it can be im plemented by constructing a heap after negating all the key value namely a min heap is a complete binary tree in which every element is le than or equal if the implementation with the fringeunseen split is pursued all the unseen vertex adjacent to u must also be moved to the a f d e tree vertex remaining vertex illustration a ba c d b c ea fa a f d e ba cb d ea b c fb a f d e cb dc ea fb b c a f d e fb df ef b c a f d e ef df b c a f d e df figure application of prims the parenthesized label of a vertex in the middle column indicate the nearest tree vertex and edge weight selected vertex and edge are shown in v u v ei u ti figure correctness proof of prims to it all the principal property of heap remain valid for min heap with some obvious for example the root of a min heap contains the smallest rather than the largest deletion of the smallest element from and insertion of a new element into a min heap of size n are olog n operation and so is the operation of changing an element priority see problem in this section if a graph is represented by it adjacency list and the priority queue is im plemented a a min heap the running time of the algorithm is in oe log v this is because the algorithm performs v deletion of the smallest element and make e verification and possibly change of an element priority in a min heap of size not exceeding v each of these operation a noted earlier is a olog v hence the running time of this implementation of prims algorithm is in v eolog v oe log v because in a connected graph v in the next section you will find another greedy algorithm for the minimum spanning tree problem which is greedy in a manner different from that of prims exercise write pseudocode of the greedy algorithm for the change making problem with an amount n and coin denomination d d dm a it what is the time efficiency class of your design a greedy algorithm for the assignment problem see section doe your greedy algorithm always yield an optimal job scheduling consider the problem of scheduling n job of known dura tions t t tn for execution by a single the job can be executed in any order one job at a you want to find a schedule that minimizes the total time spent by all the job in the the time spent by one job in the system is the sum of the time spent by this job in waiting plus the time spent on it design a greedy algorithm for this doe the greedy algorithm always yield an optimal compatible interval given n open interval a b a b an bn on the real line each representing start and end time of some activity requiring the same resource the task is to find the largest number of these interval so that no two of them investigate the three greedy algorithm based on earliest start shortest duration earliest finish for each of the three algorithm either prove that the algorithm always yield an optimal solution or give a counterexample showing this not to be the bridge crossing revisited consider the generalization of the bridge crossing puzzle problem in exercise in which we have n people whose bridge crossing time are t t all the other condition of the problem remain the same at most two people at a time can cross the bridge and they move with the speed of the slower of the two and they must carry with them the only flashlight the group design a greedy algorithm for this problem and find how long it will take to cross the bridge by using this doe your algorithm yield a minimum crossing time for every instance of the if it doe prove it if it doe not find an instance with the smallest number of people for which this averaging down there are n identical vessel one of them with w pint of water and the others you are allowed to perform the following operation take two of the vessel and split the total amount of water in them equally between the object is to achieve a minimum amount of water in the vessel containing all the water in the initial set up by a sequence of such what is the best way to do rumor spreading there are n people each in possession of a different they want to share all the rumor with each other by sending electronic assume that a sender includes all the rumor he or she know at the time the message is sent and that a message may only have one design a greedy algorithm that always yield the minimum number of message they need to send to guarantee that every one of them get all the bachets problem of weight find an optimal set of n weight w w wn so that it would be possible to weigh on a balance scale any integer load in the largest possible range from to w provided weight can be put only on the free cup of the weight can be put on both cup of the apply prims algorithm to the following include in the priority queue all the vertex not already in the a b e c d apply prims algorithm to the following include in the priority queue only the fringe vertex the vertex not in the current tree which are adjacent to at least one tree a b c d e f g h i j k l the notion of a minimum spanning tree is applicable to a connected weighted do we have to check a graph connectivity before applying prims algorithm or can the algorithm do it by doe prims algorithm always work correctly on graph with negative edge let t be a minimum spanning tree of graph g obtained by prims let gnew be a graph obtained by adding to g a new vertex and some edge with weight connecting the new vertex to some vertex in can we con struct a minimum spanning tree of gnew by adding one of the new edge to t if you answer yes explain how if you answer no explain why how can one use prims algorithm to find a spanning tree of a connected graph with no weight on it is it a good algorithm for this prove that any weighted connected graph with distinct weight ha exactly one minimum spanning outline an efficient algorithm for changing an element value in a min what is the time efficiency of your kruskals algorithm in the previous section we considered the greedy algorithm that grows a mini mum spanning tree through a greedy inclusion of the nearest vertex to the vertex already in the remarkably there is another greedy algorithm for the mini mum spanning tree problem that also always yield an optimal it is named kruskals algorithm after joseph kruskal who discovered this algorithm when he wa a second year graduate student kruskals algorithm look at a minimum spanning tree of a weighted connected graph g v e a an acyclic subgraph with v edge for which the sum of the edge weight is the it is not difficult to prove that such a subgraph must be a consequently the algorithm construct a minimum spanning tree a an expanding sequence of subgraphs that are always acyclic but are not necessarily connected on the inter mediate stage of the the algorithm begin by sorting the graph edge in nondecreasing order of their then starting with the empty subgraph it scan this sorted list adding the next edge on the list to the current subgraph if such an inclusion doe not create a cycle and simply skipping the edge algorithm kruskalg kruskals algorithm for constructing a minimum spanning tree input a weighted connected graph g v e output et the set of edge composing a minimum spanning tree of g sort e in nondecreasing order of the edge weight wei weie et ecounter initialize the set of tree edge and it size k initialize the number of processed edge while ecounter v do kk if et eik is acyclic et et eik ecounter ecounter return et the correctness of kruskals algorithm can be proved by repeating the essen tial step of the proof of prims algorithm given in the previous the fact that et is actually a tree in prims algorithm but generally just an acyclic subgraph in kruskals algorithm turn out to be an obstacle that can be figure demonstrates the application of kruskals algorithm to the same graph we used for illustrating prims algorithm in section a you trace the algorithm operation note the disconnectedness of some of the intermediate applying prims and kruskals algorithm to the same small graph by hand may create the impression that the latter is simpler than the this impres sion is wrong because on each of it iteration kruskals algorithm ha to check whether the addition of the next edge to the edge already selected would create a a f d e tree edge sorted list of edge illustration bc ef ab bf cf af df ae cd de b c a f d e bc bc ef ab bf cf af df ae cd de b c a f d e ef bc ef ab bf cf af df ae cd de b c a f d e ab bc ef ab bf cf af df ae cd de b c a f d e bf bc ef ab bf cf af df ae cd de b c a f d e df figure application of kruskals selected edge are shown in v v u u a b figure new edge connecting two vertex may a or may not b create a it is not difficult to see that a new cycle is created if and only if the new edge connects two vertex already connected by a path if and only if the two vertex belong to the same connected component figure note also that each connected component of a subgraph generated by kruskals algorithm is a tree because it ha no in view of these observation it is convenient to use a slightly different interpretation of kruskals we can consider the algorithm operation a a progression through a series of forest containing all the vertex of a given graph and some of it the initial forest consists of v trivial tree each comprising a single vertex of the the final forest consists of a single tree which is a minimum spanning tree of the on each iteration the algorithm take the next edge u v from the sorted list of the graph edge find the tree containing the vertex u and v and if these tree are not the same unites them in a larger tree by adding the edge u fortunately there are efficient algorithm for doing so including the crucial check for whether two vertex belong to the same they are called unionfind we discus them in the following with an efficient union find algorithm the running time of kruskals algorithm will be dominated by the time needed for sorting the edge weight of a given hence with an efficient sorting algorithm the time efficiency of kruskals algorithm will be in oe log disjoint subset and union find algorithm kruskals algorithm is one of a number of application that require a dynamic partition of some n element set s into a collection of disjoint subset s s after being initialized a a collection of n one element subset each containing a different element of s the collection is subjected to a sequence of intermixed union and find note that the number of union operation in any such sequence must be bounded above by n because each union increase a subset size at least by and there are only n element in the entire set thus we are dealing here with an abstract data type of a collection of disjoint subset of a finite set with the following operation makesetx creates a one element set it is assumed that this operation can be applied to each of the element of set s only findx return a subset containing unionx y construct the union of the disjoint subset sx and sy containing x and y respectively and add it to the collection to replace sx and sy which are deleted from for example let s then makeseti creates the set i and applying this operation six time initializes the structure to the collection of six singleton set performing union and union yield and if followed by union and then by union we end up with the disjoint subset most implementation of this abstract data type use one element from each of the disjoint subset in a collection a that subset some implemen tations do not impose any specific constraint on such a representative others do so by requiring say the smallest element of each subset to be used a the subset also it is usually assumed that set element are or can be mapped into there are two principal alternative for implementing this data the first one called the quick find optimizes the time efficiency of the find operation the second one called the quick union optimizes the union the quick find us an array indexed by the element of the underlying set s the array value indicate the representative of the subset containing those each subset is implemented a a linked list whose header contains the pointer to the first and last element of the list along with the number of element in the list see figure for an under this scheme the implementation of makesetx requires assigning the corresponding element in the representative array to x and initializing the corre sponding linked list to a single node with the x the time efficiency of this operation is obviously in and hence the initialization of n singleton subset is in the efficiency of findx is also in all we need to do is to retrieve the x representative in the representative executing unionx y take a straightforward solution would simply append the y list to the end of the x list update the information about their representative for all the element in the subset representative element index representative size last first list null list null null list null list null null list null null list null null figure linked list representation of subset and obtained by quick find after performing union union union and union the list of size are considered deleted from the y list and then delete the y list from the it is easy to verify however that with this algorithm the sequence of union operation union union unioni i unionn n run in n time which is slow compared with several known a simple way to improve the overall efficiency of a sequence of union operation is to always append the shorter of the two list to the longer one with tie broken of course the size of each list is assumed to be available by say storing the number of element in the list this modification is called the a b figure a forest representation of subset and used by quick b result of union union by though it doe not improve the worst case efficiency of a single ap plication of the union operation it is still in n the worst case running time of any legitimate sequence of union by size operation turn out to be in on log here is a proof of this let ai be an element of set s whose disjoint subset we manipulate and let ai be the number of time ai representative is updated in a sequence of union by size how large can ai get if set s ha n each time ai representative is updated ai must be in a smaller subset involved in computing the union whose size will be at least twice a large a the size of the subset containing hence when ai representative is updated for the first time the resulting set will have at least two element when it is updated for the second time the resulting set will have at least four element and in general if it is updated ai time the resulting set will have at least ai since the entire set s ha n element ai n and hence ai log therefore the total number of possible update of the representative for all n element in s will not exceed n log thus for union by size the time efficiency of a sequence of at most n union and m find is in on log n the quick union the second principal alternative for implementing disjoint subset represents each subset by a rooted the node of the tree contain the subset element one per node with the root element considered the subset representative the tree edge are directed from child to their parent figure in addition a mapping of the set element to their tree node implemented say a an array of pointer is this mapping is not shown in figure for the sake of for this implementation makesetx requires the creation of a single node tree which is a operation hence the initialization of n singleton subset is in a unionx y is implemented by attaching the root of the y tree to the root of the x tree and deleting the y tree from the collection by making the pointer to it root the time efficiency of this operation is clearly a findx is this is a specific example of the usefulness of the amortized efficiency we mentioned back in chapter x t t t x t t t t t figure path performed by following the pointer chain from the node containing x to the tree root whose element is returned a the subset accordingly the time efficiency of a single find operation is in on because a tree representing a subset can degenerate into a linked list with n this time bound can be the straightforward way for doing so is to always perform a union operation by attaching a smaller tree to the root of a larger one with tie broken the size of a tree can be measured either by the number of node this version is called union by size or by it height this version is called union by of course these option require storing for each node of the tree either the number of node descendant or the height of the subtree rooted at that node one can easily prove that in either case the height of the tree will be logarithmic making it possible to execute each find in olog n thus for quick union the time efficiency of a sequence of at most n union and m find is in on m log in fact an even better efficiency can be obtained by combining either variety of quick union with path this modification make every node encountered during the execution of a find operation point to the tree root figure according to a quite sophisticated analysis that go beyond the level of this book see tar this and similar technique improve the efficiency of a sequence of at most n union and m find to only slightly worse than exercise apply kruskals algorithm to find a minimum spanning tree of the following b c a d e a b c d e f g h i j k l indicate whether the following statement are true or false if e is a minimum weight edge in a connected weighted graph it must be among edge of at least one minimum spanning tree of the if e is a minimum weight edge in a connected weighted graph it must be among edge of each minimum spanning tree of the if edge weight of a connected weighted graph are all distinct the graph must have exactly one minimum spanning if edge weight of a connected weighted graph are not all distinct the graph must have more than one minimum spanning what change if any need to be made in algorithm kruskal to make it find a minimum spanning forest for an arbitrary a minimum spanning forest is a forest whose tree are minimum spanning tree of the graph connected doe kruskals algorithm work correctly on graph that have negative edge design an algorithm for finding a maximum spanning tree a spanning tree with the largest possible edge weight of a weighted connected rewrite pseudocode of kruskals algorithm in term of the operation of the disjoint subset prove the correctness of kruskals prove that the time efficiency of findx is in olog n for the union by size version of quick find at least two web site with animation of kruskals and prims discus their merit and design and conduct an experiment to empirically compare the efficiency of prims and kruskals algorithm on random graph of different size and steiner tree four village are located at the vertex of a unit square in the euclidean you are asked to connect them by the shortest network of road so that there is a path between every pair of the village along those find such a write a program generating a random maze based on prims kruskals dijkstras algorithm in this section we consider the single source shortest path problem for a given vertex called the source in a weighted connected graph find shortest path to all it other it is important to stress that we are not interested here in a single shortest path that start at the source and visit all the other this would have been a much more difficult problem actually a version of the traveling salesman problem introduced in section and discussed again later in the the single source shortest path problem asks for a family of path each leading from the source to a different vertex in the graph though some path may of course have edge in a variety of practical application of the shortest path problem have made the problem a very popular object of the obvious but probably most widely used application are transportation planning and packet routing in communi cation network including the multitude of le obvious application include finding shortest path in social network speech recognition document formatting robotics compiler and airline crew in the world of enter tainment one can mention pathfinding in video game and finding best solution to puzzle using their state space graph see section for a very simple example of the there are several well known algorithm for finding shortest path including floyds algorithm for the more general all pair shortest path problem discussed in chapter here we consider the best known algorithm for the single source shortest path problem called dijkstras this algorithm is applicable to undirected and directed graph with nonnegative weight since in most ap plication this condition is satisfied the limitation ha not impaired the popularity of dijkstras dijkstras algorithm find the shortest path to a graph vertex in order of their distance from a given first it find the shortest path from the source edsger dijkstra a noted dutch pioneer of the science and industry of computing discovered this algorithm in the mid dijkstra said about his algorithm this wa the first graph problem i ever posed myself and the amazing thing wa that i didnt publish it wa not amazing at the at the time algorithm were hardly considered a scientific u v v figure idea of dijkstras the subtree of the shortest path already found is shown in the next nearest to the source v vertex u is selected by comparing the length of the subtrees path increased by the distance to vertex adjacent to the subtrees to a vertex nearest to it then to a second nearest and so in general before it ith iteration commences the algorithm ha already identified the shortest path to i other vertex nearest to the these vertex the source and the edge of the shortest path leading to them from the source form a subtree ti of the given graph figure since all the edge weight are nonnegative the next vertex nearest to the source can be found among the vertex adjacent to the vertex of the set of vertex adjacent to the vertex in ti can be referred to a fringe vertex they are the candidate from which dijkstras algorithm selects the next vertex nearest to the actually all the other vertex can be treated a fringe vertex connected to tree vertex by edge of infinitely large to identify the ith nearest vertex the algorithm computes for every fringe vertex u the sum of the distance to the nearest tree vertex v given by the weight of the edge v u and the length dv of the shortest path from the source to v previously determined by the algorithm and then selects the vertex with the smallest such the fact that it suffices to compare the length of such special path is the central insight of dijkstras to facilitate the algorithm operation we label each vertex with two the numeric label d indicates the length of the shortest path from the source to this vertex found by the algorithm so far when a vertex is added to the tree d indicates the length of the shortest path from the source to that the other label indicates the name of the next to last vertex on such a path the parent of the vertex in the tree being it can be left unspecified for the source s and vertex that are adjacent to none of the current tree with such labeling finding the next nearest vertex u becomes a simple task of finding a fringe vertex with the smallest d tie can be broken after we have identified a vertex u to be added to the tree we need to perform two operation move u from the fringe to the set of tree for each remaining fringe vertex u that is connected to u by an edge of weight wu u such that du wu u du update the label of u by u and du wu u figure demonstrates the application of dijkstras algorithm to a specific the labeling and mechanic of dijkstras algorithm are quite similar to those used by prims algorithm see section both of them construct an expanding subtree of vertex by selecting the next vertex from the priority queue of the remaining it is important not to mix them up they solve different problem and therefore operate with priority computed in a different manner dijkstras algorithm compare path length and therefore must add edge weight while prims algorithm compare the edge weight a now we can give pseudocode of dijkstras it is spelled out in more detail than prims algorithm wa in section in term of explicit operation on two set of labeled vertex the set vt of vertex for which a shortest path ha already been found and the priority queue q of the fringe note that in the following pseudocode vt contains a given source vertex and the fringe contains the vertex adjacent to it after iteration is algorithm dijkstrag s dijkstras algorithm for single source shortest path input a weighted connected graph g v e with nonnegative weight and it vertex s output the length dv of a shortest path from s to v and it penultimate vertex pv for every vertex v in v initializeq initialize priority queue to empty for every vertex v in v dv pv null insertq v dv initialize vertex priority in the priority queue d decreaseq s d update priority of s with d vt for i to v do u deleteminq delete the minimum priority element vt vt u for every vertex u in v vt that is adjacent to u do if du wu u du du du wu u pu u decreaseq u du the time efficiency of dijkstras algorithm depends on the data structure used for implementing the priority queue and for representing an input graph for the reason explained in the analysis of prims algorithm in section it is b c a d e tree vertex remaining vertex illustration a ba c da e b c a d e ba cb db e b c a d e db cb ed b c a d e cb ed b c a d e ed the shortest path identified by following nonnumeric label backward from a destination vertex in the left column to the source and their length given by numeric label of the tree vertex are a follows from a to b a b of length from a to d a b d of length from a to c a b c of length from a to e a b d e of length figure application of dijkstras the next closest vertex is shown in in v for graph represented by their weight matrix and the priority queue implemented a an unordered for graph represented by their adjacency list and the priority queue implemented a a min heap it is in oe log v a still better upper bound can be achieved for both prims and dijkstras algorithm if the priority queue is implemented using a sophisticated data structure called the fibonacci heap however it complexity and a considerable overhead make such an improvement primarily of theoretical exercise explain what adjustment if any need to be made in dijkstras algorithm andor in an underlying graph to solve the following solve the single source shortest path problem for directed weighted find a shortest path between two given vertex of a weighted graph or this variation is called the single pair shortest path find the shortest path to a given vertex from each other vertex of a weighted graph or this variation is called the single destination shortest path solve the single source shortest path problem in a graph with nonnegative number assigned to it vertex and the length of a path defined a the sum of the vertex number on the solve the following instance of the single source shortest path problem with vertex a a the source b c a d e a b c d e f g h i j k l give a counterexample that show that dijkstras algorithm may not work for a weighted connected graph with negative let t be a tree constructed by dijkstras algorithm in the process of solving the single source shortest path problem for a weighted connected graph true or false t is a spanning tree of true or false t is a minimum spanning tree of write pseudocode for a simpler version of dijkstras algorithm that find only the distance the length of shortest path but not shortest path themselves from a given vertex to all other vertex of a graph represented by it weight prove the correctness of dijkstras algorithm for graph with positive design a linear time algorithm for solving the single source shortest path problem for dag directed acyclic graph represented by their adjacency explain how the minimum sum descent problem problem in exercise can be solved by dijkstras shortest path modeling assume you have a model of a weighted connected graph made of ball representing the vertex connected by string of appro priate length representing the describe how you can solve the single pair shortest path problem with this describe how you can solve the single source shortest path problem with this revisit the exercise from section about determining the best route for a subway passenger to take from one designated station to another in a well developed subway system like those in washington dc or london write a program for this huffman tree and code suppose we have to encode a text that comprises symbol from some n symbol alphabet by assigning to each of the text symbol some sequence of bit called the for example we can use a fixed length encoding that assigns to each symbol a bit string of the same length m m log this is exactly what the standard ascii code one way of getting a coding scheme that yield a shorter bit string on the average is based on the old idea of assigning shorter code word to more frequent symbol and longer codewords to le frequent this idea wa used in particular in the telegraph code invented in the mid th century by samuel in that code frequent letter such a e and a are assigned short sequence of dot and dash while infrequent letter such a q and z have longer variable length encoding which assigns codewords of different length to different symbol introduces a problem that fixed length encoding doe not namely how can we tell how many bit of an encoded text represent the first or more generally the ith to avoid this complication we can limit ourselves to the so called prefix free or simply prefix in a prefix code no codeword is a prefix of a codeword of another hence with such an encoding we can simply scan a bit string until we get the first group of bit that is a codeword for some symbol replace these bit by this symbol and repeat this operation until the bit string end is if we want to create a binary prefix code for some alphabet it is natural to associate the alphabet symbol with leaf of a binary tree in which all the left edge are labeled by and all the right edge are labeled by the codeword of a symbol can then be obtained by recording the label on the simple path from the root to the symbol since there is no simple path to a leaf that continues to another leaf no codeword can be a prefix of another codeword hence any such tree yield a prefix among the many tree that can be constructed in this manner for a given alphabet with known frequency of the symbol occurrence how can we construct a tree that would assign shorter bit string to high frequency symbol and longer one to low frequency it can be done by the following greedy algorithm invented by david huffman while he wa a graduate student at mit huffmans algorithm step initialize n one node tree and label them with the symbol of the alphabet record the frequency of each symbol in it tree root to indicate the tree more generally the weight of a tree will be equal to the sum of the frequency in the tree step repeat the following operation until a single tree is find two tree with the smallest weight tie can be broken arbitrarily but see problem in this section make them the left and right subtree of a new tree and record the sum of their weight in the root of the new tree a it a tree constructed by the above algorithm is called a huffman it defines in the manner described above a huffman example consider the five symbol alphabet a b c d with the following occurrence frequency in a text made up of these symbol symbol a b c d frequency the huffman tree construction for this input is shown in figure b c d a c d a b a b c d c d a b c d a b figure example of constructing a huffman coding the resulting codewords are a follows symbol a b c d frequency codeword hence dad is encoded a and is decoded a with the occurrence frequency given and the codeword length obtained the average number of bit per symbol in this code is had we used a fixed length encoding for the same alphabet we would have to use at least bit per each thus for this toy example huffmans code achieves the compression ratio a standard measure of a compression algorithm effectiveness of in other word huffmans encoding of the text will use le memory than it fixed length extensive experiment with huffman code have shown that the compression ratio for this scheme typically fall between and depending on the characteristic of the text being huffmans encoding is one of the most important file compression in addition to it simplicity and versatility it yield an optimal minimal length encoding provided the frequency of symbol occurrence are independent and known in the simplest version of huffman compression call in fact for a preliminary scanning of a given text to count the frequency of symbol occurrence in then these frequency are used to construct a huffman coding tree and encode the text a described this scheme make it necessary however to include the coding table into the encoded text to make it decoding this drawback can be overcome by using dynamic huffman encoding in which the coding tree is updated each time a new symbol is read from the source further modern alternative such a lempel ziv algorithm say assign codewords not to individual symbol but to string of symbol allowing them to achieve better and more robust compression in many it is important to note that application of huffmans algorithm are not limited to data suppose we have n positive number w w wn that have to be assigned to n leaf of a binary tree one per if we define the weighted path length a the sum n li wi where li is the length of the simple i path from the root to the ith leaf how can we construct a binary tree with minimum weighted path it is this more general problem that huffmans algorithm actually for the coding application li and wi are the length of the codeword and the frequency of the ith symbol this problem arises in many situation involving decision consider for example the game of guessing a chosen object from n possibility say an integer between and n by asking question answerable by yes or different strategy for playing this game can be modeled by decision tree such a those depicted in figure for n the length of the simple path from the root to a leaf in such a tree is equal to the number of question needed to get to the chosen number represented by the if number i is chosen with probability pi the sum decision tree are discussed in more detail in section no n no n yes yes n n n n no yes no yes no yes n n n n n n no yes n n figure two decision tree for guessing an integer between and n li pi where li is the length of the path from the root to the ith leaf indicates i the average number of question needed to guess the chosen number with a game strategy represented by it decision if each of the number is chosen with the same probability of n the best strategy is to successively eliminate half or almost half the candidate a binary search this may not be the case for arbitrary pi for example if n and p p p and p the minimum weighted path tree is the rightmost one in figure thus we need huffmans algorithm to solve this problem in it general note that this is the second time we are encountering the problem of con structing an optimal binary in section we discussed the problem of constructing an optimal binary search tree with positive number the search prob ability assigned to every node of the in this section given number are assigned just to the latter problem turn out to be easier it can be solved by the greedy algorithm whereas the former is solved by the more complicated dynamic programming exercise construct a huffman code for the following data symbol a b c d frequency encode abacabad using the code of question decode using the code of question for data transmission purpose it is often desirable to have a code with a minimum variance of the codeword length among code of the same average compute the average and variance of the codeword length in two huffman code that result from a different tie breaking during a huffman code construction for the following data symbol a b c d e probability indicate whether each of the following property is true for every huffman the codewords of the two least frequent symbol have the same the codewords length of a more frequent symbol is always smaller than or equal to the codewords length of a le frequent what is the maximal length of a codeword possible in a huffman encoding of an alphabet of n write pseudocode of the huffman tree construction what is the time efficiency class of the algorithm for constructing a huff man tree a a function of the alphabet show that a huffman tree can be constructed in linear time if the alphabet symbol are given in a sorted order of their given a huffman coding tree which algorithm would you use to get the codewords for all the what is it time efficiency class a a function of the alphabet explain how one can generate a huffman code without an explicit generation of a huffman coding write a program that construct a huffman code for a given english text and encode write a program for decoding of an english text which ha been encoded with a huffman experiment with your encoding program to find a range of typical compres sion ratio for huffmans encoding of english text of say experiment with your encoding program to find out how sensitive the compression ratio are to using standard estimate of frequency instead of actual frequency of symbol occurrence in english card guessing design a strategy that minimizes the expected number of question asked in the following game you have a deck of card that consists of one ace of spade two deuce of spade three three and on up to nine nine making card in someone draw a card from the shuffled deck which you have to identify by asking question answerable with yes or summary the greedy technique suggests constructing a solution to an optimization problem through a sequence of step each expanding a partially constructed solution obtained so far until a complete solution to the problem is on each step the choice made must be feasible locally optimal and prims algorithm is a greedy algorithm for constructing a minimum spanning tree of a weighted connected it work by attaching to a previously constructed subtree a vertex closest to the vertex already in the kruskals algorithm is another greedy algorithm for the minimum spanning tree it construct a minimum spanning tree by selecting edge in nondecreasing order of their weight provided that the inclusion doe not create a checking the latter condition efficiently requires an application of one of the so called union find dijkstras algorithm solves the single source shortest path problem of finding shortest path from a given vertex the source to all the other vertex of a weighted graph or it work a prims algorithm but compare path length rather than edge dijkstras algorithm always yield a correct solution for a graph with nonnegative a huffman tree is a binary tree that minimizes the weighted path length from the root to the leaf of predefined the most important application of huffman tree is huffman a huffman code is an optimal prefix free variable length encoding scheme that assigns bit string to symbol based on their frequency in a given this is accomplished by a greedy construction of a binary tree whose leaf represent the alphabet symbol and whose edge are labeled with s and iterative improvement the most successful men in the end are those whose success is the result of steady alexander graham bell the greedy strategy considered in the preceding chapter construct a solution to an optimization problem piece by piece always adding a locally optimal piece to a partially constructed in this chapter we discus a different approach to designing algorithm for optimization it start with some feasible solution a solution that satisfies all the constraint of the problem and proceeds to improve it by repeated application of some simple this step typically involves a small localized change yielding a feasible solution with an improved value of the objective when no such change improves the value of the objective function the algorithm return the last feasible solution a optimal and there can be several obstacle to the successful implementation of this first we need an initial feasible for some problem we can always start with a trivial solution or use an approximate solution obtained by some other greedy but for others finding an initial solution may require a much effort a solving the problem after a feasible solution ha been second it is not always clear what change should be allowed in a feasible solution so that we can check efficiently whether the current solution is locally optimal and if not replace it with a better third and this is the most fundamental difficulty is an issue of local versus global extremum maximum or think about the problem of finding the highest point in a hilly area with no map on a foggy a logical thing to do would be to start walking up the hill from the point you are at until it becomes impossible to do so because no direction would lead you will have reached a local highest point but because of a limited feasibility there will be no simple way to tell whether the point is the highest global maximum you are after in the entire fortunately there are important problem that can be solved by iterativeimprovement the most important of them is linear we have already encountered this topic in section here in section we introduce the simplex method the classic algorithm for linear discovered by the mathematician george dantzig in this algorithm ha proved to be one of the most consequential achievement in the history of in section we consider the important problem of maximizing the amount of flow that can be sent through a network with link of limited this problem is a special case of linear however it special structure make it possible to solve the problem by algorithm that are more efficient than the simplex we outline the classic iterative improvement algorithm for this problem discovered by the american mathematician ford and fulkerson in the the last two section of the chapter deal with bipartite this is the problem of finding an optimal pairing of element taken from two disjoint example include matching worker and job high school graduate and college and men and woman for section deal with the problem of maximizing the number of matched pair section is concerned with the matching we also discus several iterative improvement algorithm in section where we consider approximation algorithm for the traveling salesman and knap sack other example of iterative improvement algorithm can be found in the algorithm textbook by moret and shapiro mor book on continuous and discrete optimization nem and the literature on heuristic search the simplex method we have already encountered linear programming see section the general problem of optimizing a linear function of several variable subject to a set of linear constraint maximize or minimize cx cnxn subject to aix ainxn or or bi for i m x xn we mentioned there that many important practical problem can be modeled a instance of linear two researcher kantorovich of the former soviet union and the dutch american koopmans were even awarded the nobel prize in for their contribution to linear programming theory and it application to apparently because there is no nobel prize in mathematics the royal swedish academy of science failed to honor the mathematician dantzig who is universally recognized a the father of linear programming in it modern form and the inventor of the simplex method the classic algorithm for solving such geometric interpretation of linear programming before we introduce a general method for solving linear programming problem let u consider a small example which will help u to see the fundamental prop erties of such example consider the following linear programming problem in two vari ables maximize x y subject to x y x y x y by definition a feasible solution to this problem is any point x y that satisfies all the constraint of the problem the problem feasible region is the set of all it feasible it is instructive to sketch the feasible region in the cartesian recall that any equation ax by c where coefficient a and b are not both equal to zero defines a straight such a line divide the plane into two half plane for all the point in one of them ax by c while for all the point in the other ax by it is easy to determine which of the two half plane is which take any point x y not on the line ax by c and check which of the two inequality hold ax by c or ax by in particular the set of point defined by inequality x y comprises the point on and below the line x y and the set of point defined by inequality x y comprises the point on and below the line x y since the point of the feasible region must satisfy all the constraint of the problem the feasible region is obtained by the intersection of these two half plane and the first quadrant of the cartesian plane defined by the nonnegativity constraint x y see figure thus the feasible region for problem is the convex polygon with the vertex and the last point which is the point of intersection of the line x y and x y is obtained by solving the system of these two linear our task is to find an optimal solution a point in the feasible region with the largest value of the objective function z x are there feasible solution for which the value of the objective function equal say the point x y for which the objective function z x y is equal to form the line x y since this line doe not have common point george dantzig ha received many honor including the national medal of science presented by the president of the united state in the citation state that the national medal wa awarded for inventing linear programming and discovering method that led to wide scale scientific and technical application to important problem in logistics scheduling and network optimization and to the use of computer in making efficient use of the mathematical y x y x xy figure feasible region of problem with the feasible region see figure the answer to the posed question is on the other hand there are infinitely many feasible point for which the objective function is equal to say they are the intersection point of the line x y with the feasible note that the line x y and x y have the same slope a would any line defined by equation x y z where z is some such line are called level line of the objective thus our problem can be restated a finding the largest value of the parameter z for which the level line x y z ha a common point with the feasible we can find this line either by shifting say the line x y south west without changing it toward the feasible region until it hit the region for the first time or by shifting say the line x y north east until it hit the feasible region for the last either way it will happen at the point with the corresponding z value this mean that the optimal solution to the linear programming problem in question is x y with the maximal value of the objective function equal to note that if we had to maximize z x y a the objective function in problem the level line x y z for the largest value of z would coincide with the boundary line segment that ha the same slope a the level line draw this line in figure consequently all the point of the line segment between vertex and including the vertex themselves would be optimal solution yielding of course the same maximal value of the objective y x x y x y x y figure solving a two dimensional linear programming problem doe every linear programming problem have an optimal solution that can be found at a vertex of it feasible without appropriate qualification the answer to this question is to begin with the feasible region of a linear programming problem can be for example if the constraint include two contradictory requirement such a x y and x y there can be no point in the problem feasible linear programming problem with the empty feasible region are called obviously infeasible problem do not have optimal another complication may arise if the problem feasible region is unbounded a the following example example if we reverse the inequality in problem to x y and x y the feasible region of the new problem will become unbounded see figure if the feasible region of a linear programming problem is unbounded it objective function may or may not attain a finite optimal value on for example the problem of maximizing z x y subject to the constraint x y x y x y ha no optimal solution because there are point in the feasible region making x y a large a we such problem are called on the other hand the problem of minimizing z x y subject to the same constraint ha an optimal solution y x x y x y x y figure unbounded feasible region of a linear programming problem with constraint x y x y x y and three level line of the function x fortunately the most important feature of the example we considered above hold for problem with more than two in particular a feasible region of a typical linear programming problem is in many way similar to convex polygon in the two dimensional cartesian specifically it always ha a finite number of vertex which mathematician prefer to call extreme point see section furthermore an optimal solution to a linear programming problem can be found at one of the extreme point of it feasible we reiterate these property in the following theorem extreme point theorem any linear programming problem with a nonempty bounded feasible region ha an optimal solution moreover an op timal solution can always be found at an extreme point of the problem feasible this theorem implies that to solve a linear programming problem at least in the case of a bounded feasible region we can ignore all but a finite number of except for some degenerate instance such a maximizing z x y subject to x y if a linear programming problem with an unbounded feasible region ha an optimal solution it can also be found at an extreme point of the feasible point in it feasible in principle we can solve such a problem by computing the value of the objective function at each extreme point and selecting the one with the best there are two major obstacle to implementing this plan the first lie in the need for a mechanism for generating the extreme point of the feasible a we are going to see below a rather straightforward algebraic procedure for this task ha been the second obstacle lie in the number of extreme point a typical feasible region here the news is bad the number of extreme point is known to grow exponentially with the size of the this make the exhaustive inspection of extreme point unrealistic for most linear programming problem of nontrivial fortunately it turn out that there exists an algorithm that typically inspects only a small fraction of the extreme point of the feasible region before reaching an optimal this famous algorithm is called the simplex the idea of this algorithm can be described in geometric term a start by identifying an extreme point of the feasible then check whether one can get an improved value of the objective function by going to an adjacent extreme if it is not the case the current point is optimal stop if it is the case proceed to an adjacent extreme point with an improved value of the objective after a finite number of step the algorithm will either reach an extreme point where an optimal solution occurs or determine that no optimal solution an outline of the simplex method our task now is to translate the geometric description of the simplex method into the more algorithmically precise language of to begin with before we can apply the simplex method to a linear programming problem it ha to be represented in a special form called the standard the standard form ha the following requirement it must be a maximization all the constraint except the nonnegativity constraint must be in the form of linear equation with nonnegative right hand all the variable must be required to be thus the general linear programming problem in standard form with m constraint and n unknown n m is maximize cx cnxn subject to aix ainxn bi where bi for i m x xn it can also be written in compact matrix notation maximize cx subject to ax b x where x b x a b c c c cn xn am am amn bm any linear programming problem can be transformed into an equivalent problem in standard if an objective function need to be minimized it can be replaced by the equivalent problem of maximizing the same objective function with all it coefficient cj replaced by cj j n see section for a more general discussion of such if a constraint is given a an inequality it can be replaced by an equivalent equation by adding a slack variable representing the difference between the two side of the original for example the two inequality of problem can be transformed respectively into the following equation x y u where u and x y v where v finally in most linear programming problem the variable are required to be nonnegative to begin with because they represent some physical if this is not the case in an initial statement of a problem an unconstrained variable xj can be replaced by the difference between two new nonnegative variable xj xj xj xj xj thus problem in standard form is the following linear programming problem in four variable maximize x y u v subject to x y u x y v x y u v it is easy to see that if we find an optimal solution x y u v to problem we can obtain an optimal solution to problem by simply ignoring it last two the principal advantage of the standard form lie in the simple mechanism it provides for identifying extreme point of the feasible to do this for problem for example we need to set two of the four variable in the con straint equation to zero to get a system of two linear equation in two unknown and solve this for the general case of a problem with m equation in n unknown n m n m variable need to be set to zero to get a system of m equation in m if the system obtained ha a unique solution a any nondegenerate system of linear equation with the number of equation equal to the number of unknown doe we have a basic solution it coordinate set to zero before solving the system are called nonbasic and it coordinate obtained by solving the system are called this terminology come from linear specifically we can rewrite the system of constraint equation of a x y u v a basis in the two dimensional vector space is composed of any two vector that are not proportional to each other once a basis is chosen any vector can be uniquely expressed a a sum of multiple of the basis basic and nonbasic variable indicate which of the given vector are respectively included and excluded in a particular basis if all the coordinate of a basic solution are nonnegative the basic solution is called a basic feasible for example if we set to zero variable x and y and solve the resulting system for u and v we obtain the basic feasible solution if we set to zero variable x and u and solve the resulting system for y and v we obtain the basic solution which is not the importance of basic feasible solution lie in the one to one correspondence between them and the extreme point of the feasible for example is an extreme point of the feasible region of problem with the point in figure being it projection on the x y incidentally is a natural starting point for the simplex method application to this a mentioned above the simplex method progress through a series of adjacent extreme point basic feasible solution with increasing value of the objective each such point can be represented by a simplex tableau a table storing the information about the basic feasible solution corresponding to the extreme for example the simplex tableau for of problem is presented below x y u v u v in general a simplex tableau for a linear programming problem in standard form with n unknown and m linear equality constraint n m ha m row and n each of the first m row of the table contains the coefficient of a corresponding constraint equation with the last column entry containing the equation right hand the column except the last one are labeled by the name of the the row are labeled by the basic variable of the basic feasible solution the tableau represents the value of the basic variable of this solution are in the last also note that the column labeled by the basic variable form the m m identity the last row of a simplex tableau is called the objective it is initialized by the coefficient of the objective function with their sign reversed in the first n column and the value of the objective function at the initial point in the last on subsequent iteration the objective row is transformed the same way a all the other the objective row is used by the simplex method to check whether the current tableau represents an optimal solution it doe if all the entry in the objective row except possibly the one in the last column are if this is not the case any of the negative entry indicates a nonbasic variable that can become basic in the next for example according to this criterion the basic feasible solution represented by tableau is not the negative value in the x column signal the fact that we can increase the value of the objective function z x y u v by increasing the value of the x coordinate in the current basic feasible solution indeed since the coefficient for x in the objective function is positive the larger the x value the larger the value of this of course we will need to compensate an increase in x by adjusting the value of the basic variable u and v so that the new point is still for this to be the case both condition xu where u xv where v must be satisfied which mean that x min note that if we increase the value of x from to the largest amount possible we will find ourselves at the point an adjacent to extreme point of the feasible region with z similarly the negative value in the y column of the objective row signal the fact that we can also increase the value of the objective function by increasing the value of the y coordinate in the initial basic feasible solution this requires yu where u y v where v which mean that y min if we increase the value of y from to the largest amount possible we will find ourselves at the point another adjacent to extreme point with z if there are several negative entry in the objective row a commonly used rule is to select the most negative one the negative number with the largest absolute this rule is motivated by the observation that such a choice yield the largest increase in the objective function value per unit of change in a variable in our example an increase in the x value from to at change the value of z x y u v from to while an increase in the y value from to at change z from to note however that the feasibility constraint impose different limit on how much each of the variable may in our example in particular the choice of the y variable over the x variable lead to a smaller increase in the value of the objective still we will employ this commonly used rule and select variable y a we continue with our a new basic variable is called the entering variable while it column is referred to a the pivot column we mark the pivot column by now we will explain how to choose a departing variable a basic variable to become nonbasic in the next the total number of basic variable in any basic solution must be equal to m the number of the equality a we saw above to get to an adjacent extreme point with a larger value of the objective function we need to increase the entering variable by the largest amount possible to make one of the old basic variable zero while preserving the nonnegativity of all the we can translate this observation into the following rule for choosing a departing variable in a simplex tableau for each positive entry in the pivot column compute the ratio by dividing the row last entry by the entry in the pivot for the example of tableau these ratio are u v the row with the smallest ratio determines the departing variable the variable to become tie may be broken for our example it is variable we mark the row of the departing variable called the pivot row by and denote it ro w note that if there are no positive entry in the pivot column no ratio can be computed which indicates that the problem is unbounded and the algorithm finally the following step need to be taken to transform a current tableau into the next this transformation called pivoting is similar to the principal step of the gauss jordan elimination algorithm for solving system of linear equation see problem in exercise first divide all the entry of the pivot row by the pivot it entry in the pivot column to obtain ro w for tableau we obtain ro w new then replace each of the other row including the objective row by the difference row c ro w new where c is the row entry in the pivot for tableau this yield row ro w new row ro w new thus the simplex method transforms tableau into the following tableau x y u v u y tableau represents the basic feasible solution with an increased value of the objective function which is equal to it is not optimal however the next iteration do it yourself a a good yield tableau x y u v x y this tableau represents the basic feasible solution it is optimal because all the entry in the objective row of tableau are the maximal value of the objective function is equal to the last entry in the objective let u summarize the step of the simplex summary of the simplex method step initialization present a given linear programming problem in stan dard form and set up an initial tableau with nonnegative entry in the rightmost column and m other column composing the m m identity entry in the objective row are to be disregarded in verifying these these m column define the basic variable of the initial basic feasible solution used a the label of the tableau step optimality test if all the entry in the objective row except possibly the one in the rightmost column which represents the value of the objective function are nonnegative stop the tableau represents an optimal solution whose basic variable value are in the rightmost column and the remaining nonbasic variable value are step finding the entering variable select a negative entry from among the first n element of the objective a commonly used rule is to select the negative entry with the largest absolute value with tie broken mark it column to indicate the entering variable and the pivot step finding the departing variable for each positive entry in the pivot column calculate the ratio by dividing that row entry in the right most column by it entry in the pivot if all the entry in the pivot column are negative or zero the problem is unbounded find the row with the smallest ratio tie may be broken arbitrarily and mark this row to indicate the departing variable and the pivot step forming the next tableau divide all the entry in the pivot row by it entry in the pivot subtract from each of the other row including the objective row the new pivot row multiplied by the entry in the pivot column of the row in this will make all the entry in the pivot column s except for in the pivot replace the label of the pivot row by the variable name of the pivot column and go back to step further note on the simplex method formal proof of validity of the simplex method step can be found in book devoted to a detailed discussion of linear programming a few important remark about the method still need to be made generally speaking an iteration of the simplex method lead to an extreme point of the problem feasible region with a greater value of the objective in degenerate case which arise when one or more basic variable are equal to zero the simplex method can only guarantee that the value of the objective function at the new extreme point is greater than or equal to it value at the previous in turn this open the door to the possibility not only that the objective function value stall for several iteration in a row but that the algorithm might cycle back to a previously considered point and hence never the latter phenomenon is called although it rarely if ever happens in practice specific example of problem where cycling doe occur have been a simple modification of step and of the simplex method called blands rule eliminates even the theoretical possibility of assuming that the variable are denoted by a subscripted letter x x xn this rule can be stated a follows step modified among the column with a negative entry in the objective row select the column with the smallest step modified resolve a tie among the smallest ratio by selecting the row labeled by the basic variable with the smallest another caveat deal with the assumption made in step they are automat ically satisfied if a problem is given in the form where all the constraint imposed on nonnegative variable are inequality aix ainxn bi with bi for i indeed by adding a nonnegative slack variable xni into the ith constraint we obtain the equality aix ainxn xni bi and all the re quirements imposed on an initial tableau of the simplex method are satisfied for the obvious basic feasible solution x xn xn xnm but if a problem is not given in such a form finding an initial basic feasible solution may present a nontrivial moreover for problem with an empty feasible region no initial basic feasible solution exists and we need an algorithmic way to identify such one of the way to address these issue is to use an exten sion to the classic simplex method called the two phase simplex method see in a nutshell this method add a set of artificial variable to the equality constraint of a given problem so that the new problem ha an obvious basic fea sible it then solves the linear programming problem of minimizing the sum of the artificial variable by the simplex the optimal solution to this problem either yield an initial tableau for the original problem or indicates that the feasible region of the original problem is how efficient is the simplex since the algorithm progress through a sequence of adjacent point of a feasible region one should probably expect bad news because the number of extreme point is known to grow exponentially with the problem indeed the worst case efficiency of the simplex method ha been shown to be exponential a fortunately more than half a century of practical experience with the algorithm ha shown that the number of iteration in a typical application range between m and m with the number of operation per iteration proportional to mn where m and n are the number of equality constraint and variable since it discovery in the simplex method ha been a subject of intensive study by many some of them have worked on improvement to the original algorithm and detail of it efficient a a result of these effort program implementing the simplex method have been polished to the point that very large problem with hundred of thousand of constraint and variable can be solved in a routine in fact such program have evolved into sophisticated software these package enable the user to enter a problem constraint and obtain a solution in a user friendly they also provide tool for investigating important property of the solution such a it sensitivity to change in the input such investigation are very important for many application including those in at the other end of the spectrum linear programming problem of a moderate size can nowadays be solved on a desktop using a standard spreadsheet facility or by taking advantage of specialized software available on the researcher have also tried to find algorithm for solving linear programming problem with polynomial time efficiency in the worst an important mile stone in the history of such algorithm wa the proof by khachian kha showing that the ellipsoid method can solve any linear programming problem in polynomial although the ellipsoid method wa much slower than the simplex method in practice it better worst case efficiency encouraged a search for alternative to the simplex in narendra karmarkar published an algorithm that not only had a polynomial worst case efficiency but also wa competitive with the simplex method in empirical test a although we are not going to discus karmarkars algorithm kar here it is worth pointing out that it is also based on the iterative improvement however karmarkars algorithm generates a sequence of feasible solution that lie within the feasible region rather than going through a sequence of adjacent extreme point a the simplex method such algorithm are called interior point method see exercise consider the following version of the post office location problem problem in exercise given n integer x x xn representing coordinate of n village located along a straight road find a location for a post office that minimizes the average distance between the the post office may be but is not required to be located at one of the devise an iterative improvement algorithm for this is this an efficient way to solve this solve the following linear programming problem maximize x y subject to x y x y x y maximize x y subject to x y y x x y consider the linear programming problem minimize cx cy subject to x y x y x y where c and c are some real number not both equal to give an example of the coefficient value c and c for which the problem ha a unique optimal give an example of the coefficient value c and c for which the problem ha infinitely many optimal give an example of the coefficient value c and c for which the problem doe not have an optimal would the solution to problem be different if it inequality constraint were strict x y and x y trace the simplex method on the problem of exercise the problem of exercise trace the simplex method on the problem of example in section by by using one of the implementation available on the determine how many iteration the simplex method need to solve the problem n maximize xj j subject to xj bj where bj for j can we apply the simplex method to solve the knapsack problem see exam ple in section if you answer yes indicate whether it is a good algorithm for the problem in question if you answer no explain why prove that no linear programming problem can have exactly k optimal solution unless k if a linear programming problem n maximize cj xj j n subject to aij xj bi for i m j x x xn is considered a primal then it dual is defined a the linear programming problem m minimize bi yi i m subject to aij yi cj for j n i y y ym express the primal and dual problem in matrix find the dual of the linear programming problem maximize x x x subject to x x x x x x x x x solve the primal and dual problem and compare the optimal value of their objective the maximum flow problem in this section we consider the important problem of maximizing the flow of a ma terial through a transportation network pipeline system communication system electrical distribution system and so we will assume that the transportation network in question can be represented by a connected weighted digraph with n vertex numbered from to n and a set of edge e with the following property it contains exactly one vertex with no entering edge this vertex is called the source and assumed to be numbered it contains exactly one vertex with no leaving edge this vertex is called the sink and assumed to be numbered the weight uij of each directed edge i j is a positive integer called the edge this number represents the upper bound on the amount of the material that can be sent from i to j through a link represented by this a digraph satisfying these property is called a flow network or simply a a small instance of a network is given in figure it is assumed that the source and the sink are the only source and destination of the material respectively all the other vertex can serve only a point where a flow can be redirected without consuming or adding any amount of the in other word the total amount of the material entering an intermediate vertex must be equal to the total amount of the material leaving the this con dition is called the flow conservation if we denote the amount sent through edge i j by xij then for any intermediate vertex i the flow conservation requirement can be expressed by the following equality constraint xji xij for i n j jie j ij e in a slightly more general model one can consider a network with several source and sink and allow capacity uij to be infinitely figure example of a network the vertex number are vertex name the edge number are edge where the sum in the left and right hand side express the total inflow and outflow entering and leaving vertex i since no amount of the material can change by going through intermediate vertex of the network it stand to reason that the total amount of the material leaving the source must end up at the this observation can also be derived formally from equality a task you will be asked to do in the thus we have the following equality xj xj j j e j jne this quantity the total outflow from the source or equivalently the total inflow into the sink is called the value of the we denote it by it is this quantity that we will want to maximize over all possible flow in a thus a feasible flow is an assignment of real number xij to edge i j of a given network that satisfy flow conservation constraint and the capacity constraint xij uij for every edge i j the maximum flow problem can be stated formally a the following optimization problem maximize v xj j j e subject to xji xij for i n j jie j ij e xij uij for every edge i j we can solve linear programming problem by the simplex method or by another algorithm for general linear programming problem see section however the special structure of problem can be exploited to design faster in particular it is quite natural to employ the iterative improvement idea a we can always start with the zero flow set xij for every edge i j in the then on each iteration we can try to find a path from source to sink along which some additional flow can be such a path is called flow if a flow augmenting path is found we adjust the flow along the edge of this path to get a flow of an increased value and try to find an augmenting path for the new if no flow augmenting path can be found we conclude that the current flow is this general template for solving the maximum flow problem is called the augmenting path method also known a the ford fulkerson method after ford and fulkerson who discovered it see an actual implementation of the augmenting path idea is however not quite to see this let u consider the network in figure we start with the zero flow shown in figure in that figure the zero amount sent through each edge are separated from the edge capacity by the slash we will use this notation in the other example a it is natural to search for a flowaugmenting path from source to sink by following directed edge i j for which the current flow xij is le than the edge capacity uij among several possibility let u assume that we identify the augmenting path we can increase the flow along this path by a maximum of unit which is the smallest unused capacity of it the new flow is shown in figure this is a far a our simpleminded idea about flow augmenting path will be able to take unfortunately the flow shown in figure is not optimal it value can still be increased along the path by increasing the flow by on edge and and decreasing it by on edge the flow obtained a the result of this augmentation is shown in figure it is indeed can you tell thus to find a flow augmenting path for a flow x we need to consider path from source to sink in the underlying undirected graph in which any two consecutive vertex i j are either connected by a directed edge from i to j with some positive unused capacity rij uij xij so that we can increase the flow through that edge by up to rij unit or connected by a directed edge from j to i with some positive flow xji so that we can decrease the flow through that edge by up to xji edge of the first kind are called forward edge because their tail is listed before their head in the vertex list i j n defining the path edge of the second kind are called backward edge because their tail is listed after their head in the path list i j to illustrate for the path of the last example and are the forward edge and is the backward for a given flow augmenting path let r be the minimum of all the unused capacity rij of it forward edge and all the flow xji of it backward it is easy to see that if we increase the current flow by r on each forward edge and decrease it by this amount on each backward edge we will obtain a feasible a b c figure illustration of the augmenting path flow augmenting path are shown in the flow amount and edge capacity are indicated by the number before and after the slash flow whose value is r unit greater than the value of it indeed let i be an intermediate vertex on a flow augmenting there are four possible combination of forward and backward edge incident to vertex i r i r r i r r i r r i r for each of them the flow conservation requirement for vertex i will still hold after the flow adjustment indicated above the edge further since r is the minimum among all the positive unused capacity on the forward edge and all the positive flow on the backward edge of the flow augmenting path the new flow will satisfy the capacity constraint a finally adding r to the flow on the first edge of the augmenting path will increase the value of the flow by under the assumption that all the edge capacity are integer r will be a positive integer hence the flow value increase at least by on each iteration of the augmenting path since the value of a maximum flow is bounded above by the sum of the capacity of the source edge the augmenting path method ha to stop after a finite number of surprisingly the final flow always turn out to be maximal irrespective of a sequence of augmenting this remarkable result stem from the proof of the max flow min cut theorem see for which we replicate later in this the augmenting path method a described above in it general form doe not indicate a specific way for generating flow augmenting a bad sequence of such path may however have a dramatic impact on the method consider for example the network in figure in which u stand for some large positive if we augment the zero flow along the path we shall obtain the flow of value shown in figure augmenting that flow along the path will increase the flow value to figure if we continue selecting this pair of flow augmenting path we will need a total of u iteration to reach the maximum flow of value u figure of course we can obtain the maximum flow in just two iteration by augmenting the initial zero flow along the path followed by augmenting the new flow along the path the dramatic difference between u and iteration make the fortunately there are several way to generate flow augmenting path ef ficiently and avoid the degradation in performance illustrated by the previous the simplest of them us breadth first search to generate augment ing path with the least number of edge see section this version of the augmenting path method called shortest augmenting path or first labeled first scanned algorithm wa suggested by edmonds and karp the labeling refers to marking a new unlabeled vertex with two the first label indicates the amount of additional flow that can be brought from the source to the vertex being the second label is the name of the vertex from which the vertex being labeled wa it can be left undefined for the it is also convenient to add the or sign to the second label to indicate whether the vertex wa reached via a forward or backward edge the source can be always labeled with for the other vertex the label are computed a if capacity upper bound are irrational number the augmenting path method may not terminate see chv for a cleverly devised example demonstrating such a this limitation is only of theoretical interest because we cannot store irrational number in a computer and rational number can be transformed into integer by changing the capacity measurement u u u u u u u u a b u u uu uu u u uu uu c d figure efficiency degradation of the augmenting path if unlabeled vertex j is connected to the front vertex i of the traversal queue by a directed edge from i to j with positive unused capacity rij uij xij then vertex j is labeled with lj i where lj minli rij if unlabeled vertex j is connected to the front vertex i of the traversal queue by a directed edge from j to i with positive flow xji then vertex j is labeled with lj i where lj minli if this labeling enhanced traversal end up labeling the sink the current flow can be augmented by the amount indicated by the sink first the augmentation is performed along the augmenting path traced by following the vertex second label from sink to source the current flow quantity are increased on the forward edge and decreased on the backward edge of this if on the other hand the sink remains unlabeled after the traversal queue becomes empty the algorithm return the current flow a maximum and algorithm shortestaugmentingpathg implement the shortest augmenting path algorithm input a network with single source single sink n and positive integer capacity uij on it edge i j output a maximum flow x assign xij to every edge i j in the network label the source with and add the source to the empty queue q while not emptyq do i frontq dequeueq for every edge from i to j do forward edge if j is unlabeled rij uij xij if rij lj minli rij label j with lj i enqueueq j for every edge from j to i do backward edge if j is unlabeled if xji lj minli xji label j with lj i enqueueq j if the sink ha been labeled augment along the augmenting path found j n start at the sink and move backwards using second label while j the source hasnt been reached if the second label of vertex j is i xij xij ln else the second label of vertex j is i xji xji ln j i i the vertex indicated by is second label erase all vertex label except the one of the source reinitialize q with the source return x the current flow is maximum an application of this algorithm to the network in figure is illustrated in figure the optimality of a final flow obtained by the augmenting path method stem from a theorem that relates network flow to network a cut induced by partitioning vertex of a network into some subset x containing the source and x the complement of x containing the sink is the set of all the edge with a tail in x and a head in x we denote a cut cx x or simply for example for the network in figure if x and hence x cx x if x and hence x cx x if x and hence x cx x the name cut stem from the following property if all the edge of a cut were deleted from the network there would be no directed path from source to indeed let cx x be a consider a directed path from source to if vi is the first vertex of that path which belongs to x the set of such vertex is not queue augment the flow by the sink first label along the path queue augment the flow by the sink first label along the path queue no augmenting path the sink is unlabeled the current flow is figure illustration of the shortest augmenting path the diagram on the left show the current flow before the next iteration begin the diagram on the right show the result of the vertex labeling on that iteration the augmenting path found in bold and the flow before it vertex deleted from the queue are indicated by the empty because it contains the sink then vi is not the source and it immediate predecessor vi on that path belongs to hence the edge from vi to vi must be an element of the cut cx x this prof the property in the capacity of a cut cx x denoted cx x is defined a the sum of capacity of the edge that compose the for the three example of cut given above the capacity are equal to and since the number of different cut in a network is nonempty and finite there always exists a minimum cut a cut with the smallest what is a minimum cut in the network of figure the following theorem establishes an important relationship between the notion of maximum flow and minimum theorem max flow min cut theorem the value of a maximum flow in a network is equal to the capacity of it minimum proof first let x be a feasible flow of value v and let cx x be a cut of capacity c in the same consider the flow across this cut defined a the difference between the sum of the flow on the edge from x to x and the sum of the flow on the edge from x to it is intuitively clear and can be formally derived from the equation expressing the flow conservation requirement and the definition of the flow value problem b in this section exercise that the flow across the cut cx x is equal to v the value of the flow v xij xj i ix j x j x ix since the second sum is nonnegative and the flow xij on any edge i j cannot exceed the edge capacity uij equality implies that v xij uij ix j x ix j x v thus the value of any feasible flow in a network cannot exceed the capacity of any cut in that let v be the value of a final flow x obtained by the augmenting path if we now find a cut whose capacity is equal to v we will have to conclude in view of inequality that i the value v of the final flow is maximal among all feasible flow ii the cut capacity is minimal among all cut in the network and iii the maximum flow value is equal to the minimum cut to find such a cut consider the set of vertex x that can be reached from the source by following an undirected path composed of forward edge with positive unused capacity with respect to the final flow x and backward edge with positive flow on this set contains the source but doe not contain the sink if it did we would have an augmenting path for the flow x which would contradict the assumption that the flow x is consider the cut cx by the definition of set x each edge i j from x to x ha zero unused capacity xij uij and each edge j i from x to x ha the zero flow on it otherwise j would be in applying equality to the final flow x and the set x defined above we obtain v xij xji uij cx x ix j x j x ix ix j x which prof the the proof outlined above accomplishes more than proving the equality of the maximum flow value and the minimum cut it also implies that when the augmenting path method terminates it yield both a maximum flow and a mini mum if labeling of the kind utilized in the shortest augmenting path algorithm is used a minimum cut is formed by the edge from the labeled to unlabeled ver tices on the last iteration of the finally the proof implies that all such edge must be full the flow must be equal to the edge capacity and all the edge from unlabeled vertex to labeled if any must be empty have zero flow on in particular for the network in figure the algorithm find the cut of minimum capacity both edge of which are full a edmonds and karp proved in their paper edm that the number of aug menting path needed by the shortest augmenting path algorithm never exceeds nm where n and m are the number of vertex and edge since the time required to find a shortest augmenting path by breadth first search is in on m om for network represented by their adjacency list the time efficiency of the shortest augmenting path algorithm is in more efficient algorithm for the maximum flow problem are known see the monograph ahu a well a appropriate chapter in such book a cor and some of them implement the augmenting path idea in a more efficient others are based on the concept of a preflow is a flow that satisfies the capacity constraint but not the flow conservation any vertex is allowed to have more flow entering the vertex than leaving a preflow push algorithm move the excess flow toward the sink until the flow conservation requirement is reestablished for all intermediate vertex of the faster al gorithms of this kind have worst case efficiency close to note that preflow push algorithm fall outside the iterative improvement paradigm because they do not generate a sequence of improving solution that satisfy all the constraint of the to conclude this section it is worth pointing out that although the initial interest in studying network flow wa caused by transportation application this model ha also proved to be useful for many other we discus one of them in the next exercise since maximum flow algorithm require processing edge in both direction it is convenient to modify the adjacency matrix representation of a network a if there is a directed edge from vertex i to vertex j of capacity uij then the element in the ith row and the j th column is set to uij and the element in the j th row and the ith column is set to uij if there is no edge between vertex i and j both these element are set to outline a simple algorithm for identifying a source and a sink in a network presented by such a matrix and indicate it time apply the shortest augmenting path algorithm to find a maximum flow and a minimum cut in the following doe the maximum flow problem always have a unique would your answer be different for network with different capacity on all their answer the same question for the minimum cut problem of finding a cut of the smallest capacity in a given explain how the maximum flow problem for a network with several source and sink can be transformed into the same problem for a network with a single source and a single some network have capacity constraint on the flow amount that can flow through their intermediate explain how the maximum flow problem for such a network can be transformed to the maximum flow problem for a network with edge capacity constraint consider a network that is a rooted tree with the root a it source the leaf a it sink and all the edge directed along the path from the root to the design an efficient algorithm for finding a maximum flow in such a what is the time efficiency of your prove equality prove that for any flow in a network and any cut in it the value of the flow is equal to the flow across the cut see equality explain the relationship between this property and equality express the maximum flow problem for the network in figure a a linear programming solve this linear programming problem by the simplex a an alternative to the shortest augmenting path algorithm edmonds and karp edm suggested the maximum capacity augmenting path algorithm in which a flow is augmented along the path that increase the flow by the largest implement both these algorithm in the language of your choice and perform an empirical investigation of their relative write a report on a more advanced maximum flow algorithm such a i dinitzs algorithm ii karzanovs algorithm iii malhotra kamar maheshwari algorithm or iv goldberg tarjan dining problem several family go out to dinner to increase their social interaction they would like to sit at table so that no two member of the same family are at the same show how to find a seating arrangement that meet this objective or prove that no such arrangement exists by using a maximum flow assume that the dinner contingent ha p family and that the ith family ha ai also assume that q table are available and the j th table ha a seating capacity of bj ahu maximum matching in bipartite graph in many situation we are faced with a problem of pairing element of two the traditional example is boy and girl for a dance but you can easily think of more serious it is convenient to represent element of two given set by vertex of a graph with edge between vertex that can be a matching in a graph is a subset of it edge with the property that no two edge share a a maximum matching more precisely a maximum cardinality matching is a matching with the largest number of what is it for the graph in figure is it the maximum matching problem is the problem of finding a maximum matching in a given for an arbitrary graph this is a rather difficult it wa solved in by jack edmonds see gal for a good survey and more recent we limit our discussion in this section to the simpler case of bipartite in a bipartite graph all the vertex can be partitioned into two disjoint set v and u not necessarily of the same size so that every edge connects a vertex in one of these set to a vertex in the other in other word a graph is bipartite if it vertex can be colored in two color so that every edge ha it vertex colored in different color such graph are also said to be the graph in figure is it is not difficult to prove that a graph is bipartite if and only if it doe not have a cycle of an odd we will assume for the rest of this section that v u figure example of a bipartite the vertex set of a given bipartite graph ha been already partitioned into set v and u a required by the definition see problem in exercise let u apply the iterative improvement technique to the maximumcardinality matching let m be a matching in a bipartite graph g v u e how can we improve it find a new matching with more obviously if every vertex in either v or u is matched ha a mate serf a an endpoint of an edge in m this cannot be done and m is a maximum therefore to have a chance at improving the current matching both v and u must contain unmatched also called free vertex vertex that are not incident to any edge in for example for the matching ma in the graph in figure vertex and are free and vertex and are another obvious observation is that we can immediately increase a current matching by adding an edge between two free for example adding to the matching ma in the graph in figure yield a larger matching mb figure let u now try to find a matching larger than mb by matching vertex the only way to do this would be to include the edge in a new this inclusion requires removal of which can be compensated by inclusion of in the new this new matching mc is shown in figure in general we increase the size of a current matching m by constructing a simple path from a free vertex in v to a free vertex in u whose edge are alternately in e m and in that is the first edge of the path doe not belong to m the second one doe and so on until the last edge that doe not belong to such a path is called augmenting with respect to the matching for example the path is an augmenting path with respect to the matching mb in figure since the length of an augmenting path is always odd adding to the matching m the path edge in the odd numbered position and deleting from it the path edge in the even numbered position yield a matching with one more edge than in such a matching adjustment is called thus in figure the matching mb wa obtained by augmentation of the matching ma along the augmenting path and the matching mc wa obtained by augmentation of the matching mb along the augmenting path moving further is an augmenting path for the matching mc figure after adding to mc the edge and and deleting and we obtain the matching md shown in figure the v u a augmenting path b augmenting path c augmenting path d maximum matching figure augmenting path and matching matching md is not only a maximum matching but also perfect a matching that match all the vertex of the before we discus an algorithm for finding an augmenting path let u settle the issue of what nonexistence of such a path according to the theorem discovered by the french mathematician claude berge it mean the current matching is theorem a matching m is a maximum matching if and only if there exists no augmenting path with respect to proof if an augmenting path with respect to a matching m exists then the size of the matching can be increased by let u prove the more difficult part if no augmenting path with respect to a matching m exists then the matching is a maximum assume that on the contrary this is not the case for a certain matching m in a graph let m be a maximum matching in g by our assumption the number of edge in m is at least one more than the number of edge in m m consider the edge in the symmetric difference m m m m m m the set of all the edge that are either in m or in m but not in note that m m m m because m m by let g be the subgraph of g made up of all the edge in m m and their by definition of a matching any vertex in g g can be incident to no more than one edge in m and no more than one edge in hence each of the vertex in g ha degree or le and therefore every connected component of g is either a path or an even length cycle of alternating edge from m m and m since m m m m and the number of edge from m m and m m is the same for any even length cycle of alternating edge in g there must exist at least one path of alternating edge that start and end with an edge from m hence this is an augmenting path for the matching m which contradicts the assumption that no such path our discussion of augmenting path lead to the following general method for constructing a maximum matching in a bipartite start with some initial matching the empty find an augmenting path and augment the current matching along this when no augmenting path can be found terminate the algorithm and return the last matching which is we now give a specific algorithm implementing this general we will search for an augmenting path for a matching m by a bfs like traversal of the graph that start simultaneously at all the free vertex in one of the set v and u say v it would be logical to select the smaller of the two vertex set but we will ignore this observation in the pseudocode recall that an augmenting path if it exists is an odd length path that connects a free vertex in v with a free vertex in u and which unless it consists of a single edge zig from a vertex in v to another vertex mate in u then zag back to v along the uniquely defined edge from m and so on until a free vertex in u is draw augmenting path for the matchings in figure for hence any candidate to be such a path must have it edge alternate in the pattern just this motivates the following rule for labeling vertex during the bfs like traversal of the case the queue front vertex w is in v if u is a free vertex adjacent to w it is used a the other endpoint of an augmenting path so the labeling stop and augmentation of the matching the augmenting path in question is obtained by moving backward along the vertex label see below to alternately add and delete it edge to and from the current if u is not free and connected to w by an edge not in m label u with w unless it ha been already case the front vertex w is in u in this case w must be matched and we label it mate in v with here is pseudocode of the algorithm in it algorithm maximumbipartitematchingg find a maximum matching in a bipartite graph by a bfs like traversal input a bipartite graph g v u e output a maximum cardinality matching m in the input graph initialize set m of edge with some valid matching the empty set initialize queue q with all the free vertex in v in any order while not emptyq do w frontq dequeueq if w v for every vertex u adjacent to w do if u is free augment m m w u vw while v is labeled do u vertex indicated by v label m m v u v vertex indicated by u label m m v u remove all vertex label reinitialize q with all free vertex in v break exit the for loop else u is matched if w u m and u is unlabeled label u with w enqueueq u else w u and matched label the mate v of w with w enqueueq v return m current matching is maximum an application of this algorithm to the matching in figure is shown in figure note that the algorithm find a maximum matching that differs from the one in figure v u queue queue augment from queue queue augment from queue queue augment from queue empty maximum matching figure application of the maximum cardinality matching the left column show a current matching and initialized queue at the next iteration start the right column show the vertex labeling generated by the algorithm before augmentation is matching edge are shown in vertex label indicate the vertex from which the labeling is the discovered endpoint of an augmenting path is shaded and labeled for vertex deleted from the queue are indicated by how efficient is the maximum matching each iteration except the last one match two previously free vertex one from each of the set v and therefore the total number of iteration cannot exceed n where n v u is the number of vertex in the the time spent on each iteration is in on m where m e is the number of edge in the this assumes that the information about the status of each vertex free or matched and the vertex mate if the latter can be retrieved in constant time by storing it in an hence the time efficiency of the algorithm is in onn hopcroft and karp hop showed how the efficiency can be improved to o nn m by combining several iteration into a single stage to maximize the number of edge added to the matching with one we were concerned in this section with matching the largest possible number of vertex pair in a bipartite some application may require taking into ac count the quality or cost of matching different for example worker may execute job with different efficiency or girl may have different preference for their potential dance it is natural to model such situation by bipartite graph with weight assigned to their this lead to the problem of maxi mizing the sum of the weight on edge connecting matched pair of this problem is called maximum weight we encountered it under a differ ent name the assignment problem in section there are several sophisti cated algorithm for this problem which are much more efficient than exhaustive search see pap gal we have to leave them outside of our discussion however because of their complexity especially for general exercise for each matching shown below in bold find an augmentation or explain why no augmentation apply the maximum matching algorithm to the following bipartite graph what is the largest and what is the smallest possible cardinality of a match ing in a bipartite graph g v u e with n vertex in each vertex set v and u and at least n what is the largest and what is the smallest number of distinct solution the maximum cardinality matching problem can have for a bipartite graph g v u e with n vertex in each vertex set v and u and at least n hall marriage theorem asserts that a bipartite graph g v u e ha a matching that match all vertex of the set v if and only if for each subset s v r s where r is the set of all vertex adjacent to a vertex in check this property for the following graph with i v and ii v you have to devise an algorithm that return yes if there is a matching in a bipartite graph g v u e that match all vertex in v and return no would you base your algorithm on checking the condition of hall marriage suppose there are five committee a b c d and e composed of six person a b c d e and f a follows committee a member are b and e committee b member are b d and e committee c member are a c d e and f committee d member are b d and e committee e member are b and is there a system of distinct representative is it possible to select a representative from each committee so that all the selected person are show how the maximum cardinality matching problem for a bipartite graph can be reduced to the maximum flow problem discussed in section consider the following greedy algorithm for finding a maximum matching in a bipartite graph g v u e sort all the vertex in nondecreasing order of their scan this sorted list to add to the current matching initially empty the edge from the list free vertex to an adjacent free vertex of the lowest if the list vertex is matched or if there are no adjacent free vertex for it the vertex is simply doe this algorithm always produce a maximum matching in a bipartite design a linear time algorithm for finding a maximum matching in a implement the maximum matching algorithm of this section in the language of your experiment with it performance on bipartite graph with n vertex in each of the vertex set and randomly generated edge in both dense and sparse mode to compare the observed running time with the algorithm theoretical domino puzzle a domino is a tile that can be oriented either hori zontally or a tiling of a given board composed of square is covering it with domino exactly and without is it possible to tile with domino an board without two unit square at it diagonally opposite the stable marriage problem in this section we consider an interesting version of bipartite matching called the stable marriage consider a set y m m mn of n men and a set x w w wn of n each man ha a preference list ordering the woman a potential marriage partner with no tie similarly each woman ha a preference list of the men also with no example of these two set of list are given in figure and the same information can also be presented by an n n ranking matrix see figure the row and column of the matrix represent the men and woman of the two set a cell in row m and column w contains two ranking the first is the position ranking of w in the m preference list the second is the position ranking of m in the w preference for example the pair in jims row and anns column in the matrix in figure indicates that ann is jims third choice while jim is anns which of these two way to represent such information is better depends on the task at for example it is easier to specify a match of the set element by using the ranking matrix whereas the preference list might be a more efficient data structure for implementing a matching a marriage matching m is a set of n m w pair whose member are selected from disjoint n element set y and x in a one one fashion each man m from y is paired with exactly one woman w from x and vice if we represent y and x a vertex of a complete bipartite graph with edge connecting possible marriage partner then a marriage matching is a perfect matching in such a men preference woman preference ranking matrix st nd rd st nd rd ann lea sue bob lea ann sue ann jim tom bob bob jim lea sue ann lea tom bob jim jim tom sue lea ann sue jim tom bob tom a b c figure data for an instance of the stable marriage a men preference list b woman preference c ranking matrix with the boxed cell composing an unstable a pair m w where m y w x is said to be a blocking pair for a marriage matching m if man m and woman w are not matched in m but they prefer each other to their mate in for example bob lea is a blocking pair for the marriage matching m bob ann jim lea tom sue figure because they are not matched in m while bob prefers lea to ann and lea prefers bob to a marriage matching m is called stable if there is no blocking pair for it otherwise m is called according to this definition the marriage matching in figure is unstable because bob and lea can drop their designated mate to join in a union they both the stable marriage problem is to find a stable marriage matching for men and woman given surprisingly this problem always ha a can you find it for the instance in figure it can be found by the following stable marriage algorithm input a set of n men and a set of n woman along with ranking of the woman by each man and ranking of the men by each woman with no tie allowed in the ranking output a stable marriage matching step start with all the men and woman being step while there are free men arbitrarily select one of them and do the following proposal the selected free man m proposes to w the next woman on his preference list who is the highest ranked woman who ha not rejected him response if w is free she accepts the proposal to be matched with if she is not free she compare m with her current if she prefers m to him she accepts m proposal making her former mate free otherwise she simply reject m proposal leaving m step return the set of n matched before we analyze this algorithm it is useful to trace it on some such an example is presented in figure let u discus property of the stable marriage theorem the stable marriage algorithm terminates after no more than n iteration with a stable marriage proof the algorithm start with n men having the total of n woman on their ranking on each iteration one man make a proposal to a this reduces the total number of woman to whom the men can still propose in the future because no man proposes to the same woman more than hence the algorithm must stop after no more than n ann lea sue free men bob bob proposed to lea bob jim tom jim lea accepted tom ann lea sue free men bob jim proposed to lea jim tom jim lea rejected tom ann lea sue free men bob jim proposed to sue jim tom jim sue accepted tom ann lea sue free men bob tom proposed to sue tom jim sue rejected tom ann lea sue free men bob tom proposed to lea tom jim lea replaced bob with tom tom ann lea sue free men bob bob proposed to ann bob jim ann accepted tom figure application of the stable marriage an accepted proposal is indicated by a boxed cell a rejected proposal is shown by an underlined let u now prove that the final matching m is a stable marriage since the algorithm stop after all the n men are one one matched to the n woman the only thing that need to be proved is the stability of suppose on the contrary that m is then there exists a blocking pair of a man m and a woman w who are unmatched in m and such that both m and w prefer each other to the person they are matched with in since m proposes to every woman on his ranking list in decreasing order of preference and w precedes m match in m m must have proposed to w on some whether w refused m proposal or accepted it but replaced him on a subsequent iteration with a higher ranked match w mate in m must be higher on w preference list than m because the ranking of the men matched to a given woman may only improve on each iteration of the this contradicts the assumption that w prefers m to her final match in the stable marriage algorithm ha a notable it is not gender in the form presented above it favor men preference over woman we can easily see this by tracing the algorithm on the following instance of the problem woman woman man man the algorithm obviously yield the stable matching m man woman man woman in this matching both men are matched to their first choice which is not the case for the one can prove that the algorithm always yield a stable matching that is man optimal it assigns to each man the highest ranked woman possible under any stable of course this gender bias can be reversed but not eliminated by reversing the role played by men and woman in the algorithm by making woman propose and men accept or reject their there is another important corollary to the fact that the stable marriage algorithm always yield a gender optimal stable it is easy to prove that a man woman optimal matching is unique for a given set of participant therefore the algorithm output doe not depend on the order in which the free men woman make their consequently we can use any data structure we might prefer a queue or a stack for representing this set with no impact on the algorithm the notion of the stable matching a well a the algorithm discussed above wa introduced by gale and shapley in the paper titled college admission and the stability of marriage i do not know which of the two application mentioned in the title you would consider more the point is that stability is a matching property that can be desirable in a variety of for example it ha been used for many year in the united state for matching medical school graduate with hospital for residency for a brief history of this application and an in depth discussion of the stable marriage problem and it extension see the monograph by gusfield and irwing exercise consider an instance of the stable marriage problem given by the following ranking matrix a b c for each of it marriage matchings indicate whether it is stable or for the unstable matchings specify a blocking for the stable matchings indicate whether they are man optimal woman optimal or assume that the greek and roman letter denote the men and woman design a simple algorithm for checking whether a given marriage matching is stable and determine it time efficiency find a stable marriage matching for the instance given in problem by apply ing the stable marriage algorithm in it men proposing in it woman proposing find a stable marriage matching for the instance defined by the following ranking matrix a b c d determine the time efficiency class of the stable marriage algorithm in the worst in the best prove that a man optimal stable marriage set is always is it also true for a woman optimal stable marriage prove that in the man optimal stable matching each woman ha the worst partner that she can have in any stable marriage implement the stable marriage algorithm given in section so that it running time is in run an experiment to ascertain it average case write a report on the college admission problem resident hospital assign ment that generalizes the stable marriage problem in that a college can accept proposal from more than one consider the problem of the roommate which is related to but more difficult than the stable marriage problem an even number of boy wish to divide up into pair of a set of pairing is called stable if under it there are no two boy who are not roommate and who prefer each other to their actual gal give an instance of this problem that doe not have a stable summary the iterative improvement technique involves finding a solution to an op timization problem by generating a sequence of feasible solution with improving value of the problem objective each subsequent so lution in such a sequence typically involves a small localized change in the previous feasible when no such change improves the value of the objective function the algorithm return the last feasible solution a optimal and important problem that can be solved exactly by iterative improvement algorithm include linear programming maximizing the flow in a network and matching the maximum possible number of vertex in a the simplex method is the classic method for solving the general linear programming it work by generating a sequence of adjacent extreme point of the problem feasible region with improving value of the objective the maximum flow problem asks to find the maximum flow possible in a network a weighted directed graph with a source and a the ford fulkerson method is a classic template for solving the maximumflow problem by the iterative improvement the shortestaugmenting path method implement this idea by labeling network vertex in the breadth first search the ford fulkerson method also find a minimum cut in a given a maximum cardinality matching is the largest subset of edge in a graph such that no two edge share the same for a bipartite graph it can be found by a sequence of augmentation of previously obtained the stable marriage problem is to find a stable matching for element of two nelement set based on given matching this problem always ha a solution that can be found by the gale shapley limitation of algorithm power intellect distinguishes between the possible and the impossible reason distinguishes between the sensible and the even the possible can be max born my life and my view in the preceding chapter of this book we encountered dozen of algorithm for solving a variety of different a fair assessment of algorithm a problem solving tool is inescapable they are very powerful instrument especially when they are executed by modern but the power of algorithm is not unlimited and it limit are the subject of this a we shall see some problem cannot be solved by any other problem can be solved algorithmically but not in polynomial and even when a problem can be solved in polynomial time by some algorithm there are usually lower bound on their we start in section with method for obtaining lower bound which are estimate on a minimum amount of work needed to solve a in general obtaining a nontrivial lower bound even for a simple sounding problem is a very difficult a opposed to ascertaining the efficiency of a particular algorithm the task here is to establish a limit on the efficiency of any algorithm known or this also necessitates a careful description of the operation such algorithm are allowed to if we fail to define carefully the rule of the game so to speak our claim may end up in the large dustbin of impossibility related statement a for example the one made by the celebrated british physicist lord kelvin in heavier than air flying machine are section discus decision this technique allows u among other application to establish lower bound on the efficiency of comparison based algorithm for sorting and for searching in sorted a a result we will be able to answer such question a whether it is possible to invent a faster sorting algorithm than mergesort and whether binary search is the fastest algorithm for searching in a sorted what doe your intuition tell you the answer to these question will turn out to incidentally decision tree are also a great vehicle for directing u to a solution of some puzzle such a the coin weighing problem discussed in section section deal with the question of intractability which problem can and cannot be solved in polynomial this well developed area of theoretical computer science is called computational complexity we present the basic element of this theory and discus informally such fundamental notion a p np and np complete problem including the most important unresolved question of theoretical computer science about the relationship between p and np the last section of this chapter deal with numerical this branch of computer science concern algorithm for solving problem of continuous mathematics solving equation and system of equation evaluating such func tions a sin x and ln x computing integral and so the nature of such problem imposes two type of first most cannot be solved second solving them even approximately requires dealing with number that can be rep resented in a digital computer with only a limited level of manipulating approximate number without proper care can lead to very inaccurate we will see that even solving a basic quadratic equation on a computer pose sig nificant difficulty that require a modification of the canonical formula for the equation lower bound argument we can look at the efficiency of an algorithm two we can establish it asymp totic efficiency class say for the worst case and see where this class stand with respect to the hierarchy of efficiency class outlined in section for exam ple selection sort whose efficiency is quadratic is a reasonably fast algorithm whereas the algorithm for the tower of hanoi problem is very slow because it ef ficiency is we can argue however that this comparison is akin to the proverbial comparison of apple to orange because these two algorithm solve different the alternative and possibly fairer approach is to ask how efficient a particular algorithm is with respect to other algorithm for the same seen in this light selection sort ha to be considered slow because there are on log n sorting algorithm the tower of hanoi algorithm on the other hand turn out to be the fastest possible for the problem it when we want to ascertain the efficiency of an algorithm with respect to other algorithm for the same problem it is desirable to know the best possible efficiency any algorithm solving the problem may knowing such a lower bound can tell u how much improvement we can hope to achieve in our quest for a better algorithm for the problem in if such a bound is tight we already know an algorithm in the same efficiency class a the lower bound we can hope for a constant factor improvement at if there is a gap between the efficiency of the fastest algorithm and the best lower bound known the door for possible improvement remains open either a faster algorithm matching the lower bound could exist or a better lower bound could be in this section we present several method for establishing lower bound and illustrate them with specific a we did in analyzing the efficiency of specific algorithm in the preceding chapter we should distinguish between a lower bound class and a minimum number of time a particular operation need to be a a rule the second problem is more difficult than the for example we can immediately conclude that any algorithm for finding the median of n number must be in n but it is not simple at all to prove that any comparison based algorithm for this problem must do at least n comparison in the worst case for odd trivial lower bound the simplest method of obtaining a lower bound class is based on counting the number of item in the problem input that must be processed and the number of output item that need to be since any algorithm must at least read all the item it need to process and write all it output such a count yield a trivial lower for example any algorithm for generating all permutation of n distinct item must be in because the size of the output is and this bound is tight because good algorithm for generating permutation spend a constant time on each of them except the initial one see section a another example consider the problem of evaluating a polynomial of degree n px anxn an xn a at a given point x given it coefficient an an it is easy to see that all the coefficient have to be processed by any polynomial evaluation indeed if it were not the case we could change the value of an unprocessed coefficient which would change the value of the polynomial at a nonzero point this mean that any such algorithm must be in this lower bound is tight because both the right to left evaluation algorithm problem in exercise and horners rule section are both in a similar vein a trivial lower bound for computing the product of two n n matrix is n because any such algorithm ha to process n element in the input matrix and generate n element of the it is still unknown however whether this bound is trivial lower bound are often too low to be for example the trivial bound for the traveling salesman problem is n because it input is nn intercity distance and it output is a list of n city making up an optimal but this bound is all but useless because there is no known algorithm with the running time being a polynomial function of any there is another obstacle to deriving a meaningful lower bound by this it lie in determining which part of an input must be processed by any algorithm solving the problem in for example searching for an element of a given value in a sorted array doe not require processing all it element a another example consider the problem of determining connectivity of an undirected graph defined by it adjacency it is plausible to expect that any such algorithm would have to check the existence of each of the nn potential edge but the proof of this fact is not information theoretic argument while the approach outlined above take into account the size of a problem output the information theoretical approach seek to establish a lower bound based on the amount of information it ha to consider a an example the well known game of deducing a positive integer between and n selected by somebody by asking that person question with yesno the amount of uncertainty that any algorithm solving this problem ha to resolve can be measured by log n the number of bit needed to specify a particular number among the n we can think of each question or to be more accurate an answer to each question a yielding at most bit of information about the algorithm output the selected consequently any such algorithm will need at least log n such step before it can determine it output in the worst the approach we just exploited is called the information theoretic argument because of it connection to information it ha proved to be quite useful for finding the so called information theoretic lower bound for many problem involving comparison including sorting and it underlying idea can be realized much more precisely through the mechanism of decision because of the importance of this technique we discus it separately and in more detail in section adversary argument let u revisit the same game of guessing a number used to introduce the idea of an information theoretic we can prove that any algorithm that solves this problem must ask at least log n question in it worst case by playing the role of a hostile adversary who want to make an algorithm ask a many question a the adversary start by considering each of the number between and n a being potentially this is cheating of course a far a the game is concerned but not a a way to prove our after each question the adversary give an answer that leaf him with the largest set of number consistent with this and all the previously given this strategy leaf him with at least one half of the number he had before his last if an algorithm stop before the size of the set is reduced to the adversary can exhibit a number that could be a legitimate input the algorithm failed to it is a simple technical matter now to show that one need log n iteration to shrink an n element set to a one element set by halving and rounding up the size of the remaining hence at least log n question need to be asked by any algorithm in the worst this example illustrates the adversary method for establishing lower it is based on following the logic of a malevolent but honest adversary the malevolence make him push the algorithm down the most time consuming path and his honesty force him to stay consistent with the choice already a lower bound is then obtained by measuring the amount of work needed to shrink a set of potential input to a single input along the most time consuming a another example consider the problem of merging two sorted list of size n a a an and b b bn into a single sorted list of size for simplicity we assume that all the a and b are distinct which give the problem a unique we encountered this problem when discussing mergesort in section recall that we did merging by repeatedly comparing the first element in the remaining list and outputting the smaller among the number of key comparison in the worst case for this algorithm for merging is n is there an algorithm that can do merging the answer turn out to be knuth knuiii quote the following adversary method for proving that n is a lower bound on the number of key comparison made by any comparison based algorithm for this the adversary will employ the following rule reply true to the comparison ai bj if and only if i this will force any correct merging algorithm to produce the only combined list consistent with this rule b a b a bn to produce this combined list any correct algorithm will have to explicitly compare n adjacent pair of it element b to a a to b and so if one of these comparison ha not been made a ha not been compared to b we can transpose these key to get b b a a bn an which is consistent with all the comparison made but cannot be distinguished from the correct configuration given hence n is indeed a lower bound for the number of key comparison needed for any merging problem reduction we have already encountered the problem reduction approach in section there we discussed getting an algorithm for problem p by reducing it to another problem q solvable with a known a similar reduction idea can be used for finding a lower to show that problem p is at least a hard a another problem q with a known lower bound we need to reduce q to p not p to in other word we should show that an arbitrary instance of problem q can be transformed in a reasonably efficient fashion to an instance of problem p so any algorithm solving p would solve q a then a lower bound for q will be a lower bound for p table list several important problem that are often used for this table problem often used for establishing lower bound by problem reduction problem lower bound tightness sorting n log n yes searching in a sorted array log n yes element uniqueness problem n log n yes multiplication of n digit integer n unknown multiplication of n n matrix n unknown we will establish the lower bound for sorting and searching in the next sec the element uniqueness problem asks whether there are duplicate among n given we encountered this problem in section and the proof of the lower bound for this seemingly simple problem is based on a very sophisti cated mathematical analysis that is well beyond the scope of this book see pre for a rather elementary a to the last two algebraic prob lem in table the lower bound quoted are trivial but whether they can be improved remains a an example of establishing a lower bound by reduction let u consider the euclidean minimum spanning tree problem given n point in the cartesian plane construct a tree of minimum total length whose vertex are the given a a problem with a known lower bound we use the element uniqueness we can transform any set x x xn of n real number into a set of n point in the cartesian plane by simply adding a the point y coordinate x x xn let t be a minimum spanning tree found for this set of since t must contain a shortest edge checking whether t contains a zero length edge will answer the question about uniqueness of the given this reduction implies that n log n is a lower bound for the euclidean minimum spanning tree problem since the final result about the complexity of many problem are not known the reduction technique is often used to compare the relative complexity of prob for example the formula x y x y x y and x x x show that the problem of computing the product of two n digit integer and squaring an n digit integer belong to the same complexity class despite the latter being seemingly simpler than the there are several similar result for matrix for example multi plying two symmetric matrix turn out to be in the same complexity class a multiplying two arbitrary square this result is based on the observation that not only is the former problem a special case of the latter one but also that we can reduce the problem of multiplying two arbitrary square matrix of order n say a and b to the problem of multiplying two symmetric matrix x a and y bt at b where at and bt are the transpose matrix of a and b at i j aj i and bt i j bj i respectively and stand for the n n matrix whose element are all indeed xy a bt ab at b at bt from which the needed product ab can be easily true we will have to multiply matrix twice the original size but this is just a minor technical complication with no impact on the complexity though such result are interesting we will encounter even more important application of the reduction approach to comparing problem complexity in section exercise prove that any algorithm solving the alternating disk puzzle problem in exercise must make at least nn move to solve is this lower bound prove that the classic recursive algorithm for the tower of hanoi puzzle section make the minimum number of disk move needed to solve the find a trivial lower bound class for each of the following problem and indi cate if you can whether this bound is finding the largest element in an array checking completeness of a graph represented by it adjacency matrix generating all the subset of an n element set determining whether n given real number are all distinct consider the problem of identifying a lighter fake coin among n identical looking coin with the help of a balance can we use the same information theoretic argument a the one in the text for the number of ques tions in the guessing game to conclude that any algorithm for identifying the fake will need at least log n weighing in the worst prove that any comparison based algorithm for finding the largest element of an n element set of real number must make n comparison in the worst find a tight lower bound for sorting an array by exchanging it adjacent give an adversary argument proof that the time efficiency of any algorithm that check connectivity of a graph with n vertex is in n provided the only operation allowed for an algorithm is to inquire about the presence of an edge between two vertex of the is this lower bound what is the minimum number of comparison needed for a comparison based sorting algorithm to merge any two sorted list of size n and n element prove the validity of your find the product of matrix a and b through a transformation to a product of two symmetric matrix if a and b can one use this section formula that indicate the complexity equiva lence of multiplication and squaring of integer to show the complexity equivalence of multiplication and squaring of square show that multiplication of two matrix of order n can be reduced to squaring a matrix of order find a tight lower bound class for the problem of finding two closest number among n real number x x find a tight lower bound class for the number placement problem problem in exercise decision tree many important algorithm especially those for sorting and searching work by comparing item of their we can study the performance of such algorithm with a device called a decision a an example figure present a decision tree of an algorithm for finding a minimum of three each internal node of a binary decision tree represents a key comparison indicated in the node k k the node left subtree contains the information about subsequent comparison made if k k and it right subtree doe the same for the case of k k for the sake of simplicity we assume throughout this section that all input item are each leaf represents a possible outcome of the algorithm run on some input of size note that the number of leaf can be greater than the number of outcome because for some algorithm the same outcome can be arrived at through a different chain of this happens to be the case for the decision tree in figure an important point is that the number of leaf must be at least a large a the number of possible the algorithm work on a particular input of size n can be traced by a path from the root to a leaf in it decision tree and the number of comparison made by the algorithm on such yes a b no yes a c no yes b c no a c b c figure decision tree for finding a minimum of three a run is equal to the length of this hence the number of comparison in the worst case is equal to the height of the algorithm decision the central idea behind this model lie in the observation that a tree with a given number of leaf which is dictated by the number of possible outcome ha to be tall enough to have that many specifically it is not difficult to prove that for any binary tree with l leaf and height h h log l indeed a binary tree of height h with the largest number of leaf ha all it leaf on the last level hence the largest number of leaf in such a tree is in other word h l which immediately implies inequality put a lower bound on the height of binary decision tree and hence the worst case number of comparison made by any comparison based algorithm for the problem in such a bound is called the informationtheoretic lower bound see section we illustrate this technique below on two important problem sorting and searching in a sorted decision tree for sorting most sorting algorithm are comparison based they work by comparing element in a list to be by studying property of decision tree for such algorithm we can derive important lower bound on their time we can interpret an outcome of a sorting algorithm a finding a permutation of the element index of an input list that put the list element in ascending consider a an example a three element list a b c of orderable item such a real number or for the outcome a c b obtained by sorting this list see figure the permutation in question is in general the number of possible outcome for sorting an arbitrary n element list is equal to abc yes a b no abc abc yes a c no yes b c no abc cba bac cba yes b c no b a no yes a c no yes b a a bc a cb c ab b ac b ca c ba figure decision tree for the tree element selection a triple above a node indicates the state of the array being note two redundant comparison b a with a single possible outcome because of the result of some previously made inequality implies that the height of a binary decision tree for any comparison based sorting algorithm and hence the worst case number of com parisons made by such an algorithm cannot be le than log cworst n log using stirlings formula for we get log log nnen log log log n log n log n n n e in other word about n log n comparison are necessary in the worst case to sort an arbitrary n element list by any comparison based sorting note that mergesort make about this number of comparison in it worst case and hence is asymptotically this also implies that the asymptotic lower bound n log n is tight and therefore cannot be substantially we should point out however that the lower bound of log can be improved for some value of for example log but it ha been proved that comparison are necessary and sufficient to sort an array of element in the worst we can also use decision tree for analyzing the average case efficiency of comparison based sorting we can compute the average number of comparison for a particular algorithm a the average depth of it decision tree leaf a the average path length from the root to the for example for abc yes a b no abc bac yes b c no yes a c no a bc acb b a c bca yes a c no yes b c no a cb c ab b ca c ba figure decision tree for the three element insertion the three element insertion sort whose decision tree is given in figure this number is under the standard assumption that all outcome of sorting are equally likely the following lower bound on the average number of comparison cavg made by any comparison based algorithm in sorting an n element list ha been proved cavgn log a we saw earlier this lower bound is about n log you might be surprised that the lower bound for the average and worst case are almost remember however that these bound are obtained by maximizing the number of compar isons made in the average and worst case for a particular sorting algorithm the average case efficiency can of course be significantly better than their worst case decision tree for searching a sorted array in this section we shall see how decision tree can be used for establishing lower bound on the number of key comparison in searching a sorted array of n key a a an the principal algorithm for this problem is binary a we saw in section the number of comparison made by binary search in the worst case cwbsorstn is given by the formula cwbsorst n log n logn a a a a a a a a a a a a a a a a figure ternary decision tree for binary search in a four element we will use decision tree to determine whether this is the smallest possible number of since we are dealing here with three way comparison in which search key k is compared with some element ai to see whether k ai k ai or k ai it is natural to try using ternary decision figure present such a tree for the case of n the internal node of that tree indicate the array element being compared with the search the leaf indicate either a matching element in the case of a successful search or a found interval that the search key belongs to in the case of an unsuccessful we can represent any algorithm for searching a sorted array by three way comparison with a ternary decision tree similar to that in figure for an array of n element all such decision tree will have n leaf n for successful search and n for unsuccessful since the minimum height h of a ternary tree with l leaf is log l we get the following lower bound on the number of worst case comparison cworst n logn this lower bound is smaller than logn the number of worst case comparison for binary search at least for large value of n and smaller than or equal to logn for every positive integer n see problem in this section can we prove a better lower bound or is binary search far from being the answer turn out to be the to obtain a better lower bound we should consider binary rather than ternary decision tree such a the one in figure internal node in such a tree correspond to the same three way comparison a before but they also serve a terminal node for successful leaf therefore represent only unsuccessful search and there are n of them for searching an n element a a a a a a a a a a a a figure binary decision tree for binary search in a four element a comparison of the decision tree in figure and illustrates the binary decision tree is simply the ternary decision tree with all the middle subtrees applying inequality to such binary decision tree immediately yield cworst n logn this inequality close the gap between the lower bound and the number of worstcase comparison made by binary search which is also logn a much more sophisticated analysis see knuiii section show that under the standard assumption about search binary search make the smallest number of comparison on the average a the average number of comparison made by this algorithm turn out to be about log n and logn for successful and unsuccessful search exercise prove by mathematical induction that h log l for any binary tree with height h and the number of leaf h log l for any ternary tree with height h and the number of leaf consider the problem of finding the median of a three element set a b c of orderable what is the information theoretic lower bound for comparison based al gorithms solving this draw a decision tree for an algorithm solving this if the worst case number of comparison in your algorithm is greater than the information theoretic lower bound do you think an algorithm matching the lower bound either find such an algorithm or prove it draw a decision tree and find the number of key comparison in the worst and average case for the three element basic bubble the three element enhanced bubble sort which stop if no swap have been made on it last design a comparison based algorithm for sorting a four element array with the smallest number of element comparison design a comparison based algorithm for sorting a five element array with seven comparison in the worst draw a binary decision tree for searching a four element sorted list by sequen tial compare the two lower bound for searching a sorted array logn and logn to show that logn logn for every positive integer logn logn for every positive integer n what is the information theoretic lower bound for finding the maximum of n number by comparison based is this bound a tournament tree is a complete binary tree reflecting result of a knockout tournament it leaf represent n player entering the tournament and each internal node represents a winner of a match played by the player represented by the node hence the winner of the tournament is represented by the root of the what is the total number of game played in such a how many round are there in such a design an efficient algorithm to determine the second best player using the information produced by the how many extra game doe your algorithm advanced fake coin problem there are n coin identical in appearance either all are genuine or exactly one of them is it is unknown whether the fake coin is lighter or heavier than the genuine you have a balance scale with which you can compare any two set of that is by tipping to the left to the right or staying even the balance scale will tell whether the set weigh the same or which of the set is heavier than the other but not by how the problem is to find whether all the coin are genuine and if not to find the fake coin and establish whether it is lighter or heavier than the genuine prove that any algorithm for this problem must make at least logn weighing in the worst draw a decision tree for an algorithm that solves the problem for n coin in two prove that there exists no algorithm that solves the problem for n coin in two draw a decision tree for an algorithm that solves the problem for n coin in two weighing by using an extra coin known to be draw a decision tree for an algorithm that solves the classic version of the problem that for n coin in three weighing with no extra coin being jigsaw puzzle a jigsaw puzzle contains n a section of the puzzle is a set of one or more piece that have been connected to each a move consists of connecting two what algorithm will minimize the number of move required to complete the p np and np complete problem in the study of the computational complexity of problem the first concern of both computer scientist and computing professional is whether a given problem can be solved in polynomial time by some definition we say that an algorithm solves a problem in polynomial time if it worst case time efficiency belongs to opn where pn is a polynomial of the problem input size note that since we are using big oh notation here problem solvable in say logarithmic time are solvable in polynomial time a problem that can be solved in polynomial time are called tractable and problem that cannot be solved in polynomial time are called there are several reason for drawing the intractability line in this first the entry of table and their discussion in section imply that we cannot solve arbitrary instance of intractable problem in a reasonable amount of time unless such instance are very second although there might be a huge difference between the running time in opn for polynomial of drastically different degree there are very few useful polynomial time algorithm with the degree of a polynomial higher than in addition polynomial that bound running time of algorithm do not usually have extremely large third polynomial function posse many convenient property in particular both the sum and composition of two polynomial are always polynomial fourth the choice of this class ha led to a development of an extensive theory called computational complexity which seek to classify problem according to their inherent and according to this theory a problem intractability remains the same for all principal model of computation and all reasonable input encoding scheme for the problem under we just touch on some basic notion and idea of complexity theory in this if you are interested in a more formal treatment of this theory you will have no trouble finding a wealth of textbook devoted to the subject sip p and np problem most problem discussed in this book can be solved in polynomial time by some they include computing the product and the greatest common divisor of two integer sorting a list searching for a key in a list or for a pattern in a text string checking connectivity and acyclicity of a graph and finding a minimum spanning tree and shortest path in a weighted you are invited to add more example to this informally we can think about problem that can be solved in polynomial time a the set that computer science theoretician call p a more formal definition includes in p only decision problem which are problem with yesno definition class p is a class of decision problem that can be solved in polynomial time by deterministic this class of problem is called the restriction of p to decision problem can be justified by the following first it is sensible to exclude problem not solvable in polynomial time because of their exponentially large such problem do arise naturally generating subset of a given set or all the permutation of n distinct item but it is apparent from the outset that they cannot be solved in polynomial second many important problem that are not decision problem in their most natural formulation can be reduced to a series of decision problem that are easier to for example instead of asking about the minimum number of color needed to color the vertex of a graph so that no two adjacent vertex are colored the same color we can ask whether there exists such a coloring of the graph vertex with no more than m color for m the latter is called the m coloring the first value of m in this series for which the decision problem of m coloring ha a solution solves the optimization version of the graph coloring problem a it is natural to wonder whether every decision problem can be solved in polynomial the answer to this question turn out to be in fact some decision problem cannot be solved at all by any such problem are called undecidable a opposed to decidable problem that can be solved by an a famous example of an undecidable problem wa given by alan turing in the problem in question is called the halting problem given a computer program and an input to it determine whether the program will halt on that input or continue working indefinitely on here is a surprisingly short proof of this remarkable by way of contra diction assume that a is an algorithm that solves the halting that is for any program p and input i ap i if program p halt on input i if program p doe not halt on input i we can consider program p a an input to itself and use the output of algorithm a for pair p p to construct a program q a follows qp halt if ap p if program p doe not halt on input p doe not halt if ap p if program p halt on input p then on substituting q for p we obtain qq halt if aq q if program q doe not halt on input q doe not halt if aq q if program q halt on input this is a contradiction because neither of the two outcome for program q is possible which completes the are there decidable but intractable yes there are but the number of known example is surprisingly small especially of those that arise naturally rather than being constructed for the sake of a theoretical there are many important problem however for which no polynomial time algorithm ha been found nor ha the impossibility of such an algorithm been the classic monograph by garey and johnson gar contains a list of several hundred such problem from different area of computer science mathematics and operation here is just a small sample of some of the best known problem that fall into this category hamiltonian circuit problem determine whether a given graph ha a hamiltonian circuit a path that start and end at the same vertex and pass through all the other vertex exactly traveling salesman problem find the shortest tour through n city with known positive integer distance between them find the shortest hamiltonian circuit in a complete graph with positive integer this wa just one of many breakthrough contribution to theoretical computer science made by the english mathematician and computer science pioneer alan turing in recognition of this the acm the principal society of computing professional and researcher ha named after him an award given for outstanding contribution to theoretical computer a lecture given on such an occasion by richard karp kar provides an interesting historical account of the development of complexity knapsack problem find the most valuable subset of n item of given positive integer weight and value that fit into a knapsack of a given positive integer partition problem given n positive integer determine whether it is possi ble to partition them into two disjoint subset with the same bin packing problem given n item whose size are positive rational num bers not larger than put them into the smallest number of bin of size graph coloring problem for a given graph find it chromatic number which is the smallest number of color that need to be assigned to the graph vertex so that no two adjacent vertex are assigned the same integer linear programming problem find the maximum or minimum value of a linear function of several integer valued variable subject to a finite set of constraint in the form of linear equality and some of these problem are decision those that are not have decision version counterpart the m coloring problem for the graph coloring what all these problem have in common is an exponential or worse growth of choice a a function of input size from which a solution need to be note however that some problem that also fall under this umbrella can be solved in polynomial for example the eulerian circuit problem the problem of the existence of a cycle that traverse all the edge of a given graph exactly once can be solved in on time by checking in addition to the graph connectivity whether all the graph vertex have even this example is particularly striking it is quite counterintuitive to expect that the problem about cycle traversing all the edge exactly once eulerian circuit can be so much easier than the seemingly similar problem about cycle visiting all the vertex exactly once hamiltonian another common feature of a vast majority of decision problem is the fact that although solving such problem can be computationally difficult checking whether a proposed solution actually solves the problem is computationally easy it can be done in polynomial we can think of such a proposed solution a being randomly generated by somebody leaving u with the task of verifying it for example it is easy to check whether a proposed list of vertex is a hamiltonian circuit for a given graph with n all we need to check is that the list contains n vertex of the graph in question that the first n vertex are distinct whereas the last one is the same a the first and that every consecutive pair of the list vertex is connected by an this general observation about decision problem ha led computer scientist to the notion of a nondeterministic definition a nondeterministic algorithm is a two stage procedure that take a it input an instance i of a decision problem and doe the nondeterministic guessing stage an arbitrary string s is generated that can be thought of a a candidate solution to the given instance i but may be complete gibberish a deterministic verification stage a deterministic algorithm take both i and s a it input and output yes if s represents a solution to instance if s is not a solution to instance i the algorithm either return no or is allowed not to halt at we say that a nondeterministic algorithm solves a decision problem if and only if for every yes instance of the problem it return yes on some in other word we require a nondeterministic algorithm to be capable of guessing a solution at least once and to be able to verify it and of course we do not want it to ever output a yes answer on an instance for which the answer should be finally a nondeterministic algorithm is said to be nondeterministic polynomial if the time efficiency of it verification stage is now we can define the class of np definition class np is the class of decision problem that can be solved by nondeterministic polynomial this class of problem is called nondeterministic most decision problem are in first of all this class includes all the problem in p p this is true because if a problem is in p we can use the deterministic polynomialtime algorithm that solves it in the verification stage of a nondeterministic algorithm that simply ignores string s generated in it nondeterministic guessing but np also contains the hamiltonian circuit problem the partition problem decision version of the traveling salesman the knapsack graph coloring and many hundred of other difficult combinatorial optimization problem cataloged in the halting problem on the other hand is among the rare example of decision problem that are known not to be in this lead to the most important open question of theoretical computer science is p a proper subset of np or are these two class in fact the we can put this symbolically a p note that p np would imply that each of many hundred of difficult combinatorial decision problem can be solved by a polynomial time algorithm although computer scientist have failed to find such algorithm despite their persistent effort over many moreover many well known decision problem are known to be np complete see below which seems to cast more doubt on the possibility that p np complete problem informally an np complete problem is a problem in np that is a difficult a any other problem in this class because by definition any other problem in np can be reduced to it in polynomial time shown symbolically in figure here are more formal definition of these definition a decision problem d is said to be polynomially reducible to a decision problem d if there exists a function t that transforms instance of d to instance of d such that t map all yes instance of d to yes instance of d and all no instance of d to no instance of d t is computable by a polynomial time algorithm this definition immediately implies that if a problem d is polynomially reducible to some problem d that can be solved in polynomial time then problem d can also be solved in polynomial time definition a decision problem d is said to be np complete if it belongs to class np every problem in np is polynomially reducible to d the fact that closely related decision problem are polynomially reducible to each other is not very for example let u prove that the hamiltonian circuit problem is polynomially reducible to the decision version of the traveling np problem np complete problem figure notion of an np complete polynomial time reduction of np problem to an np complete problem are shown by salesman the latter can be stated a the existence problem of a hamil tonian circuit not longer than a given positive integer m in a given complete graph with positive integer we can map a graph g of a given instance of the hamiltonian circuit problem to a complete weighted graph g representing an in stance of the traveling salesman problem by assigning a the weight to each edge in g and adding an edge of weight between any pair of nonadjacent vertex in a the upper bound m on the hamiltonian circuit length we take m n where n is the number of vertex in g and g obviously this transformation can be done in polynomial let g be a yes instance of the hamiltonian circuit then g ha a hamiltonian circuit and it image in g will have length n making the image a yes instance of the decision traveling salesman conversely if we have a hamiltonian circuit of the length not larger than n in g then it length must be exactly n and hence the circuit must be made up of edge present in g making the inverse image of the yes instance of the decision traveling salesman problem be a yes instance of the hamiltonian circuit this completes the the notion of np completeness requires however polynomial reducibility of all problem in np both known and unknown to the problem in given the bewildering variety of decision problem it is nothing short of amazing that specific example of np complete problem have been actually neverthe le this mathematical feat wa accomplished independently by stephen cook in the united state and leonid levin in the former soviet in his paper cook coo showed that the so called cnf satisfiability problem is np the cnf satisfiability problem deal with boolean each boolean expression can be represented in conjunctive normal form such a the following expression involving three boolean variable x x and x and their negation denoted x x and x respectively x x xx xx x the cnf satisfiability problem asks whether or not one can assign value true and false to variable of a given boolean expression in it cnf form to make the entire expression it is easy to see that this can be done for the above formula if x true x true and x false the entire expression is since the cook levin discovery of the first known np complete problem computer scientist have found many hundred if not thousand of other exam in particular the well known problem or their decision version men tioned above hamiltonian circuit traveling salesman partition bin packing and graph coloring are all np it is known however that if p np there must exist np problem that neither are in p nor are np a it often happens in the history of science breakthrough discovery are made independently and almost simultaneously by several in fact levin introduced a more general notion than np completeness which wa not limited to decision problem but his paper lev wa published two year after for a while the leading candidate to be such an example wa the problem of determining whether a given integer is prime or but in an im portant theoretical breakthrough professor manindra agrawal and his student neeraj kayal and nitin saxena of the indian institute of technology in kanpur announced in a discovery of a deterministic polynomial time algorithm for primality testing their algorithm doe not solve however the related problem of factoring large composite integer which lie at the heart of the widely used encryption method called the rsa algorithm showing that a decision problem is np complete can be done in two first one need to show that the problem in question is in np a randomly generated string can be checked in polynomial time to determine whether or not it represents a solution to the typically this step is the second step is to show that every problem in np is reducible to the problem in question in polynomial because of the transitivity of polynomial reduction this step can be done by showing that a known np complete problem can be transformed to the problem in question in polynomial time see figure although such a transformation may need to be quite ingenious it is incomparably simpler than proving the existence of a transformation for every problem in for example if we already know that the hamiltonian circuit problem is np complete it polynomial reducibility to the decision traveling salesman problem implies that the latter is also np complete after an easy check that the decision traveling salesman problem is in class the definition of np completeness immediately implies that if there exists a deterministic polynomial time algorithm for just one np complete problem then every problem in np can be solved in polynomial time by a deterministic algo rithm and hence p in other word finding a polynomial time algorithm np problem known np complete problem candidate for np completeness figure proving np completeness by for one np complete problem would mean that there is no qualitative difference between the complexity of checking a proposed solution and finding it in polyno mial time for the vast majority of decision problem of all such implication make most computer scientist believe that p np although nobody ha been successful so far in finding a mathematical proof of this intriguing sur prisingly in interview with the author of a book about the life and discovery of prominent computer scientist sha cook seemed to be uncertain about the eventual resolution of this dilemma whereas levin contended that we should expect the p np whatever the eventual answer to the p np question prof to be knowing that a problem is np complete ha important practical implication for it mean that faced with a problem known to be np complete we should probably not aim at gaining fame and fortune by designing a polynomial time algorithm for solving all it rather we should concentrate on several approach that seek to alleviate the intractability of such these approach are outlined in the next chapter of the exercise a game of chess can be posed a the following decision problem given a legal positioning of chess piece and information about which side is to move determine whether that side can is this decision problem a certain problem can be solved by an algorithm whose running time is in onlog which of the following assertion is the problem is the problem is impossible to give example of the following graph or explain why such example cannot graph with a hamiltonian circuit but without an eulerian circuit graph with an eulerian circuit but without a hamiltonian circuit graph with both a hamiltonian circuit and an eulerian circuit graph with a cycle that includes all the vertex but with neither a hamil tonian circuit nor an eulerian circuit in the clay mathematics institute cmi of cambridge massachusetts designated a million prize for the solution to this for each of the following graph find it chromatic a e a a f e b d b f b g d c e c g c h d h design a polynomial time algorithm for the graph coloring problem deter mine whether vertex of a given graph can be colored in no more than two color so that no two adjacent vertex are colored the same consider the following brute force algorithm for solving the composite num ber problem check successive integer from to n a possible divisor of if one of them divide n evenly return yes the number is composite if none of them doe return why doe this algorithm not put the problem in class p state the decision version for each of the following problem and outline a polynomial time algorithm that verifies whether or not a proposed solution solves the you may assume that a proposed solution represents a legitimate input to your verification knapsack problem bin packing problem show that the partition problem is polynomially reducible to the decision version of the knapsack show that the following three problem are polynomially reducible to each i determine for a given graph g v e and a positive integer m v whether g contains a clique of size m or a clique of size k in a graph is it complete subgraph of k ii determine for a given graph g v e and a positive integer m v whether there is a vertex cover of size m or le for a vertex cover of size k for a graph g v e is a subset v v such that v k and for each edge u v e at least one of u and v belongs to v iii determine for a given graph g v e and a positive integer m v whether g contains an independent set of size m or an independent set of size k for a graph g v e is a subset v v such that v k and for all u v v vertex u and v are not adjacent in determine whether the following problem is np given several sequence of uppercase and lowercase letter is it possible to select a letter from each sequence without selecting both the upper and lowercase version of any for example if the sequence are abc bc ab and ac it is possible to choose a from the first sequence b from the second and third and c from the an example where there is no way to make the required selection is given by the four sequence ab ab ab and kar which of the following diagram do not contradict the current state of our knowledge about the complexity class p np and npc np complete p np p np npc npc np np p npc p npc np p npc king arthur expects knight for an annual dinner at unfortu nately some of the knight quarrel with each other and arthur know who quarrel with arthur want to seat his guest around a table so that no two quarreling knight sit next to each which standard problem can be used to model king arthur a a research project find a proof that arthur problem ha a solution if each knight doe not quarrel with at least other fundamental of the analysis of algorithm efficiency i often say that when you can measure what you are speaking about and express it in number you know something about it but when you cannot express it in number your knowledge is a meagre and unsatisfactory kind it may be the beginning of knowledge but you have scarcely in your thought advanced to the stage of science whatever the matter may lord kelvin not everything that can be counted count and not everything that count can be albert einstein this chapter is devoted to analysis of the american heritage dic tionary defines analysis a the separation of an intellectual or substantial whole into it constituent part for individual accordingly each of the prin cipal dimension of an algorithm pointed out in section is both a legitimate and desirable subject of but the term analysis of algorithm is usually used in a narrower technical sense to mean an investigation of an algorithm efficiency with respect to two resource running time and memory this emphasis on efficiency is easy to first unlike such dimension a simplicity and gen erality efficiency can be studied in precise quantitative second one can argue although this is hardly always the case given the speed and memory of today computer that the efficiency consideration are of primary importance from a practical point of in this chapter we too will limit the discussion to an algorithm we start with a general framework for analyzing algorithm efficiency in sec tion this section is arguably the most important in the chapter the funda mental nature of the topic make it also one of the most important section in the entire in section we introduce three notation o big oh big omega and big borrowed from mathematics these notation have become the language for discussing the efficiency of in section we show how the general framework outlined in section can be systematically applied to analyzing the efficiency of nonrecursive the main tool of such an analysis is setting up a sum representing the algorithm running time and then simplifying the sum by using standard sum manipulation in section we show how the general framework outlined in section can be systematically applied to analyzing the efficiency of recursive here the main tool is not a summation but a special kind of equation called a recurrence we explain how such recurrence relation can be set up and then introduce a method for solving although we illustrate the analysis framework and the method of it appli cation by a variety of example in the first four section of this chapter section is devoted to yet another example that of the fibonacci discov ered year ago this remarkable sequence appears in a variety of application both within and outside computer a discussion of the fibonacci sequence serf a a natural vehicle for introducing an important class of recurrence rela tions not solvable by the method of section we also discus several algorithm for computing the fibonacci number mostly for the sake of a few general obser vations about the efficiency of algorithm and method of analyzing the method of section and provide a powerful technique for analyz ing the efficiency of many algorithm with mathematical clarity and precision but these method are far from being the last two section of the chapter deal with two approach empirical analysis and algorithm visualization that complement the pure mathematical technique of section and much newer and hence le developed than their mathematical counterpart these ap proaches promise to play an important role among the tool available for analysis of algorithm challenge of numerical algorithm numerical analysis is usually described a the branch of computer science con cerned with algorithm for solving mathematical this description need an important clarification the problem in question are problem of continuous mathematics solving equation and system of equation evaluating such func tions a sin x and ln x computing integral and so on a opposed to problem of discrete mathematics dealing with such structure a graph tree permutation and our interest in efficient algorithm for mathematical problem stem from the fact that these problem arise a model of many real life phe nomena both in the natural world and in the social in fact numerical analysis used to be the main area of research study and application of computer with the rapid proliferation of computer in business and everyday life application which deal primarily with storage and retrieval of information the relative importance of numerical analysis ha shrunk in the last however it application enhanced by the power of modern computer continue to expand in all area of fundamental research and thus wherever one inter est lie in the wide world of modern computing it is important to have at least some understanding of the special challenge posed by continuous mathematical we are not going to discus the variety of difficulty posed by modeling the task of describing a real life phenomenon in mathematical assuming that this ha already been done what principal obstacle to solving a mathematical problem do we the first major obstacle is the fact that most numerical analy si problem cannot be solved they have to be solved approximately and this is usually done by replacing an infinite object by a finite for example the value of ex at a given point x can be computed by approximating it infinite taylor series about x by a finite sum of it first term called the nth degree taylor polynomial ex x x xn to give another example the definite integral of a function can be approximated by a finite weighted sum of it value a in the composite trapezoidal rule that you might remember from your calculus class b h f n f x d x a f xi f b a i where h b an xi a ih for i n figure the error of such approximation are called truncation one of the major task in numerical analysis is to estimate the magnitude of truncation solving a system of linear equation and polynomial evaluation discussed in section and respectively are rare exception to this h h h h x a x xi xi xi xn b figure composite trapezoidal this is typically done by using calculus tool from elementary to quite for example for approximation we have ex x x xn m xn n where m max e on the segment with the endpoint at and this formula make it possible to determine the degree of taylor polynomial needed to guarantee a predefined accuracy level of approximation for example if we want to compute by formula and guarantee the truncation error to be smaller than we can proceed a first we estimate m of formula m max e using this bound and the desired accuracy level of we obtain from m n n to solve the last inequality we can compute the first few value of n n n to see that the smallest value of n for which this inequality hold is similarly for approximation the standard bound of the truncation error is given by the inequality b h n b ah f xdx f a f xi f b m a i where m max f x on the interval a x you are asked to use this inequality in the exercise for this section problem and the other type of error called round off error are caused by the limited accuracy with which we can represent real number in a digital these error arise not only for all irrational number which by definition require an infinite number of digit for their exact representation but for many rational number a in the overwhelming majority of situation real number are represented a floating point number dp be where b is the number base usually or or for unsophisticated calculator d d dp are digit di b for i p and d unless the number is representing together the fractional part of the number and called it mantissa and e is an integer exponent with the range of value approximately symmetric about the accuracy of the floating point representation depends on the number of significant digit p in representation most computer permit two or even three level of precision single precision typically equivalent to between and significant decimal digit double precision to significant decimal digit and extended precision to significant decimal using higher precision arithmetic slows computation but may help to overcome some of the problem caused by round off higher precision may need to be used only for a particular step of the algorithm in a with an approximation of any kind it is important to distinguish between the absolute error and the relative error of representing a number by it approximation absolute error relative error the relative error is undefined if very large and very small number cannot be represented in floating point arithmetic because of the phenomenon called overflow and underflow respec an overflow happens when an arithmetic operation yield a result out side the range of the computer floating point typical example of overflow arise from the multiplication of large number or division by a very small sometimes we can eliminate this problem by making a simple change in the order in which an expression is evaluated by replacing an expression with an equal one computing not a but a or by computing a logarithm of an expression instead of the expression underflow occurs when the result of an operation is a nonzero fraction of such a small magnitude that it cannot be represented a a nonzero floating point usually underflow number are replaced by zero but a special signal is generated by hardware to indicate such an event ha it is important to remember that in addition to inaccurate representation of number the arithmetic operation performed in a computer are not always exact in particular subtracting two nearly equal floating point number may cause a large increase in relative this phenomenon is called subtractive example consider two irrational number and represented by floating point number and the relative error of these approximation are small and the relative error of representing the difference by the difference of the floating point representation is which is very large for a relative error despite quite accurate approximation for both and note that we may get a significant magnification of round off error if a lowaccuracy difference is used a a we already encountered this problem in discussing gaussian elimination in section our solution there wa to use partial many numerical algorithm involve thousand or even million of arithmetic operation for typical for such algorithm the propagation of round off error becomes a major concern from both the practical and theoretical for some algorithm round off error can propagate through the algorithm operation with increasing this highly undesirable property of a numerical algorithm is called some problem exhibit such a high level of sensitivity to change in their input that it is all but impossible to design a stable algorithm to solve such problem are called ill example consider the following system of two linear equation in two unknown it only solution is x y to see how sensitive this system is to small change to it right hand side consider the system with the same coefficient matrix but slightly different right hand side value the only solution to this system is x y which is quite far from the solution to the previous note that the coefficient matrix of this system is close to being singular hence a minor change in it coefficient may yield a system with either no solution or infinitely many solution depending on it right hand side you can find a more formal and detailed discussion of how we can measure the degree of ill condition of the coefficient matrix in numerical analysis textbook we conclude with a well known problem of finding real root of the quadratic equation ax bx c for any real coefficient a b and c a according to secondary school algebra equation ha real root if and only if it discriminant d b ac is nonnegative and these root can be found by the following formula x b b ac a although formula provides a complete solution to the posed problem a far a a mathematician is concerned it is far from being a complete solution for an algorithm the first major obstacle is evaluating the square even for most positive integer d d is an irrational number that can be computed only there is a method of computing square root that is much better than the one commonly taught in secondary it follows from new ton method a very important algorithm for solving equation which we discus in section this method generates the sequence xn of approximation to d where d is a given nonnegative number according to the formula xn d for n xn xn where the initial approximation x can be chosen among other possibility a x it is not difficultto prove that sequence is decreasing if d and always converges to we can stop generating it element either when the difference between it two consecutive element is le than a predefined error tolerance xn xn or when xnis sufficiently close to approximation sequence converges very fast to d for most value of in particular one can prove that if d then no more than four iteration are needed to guarantee that xn d and we can always scale a given value of d to one in the interval by the formula d dp where p is an even example let u apply newton algorithm to compute for simplicity we ignore we will round off the number to six decimal place and use the standard numerical analysis notation to indicate the round x x x x x x x x x x x x x at this point we have to stop because x x and hence all other approximation will be the the exact value of is with the issue of computing square root squared away i do not know whether or not the pun wa intended are we home free to write a program based on formula the answer is no because of the possible impact of round off among other obstacle we are faced here with the menace of subtractive if b is much larger than ac b ac will be very close to b and a root computed by formula might have a large relative example let u follow a paper by george forsythe for and consider the equation x x it true root to significant digit are x george forsythe a noted numerical analyst played a leading role in establishing computer science a a separate academic discipline in the united it is his word that are used a the epigraph to this book and x if we use formula and perform all the computation in decimal floating point arithmetic with say seven significant digit we obtain b ac d d b d x a x b d a and although the relative error of approximating x by x is very small for the second root it is very large x x x to avoid the possibility of subtractive cancellation in formula we can use instead another formula obtained a follows x b b ac a b b ac b b ac a b b ac c b b ac with no danger of subtractive cancellation in the denominator if b a to x it can be computed by the standard formula x b b ac a with no danger of cancellation either for a positive value of the case of b is symmetric we can use the formula x b b ac a and x c b b ac the case of b can be considered with either of the other two there are several other obstacle to applying formula which are related to limitation of floating point arithmetic if a is very small division by a can cause an overflow there seems to be no way to fight the danger of subtractive cancellation in computing b ac other than calculating it with double precision and so these problem have been overcome by william kahan of the university of toronto see for and his algorithm is considered to be a significant achievement in the history of numerical hopefully this brief overview ha piqued your interest enough for you to seek more information in the many book devoted exclusively to numerical in this book we discus one more topic in the next chapter three classic method for solving equation in one exercise some textbook define the number of significant digit in the approximation of number by number a the largest nonnegative integer k for which according to this definition how many significant digit are there in the approximation of by if is known to approximate some number with the absolute error not exceeding find the range of possible value of the range of the relative error of these find the approximate value of e obtained by the fifth degree taylor polynomial about and compute the truncation error of this approx doe the result agree with the theoretical prediction made in the derive formula of the composite trapezoidal use the composite trapezoidal rule with n to approximate the following definite find the truncation error of each approximation and com pare it with the one given by formula xd x x d x if esin xdx is to be computed by the composite trapezoidal rule how large should the number of subintervals be to guarantee a truncation error smaller than smaller than solve the two system of linear equation and indicate whether they are ill x y x y x x write a computer program to solve the equation ax bx c prove that for any nonnegative number d the sequence of newton method for computing d is strictly decreasing and converges to d for any value of the initial approximation x prove that if d and x d no more than four iteration of newton method are needed to guarantee that xn d apply four iteration of newton method to compute and estimate the absolute and relative error of this summary given a class of algorithm for solving a particular problem a lower bound indicates the best possible efficiency any algorithm from this class can a trivial lower bound is based on counting the number of item in the problem input that must be processed and the number of output item that need to be an information theoretic lower bound is usually obtained through a mecha nism of decision this technique is particularly useful for comparison based algorithm for sorting and specifically any general comparison based sorting algorithm must perform at least log n log n key comparison in the worst any general comparison based algorithm for searching a sorted array must perform at least logn key comparison in the worst the adversary method for establishing lower bound is based on following the logic of a malevolent adversary who force the algorithm into the most time consuming a lower bound can also be established by reduction by reducing a problem with a known lower bound to the problem in complexity theory seek to classify problem according to their computational the principal split is between tractable and intractable problem problem that can and cannot be solved in polynomial time for purely technical reason complexity theory concentrate on decision problem which are problem with yesno the halting problem is an example of an undecidable decision problem it cannot be solved by any p is the class of all decision problem that can be solved in polynomial np is the class of all decision problem whose randomly guessed solution can be verified in polynomial many important problem in np such a the hamiltonian circuit problem are known to be np complete all other problem in np are reducible to such a problem in polynomial the first proof of a problem np completeness wa published by cook for the cnf satisfiability it is not known whether p np or p is just a proper subset of this question is the most important unresolved issue in theoretical computer a discovery of a polynomial time algorithm for any of the thousand of known np complete problem would imply that p numerical analysis is a branch of computer science dealing with solving continuous mathematical two type of error occur in solving a majority of such problem truncation error and round off truncation error stem from replacing infinite object by their finite round off error are due to inaccuracy of representing number in a digital subtractive cancellation happens a a result of subtracting two near equal floating point it may lead to a sharp increase in the relative roundoff error and therefore should be avoided by either changing the expression form or by using a higher precision in computing such a writing a general computer program for solving quadratic equation ax bx c is a difficult the problem of computing square root can be solved by utilizing newton method the problem of subtractive cancellation can be dealt with by using different formula depending on whether coefficient b is positive or negative and by computing the discriminant b ac with double the analysis framework in this section we outline a general framework for analyzing the efficiency of algo we already mentioned in section that there are two kind of efficiency time efficiency and space time efficiency also called time complexity indicates how fast an algorithm in question space efficiency also called space complexity refers to the amount of memory unit required by the algorithm in ad dition to the space needed for it input and in the early day of electronic computing both resource time and space were at a half a century of relentless technological innovation have improved the computer speed and memory size by many order of now the amount of extra space re quired by an algorithm is typically not of a much concern with the caveat that there is still of course a difference between the fast main memory the slower secondary memory and the the time issue ha not diminished quite to the same extent in addition the research experience ha shown that for most problem we can achieve much more spectacular progress in speed than in therefore following a well established tradition of algorithm textbook we primarily concentrate on time efficiency but the analytical framework introduced here is applicable to analyzing space efficiency a measuring an input size let start with the obvious observation that almost all algorithm run longer on larger for example it take longer to sort larger array multiply larger matrix and so therefore it is logical to investigate an algorithm efficiency a a function of some parameter n indicating the algorithm input in most case selecting such a parameter is quite for example it will be the size of the list for problem of sorting searching finding the list smallest element and most other problem dealing with for the problem of evaluating a polynomial px anxn a of degree n it will be the polynomial degree or the number of it coefficient which is larger by than it youll see from the discussion that such a minor difference is inconsequential for the efficiency there are situation of course where the choice of a parameter indicating an input size doe one such example is computing the product of two n n there are two natural measure of size for this the first and more frequently used is the matrix order but the other natural contender is the total number of element n in the matrix being the latter is also more general since it is applicable to matrix that are not necessarily since there is a simple formula relating these two measure we can easily switch from one to the other but the answer about an algorithm efficiency will be qualitatively different depending on which of these two measure we use see problem in this section the choice of an appropriate size metric can be influenced by operation of the algorithm in for example how should we measure an input size for a spell checking if the algorithm examines individual character of it input we should measure the size by the number of character if it work by processing word we should count their number in the we should make a special note about measuring input size for algorithm solving problem such a checking primality of a positive integer here the input is just one number and it is this number magnitude that determines the input some algorithm require more than one parameter to indicate the size of their input the number of vertex and the number of edge for algorithm on graph represented by their adjacency in such situation it is preferable to measure size by the number b of bit in the n binary representation b log n this metric usually give a better idea about the efficiency of algorithm in ques unit for measuring running time the next issue concern unit for measuring an algorithm running of course we can simply use some standard unit of time measurement a second or millisecond and so on to measure the running time of a program implement ing the there are obvious drawback to such an approach however dependence on the speed of a particular computer dependence on the quality of a program implementing the algorithm and of the compiler used in generating the machine code and the difficulty of clocking the actual running time of the pro since we are after a measure of an algorithm efficiency we would like to have a metric that doe not depend on these extraneous one possible approach is to count the number of time each of the algorithm operation is this approach is both excessively difficult and a we shall see usually the thing to do is to identify the most important operation of the algorithm called the basic operation the operation contributing the most to the total running time and compute the number of time the basic operation is a a rule it is not difficult to identify the basic operation of an algorithm it is usually the most time consuming operation in the algorithm innermost for example most sorting algorithm work by comparing element key of a list being sorted with each other for such algorithm the basic operation is a key a another example algorithm for mathematical problem typically involve some or all of the four arithmetical operation addition subtraction multiplication and of the four the most time consuming operation is division followed by multiplication and then addition and subtraction with the last two usually considered thus the established framework for the analysis of an algorithm time ef ficiency suggests measuring it by counting the number of time the algorithm basic operation is executed on input of size we will find out how to compute such a count for nonrecursive and recursive algorithm in section and here is an important let cop be the execution time of an algo rithms basic operation on a particular computer and let cn be the number of time this operation need to be executed for this then we can estimate on some computer multiplication doe not take longer than additionsubtraction see for example the timing data provided by kernighan and pike in ker the running time t n of a program implementing this algorithm on that computer by the formula t n of course this formula should be used with the count cn doe not contain any information about operation that are not basic and in fact the count itself is often computed only further the constant cop is also an approximation whose reliability is not always easy to still unless n is extremely large or very small the formula can give a reasonable estimate of the algorithm running it also make it possible to answer such question a how much faster would this algorithm run on a machine that is time faster than the one we the answer is obviously or assuming that cn nn how much longer will the algorithm run if we double it input the answer is about four time indeed for all but very small value of n cn nn n n n and therefore t n copcn n t n copcn n note that we were able to answer the last question without actually knowing the value of cop it wa neatly cancelled out in the also note that the multiplicative constant in the formula for the count cn wa also cancelled it is for these reason that the efficiency analysis framework ignores multiplicative constant and concentrate on the count order of growth to within a constant multiple for large size order of growth why this emphasis on the count order of growth for large input a difference in running time on small input is not what really distinguishes efficient algorithm from inefficient when we have to compute for example the greatest common divisor of two small number it is not immediately clear how much more efficient euclid algorithm is compared to the other two algorithm discussed in section or even why we should care which of them is faster and by how it is only when we have to find the greatest common divisor of two large number that the difference in algorithm efficiency becomes both clear and for large value of n it is the function order of growth that count just look at table which contains value of a few function particularly important for analysis of the magnitude of the number in table ha a profound significance for the analysis of the function growing the slowest among these is the logarithmic it grows so slowly in fact that we should expect a program table value some approximate of several function important for analysis of algorithm n log n n n log n n n n implementing an algorithm with a logarithmic basic operation count to run practi cally instantaneously on input of all realistic also note that although specific value of such a count depend of course on the logarithm base the formula loga n loga b logb n make it possible to switch from one base to another leaving the count logarithmic but with a new multiplicative this is why we omit a logarithm base and write simply log n in situation where we are interested just in a function order of growth to within a multiplicative on the other end of the spectrum are the exponential function n and the factorial function both these function grow so fast that their value become astronomically large even for rather small value of this is the reason why we did not include their value for n in table for example it would take about year for a computer making a trillion operation per second to execute though this is incomparably faster than it would have taken to execute operation it is still longer than billion year the estimated age of the planet there is a tremendous difference between the order of growth of the function n and yet both are often referred to a exponential growth function or simply exponential despite the fact that strictly speaking only the former should be referred to a the bottom line which is important to remember is this algorithm that require an exponential number of operation are practical for solving only problem of very small another way to appreciate the qualitative difference among the order of growth of the function in table is to consider how they react to say a twofold increase in the value of their argument the function log n increase in value by just because log n log log n log n the linear function increase twofold the linearithmic function n log n increase slightly more than twofold the quadratic function n and cubic function n increase fourfold and eightfold respectively because n n and n n the value of n get squared because n n and increase much more than that yes even mathematics refuse to cooperate to give a neat answer for worst case best case and average case efficiency in the beginning of this section we established that it is reasonable to measure an algorithm efficiency a a function of a parameter indicating the size of the algorithm but there are many algorithm for which running time depends not only on an input size but also on the specific of a particular consider a an example sequential this is a straightforward algorithm that search for a given item some search key k in a list of n element by checking successive element of the list until either a match with the search key is found or the list is here is the algorithm pseudocode in which for simplicity a list is implemented a an it also assumes that the second condition ai k will not be checked if the first one which check that the array index doe not exceed it upper bound algorithm k search for a given value in a given array by sequential search input an array and a search key k output the index of the first element in a that match k or if there are no matching element i while i n and ai k do i i if i n return i else return clearly the running time of this algorithm can be quite different for the same list size in the worst case when there are no matching element or the first matching element happens to be the last one on the list the algorithm make the largest number of key comparison among all possible input of size n cworst n the worst case efficiency of an algorithm is it efficiency for the worst case input of size n which is an input or input of size n for which the algorithm run the longest among all possible input of that the way to determine the worst case efficiency of an algorithm is in principle quite straightforward analyze the algorithm to see what kind of input yield the largest value of the basic operation count cn among all possible input of size n and then compute this worst case value for sequential search the answer wa the method for handling le trivial situation are explained in subsequent section of this clearly the worst case analysis provides very important information about an algorithm efficiency by bounding it running time from in other word it guarantee that for any instance of size n the running time will not exceed cworstn it running time on the worst case the best case efficiency of an algorithm is it efficiency for the best case input of size n which is an input or input of size n for which the algorithm run the fastest among all possible input of that accordingly we can analyze the best case efficiency a first we determine the kind of input for which the count cn will be the smallest among all possible input of size note that the best case doe not mean the smallest input it mean the input of size n for which the algorithm run the then we ascertain the value of cn on these most convenient for example the best case input for sequential search are list of size n with their first element equal to a search key accordingly cbestn for this the analysis of the best case efficiency is not nearly a important a that of the worst case but it is not completely useless though we should not expect to get best case input we might be able to take advantage of the fact that for some algorithm a good best case performance extends to some useful type of input close to being the best case for example there is a sorting algorithm insertion sort for which the best case input are already sorted array on which the algorithm work very moreover the best case efficiency deteriorates only slightly for almost sorted therefore such an algorithm might well be the method of choice for application dealing with almost sorted and of course if the best case efficiency of an algorithm is unsatisfactory we can immediately discard it without further it should be clear from our discussion however that neither the worst case analysis nor it best case counterpart yield the necessary information about an algorithm behavior on a typical or random this is the information that the average case efficiency seek to to analyze the algorithm average case efficiency we must make some assumption about possible input of size let consider again sequential the standard assumption are that a the probability of a successful search is equal to p p and b the probability of the first match occurring in the ith position of the list is the same for every under these assumption the validity of which is usually difficult to verify their reasonableness notwithstanding we can find the average number of key comparison cavgn a in the case of a successful search the probability of the first match occurring in the ith position of the list is pn for every i and the number of comparison made by the algorithm in such a situation is obviously in the case of an unsuccessful search the number of comparison will be n with the probability of such a search being therefore cavgn p p i p n p n p n n n n p i n n p n p nn n p pn n n this general formula yield some quite reasonable for example if p the search must be successful the average number of key comparison made by sequential search is n that is the algorithm will inspect on average about half of the list if p the search must be unsuccessful the average number of key comparison will be n because the algorithm will inspect all n element on all such a you can see from this very elementary example investigation of the average case efficiency is considerably more difficult than investigation of the worst case and best case the direct approach for doing this involves dividing all instance of size n into several class so that for each instance of the class the number of time the algorithm basic operation is executed is the what were these class for sequential then a probability distribution of input is obtained or assumed so that the expected value of the basic operation count can be the technical implementation of this plan is rarely easy however and probabilistic assumption underlying it in each particular case are usually difficult to given our quest for simplicity we will mostly quote known result about the average case efficiency of algorithm under if you are interested in derivation of these result consult such book a baa sed knui knuii and it should be clear from the preceding discussion that the average case efficiency cannot be obtained by taking the average of the worst case and the best case even though this average doe occasionally coincide with the average case cost it is not a legitimate way of performing the average case doe one really need the average case efficiency the answer is unequivocally yes there are many important algorithm for which the averagecase efficiency is much better than the overly pessimistic worst case efficiency would lead u to so without the average case analysis computer scientist could have missed many important yet another type of efficiency is called amortized it applies not to a single run of an algorithm but rather to a sequence of operation performed on the same data it turn out that in some situation a single operation can be expensive but the total time for an entire sequence of n such operation is always significantly better than the worst case efficiency of that single operation multiplied by so we can amortize the high cost of such a worst case occurrence over the entire sequence in a manner similar to the way a business would amortize the cost of an expensive item over the year of the item productive this sophisticated approach wa discovered by the american computer scientist robert tarjan who used it among other application in developing an interesting variation of the classic binary search tree see tar for a quite readable nontechnical discussion and tar for a technical we will see an example of the usefulness of amortized efficiency in section when we consider algorithm for finding union of disjoint recapitulation of the analysis framework before we leave this section let u summarize the main point of the framework outlined both time and space efficiency are measured a function of the algorithm input time efficiency is measured by counting the number of time the algorithm basic operation is space efficiency is measured by counting the number of extra memory unit consumed by the the efficiency of some algorithm may differ significantly for input of the same for such algorithm we need to distinguish between the worst case average case and best case the framework primary interest lie in the order of growth of the algorithm running time extra memory unit consumed a it input size go to in the next section we look at formal mean to investigate order of in section and we discus particular method for investigating nonrecursive and recursive algorithm it is there that you will see how the analysis framework outlined here can be applied to investigating the efficiency of specific you will encounter many more example throughout the rest of the exercise for each of the following algorithm indicate i a natural size metric for it input ii it basic operation and iii whether the basic operation count can be different for input of the same size computing the sum of n number computing finding the largest element in a list of n number euclid algorithm sieve of eratosthenes pen and pencil algorithm for multiplying two n digit decimal integer consider the definition based algorithm for adding two n n what is it basic how many time is it performed a a function of the matrix order a a function of the total number of element in the input answer the same question for the definition based algorithm for matrix consider a variation of sequential search that scan a list to return the number of occurrence of a given search key in the doe it efficiency differ from the efficiency of classic sequential glove selection there are glove in a drawer pair of red glove pair of yellow and pair of you select the glove in the dark and can check them only after a selection ha been what is the smallest number of glove you need to select to have at least one matching pair in the best in the worst missing sock imagine that after washing distinct pair of sock you discover that two sock are of course you would like to have the largest number of complete pair thus you are left with complete pair in the best case scenario and with complete pair in the worst assuming that the probability of disappearance for each of the sock is the same find the probability of the best case scenario the probability of the worst case scenario the number of pair you should expect in the average prove formula for the number of bit in the binary representation of a positive decimal prove the alternative formula for the number of bit in the binary repre sentation of a positive integer n b logn what would be the analogous formula for the number of decimal explain why within the accepted analysis framework it doe not matter whether we use binary or decimal digit in measuring n suggest how any sorting algorithm can be augmented in a way to make the best case count of it key comparison equal to just n n is a list size of do you think it would be a worthwhile addition to any sorting gaussian elimination the classic algorithm for solving system of n linear equation in n unknown requires about n multiplication which is the algorithm basic how much longer should you expect gaussian elimination to work on a system of equation versus a system of you are considering buying a computer that is time faster than the one you currently by what factor will the faster computer increase the size of system solvable in the same amount of time a on the old for each of the following function indicate how much the function value will change if it argument is increased log n n n n n n for each of the following pair of function indicate whether the first function of each of the following pair ha a lower same or higher order of growth to within a constant multiple than the second nn and n n and log n and ln n log n and log n n and n n and invention of chess according to a well known legend the game of chess wa invented many century ago in northwestern india by a certain when he took his invention to his king the king liked the game so much that he offered the inventor any reward he the inventor asked for some grain to be obtained a follows just a single grain of wheat wa to be placed on the first square of the chessboard two on the second four on the third eight on the fourth and so on until all square had been if it took just second to count each grain how long would it take to count all the grain due to how long would it take if instead of doubling the number of grain for each square of the chessboard the inventor asked for adding two coping with the limitation of algorithm power keep on the lookout for novel idea that others have used your idea ha to be original only in it adaptation to the problem youre working thomas edison a we saw in the previous chapter there are problem that are difficult to solve at the same time some of them are so important that we cannot just sigh in resignation and do this chapter outline several way of dealing with such difficult section and introduce two algorithm design technique backtracking and branch and bound that often make it possible to solve at least some large instance of difficult combinatorial both strategy can be considered an improvement over exhaustive search discussed in section unlike exhaustive search they construct candidate solution one component at a time and evaluate the partially constructed solution if no potential value of the remaining component can lead to a solution the remaining component are not generated at this approach make it possible to solve some large instance of difficult combinatorial problem though in the worst case we still face the same curse of exponential explosion encountered in exhaustive both backtracking and branch and bound are based on the construction of a state space tree whose node reflect specific choice made for a solution both technique terminate a node a soon a it can be guaranteed that no solution to the problem can be obtained by considering choice that correspond to the node the technique differ in the nature of problem they can be applied branch and bound is applicable only to optimization problem because it is based on computing a bound on possible value of the problem objective backtracking throughout the book see in particular section and we have encoun tered problem that require finding an element with a special property in a domain that grows exponentially fast or faster with the size of the problem input a hamiltonian circuit among all permutation of a graph vertex the most valu able subset of item for an instance of the knapsack problem and the we addressed in section the reason for believing that many such problem might not be solvable in polynomial also recall that we discussed in section how such problem can be solved at least in principle by exhaustive the exhaustive search technique suggests generating all candidate solution and then identifying the one or the one with a desired backtracking is a more intelligent variation of this the principal idea is to construct solution one component at a time and evaluate such partially constructed candidate a if a partially constructed solution can be de veloped further without violating the problem constraint it is done by taking the first remaining legitimate option for the next if there is no legiti mate option for the next component no alternative for any remaining component need to be in this case the algorithm backtracks to replace the last component of the partially constructed solution with it next it is convenient to implement this kind of processing by constructing a tree of choice being made called the state space it root represents an initial state before the search for a solution the node of the first level in the tree represent the choice made for the first component of a solution the node of the second level represent the choice for the second component and so a node in a state space tree is said to be promising if it corresponds to a partially constructed solution that may still lead to a complete solution otherwise it is called leaf represent either nonpromising dead end or complete solution found by the in the majority of case a statespace tree for a backtracking algorithm is constructed in the manner of depthfirst if the current node is promising it child is generated by adding the first remaining legitimate option for the next component of a solution and the processing move to this if the current node turn out to be nonpromising the algorithm backtracks to the node parent to consider the next possible option for it last component if there is no such option it backtracks one more level up the tree and so finally if the algorithm reach a complete solution to the problem it either stop if just one solution is required or continues searching for other possible n queen problem a our first example we use a perennial favorite of textbook writer the n queen the problem is to place n queen on an n n chessboard so that no two queen attack each other by being in the same row or in the same column or on the same for n the problem ha a trivial solution and it is easy to see that there is no solution for n and n so let u consider the four queen problem and solve it by the backtracking since each of the four queen ha to be placed in it own row all we need to do is to assign a column for each queen on the board presented in figure we start with the empty board and then place queen in the first possible position of it row which is in column of row then we place queen after trying unsuccessfully column and in the first acceptable position for it which is square the square in row and column this prof to be a dead end because there is no acceptable position for queen so the algorithm backtracks and put queen in the next possible position at then queen is placed at which prof to be another dead the algorithm then backtracks all the way to queen and move it to queen then go to queen to and queen to which is a solution to the the state space tree of this search is shown in figure if other solution need to be found how many of them are there for the fourqueens the algorithm can simply resume it operation at the leaf at which it alternatively we can use the board symmetry for this queen queen queen queen figure board for the four queen q q q q q q q q q q q q q q q q q q solution figure state space tree of solving the four queen problem by denotes an unsuccessful attempt to place a queen in the indicated the number above the node indicate the order in which the node are finally it should be pointed out that a single solution to the n queen problem for any n can be found in linear in fact over the last year mathe maticians have discovered several alternative formula for nonattacking position of n queen such position can also be found by applying some general algorithm design strategy problem in this section hamiltonian circuit problem a our next example let u consider the problem of finding a hamiltonian circuit in the graph in figure without loss of generality we can assume that if a hamiltonian circuit exists it start at vertex accordingly we make vertex a the root of the state space a b a c f b d e e c f d e e d f c dead end dead end f d dead end a solution a b figure a b state space tree for finding a hamiltonian the number above the node of the tree indicate the order in which the node are tree figure the first component of our future solution if it exists is a first intermediate vertex of a hamiltonian circuit to be using the alphabet order to break the three way tie among the vertex adjacent to a we select vertex from b the algorithm proceeds to c then to d then to e and finally to f which prof to be a dead so the algorithm backtracks from f to e then to d and then to c which provides the first alternative for the algorithm to going from c to e eventually prof useless and the algorithm ha to backtrack from e to c and then to from there it go to the vertex f e c and d from which it can legitimately return to a yielding the hamiltonian circuit a b f e c d if we wanted to find another hamiltonian circuit we could continue this process by backtracking from the leaf of the solution subset sum problem a our last example we consider the subset sum problem find a subset of a given set a a an of n positive integer whose sum is equal to a given positive integer for example for a and d there are two solution and of course some instance of this problem may have no it is convenient to sort the set element in increasing so we will assume that a a with wo with wo with wo with wo with wo with wo with wo solution figure complete state space tree of the backtracking algorithm applied to the instance a and d of the subset sum the number inside a node is the sum of the element already included in the subset represented by the the inequality below a leaf indicates the reason for it the state space tree can be constructed a a binary tree like that in figure for the instance a and d the root of the tree represents the starting point with no decision about the given element made a it left and right child represent respectively inclusion and exclusion of a in a set being similarly going to the left from a node of the first level corresponds to inclusion of a while going to the right corresponds to it exclusion and so thus a path from the root to a node on the ith level of the tree indicates which of the first i number have been included in the subset represented by that we record the value of s the sum of these number in the if s is equal to d we have a solution to the we can either report this result and stop or if all the solution need to be found continue by backtracking to the node if s is not equal to d we can terminate the node a nonpromising if either of the following two inequality hold s ai d the sum s is too large n s aj d the sum s is too j i general remark from a more general perspective most backtracking algorithm fit the follow ing an output of a backtracking algorithm can be thought of a an n tuple x x xn where each coordinate xi is an element of some finite linearly ordered set for example for the n queen problem each si is the set of integer column number through the tuple may need to satisfy some additional constraint the nonattacking requirement in the n queen depending on the problem all solution tuples can be of the same length the n queen and the hamiltonian circuit problem and of different length the subset sum a backtracking algorithm generates explicitly or implicitly a state space tree it node represent partially constructed tuples with the first i coordinate defined by the earlier action of the if such a tuple x x xi is not a solution the algorithm find the next element in si that is consistent with the value of x x xi and the problem constraint and add it to the tuple a it i st if such an element doe not exist the algorithm backtracks to consider the next value of xi and so to start a backtracking algorithm the following pseudocode can be called for i represents the empty algorithm give a template of a generic backtracking algorithm input specifies first i promising component of a solution output all the tuples representing the problem solution if is a solution write else see problem in this section exercise for each element x si consistent with and the constraint do xi x our success in solving small instance of three difficult problem earlier in this section should not lead you to the false conclusion that backtracking is a very efficient in the worst case it may have to generate all possible candidate in an exponentially or faster growing state space of the problem at the hope of course is that a backtracking algorithm will be able to prune enough branch of it state space tree before running out of time or memory or the success of this strategy is known to vary widely not only from problem to problem but also from one instance to another of the same there are several trick that might help reduce the size of a state space one is to exploit the symmetry often present in combinatorial for example the board of the n queen problem ha several symmetry so that some solution can be obtained from others by reflection or this implies in particular that we need not consider placement of the first queen in the last n column because any solution with the first queen in square i n i n can be obtained by reflection from a solution with the first queen in square n i this observation cut the size of the tree by about another trick is to preassign value to one or more component of a solution a we did in the hamiltonian circuit data presorting in the subset sum example demonstrates potential benefit of yet another opportunity rearrange data of an instance it would be highly desirable to be able to estimate the size of the state space tree of a backtracking a a rule this is too difficult to do analytically knuth knu suggested generating a random path from the root to a leaf and using the information about the number of choice available during the path generation for estimating the size of the specifically let c be the number of value of the first component x that are consistent with the problem we randomly select one of these value with equal probability c to move to one of the root c repeating this operation for c possible value for x that are consistent with x and the other constraint we move to one of the c child of that we continue this process until a leaf is reached after randomly selecting value for x x by assuming that the node on level i have ci child on average we estimate the number of node in the tree a c cc cc generating several such estimate and computing their average yield a useful estimation of the actual size of the tree although the standard deviation of this random variable can be in conclusion three thing on behalf of backtracking need to be first it is typically applied to difficult combinatorial problem for which no efficient algo rithms for finding exact solution possibly second unlike the exhaustive search approach which is doomed to be extremely slow for all instance of a problem backtracking at least hold a hope for solving some instance of nontriv ial size in an acceptable amount of this is especially true for optimization problem for which the idea of backtracking can be further enhanced by evaluat ing the quality of partially constructed how this can be done is explained in the next third even if backtracking doe not eliminate any element of a problem state space and end up generating all it element it provides a specific technique for doing so which can be of value in it own exercise continue the backtracking search for a solution to the four queen prob lem which wa started in this section to find the second solution to the explain how the board symmetry can be used to find the second solution to the four queen which is the last solution to the five queen problem found by the back tracking use the board symmetry to find at least four other solution to the implement the backtracking algorithm for the n queen problem in the lan guage of your run your program for a sample of n value to get the number of node in the algorithm state space compare these num bers with the number of candidate solution generated by the exhaustive search algorithm for this problem see problem in exercise for each value of n for which you run your program in part a estimate the size of the state space tree by the method described in section and compare the estimate with the actual number of node you design a linear time algorithm that find a solution to the n queen problem for any n apply backtracking to the problem of finding a hamiltonian circuit in the following a b c d e f g apply backtracking to solve the coloring problem for the graph in fig ure generate all permutation of by apply backtracking to solve the following instance of the subset sum problem a and d will the backtracking algorithm work correctly if we use just one of the two inequality to terminate a node a the general template for backtracking algorithm which is given in the sec tion work correctly only if no solution is a prefix to another solution to the change the template pseudocode to work correctly without this write a program implementing a backtracking algorithm for the hamiltonian circuit the m coloring puzzle peg this puzzle like game is played on a board with small hole arranged in an equilateral in an initial position all but one of the hole are occupied by peg a in the example shown a legal move is a jump of a peg over it immediate neighbor into an empty square opposite the jump remove the jumped over neighbor from the design and implement a backtracking algorithm for solving the following version of this starting with a given location of the empty hole find a shortest sequence of move that eliminates peg with no limitation on the final position of the remaining starting with a given location of the empty hole find a shortest sequence of move that eliminates peg with the remaining peg at the empty hole of the initial branch and bound recall that the central idea of backtracking discussed in the previous section is to cut off a branch of the problem state space tree a soon a we can deduce that it cannot lead to a this idea can be strengthened further if we deal with an optimization an optimization problem seek to minimize or maximize some objective function a tour length the value of item selected the cost of an assignment and the like usually subject to some note that in the standard terminology of optimization problem a feasible solution is a point in the problem search space that satisfies all the problem constraint a hamiltonian circuit in the traveling salesman problem or a subset of item whose total weight doe not exceed the knapsack capacity in the knapsack problem whereas an optimal solution is a feasible solution with the best value of the objective function the shortest hamiltonian circuit or the most valuable subset of item that fit the compared to backtracking branch and bound requires two additional item a way to provide for every node of a state space tree a bound on the best value of the objective function on any solution that can be obtained by adding further component to the partially constructed solution represented by the node the value of the best solution seen so far if this information is available we can compare a node bound value with the value of the best solution seen so if the bound value is not better than the value of the best solution seen so far not smaller for a minimization problem this bound should be a lower bound for a minimization problem and an upper bound for a maximiza tion and not larger for a maximization problem the node is nonpromising and can be terminated some people say the branch is indeed no solution obtained from it can yield a better solution than the one already this is the principal idea of the branch and bound in general we terminate a search path at the current node in a state space tree of a branch and bound algorithm for any one of the following three reason the value of the node bound is not better than the value of the best solution seen so the node represents no feasible solution because the constraint of the problem are already the subset of feasible solution represented by the node consists of a single point and hence no further choice can be made in this case we compare the value of the objective function for this feasible solution with that of the best solution seen so far and update the latter with the former if the new solution is assignment problem let u illustrate the branch and bound approach by applying it to the problem of assigning n people to n job so that the total cost of the assignment is a small a we introduced this problem in section where we solved it by exhaustive recall that an instance of the assignment problem is specified by an n n cost matrix c so that we can state the problem a follows select one element in each row of the matrix so that no two selected element are in the same column and their sum is the smallest we will demonstrate how this problem can be solved using the branch and bound technique by considering the same small instance of the problem that we investigated in section job job job job person a c person b person c person d how can we find a lower bound on the cost of an optimal selection without actually solving the we can do this by several for example it is clear that the cost of any solution including an optimal one cannot be smaller than the sum of the smallest element in each of the matrix for the instance here this sum is it is important to stress that this is not the cost of any legitimate selection and came from the same column of the matrix it is just a lower bound on the cost of any legitimate we can and will apply the same thinking to partially constructed for example for any legitimate selection that selects from the first row the lower bound will be one more comment is in order before we embark on constructing the problem state space it deal with the order in which the tree node will be rather than generating a single child of the last promising node a we did in backtracking we will generate all the child of the most promising node among nonterminated leaf in the current nonterminated still promising leaf are also called how can we tell which of the node is most we can do this by comparing the lower bound of the live it is sensible to consider a node with the best bound a most promising although this doe not of course preclude the possibility that an optimal solution will ul timately belong to a different branch of the state space this variation of the strategy is called the best first branch and so returning to the instance of the assignment problem given earlier we start with the root that corresponds to no element selected from the cost a we already discussed the lower bound value for the root denoted lb is the node on the first level of the tree correspond to selection of an element in the first row of the matrix a job for person a figure so we have four live leaf node through that may contain an optimal the most promising of them is node because it ha the smallest lower bound following our best first search strategy we branch out from that node first by considering the three different way of selecting an element from the second row and not in the second column the three different job that can be assigned to person b figure of the six live leaf node and that may contain an optimal solution we again choose the one with the smallest lower bound node first we consider selecting the third column element from c row assigning person c to job this leaf u with no choice but to select the element from the fourth column of d row assigning person d to job this yield leaf figure which corresponds to the feasible solution a b c d with the total cost of it sibling node corresponds to the feasible solution a b c d with the total cost of since it cost is larger than the cost of the solution represented by leaf node is simply of course if start lb a a a a lb lb lb lb figure level and of the state space tree for the instance of the assignment problem being solved with the best first branch and bound the number above a node show the order in which the node wa a node field indicate the job number assigned to person a and the lower bound value lb for this start lb a a a a lb lb lb lb b b b lb lb lb figure level and of the state space tree for the instance of the assignment problem being solved with the best first branch and bound start lb a a a a lb lb lb lb x x x b b b lb lb lb x x c c d d cost cost solution inferior solution figure complete state space tree for the instance of the assignment problem solved with the best first branch and bound it cost were smaller than we would have to replace the information about the best solution seen so far with the data provided by this now a we inspect each of the live leaf of the last state space tree node and in figure we discover that their lower bound value are not smaller than the value of the best selection seen so far leaf hence we terminate all of them and recognize the solution represented by leaf a the optimal solution to the before we leave the assignment problem we have to remind ourselves again that unlike for our next example there is a polynomial time algorithm for this problem called the hungarian method in the light of this efficient algorithm solving the assignment problem by branch and bound should be con sidered a convenient educational device rather than a practical knapsack problem let u now discus how we can apply the branch and bound technique to solving the knapsack this problem wa introduced in section given n item of known weight wi and value vi i n and a knapsack of capacity w find the most valuable subset of the item that fit in the it is convenient to order the item of a given instance in descending order by their value to weight then the first item give the best payoff per weight unit and the last one give the worst payoff per weight unit with tie resolved arbitrarily vw vw it is natural to structure the state space tree for this problem a a binary tree constructed a follows see figure for an each node on the ith level of this tree i n represents all the subset of n item that include a particular selection made from the first i ordered this particular selection is uniquely determined by the path from the root to the node a branch going to the left indicates the inclusion of the next item and a branch going to the right indicates it we record the total weight w and the total value v of this selection in the node along with some upper bound ub on the value of any subset that can be obtained by adding zero or more item to this a simple way to compute the upper bound ub is to add to v the total value of the item already selected the product of the remaining capacity of the knapsack w w and the best per unit payoff among the remaining item which is viwi ub v w a a specific example let u apply the branch and bound algorithm to the same instance of the knapsack problem we solved in section by exhaustive we reorder the item in descending order of their value to weight ratio item weight value value weight the knapsack capacity w is w v ub with wo w v w v ub ub with wo x inferior to node w w v ub x with wo not feasible w v w v ub ub x with wo inferior to node w w v value x not feasible optimal solution figure state space tree of the best first branch and bound algorithm for the instance of the knapsack at the root of the state space tree see figure no item have been selected a hence both the total weight of the item already selected w and their total value v are equal to the value of the upper bound computed by formula is node the left child of the root represents the subset that include item the total weight and value of the item already included are and respectively the value of the upper bound is node represents the subset that do not include item accordingly w v and ub since node ha a larger upper bound than the upper bound of node it is more promising for this maximization problem and we branch from node it child node and represent subset with item and with and without item since the total weight w of every subset represented by node exceeds the knapsack capacity node can be terminated node ha the same value of w and v a it parent the upper bound ub is equal to selecting node over node for the next branching we get node and by respectively including and excluding item the total weight and value a well a the upper bound for these node are computed in the same way a for the preceding branching from node yield node which represents no feasible solution and node which represents just a single subset of value the remaining live node and have smaller upper bound value than the value of the solution represented by node hence both can be terminated making the subset of node the optimal solution to the solving the knapsack problem by a branch and bound algorithm ha a rather unusual typically internal node of a state space tree do not define a point of the problem search space because some of the solution component remain see for example the branch and bound tree for the assign ment problem discussed in the preceding for the knapsack problem however every node of the tree represents a subset of the item we can use this fact to update the information about the best subset seen so far after generating each new node in the if we had done this for the instance investi gated above we could have terminated node and before node wa generated because they both are inferior to the subset of value of node traveling salesman problem we will be able to apply the branch and bound technique to instance of the traveling salesman problem if we come up with a reasonable lower bound on tour one very simple lower bound can be obtained by finding the smallest element in the intercity distance matrix d and multiplying it by the number of city but there is a le obvious and more informative lower bound for instance with symmetric matrix d which doe not require a lot of work to it is not difficult to show problem in this section exercise that we can compute a lower bound on the length l of any tour a for each city i i n find the sum si of the distance from city i to the two nearest city compute the sum s of these n number divide the result by and if all the distance are integer round up the result to the nearest integer lb s for example for the instance in figure formula yield lb moreover for any subset of tour that must include particular edge of a given graph we can modify lower bound for example for all the hamiltonian circuit of the graph in figure that must include edge a d we get the following lower bound by summing up the length of the two shortest edge incident with each of the vertex with the required inclusion of edge a d and d a we now apply the branch and bound algorithm with the bounding function given by formula to find the shortest hamiltonian circuit for the graph in a b a lb a b a c a d a e c d lb lb lb x x x b is not lb l lb l before c e of node of node a a b c a b d a b e lb lb lb x lb l of node a b c d a b c e a b d c a b d e e a d a e a c a l l l l first tour better tour inferior tour optimal tour b figure a weighted b state space tree of the branch and bound algorithm to find a shortest hamiltonian circuit in this the list of vertex in a node specifies a beginning part of the hamiltonian circuit represented by the figure to reduce the amount of potential work we take advantage of two observation made in section first without loss of generality we can consider only tour that start at second because our graph is undirected we can generate only tour in which b is visited before in addition after visiting n city a tour ha no choice but to visit the remaining unvisited city and return to the starting the state space tree tracing the algorithm application is given in figure the comment we made at the end of the preceding section about the strength and weakness of backtracking are applicable to branch and bound a to reiterate the main point these state space tree technique enable u to solve many large instance of difficult combinatorial a a rule however it is virtually impossible to predict which instance will be solvable in a realistic amount of time and which will incorporation of additional information such a a symmetry of a game board can widen the range of solvable along this line a branch and bound algorithm can be sometimes accelerated by a knowledge of the objective function value of some nontrivial feasible the information might be obtainable say by exploiting specific of the data or even for some problem generated randomly before we start developing a state space then we can use such a solution immediately a the best one seen so far rather than waiting for the branch and bound processing to lead u to the first feasible in contrast to backtracking solving a problem by branch and bound ha both the challenge and opportunity of choosing the order of node generation and find ing a good bounding though the best first rule we used above is a sensible approach it may or may not lead to a solution faster than other arti ficial intelligence researcher are particularly interested in different strategy for developing state space finding a good bounding function is usually not a simple on the one hand we want this function to be easy to on the other hand it cannot be too simplistic otherwise it would fail in it principal task to prune a many branch of a state space tree a soon a striking a proper balance be tween these two competing requirement may require intensive experimentation with a wide variety of instance of the problem in exercise what data structure would you use to keep track of live node in a best first branch and bound solve the same instance of the assignment problem a the one solved in the section by the best first branch and bound algorithm with the bounding function based on matrix column rather than give an example of the best case input for the branch and bound algo rithm for the assignment in the best case how many node will be in the state space tree of the branch and bound algorithm for the assignment write a program for solving the assignment problem by the branch and bound experiment with your program to determine the average size of the cost matrix for which the problem is solved in a given amount of time say minute on your solve the following instance of the knapsack problem by the branch and bound algorithm item weight value w suggest a more sophisticated bounding function for solving the knapsack problem than the one used in the use your bounding function in the branch and bound algorithm applied to the instance of problem write a program to solve the knapsack problem with the branch and bound prove the validity of the lower bound given by formula for instance of the traveling salesman problem with symmetric matrix of integer intercity how would you modify lower bound for nonsymmetric distance apply the branch and bound algorithm to solve the traveling salesman prob lem for the following graph a b c d we solved this problem by exhaustive search in section a a research project write a report on how state space tree are used for programming such game a chess checker and tic tac the two principal algorithm you should read about are the minimax algorithm and alpha beta approximation algorithm for np hard problem in this section we discus a different approach to handling difficult problem of combinatorial optimization such a the traveling salesman problem and the knapsack a we pointed out in section the decision version of these problem are np their optimization version fall in the class of np hard problem problem that are at least a hard a np complete hence there are no known polynomial time algorithm for these problem and there are serious theoretical reason to believe that such algorithm do not what then are our option for handling such problem many of which are of significant practical the notion of an np hard problem can be defined more formally by extending the notion of polynomial reducibility to problem that are not necessarily in class np including optimization problem of the type discussed in this section see gar chapter if an instance of the problem in question is very small we might be able to solve it by an exhaustive search algorithm section some such problem can be solved by the dynamic programming technique we demonstrated in section but even when this approach work in principle it practicality is limited by dependence on the instance parameter being relatively the discovery of the branch and bound technique ha proved to be an important breakthrough because this technique make it possible to solve many large instance of difficult optimization problem in an acceptable amount of however such good performance cannot usually be there is a radically different way of dealing with difficult optimization prob lem solve them approximately by a fast this approach is particularly appealing for application where a good but not necessarily optimal solution will besides in real life application we often have to operate with inaccurate data to begin under such circumstance going for an approximate solution can be a particularly sensible although approximation algorithm run a gamut in level of sophistication most of them are based on some problem specific a heuristic is a common sense rule drawn from experience rather than from a mathematically proved for example going to the nearest unvisited city in the traveling salesman problem is a good illustration of this we discus an algorithm based on this heuristic later in this of course if we use an algorithm whose output is just an approximation of the actual optimal solution we would like to know how accurate this approximation we can quantify the accuracy of an approximate solution sa to a problem of minimizing some function f by the size of the relative error of this approximation r esa f sa f s f s where s is an exact solution to the alternatively since resa f sa f s we can simply use the accuracy ratio r sa f sa f s a a measure of accuracy of note that for the sake of scale uniformity the accuracy ratio of approximate solution to maximization problem is usually com puted a r sa f s f sa to make this ratio greater than or equal to a it is for minimization obviously the closer rsa is to the better the approximate solution for most instance however we cannot compute the accuracy ratio because we typically do not know f s the true optimal value of the objective therefore our hope should lie in obtaining a good upper bound on the value of this lead to the following definition a polynomial time approximation algorithm is said to be a capproximation algorithm where c if the accuracy ratio of the approximation it produce doe not exceed c for any instance of the problem in question rsa the best the smallest value of c for which inequality hold for all instance of the problem is called the performance ratio of the algorithm and denoted the performance ratio serf a the principal metric indicating the quality of the approximation we would like to have approximation algorithm with ra a close to a unfortunately a we shall see some approximation algorithm have infinitely large performance ratio ra this doe not necessarily rule out using such algorithm but it doe call for a cautious treatment of their there are two important fact about difficult combinatorial optimization problem worth keeping in first although the difficulty level of solving most such problem exactly is the same to within a polynomial time transformation of one problem to another this equivalence doe not translate into the realm of approximation finding good approximate solution is much easier for some of these problem than for second some of the problem have special class of instance that are both particularly important for real life application and easier to solve than their general the traveling salesman problem is a prime example of this approximation algorithm for the traveling salesman problem we solved the traveling salesman problem by exhaustive search in section mentioned it decision version a one of the most well known np complete problem in section and saw how it instance can be solved by a branchand bound algorithm in section here we consider several approximation algorithm a small sample of dozen of such algorithm suggested over the year for this famous for a much more detailed discussion of the topic see law hoc app and but first let u answer the question of whether we should hope to find a polynomial time approximation algorithm with a finite performance ratio on all instance of the traveling salesman a the following theorem sah show the answer turn out to be no unless p n p theorem if p np there exists no c approximation algorithm for the traveling salesman problem there exists no polynomial time approximation algorithm for this problem so that for all instance f sa cf s for some constant proof by way of contradiction suppose that such an approximation algorithm a and a constant c without loss of generality we can assume that c is a positive we will show that this algorithm could then be used for solving the hamiltonian circuit problem in polynomial we will take advantage of a variation of the transformation used in section to reduce the hamiltonian circuit problem to the traveling salesman let g be an arbitrary graph with n we map g to a complete weighted graph g by assigning weight to each edge in g and adding an edge of weight cn between each pair of vertex not adjacent in if g ha a hamiltonian circuit it length in g is n hence it is the exact solution s to the traveling salesman problem for g note that if sa is an approximate solution obtained for g by algorithm a then f sa cn by the if g doe not have a hamiltonian circuit in g the shortest tour in g will contain at least one edge of weight cn and hence f sa f s taking into account the two derived inequality we could solve the hamiltonian circuit problem for graph g in polynomial time by mapping g to g applying algorithm a to get tour sa in g and comparing it length with since the hamiltonian circuit problem is np complete we have a contradiction unless p greedy algorithm for the tsp the simplest approximation algorithm for the traveling salesman problem are based on the greedy we will discus here two such nearest neighbor algorithm the following well known greedy algorithm is based on the nearest neighbor heuristic always go next to the nearest unvisited step choose an arbitrary city a the step repeat the following operation until all the city have been visited go to the unvisited city nearest the one visited last tie can be broken step return to the starting example for the instance represented by the graph in figure with a a the starting vertex the nearest neighbor algorithm yield the tour hamiltonian circuit sa a b c d a of length a b d c figure instance of the traveling salesman the optimal solution a can be easily checked by exhaustive search is the tour s a b d c a of length thus the accuracy ratio of this approximation is r sa f sa f s tour sa is longer than the optimal tour unfortunately except for it simplicity not many good thing can be said about the nearest neighbor in particular nothing can be said in general about the accuracy of solution obtained by this algorithm because it can force u to traverse a very long edge on the last leg of the indeed if we change the weight of edge a d from to an arbitrary large number w in example the algorithm will still yield the tour a b c d a of length w and the optimal solution will still be a b d c a of length hence r sa f sa w f s which can be made a large a we wish by choosing an appropriately large value of hence ra for this algorithm a it should be according to theorem multifragment heuristic algorithm another natural greedy algorithm for the traveling salesman problem considers it a the problem of finding a minimum weight collection of edge in a given complete weighted graph so that all the vertex have degree with this emphasis on edge rather than vertex what other greedy algorithm doe it remind you an application of the greedy technique to this problem lead to the following algorithm step sort the edge in increasing order of their tie can be broken initialize the set of tour edge to be constructed to the empty step repeat this step n time where n is the number of city in the instance being solved add the next edge on the sorted edge list to the set of tour edge provided this addition doe not create a vertex of degree or a cycle of length le than n otherwise skip the step return the set of tour a an example applying the algorithm to the graph in figure yield a b c d b c a this set of edge form the same tour a the one produced by the nearest neighbor in general the multifragment heuristic algorithm tends to produce significantly better tour than the nearest neighbor algorithm a we are going to see from the experimental data quoted at the end of this but the performance ratio of the multifragment heuristic algorithm is also unbounded of there is however a very important subset of instance called euclidean for which we can make a nontrivial assertion about the accuracy of both the nearest neighbor and multifragment heuristic these are the instance in which intercity distance satisfy the following natural condition triangle inequality di j di k dk j for any triple of city i j and k the distance between city i and j cannot exceed the length of a two leg path from i to some intermediate city k to j symmetry di j dj i for any pair of city i and j the distance from i to j is the same a the distance from j to i a substantial majority of practical application of the traveling salesman prob lem are it euclidean they include in particular geometric one where city correspond to point in the plane and distance are computed by the standard euclidean although the performance ratio of the nearest neighbor and multifragment heuristic algorithm remain unbounded for euclidean instance their accuracy ratio satisfy the following inequality for any such instance with n city f sa log n f s where f sa and f s are the length of the heuristic tour and shortest tour respectively see ro and minimum spanning treebased algorithm there are approximation algori thm for the traveling salesman problem that exploit a connection between hamil tonian circuit and spanning tree of the same since removing an edge from a hamiltonian circuit yield a spanning tree we can expect that the structure of a minimum spanning tree provides a good basis for constructing a shortest tour here is an algorithm that implement this idea in a rather straight forward twice around the tree algorithm step construct a minimum spanning tree of the graph corresponding to a given instance of the traveling salesman step starting at an arbitrary vertex perform a walk around the minimum spanning tree recording all the vertex passed this can be done by a dfs step scan the vertex list obtained in step and eliminate from it all repeated occurrence of the same vertex except the starting one at the end of the this step is equivalent to making shortcut in the the vertex remaining on the list will form a hamiltonian circuit which is the output of the example let u apply this algorithm to the graph in figure the minimum spanning tree of this graph is made up of edge a b b c b d and d e figure a twice around the tree walk that start and end at a is a e a e b d b d c c a b figure illustration of the twice around the tree a b walk around the minimum spanning tree with the a b c b d e d b eliminating the second b a shortcut from c to d the second d and the third b a shortcut from e to a yield the hamiltonian circuit a b c d e a of length the tour obtained in example is not although that instance is small enough to find an optimal solution by either exhaustive search or branch andbound we refrained from doing so to reiterate a general a a rule we do not know what the length of an optimal tour actually is and therefore we cannot compute the accuracy ratio f saf for the twice around the tree algorithm we can at least estimate it above provided the graph is theorem the twice around the tree algorithm is a approximation algorithm for the traveling salesman problem with euclidean proof obviously the twice around the tree algorithm is polynomial time if we use a reasonable algorithm such a prims or kruskals in step we need to show that for any euclidean instance of the traveling salesman problem the length of a tour sa obtained by the twice around the tree algorithm is at most twice the length of the optimal tour s f sa f since removing any edge from s yield a spanning tree t of weight wt which must be greater than or equal to the weight of the graph minimum spanning tree wt we get the inequality f s wt wt this inequality implies that f s wt the length of the walk obtained in step of the the possible shortcut outlined in step of the algorithm to obtain sa cannot increase the total length of the walk in a euclidean graph the length of the walk obtained in step the length of the tour combining the last two inequality we get the inequality f s f sa which is in fact a slightly stronger assertion than the one we needed to christofides algorithm there is an approximation algorithm with a better per formance ratio for the euclidean traveling salesman problem the well known christofides algorithm it also us a minimum spanning tree but doe this in a more sophisticated way than the twice around the tree note that a twice around the tree walk generated by the latter algorithm is an eule rian circuit in the multigraph obtained by doubling every edge in the graph recall that an eulerian circuit exists in a connected multigraph if and only if all it vertex have even the christofides algorithm obtains such a multi graph by adding to the graph the edge of a minimum weight matching of all the odd degree vertex in it minimum spanning the number of such vertex is always even and hence this can always be then the algorithm find an eulerian circuit in the multigraph and transforms it into a hamiltonian circuit by shortcut exactly the same way it is done in the last step of the twice around the tree example let u trace the christofides algorithm in figure on the same instance figure used for tracing the twice around the tree algorithm in figure the graph minimum spanning tree is shown in figure it ha four odd degree vertex a b c and the minimum weight matching of these four vertex consists of edge a b and c for this tiny instance it can be found easily by comparing the total weight of just three alternative a b and c e a c and b e a e and b the traversal of the multigraph starting at vertex a produce the eulerian circuit a b c e d b a which after one shortcut yield the tour a b c e d a of length the performance ratio of the christofides algorithm on euclidean instance is see it tends to produce significantly better approximation to optimal tour than the twice around the tree algorithm doe in empirical we quote some result of such test at the end of this the quality of a tour obtained by this heuristic can be further improved by optimizing shortcut made on the last step of the algorithm a follows examine the multiply visited city in some arbitrary order and for each make the best possible this a e b d c a a e a e b d b d c c b c figure application of the christofides a b minimum spanning tree with added edge in dash of a minimum weight matching of all odd degree c hamiltonian circuit enhancement would have not improved the tour a b c e d a obtained in example from a b c e d b a because shortcutting the second occurrence of b happens to be better than shortcutting it first in general however this enhancement tends to decrease the gap between the heuristic and optimal tour length from about to about at least for randomly generated euclidean instance local search heuristic for euclidean instance surprisingly good approximation to optimal tour can be obtained by iterative improvement algorithm which are also called local search the best known of these are the opt opt and lin kernighan these algorithm start with some initial tour constructed randomly or by some simpler approximation algorithm such a the nearest on each iteration the algorithm explores a neighborhood around the current tour by replacing a few edge in the current tour by other if the change produce a shorter tour the algorithm make it the current c c c c c c c c a b figure change a original b new tour and continues by exploring it neighborhood in the same manner otherwise the current tour is returned a the algorithm output and the algorithm the opt algorithm work by deleting a pair of nonadjacent edge in a tour and reconnecting their endpoint by the different pair of edge to obtain another tour see figure this operation is called the note that there is only one way to reconnect the endpoint because the alternative produce two disjoint example if we start with the nearest neighbor tour a b c d e a in the graph of figure whose length lnn is equal to the opt algorithm will move to the next tour a shown in figure to generalize the notion of the change one can consider the k change for any k this operation replaces up to k edge in a current in addition to change only the change have proved to be of practical the two principal possibility of change are shown in figure there are several other local search algorithm for the traveling salesman the most prominent of them is the lin kernighan algorithm lin which for two decade after it publication in wa considered the best algo rithm to obtain high quality approximation of optimal the lin kernighan algorithm is a variable opt algorithm it move can be viewed a a opt move followed by a sequence of opt because of it complexity we have to re frain from discussing this algorithm the excellent survey by johnson and mcgeoch joha contains an outline of the algorithm and it modern exten sion a well a method for it efficient this survey also contain result from the important empirical study about performance of many heuris tic for the traveling salesman problem including of course the lin kernighan we conclude our discussion by quoting some of these empirical result the traveling salesman problem ha been the subject of in tense study for the last this interest wa driven by a combination of pure a e a e l lnn b d b d c c a e a e l lnn b d b d c c a e a e l lnn b d b d c c a e a e l lnn new tour b d b d c c figure change from the nearest neighbor tour of the graph in figure c c c c c c c c c c c c c c c c c c a b c figure change a original b c new theoretical interest and serious practical need stemming from such newer ap plication a circuit board and vlsi chip fabrication x ray crystallography and genetic progress in developing effective heuristic their efficient im plementation by using sophisticated data structure and the ever increasing power of computer have led to a situation that differs drastically from a pessimistic pic ture painted by the worst case theoretical this is especially true for the most important application class of instance of the traveling salesman problem point in the two dimensional plane with the standard euclidean distance be tween nowadays euclidean instance with up to city can be solved exactly in quite a reasonable amount of time typically in minute or faster on a good workstation by such optimization package a concord in fact according to the information on the web site maintained by the author of that package the largest instance of the traveling salesman problem solved exactly a of january wa a tour through point in a vlsi it significantly ex ceeded the previous record of the shortest tour through all city in there should be little doubt that the latest record will also be eventually super seded and our ability to solve ever larger instance exactly will continue to this remarkable progress doe not eliminate the usefulness of approximation al gorithms for such problem first some application lead to instance that are still too large to be solved exactly in a reasonable amount of second one may well prefer spending second to find a tour that is within a few percent of optimum than to spend many hour or even day of computing time to find the shortest tour but how can one tell how good or bad the approximate solution is if we do not know the length of an optimal a convenient way to overcome this difficulty is to solve the linear programming problem describing the instance in question by ignoring the integrality this provides a lower bound called the held karp bound on the length of the shortest the held karp bound is typically very close le than to the length of an optimal tour and this bound can be computed in second or minute unless the instance is truly thus for a tour table average tour quality and running time for various heuristic on the city random uniform euclidean instance joha excess over the running time heuristic held karp bound second nearest neighbor multifragment christofides opt opt lin kernighan sa obtained by some heuristic we estimate the accuracy ratio rsa f saf s from above by the ratio f sah k where f sa is the length of the heuristic tour sa and h k is the held karp lower bound on the shortest tour the result see table from a large empirical study joha indicate the average tour quality and running time for the discussed the instance in the reported sample have city generated randomly and uniformly a integral coordinate point in the plane with the euclidean distance rounded to the nearest the quality of tour generated by the heuristic remain about the same for much larger instance up to a million city a long a they belong to the same type of the running time quoted are for expert implementation run on a compaq e with mhz alpha processor and gigabyte of main memory or it asymmetric instance of the traveling salesman problem those with a nonsymmetic matrix of intercity distance have proved to be significantly harder to solve both exactly and approximately than euclidean in partic ular exact optimal solution for many city asymmetric instance remained unknown at the time of the state of the art survey by johnson et approximation algorithm for the knapsack problem the knapsack problem another well known np hard problem wa also intro duced in section given n item of known weight w wn and value v vn and a knapsack of weight capacity w find the most valuable sub set of the item that fit into the we saw how this problem can be solved by exhaustive search section dynamic programming section we did not include the result for the twice around the tree heuristic because of the inferior quality of it approximation with the average excess of about nor did we quote the result for the most sophisticated local search heuristic with the average excess over optimum of le than a fraction of and branch and bound section now we will solve this problem by approx imation greedy algorithm for the knapsack problem we can think of several greedy approach to this one is to select the item in decreasing order of their weight however heavier item may not be the most valuable in the alternatively if we pick up the item in decreasing order of their value there is no guarantee that the knapsack capacity will be used can we find a greedy strategy that take into account both the weight and yes we can by computing the value to weight ratio viwi i n and selecting the item in decreasing order of these in fact we already used this approach in designing the branch and bound algorithm for the problem in section here is the algorithm based on this greedy greedy algorithm for the discrete knapsack problem step compute the value to weight ratio ri viwi i n for the item step sort the item in nonincreasing order of the ratio computed in step tie can be broken step repeat the following operation until no item is left in the sorted list if the current item on the list fit into the knapsack place it in the knapsack and proceed to the next item otherwise just proceed to the next example let u consider the instance of the knapsack problem with the knapsack capacity and the item information a follows item weight value computing the value to weight ratio and sorting the item in nonincreasing order of these efficiency ratio yield item weight value valueweight the greedy algorithm will select the first item of weight skip the next item of weight select the next item of weight and skip the last item of weight the solution obtained happens to be optimal for this instance see section where we solved the same instance by the branch and bound doe this greedy algorithm always yield an optimal the answer of course is no if it did we would have a polynomial time algorithm for the nphard in fact the following example show that no finite upper bound on the accuracy of it approximate solution can be given example item weight value valueweight the knapsack capacity is w w w since the item are already ordered a required the algorithm take the first item and skip the second one the value of this subset is the optimal selection consists of item whose value is hence the accuracy ratio rsa of this approximate solution is w which is unbounded it is surprisingly easy to tweak this greedy algorithm to get an approximation algorithm with a finite performance all it take is to choose the better of two alternative the one obtained by the greedy algorithm or the one consisting of a single item of the largest value that fit into the note that for the instance of the preceding example the second alternative is better than the first it is not difficult to prove that the performance ratio of this enhanced greedy algorithm is that is the value of an optimal subset s will never be more than twice a large a the value of the subset sa obtained by this enhanced greedy algorithm and is the smallest multiple for which such an assertion can be it is instructive to consider the continuous version of the knapsack problem a in this version we are permitted to take arbitrary fraction of the item for this version of the problem it is natural to modify the greedy algorithm a greedy algorithm for the continuous knapsack problem step compute the value to weight ratio viwi i n for the item step sort the item in nonincreasing order of the ratio computed in step tie can be broken step repeat the following operation until the knapsack is filled to it full capacity or no item is left in the sorted list if the current item on the list fit into the knapsack in it entirety take it and proceed to the next item otherwise take it largest fraction to fill the knapsack to it full capacity and for example for the four item instance used in example to illustrate the greedy algorithm for the discrete version the algorithm will take the first item of weight and then of the next item on the sorted list to fill the knapsack to it full it should come a no surprise that this algorithm always yield an optimal solution to the continuous knapsack indeed the item are ordered according to their efficiency in using the knapsack if the first item on the sorted list ha weight w and value v no solution can use w unit of capacity with a higher payoff than if we cannot fill the knapsack with the first item or it fraction we should continue by taking a much a we can of the second most efficient item and so a formal rendering of this proof idea is somewhat involved and we will leave it for the note also that the optimal value of the solution to an instance of the contin uous knapsack problem can serve a an upper bound on the optimal value of the discrete version of the same this observation provides a more sophisti cated way of computing upper bound for solving the discrete knapsack problem by the branch and bound method than the one used in section approximation scheme we now return to the discrete version of the knap sack for this problem unlike the traveling salesman problem there exist polynomial time approximation scheme which are parametric family of algo rithms that allow u to get approximation sak with any predefined accuracy level f s k for any instance of size n f sak where k is an integer parameter in the range k the first approximation scheme wa suggested by sahni in this algorithm generates all subset of k item or le and for each one that fit into the knapsack it add the remaining item a the greedy algorithm would do in nonincreasing order of their value to weight the subset of the highest value obtained in this fashion is returned a the algorithm example a small example of an approximation scheme with k is pro vided in figure the algorithm yield which is the optimal solution for this you can be excused for not being overly impressed by this and indeed the importance of this scheme is mostly theoretical rather than it lie in the fact that in addition to approximating the optimal solution with any predefined accuracy level the time efficiency of this algorithm is polynomial in indeed the total number of subset the algorithm generates before adding extra element is k n k nn n j k k j nj nk k j j j j item weight value valueweight subset added item value capacity w not feasible not feasible a b figure example of applying sahnis approximation scheme for k a b subset generated by the for each of those subset it need on time to determine the subset possible thus the algorithm efficiency is in note that although it is polynomial in n the time efficiency of sahnis scheme is exponential in more sophisticated approximation scheme called fully polynomial scheme do not have this among several book that discus such algorithm the monograph mar and kel are especially recommended for their wealth of other material about the knapsack exercise apply the nearest neighbor algorithm to the instance defined by the inter city distance matrix start the algorithm at the first city assuming that the city are numbered from to compute the accuracy ratio of this approximate write pseudocode for the nearest neighbor assume that it input is given by an n n intercity distance what is the time efficiency of the nearest neighbor apply the twice around the tree algorithm to the graph in figure with a walk around the minimum spanning tree that start at the same vertex a but differs from the walk in figure is the length of the obtained tour the same a the length of the tour in figure prove that making a shortcut of the kind used by the twice around the tree algorithm cannot increase the tour length in a euclidean what is the time efficiency class of the greedy algorithm for the knapsack prove that the performance ratio ra of the enhanced greedy algorithm for the knapsack problem is equal to consider the greedy algorithm for the bin packing problem which is called the first fit ff algorithm place each of the item in the order given into the first bin the item fit in when there are no such bin place the item in a new bin and add this bin to the end of the bin apply ff to the instance s s s s s and determine whether the solution obtained is determine the worst case time efficiency of prove that ff is a approximation the first fit decreasing ffd approximation algorithm for the bin packing problem start by sorting the item in nonincreasing order of their size and then act a the first fit apply ffd to the instance s s s s s and determine whether the solution obtained is doe ffd always yield an optimal justify your prove that ffd is a approximation run an experiment to determine which of the two algorithm ff or ffd yield more accurate approximation on a random sample of the problem design a simple approximation algorithm for finding a minimum vertex cover a vertex cover with the smallest number of vertex in a given consider the following approximation algorithm for finding a maximum independent set an independent set with the largest number of vertex in a given apply the approximation algorithm of part a and output all the vertex that are not in the obtained vertex can we claim that this algorithm is a approximation algorithm design a polynomial time greedy algorithm for the graph coloring prob show that the performance ratio of your approximation algorithm is in finitely algorithm for solving nonlinear equation in this section we discus several algorithm for solving nonlinear equation in one unknown f x there are several reason for this choice among subareas of numerical first of all this is an extremely important problem from both a practical and the oretical point of it arises a a mathematical model of numerous phenomenon in the science and engineering both directly and recall for example that the standard calculus technique for finding extremum point of a function f x is based on finding it critical point which are the root of the equation f x second it represents the most accessible topic in numerical analysis and at the same time exhibit it typical tool and third some meth od for solving equation closely parallel algorithm for array searching and hence provide example of applying general algorithm design technique to problem of continuous let u start with dispelling a misconception you might have about solving your experience with equation solving from middle school to calculus course might have led you to believe that we can solve equation by factoring or by applying a readily available sorry to break it to you but you have been deceived with the best of educational intention of course you were able to solve all those equation only because they had been carefully selected to make it in general we cannot solve equation exactly and need approximation algorithm to do this is true even for solving the quadratic equation ax bx c because the standard formula for it root x b b ac a requires computing the square root which can be done only approximately for most positive in addition a we discussed in section this canonical formula need to be modified to avoid the possibility of low accuracy what about formula for root of polynomial of degree higher than such formula for third and fourth degree polynomial exist but they are too cumbersome to be of practical for polynomial of degree higher than four there can be no general formula for their root that would involve only the polynomial coefficient arithmetical operation and radical taking this remarkable result wa published first by the italian mathematician and physician paolo ruffini in and rediscovered a quarter century later by the norwegian mathematician niels abel it wa developed further by the french mathematician evariste galois the impossibility of such a formula can hardly be considered a great disap a the great german mathematician carl friedrich gauss put it in his thesis of the algebraic solution of an equation wa no better than devising a symbol for the root of the equation and then saying that the equation had a root equal to the symbol we can interpret solution to equation a point at which the graph of the function f x intersects with the x the three algorithm we discus in this section take advantage of this of course the graph of f x may intersect the x axis at a single point x at multiple or even infinitely many point sin x or at no point ex equation would then have a single root several root and no root it is a good idea to sketch a graph of the function before starting to approximate it it can help to determine the number of root and their approximate in general it is a good idea to isolate root to identify interval containing a single root of the equation in bisection method this algorithm is based on an observation that the graph of a continuous function must intersect with the x axis between two point a and b at least once if the function value have opposite sign at these two point figure the validity of this observation is proved a a theorem in calculus course and we take it for granted it serf a the basis of the following algorithm called the bisection method for solving equation starting with an interval a b at whose endpoint f x ha opposite sign the algorithm computes the value of f x at the middle point xmid a if f xmid a root wa found and the algorithm otherwise it continues the search for a root either on a xmid or on xmid b depending on which of the two half the value of f x have opposite sign at the endpoint of the new since we cannot expect the bisection algorithm to stumble on the exact value of the equation root and stop we need a different criterion for stopping the algo ruffinis discovery wa completely ignored by almost all prominent mathematician of that abel died young after a difficult life of galois wa killed in a duel when he wa only year their result on the solution of higher degree equation are now considered to be among the crowning achievement in the history of f x a x b x figure first iteration of the bisection method x is the middle point of interval a we can stop the algorithm after the interval an bn bracketing some root x becomes so small that we can guarantee that the absolute error of approximating x by xn the middle point of this interval is smaller than some small preselected number since xn is the middle point of an bn and x lie within this interval a well we have xn x bn an hence we can stop the algorithm a soon a bn an or equivalently xn an it is not difficult to prove that xn x b a for n n this inequality implies that the sequence of approximation xn can be made a close to root x a we wish by choosing n large in other word we can say that xn converges to root note however that because any digital computer represents extremely small value by zero section the convergence assertion is true in theory but not necessarily in in fact if we choose below a certain machine dependent threshold the algorithm may never another source of potential complication is round off error in computing value of the function in therefore it is a good practice to include in a program implementing the bisection method a limit on the number of iteration the algorithm is allowed to here is pseudocode of the bisection algorithm bisectionf x a b eps n implement the bisection method for finding a root of f x input two real number a and b a b a continuous function f x on a b f af b an upper bound on the absolute error eps an upper bound on the number of iteration n output an approximate or exact value x of a root in a b or an interval bracketing the root if the iteration number limit is reached n iteration count while n n do x a b if x a eps return x fval f x if fval return x if fval f a bx else a x nn return iteration limit a b note that we can use inequality to find in advance the number of iteration that should suffice at least in theory to achieve a preselected accuracy indeed choosing the number of iteration n large enough to satisfy b an n log b a doe the example let u consider equation x x it ha one real see figure for the graph of f x x x since f and f the root must lie within interval if we choose the error tolerance level a inequality would require n log or n figure contains a trace of the first eight iteration of the bisection method applied to equation thus we obtained x a an approximate value for the root x of equation and we can guarantee that x moreover if we take into account the sign of the function f x at a b and x we can assert that the root lie between and the principal weakness of the bisection method a a general algorithm for solving equation is it slow rate of convergence compared with other known it is for this reason that the method is rarely also it cannot be extended to solving more general equation and system of but it doe have several strong it always converges to a root whenever we start with an y fx x x x figure graph of function f x x x n an bn xn f xn figure trace of the bisection method for solving equation the sign after the number in the second and third column indicate the sign of f x x x at the corresponding endpoint of the interval whose property are very easy to and it doe not use derivative of the function f x a some faster method what important algorithm doe the method of bisection remind you if you have found it to closely resemble binary search you are both of them solve variation of the searching problem and they are both divide byhalf the principal difference lie in the problem domain discrete for binary search and continuous for the bisection also note that while binary search requires it input array to be sorted the bisection method doe not require it function to be nondecreasing or finally whereas binary search is very fast the bisection method is relatively fx an xn bn x figure iteration of the method of false method of false position the method of false position also known by it name in latin regula falsi is to interpolation search a the bisection method is to binary like the bisection method it ha on each iteration some interval an bn bracketing a root of a continuous function f x that ha opposite sign value at an and unlike the bisection method however it computes the next root approximation not a the middle of an bn but a the x intercept of the straight line through the point an f an and bn f bn figure you are asked in the exercise to show that the formula for this x intercept can be written a xn anf bn bnf an f bn f an example figure contains the result of the first eight iteration of this method for solving equation although for this example the method of false position doe not perform a well a the bisection method for many instance it yield a faster converging newton method newton method also called the newton raphson method is one of the most im portant general algorithm for solving when applied to equation in one unknown it can be illustrated by figure the next element xn of the method approximation sequence is obtained a the x intercept of the tangent line to the graph of function f x at the analytical formula for the element of the approximation sequence turn out to be xn xn f xn for n f xn an bn xn f xn figure trace of the method of false position for equation the sign after the number in the second and third column indicate the sign of f x x x at the corresponding endpoint of the fxn xn xn x figure iteration of newton in most case newton algorithm guarantee convergence of sequence if an initial approximation x is chosen close enough to the precisely defined prescription for choosing x can be found in numerical analysis it may converge for initial approximation far from the root a well but this is not always example computing a for a can be done by finding a nonnegative root of equation x a if we use formula for this case of f x x a and f x x we obtain xn xn f xn xn xn a xn a a xn f xn xn xn xn which is exactly the formula we used in section for computing approximate value of square example let u apply newton method to equation which we previ ously solved with the bisection method and the method of false formula for this case becomes xn xn xn xn xn a an initial element of the approximation sequence we take say x fig ure contains the result of the first five iteration of newton you cannot fail to notice how much faster newton approximation sequence converges to the root than the approximation sequence of both the bisection method and the method of false this very fast convergence is typical of newton method if an initial approximation is close to the equation note however that on each iteration of this method we need to evaluate new value of the function and it derivative whereas the previous two method require only one new value of the function also newton method doe not bracket a root a these two method moreover for an arbitrary function and arbitrarily chosen initial approximation it approximation sequence may and because formula ha the function derivative in the denominator the method may break down if it is equal to in fact newton method is most effective when f x is bounded away from zero near root in particular if f x m on the interval between xn and x we can estimate the distance between xn and x by using the mean value theorem of calculus a follows f xn f x f cxn x where c is some point between xn and since f x and f c m we obtain n xn xn f xn figure trace of newton method for equation xn x f xn m formula can be used a a criterion for stopping newton algorithm when it right hand side becomes smaller than a preselected accuracy level other possible stopping criterion are xn xn and f xn where is a small positive since the last two criterion do not necessarily imply closeness of xn to root x they should be considered inferior to the one based on the shortcoming of newton method should not overshadow it principal strength fast convergence for an appropriately chosen initial approximation and applicability to much more general type of equation and system of asymptotic notation and basic efficiency class a pointed out in the previous section the efficiency analysis framework con centrates on the order of growth of an algorithm basic operation count a the principal indicator of the algorithm to compare and rank such order of growth computer scientist use three notation o big oh big omega and big first we introduce these notation informally and then after sev eral example formal definition are in the following discussion t n and gn can be any nonnegative function defined on the set of natural in the context we are interested in t n will be an algorithm running time usually indicated by it basic operation count cn and gn will be some simple function to compare the count informal introduction informally ogn is the set of all function with a lower or same order of growth a gn to within a constant multiple a n go to thus to give a few example the following assertion are all true n on n on nn indeed the first two function are linear and hence have a lower order of growth than gn n while the last one is quadratic and hence ha the same order of growth a on the other hand n on on n n indeed the function n and are both cubic and hence have a higher order of growth than n and so ha the fourth degree polynomial n n the second notation gn stand for the set of all function with a higher or same order of growth a gn to within a constant multiple a n go to for example n n nn n but n finally gn is the set of all function that have the same order of growth a gn to within a constant multiple a n go to thus every quadratic function an bn c with a is in n but so are among infinitely many others n sin n and n log can you explain hopefully this informal introduction ha made you comfortable with the idea behind the three asymptotic so now come the formal o notation definition a function t n is said to be in ogn denoted t n ogn if t n is bounded above by some constant multiple of gn for all large n if there exist some positive constant c and some nonnegative integer n such that t n cgn for all n the definition is illustrated in figure where for the sake of visual clarity n is extended to be a real a an example let u formally prove one of the assertion made in the introduction n indeed n n n for all n n thus a value of the constant c and n required by the definition we can take and note that the definition give u a lot of freedom in choosing specific value for constant c and for example we could also reason that n n n for all n n to complete the proof with c and n cg n t n doesnt matter n n figure big oh notation t n t n cg n doesnt matter n n figure big omega notation t n notation definition a function t n is said to be in gn denoted t n gn if t n is bounded below by some positive constant multiple of gn for all large n if there exist some positive constant c and some nonnegative integer n such that t n cgn for all n the definition is illustrated in figure here is an example of the formal proof that n n n n for all n we can select c and n cg n t n cg n doesnt matter n n figure big theta notation t n notation definition a function t n is said to be in gn denoted t n gn if t n is bounded both above and below by some positive constant multiple of gn for all large n if there exist some positive constant c and c and some nonnegative integer n such that cgn t n cgn for all n the definition is illustrated in figure for example let u prove that nn first we prove the right inequality the upper bound nn n n n for all n second we prove the left inequality the lower bound nn n n n n n for all n hence we can select c c and n useful property involving the asymptotic notation using the formal definition of the asymptotic notation we can prove their general property see problem in this section exercise for a few simple the following property in particular is useful in analyzing algorithm that comprise two consecutively executed theorem if tn ogn and tn ogn then tn tn omaxgn the analogous assertion are true for the and notation a proof the proof extends to order of growth the following simple fact about four arbitrary real number a b a b if a b and a b then a a maxb since tn ogn there exist some positive constant c and some non negative integer n such that tn cgn for all n similarly since tn ogn tn cgn for all n let u denote c maxc c and consider n maxn n so that we can use both adding them yield the following tn tn cgn cgn cgn cgn cgn gn c maxgn hence tn tn omaxgn gn with the constant c and n required by the o definition being c maxc c and maxn n so what doe this property imply for an algorithm that comprises two consec utively executed it implies that the algorithm overall efficiency is deter mined by the part with a higher order of growth it least efficient part tn ogn tn tn omaxgn tn ogn for example we can check whether an array ha equal element by the following two part algorithm first sort the array by applying some known sorting algorithm second scan the sorted array to check it consecutive element for if for example a sorting algorithm used in the first part make no more than nn comparison and hence is in on while the second part make no more than n comparison and hence is in on the efficiency of the entire algorithm will be in omaxn n using limit for comparing order of growth though the formal definition of o and are indispensable for proving their abstract property they are rarely used for comparing the order of growth of two specific a much more convenient method for doing so is based on computing the limit of the ratio of two function in three principal case may arise implies that t n ha a smaller order of growth than gn t n lim c implies that t n ha the same order of growth a gn n gn implies that t n ha a larger order of growth than note that the first two case mean that t n ogn the last two mean that t n gn and the second case mean that t n the limit based approach is often more convenient than the one based on the definition because it can take advantage of the powerful calculus technique developed for computing limit such a lho pitals rule lim t n lim t n n gn n g n and stirlings formula n n n e for large value of here are three example of using the limit based approach to comparing order of growth of two example compare the order of growth of nn and this is one of the example we used at the beginning of this section to illustrate the nn n n lim lim lim n n n n n n since the limit is equal to a positive constant the function have the same order of growth or symbolically nn example compare the order of growth of log n and unlike exam ple the answer here is not immediately log n log n log e lim lim lim n log e lim n n n n n n n n since the limit is equal to zero log n ha a smaller order of growth than since limn log n we can use the so called little oh notation log n o n unlike the big oh the little oh notation is rarely used in analysis of the fourth case in which such a limit doe not exist rarely happens in the actual practice of analyzing still this possibility make the limit based approach to comparing order of growth le general than the one based on the definition of o and example compare the order of growth of and we discussed this informally in section taking advantage of stirlings formula we get n n n nn n lim lim e lim n lim n n n n n n n nen n e thus though n grows very fast still we can write symbolically that n note however that while the big omega notation doe not preclude the possibility that and n have the same order of growth the limit computed here certainly basic efficiency class even though the efficiency analysis framework put together all the function whose order of growth differ by a constant multiple there are still infinitely many such for example the exponential function an have different order of growth for different value of base therefore it may come a a surprise that the time efficiency of a large number of algorithm fall into only a few these class are listed in table in increasing order of their order of growth along with their name and a few you could raise a concern that classifying algorithm by their asymptotic effi ciency would be of little practical use since the value of multiplicative constant are usually left this leaf open the possibility of an algorithm in a worse efficiency class running faster than an algorithm in a better efficiency class for input of realistic for example if the running time of one algorithm is n while the running time of the other is n the cubic algorithm will outperform the quadratic algorithm unless n exceeds a few such anomaly are indeed fortunately multiplicative constant usually do not differ that a a rule you should expect an algorithm from a better asymptotic efficiency class to outperform an algorithm from a worse class even for moderately sized this observation is especially true for an algorithm with a better than exponential running time versus an exponential or worse exercise use the most appropriate notation among o and to indicate the time efficiency class of sequential search see section in the worst in the best in the average use the informal definition of o and to determine whether the follow ing assertion are true or table basic asymptotic efficiency class class name comment constant short of best case efficiency very few reasonable example can be given since an algorithm running time typically go to infinity when it input size grows infinitely log n logarithmic typically a result of cutting a problem size by a constant factor on each iteration of the algorithm see section note that a logarithmic algorithm cannot take into account all it input or even a fixed fraction of it any algorithm that doe so will have at least linear running n linear algorithm that scan a list of size n sequential search belong to this n log n linearithmic many divide and conquer algorithm see chapter including mergesort and quicksort in the average case fall into this n quadratic typically characterizes efficiency of algorithm with two embedded loop see the next elemen tary sorting algorithm and certain operation on n n matrix are standard n cubic typically characterizes efficiency of algorithm with three embedded loop see the next several nontrivial algorithm from linear algebra fall into this n exponential typical for algorithm that generate all subset of an n element often the term exponential is used in a broader sense to include this and larger order of growth a factorial typical for algorithm that generate all permutation of an n element nn on nn on nn n nn n for each of the following function indicate the class gn the function belongs use the simplest gn possible in your prove your n n n n lgn n lg n n n log n table contains value of several function that often arise in the analysis of these value certainly suggest that the function log n n n log n n n n are listed in increasing order of their order of do these value prove this fact with mathematical prove that the function are indeed listed in increasing order of their order of list the following function according to their order of growth from the lowest to the highest n lgn n n ln n n prove that every polynomial of degree k pn aknk ak nk a with ak belongs to prove that exponential function an have different order of growth for different value of base a prove the following assertion by using the definition of the notation in volved or disprove them by giving a specific if t n ogn then gn t gn gn where gn ogn for any two nonnegative function t n and gn defined on the set of nonnegative integer either t n ogn or t n gn or prove the section theorem for we mentioned in this section that one can check whether all element of an array are distinct by a two part algorithm based on the array if the presorting is done by an algorithm with a time efficiency in n log n what will be a time efficiency class of the entire if the sorting algorithm used for presorting need an extra array of size n what will be the space efficiency class of the entire the range of a finite nonempty set of n real number s is defined a the differ ence between the largest and smallest element of for each representation of s given below describe in english an algorithm to compute the indi cate the time efficiency class of these algorithm using the most appropriate notation o or an unsorted array a sorted array a sorted singly linked list a binary search tree lighter or you have n identical looking coin and a two pan balance scale with no one of the coin is a fake but you do not know whether it is lighter or heavier than the genuine coin which all weigh the design a algorithm to determine whether the fake coin is lighter or heavier than the door in a wall you are facing a wall that stretch infinitely in both direc there is a door in the wall but you know neither how far away nor in which you can see the door only when you are right next to de sign an algorithm that enables you to reach the door by walking at most on step where n is the unknown to you number of step between your initial position and the par mathematical analysis of nonrecursive algorithm in this section we systematically apply the general framework outlined in section to analyzing the time efficiency of nonrecursive let u start with a very simple example that demonstrates all the principal step typically taken in analyzing such example consider the problem of finding the value of the largest element in a list of n for simplicity we assume that the list is implemented a an the following is pseudocode of a standard algorithm for solving the algorithm determines the value of the largest element in a given array input an array of real number output the value of the largest element in a maxval a for i to n do if ai maxval maxval ai return maxval the obvious measure of an input size here is the number of element in the array the operation that are going to be executed most often are in the algorithm for there are two operation in the loop body the comparison ai maxval and the assignment maxval which of these two operation should we consider since the comparison is executed on each repetition of the loop and the assignment is not we should consider the comparison to be the algorithm basic note that the number of comparison will be the same for all array of size n therefore in term of this metric there is no need to distinguish among the worst average and best case let u denote cn the number of time this comparison is executed and try to find a formula expressing it a a function of size the algorithm make one comparison on each execution of the loop which is repeated for each value of the loop variable i within the bound and n therefore we get the following sum for cn n cn i this is an easy sum to compute because it is nothing other than repeated n thus n cn n i here is a general plan to follow in analyzing nonrecursive general plan for analyzing the time efficiency of nonrecursive algorithm decide on a parameter or parameter indicating an input identify the algorithm basic a a rule it is located in the inner most check whether the number of time the basic operation is executed depends only on the size of an if it also depends on some additional property the worst case average case and if necessary best case efficiency have to be investigated set up a sum expressing the number of time the algorithm basic operation is using standard formula and rule of sum manipulation either find a closed form formula for the count or at the very least establish it order of before proceeding with further example you may want to review appen dix a which contains a list of summation formula and rule that are often useful in analysis of in particular we use especially frequently two basic rule of sum manipulation u u cai c ai r il il u u u ai bi ai bi r il il il sometimes an analysis of a nonrecursive algorithm requires setting up not a sum but a recurrence relation for the number of time it basic operation is using recurrence relation is much more typical for analyzing recursive algorithm see section and two summation formula u u l where l u are some lower and upper integer limit s il n n i n nn n i s i i note that the formula n n which we used in example is a special i case of formula s for l and u n example consider the element uniqueness problem check whether all the element in a given array of n element are this problem can be solved by the following straightforward algorithm determines whether all the element in a given array are distinct input an array output return true if all the element in a are distinct and false otherwise for i to n do for j i to n do if ai aj return false return true the natural measure of the input size here is again n the number of element in the since the innermost loop contains a single operation the comparison of two element we should consider it a the algorithm basic note however that the number of element comparison depends not only on n but also on whether there are equal element in the array and if there are which array position they we will limit our investigation to the worst case by definition the worst case input is an array for which the number of element comparison cworstn is the largest among all array of size an inspection of the innermost loop reveals that there are two kind of worst case input input for which the algorithm doe not exit the loop prematurely array with no equal element and array in which the last two element are the only pair of equal for such input one comparison is made for each repetition of the innermost loop for each value of the loop variable j between it limit i and n this is repeated for each value of the outer loop for each value of the loop variable i between it limit and n accordingly we get n n n n cworst n n i n i i j i i i n n n n n n i n i i i n n n n n n we also could have computed the sum ni n i faster a follows n n n n i n n i where the last equality is obtained by applying summation formula note that this result wa perfectly predictable in the worst case the algorithm need to compare all nn distinct pair of it n example given two n n matrix a and b find the time efficiency of the definition based algorithm for computing their product c by definition c is an n n matrix whose element are computed a the scalar dot product of the row of matrix a and the column of matrix b a b c row i c i j j where ci j ai b j ai kbk j ai n bn j for every pair of index i j n algorithm multiplies two square matrix of order n by the definition based algorithm input two n n matrix a and b output matrix c ab for i to n do for j to n do ci j for k to n do ci j ci j ai k bk j return c we measure an input size by matrix order there are two arithmetical operation in the innermost loop here multiplication and addition that in principle can compete for designation a the algorithm basic actually we do not have to choose between them because on each repetition of the innermost loop each of the two is executed exactly so by counting one we automatically count the still following a well established tradition we consider multiplication a the basic operation see section let u set up a sum for the total number of multiplication mn executed by the since this count depends only on the size of the input matrix we do not have to investigate the worst case average case and best case efficiency obviously there is just one multiplication executed on each repetition of the algorithm innermost loop which is governed by the variable k ranging from the lower bound to the upper bound n therefore the number of multiplication made for every pair of specific value of variable i and j is n k and the total number of multiplication mn is expressed by the following triple sum n n n mn i j k now we can compute this sum by using formula s and rule r given starting with the innermost sum n which is equal to n we get k n n n n n n mn n n i j k i j i this example is simple enough so that we could get this result without all the summation the algorithm computes n element of the product each of the product element is computed a the scalar dot product of an n element row of the first matrix and an n element column of the second matrix which take n so the total number of multiplication is n n it is this kind of reasoning that we expected you to employ when answering this question in problem of exercise if we now want to estimate the running time of the algorithm on a particular machine we can do it by the product t n cmmn cmn where cm is the time of one multiplication on the machine in we would get a more accurate estimate if we took into account the time spent on the addition too t n cmmn caan cmn can cm can where ca is the time of one note that the estimate differ only by their multiplicative constant and not by their order of you should not have the erroneous impression that the plan outlined above always succeeds in analyzing a nonrecursive an irregular change in a loop variable a sum too complicated to analyze and the difficulty intrinsic to the average case analysis are just some of the obstacle that can prove to be insur these caveat notwithstanding the plan doe work for many simple nonrecursive algorithm a you will see throughout the subsequent chapter of the a a last example let u consider an algorithm in which the loop variable change in a different manner from that of the previous example the following algorithm find the number of binary digit in the binary representation of a positive decimal algorithm binaryn input a positive decimal integer n output the number of binary digit in n binary representation count while n do count count n n return count first notice that the most frequently executed operation here is not inside the while loop but rather the comparison n that determines whether the loop body will be since the number of time the comparison will be executed is larger than the number of repetition of the loop body by exactly the choice is not that a more significant feature of this example is the fact that the loop variable take on only a few value between it lower and upper limit therefore we have to use an alternative way of computing the number of time the loop is since the value of n is about halved on each repetition of the loop the answer should be about log the exact formula for the number of time the comparison n will be executed is actually log n the number of bit in the binary representation of n according to formula we could also get this answer by applying the analysis technique based on recurrence relation we discus this technique in the next section because it is more pertinent to the analysis of recursive exercise compute the following n n i n ii i i i n j n n ij n ii j i j i find the order of growth of the following use the gn notation with the simplest function gn ni i n lg i i ij i j n ini i i the sample variance of n measurement x xn can be computed a either inxi x n xi where x i n n or n xi n xi n i i n find and compare the number of division multiplication and addition subtraction addition and subtraction are usually bunched together that are required for computing the variance according to each of these consider the following algorithm mysteryn input a nonnegative integer n s for i to n do ssii return s what doe this algorithm what is it basic how many time is the basic operation what is the efficiency class of this suggest an improvement or a better algorithm altogether and indicate it efficiency if you cannot do it try to prove that in fact it cannot be consider the following algorithm input an array of n real number minval a maxval a for i to n do if ai minval minval ai if ai maxval maxval ai return maxval minval answer question ae of problem about this consider the following algorithm input a matrix of real number for i to n do for j i to n do if ai j aj i return false return true answer question ae of problem about this improve the implementation of the matrix multiplication algorithm see ex ample by reducing the number of addition made by the what effect will this change have on the algorithm determine the asymptotic order of growth for the total number of time all the door are toggled in the locker door puzzle problem in exercise prove the formula n i n nn i either by mathematical induction or by following the insight of a year old school boy named carl friedrich gauss who grew up to become one of the greatest mathematician of all mental arithmetic a table is filled with repeating number on it diagonal a shown calculate the total sum of the table number in your head after cra question consider the following version of an important algorithm that we will study later in the algorithm input an n n matrix of real number for i to n do for j i to n do for k i to n do aj k aj k ai k aj i ai i find the time efficiency class of this what glaring inefficiency doe this pseudocode contain and how can it be eliminated to speed the algorithm von neumann neighborhood consider the algorithm that start with a single square and on each of it n iteration add new square all around the how many one by one square are there after n gar in the parlance of cellular automaton theory the answer is the number of cell in the von neumann neighborhood of range the result for n and are illustrated n n n page numbering find the total number of decimal digit needed for num bering page in a book of assume that the page are numbered consecutively starting with mathematical analysis of recursive algorithm in this section we will see how to apply the general framework for analysis of algorithm to recursive we start with an example often used to introduce novice to the idea of a recursive example compute the factorial function f n for an arbitrary nonneg ative integer since n n n n for n and by definition we can compute f n f n n with the following recursive algorithm fn computes recursively input a nonnegative integer n output the value of if n return else return f n n for simplicity we consider n itself a an indicator of this algorithm input size rather than the number of bit in it binary the basic operation of the algorithm is multiplication whose number of execution we denote since the function f n is computed according to the formula f n f n n for n alternatively we could count the number of time the comparison n is executed which is the same a counting the total number of call made by the algorithm see problem in this section the number of multiplication mn needed to compute it must satisfy the equality mn mn for n to compute to multiply f n f n by n indeed mn multiplication are spent to compute f n and one more multiplication is needed to multiply the result by the last equation defines the sequence mn that we need to this equation defines mn not explicitly a a function of n but implicitly a a function of it value at another point namely n such equation are called recurrence relation or for brevity recurrence relation play an important role not only in analysis of algorithm but also in some area of applied they are usually studied in detail in course on discrete mathematics or discrete structure a very brief tutorial on them is provided in appendix our goal now is to solve the recurrence relation mn mn to find an explicit formula for mn in term of n note however that there is not one but infinitely many sequence that satisfy this can you give example of say two of to determine a solution uniquely we need an initial condition that tell u the value with which the sequence we can obtain this value by inspecting the condition that make the algorithm stop it recursive call if n return this tell u two first since the call stop when n the smallest value of n for which this algorithm is executed and hence mn defined is second by inspecting the pseudocodes exiting line we can see that when n the algorithm performs no therefore the initial condition we are after is m the call stop when n no multiplication when n thus we succeeded in setting up the recurrence relation and initial condition for the algorithm number of multiplication mn mn mn for n m before we embark on a discussion of how to solve this recurrence let u pause to reiterate an important we are dealing here with two recursively defined the first is the factorial function f n itself it is defined by the recurrence f n f n n for every n f the second is the number of multiplication mn needed to compute f n by the recursive algorithm whose pseudocode wa given at the beginning of the a we just showed mn is defined by recurrence and it is recurrence that we need to solve though it is not difficult to guess the solution here what sequence start with when n and increase by on each it will be more useful to arrive at it in a systematic from the several technique available for solving recurrence relation we use what can be called the method of backward the method idea and the reason for the name is immediately clear from the way it applies to solving our particular recurrence mn mn substitute mn mn mn mn substitute mn mn mn mn after inspecting the first three line we see an emerging pattern which make it possible to predict not only the next line what would it but also a general formula for the pattern mn mn i strictly speaking the correctness of this formula should be proved by mathematical induction but it is easier to get to the solution a follows and then verify it what remains to be done is to take advantage of the initial condition since it is specified for n we have to substitute i n in the pattern formula to get the ultimate result of our backward substitution mn mn mn i i mn n n you should not be disappointed after exerting so much effort to get this obvious the benefit of the method illustrated in this simple example will become clear very soon when we have to solve more difficult also note that the simple iterative algorithm that accumulates the product of n consecutive integer requires the same number of multiplication and it doe so without the overhead of time and space used for maintaining the recursion the issue of time efficiency is actually not that important for the problem of computing a we saw in section the function value get so large so fast that we can realistically compute exact value of only for very small again we use this example just a a simple and convenient vehicle to introduce the standard approach to analyzing recursive generalizing our experience with investigating the recursive algorithm for computing we can now outline a general plan for investigating recursive algo general plan for analyzing the time efficiency of recursive algorithm decide on a parameter or parameter indicating an input identify the algorithm basic check whether the number of time the basic operation is executed can vary on different input of the same size if it can the worst case average case and best case efficiency must be investigated set up a recurrence relation with an appropriate initial condition for the number of time the basic operation is solve the recurrence or at least ascertain the order of growth of it example a our next example we consider another educational workhorse of recursive algorithm the tower of hanoi in this puzzle we or mythical monk if you do not like to move disk have n disk of different size that can slide onto any of three initially all the disk are on the first peg in order of size the largest on the bottom and the smallest on the goal is to move all the disk to the third peg using the second one a an auxiliary if we can move only one disk at a time and it is forbidden to place a larger disk on top of a smaller the problem ha an elegant recursive solution which is illustrated in figure to move n disk from peg to peg with peg a auxiliary we first move recursively n disk from peg to peg with peg a auxiliary then move the largest disk directly from peg to peg and finally move recursively n disk from peg to peg using peg a of course if n we simply move the single disk directly from the source peg to the destination figure recursive solution to the tower of hanoi let u apply the general plan outlined above to the tower of hanoi the number of disk n is the obvious choice for the input size indicator and so is moving one disk a the algorithm basic clearly the number of move mn depends on n only and we get the following recurrence equation for it mn mn mn for n with the obvious initial condition m we have the following recurrence relation for the number of move mn mn mn for n m we solve this recurrence by the same method of backward substitution mn mn mn mn mn mn mn mn mn mn the pattern of the first three sum on the left suggests that the next one will be mn and generally after i substitution we get mn imn i i i imn i i since the initial condition is specified for n which is achieved for i n we get the following formula for the solution to recurrence mn n mn n n n m n n n n thus we have an exponential algorithm which will run for an unimaginably long time even for moderate value of n see problem in this section this is not due to the fact that this particular algorithm is poor in fact it is not difficult to prove that this is the most efficient algorithm possible for this it is the problem intrinsic difficulty that make it so computationally still this example make an important general point one should be careful with recursive algorithm because their succinctness may mask their when a recursive algorithm make more than a single call to itself it can be useful for analysis purpose to construct a tree of it recursive in this tree node correspond to recursive call and we can label them with the value of the parameter or more generally parameter of the for the tower of hanoi example the tree is given in figure by counting the number of node in the tree we can get the total number of call made by the tower of hanoi algorithm n cn l where l is the level in the tree in figure n l n n n n n n n figure tree of recursive call made by the recursive algorithm for the tower of hanoi the number agrees a it should with the move count obtained example a our next example we investigate a recursive version of the algorithm discussed at the end of section algorithm binrecn input a positive decimal integer n output the number of binary digit in n binary representation if n return else return binrec n let u set up a recurrence and an initial condition for the number of addition an made by the the number of addition made in computing binrec n is a n plus one more addition is made by the algorithm to increase the returned value by this lead to the recurrence an a n for n since the recursive call end when n is equal to and there are no addition made then the initial condition is a the presence of n in the function argument make the method of backward substitution stumble on value of n that are not power of therefore the standard approach to solving such a recurrence is to solve it only for n k and then take advantage of the theorem called the smoothness rule see appendix b which claim that under very broad assumption the order of growth observed for n k give a correct answer about the order of growth for all value of alternatively after getting a solution for power of we can sometimes fine tune this solution to get a formula valid for an arbitrary so let u apply this recipe to our recurrence which for n k take the form ak ak for k a now backward substitution encounter no problem ak ak substitute ak ak ak ak substitute ak ak ak ak ak i i ak k thus we end up with ak a k k or after returning to the original variable n k and hence k log n an log n log in fact one can prove problem in this section exercise that the exact solution for an arbitrary value of n is given by just a slightly more refined formula an log n this section provides an introduction to the analysis of recursive these technique will be used throughout the book and expanded further a in the next section we discus the fibonacci number their analysis involves more difficult recurrence relation to be solved by a method different from backward exercise solve the following recurrence xn xn for n x xn xn for n x xn xn n for n x xn xn n for n x solve for n k xn xn for n x solve for n k set up and solve a recurrence relation for the number of call made by f n the recursive algorithm for computing consider the following recursive algorithm for computing the sum of the first n cube sn algorithm sn input a positive integer n output the sum of the first n cube if n return else return sn n n n set up and solve a recurrence relation for the number of time the algo rithms basic operation is how doe this algorithm compare with the straightforward nonrecursive algorithm for computing this consider the following recursive algorithm qn input a positive integer n if n return else return qn n set up a recurrence relation for this function value and solve it to deter mine what this algorithm set up a recurrence relation for the number of multiplication made by this algorithm and solve set up a recurrence relation for the number of additionssubtractions made by this algorithm and solve tower of hanoi in the original version of the tower of hanoi puzzle a it wa published in the s by e douard lucas a french mathematician the world will end after disk have been moved from a mystical tower of estimate the number of year it will take if monk could move one disk per assume that monk do not eat sleep or how many move are made by the ith largest disk i n in this find a nonrecursive algorithm for the tower of hanoi puzzle and imple ment it in the language of your restricted tower of hanoi consider the version of the tower of hanoi puzzle in which n disk have to be moved from peg a to peg c using peg b so that any move should either place a disk on peg b or move a disk from that of course the prohibition of placing a larger disk on top of a smaller one remains in place design a recursive algorithm for this problem and find the number of move made by prove that the exact number of addition made by the recursive algorithm binrecn for an arbitrary positive decimal integer n is log n set up a recurrence relation for the number of addition made by the nonrecursive version of this algorithm see section example and solve design a recursive algorithm for computing n for any nonnegative integer n that is based on the formula n n n set up a recurrence relation for the number of addition made by the algorithm and solve draw a tree of recursive call for this algorithm and count the number of call made by the is it a good algorithm for solving this consider the following recursive algorithm input an array of real number if n return a else temp if temp an return temp else return an what doe this algorithm set up a recurrence relation for the algorithm basic operation count and solve consider the following algorithm to check whether a graph defined by it adjacency matrix is algorithm input adjacency matrix of an undirected graph g output true if g is complete and false otherwise if n return one vertex graph is complete by definition else if not return else for j to n do if an j return return what is the algorithm efficiency class in the worst the determinant of an n n matrix a a n a a an an n denoted det a can be defined a a for n and for n by the recursive formula n det a sj a j det aj j where sj is if j is even and if j is odd a j is the element in row and column j and aj is the n n matrix obtained from matrix a by deleting it row and column j set up a recurrence relation for the number of multiplication made by the algorithm implementing this recursive without solving the recurrence what can you say about the solution order of growth a compared to von neumann neighborhood revisited find the number of cell in the von neumann neighborhood of range n problem in exercise by setting up and solving a recurrence frying hamburger there are n hamburger to be fried on a small grill that can hold only two hamburger at a each hamburger ha to be fried on both side frying one side of a hamburger take minute regardless of whether one or two hamburger are fried at the same consider the following recursive algorithm for executing this task in the minimum amount of if n fry the hamburger or the two hamburger together on each if n fry any two hamburger together on each side and then apply the same procedure recursively to the remaining n set up and solve the recurrence for the amount of time this algorithm need to fry n explain why this algorithm doe not fry the hamburger in the minimum amount of time for all n give a correct recursive algorithm that executes the task in the minimum amount of celebrity problem a celebrity among a group of n people is a person who know nobody but is known by everybody the task is to identify a celebrity by only asking question to people of the form do you know design an efficient algorithm to identify a celebrity or determine that the group ha no such how many question doe your algorithm need in the worst example computing the nth fibonacci number in this section we consider the fibonacci number a famous sequence that can be defined by the simple recurrence f n f n f n for n and two initial condition f f the fibonacci number were introduced by leonardo fibonacci in a a solution to a problem about the size of a rabbit population problem in this section many more example of fibonacci like number have since been discovered in the natural world and they have even been used in predicting the price of stock and there are some interesting application of the fibonacci number in computer science a for example worst case input for euclid algorithm discussed in section happen to be consecutive element of the fibonacci in this section we briefly consider algorithm for computing the nth element of this among other benefit the discussion will provide u with an opportunity to introduce another method for solving recurrence relation useful for analysis of recursive to start let u get an explicit formula for f if we try to apply the method of backward substitution to solve recurrence we will fail to get an easily discernible instead we can take advantage of a theorem that describes solution to a homogeneous second order linear recurrence with constant co efficients axn bxn cxn where a b and c are some fixed real number a called the coefficient of the recurrence and xn is the generic term of an unknown sequence to be applying this theorem to our recurrence with the initial condition given see appendix b we obtain the formula f n n n where and it is hard to believe that formula which includes arbitrary integer power of irrational number yield nothing else but all the element of fibonacci sequence but it one of the benefit of formula is that it immediately implies that f n grows exponentially remember fibonaccis f n this constant is known a the golden since antiquity it ha been considered the most pleasing ratio of a rectangle two side to the human eye and might have been consciously used by ancient architect and follows from the observation that is a fraction between and and hence n get infinitely small a n go to in fact one can prove that the impact of the second term n on the value of f n can be obtained by rounding off the value of the first term to the nearest in other word for every nonnegative integer n f n n rounded to the nearest in the algorithm that follow we consider for the sake of simplicity such operation a addition and multiplication at unit since the fibonacci number grow infinitely large and grow very rapidly a more detailed analysis than the one offered here is in fact it is the size of the number rather than a time efficient method for computing them that should be of primary concern still these caveat notwithstanding the algorithm we outline and their analysis provide useful example for a student of the design and analysis of to begin with we can use recurrence and initial condition for the obvious recursive algorithm for computing f algorithm f n computes the nth fibonacci number recursively by using it definition input a nonnegative integer n output the nth fibonacci number if n return n else return f n f n before embarking on it formal analysis can you tell whether this is an efficient well we need to do a formal analysis the algorithm basic operation is clearly addition so let an be the number of addition performed by the algorithm in computing f then the number of addition needed for computing f n and f n are an and an respectively and the algorithm need one more addition to compute their thus we get the following recurrence for an an an an for n a a the recurrence an an an is quite similar to recurrence f n f n f n but it right hand side is not equal to such recurrence are called there are general technique for solving inhomogeneous recurrence see appendix b or any textbook on discrete mathematics but for this particular recurrence a special trick lead to a faster we can reduce our inhomogeneous recurrence to a homogeneous one by rewriting it a an an an and substituting bn an bn bn bn b b this homogeneous recurrence can be solved exactly in the same manner a recur rence wa solved to find an explicit formula for f but it can actually be avoided by noting that bn is in fact the same recurrence a f n except that it start with two s and thus run one step ahead of f so bn f n and an bn f n n n hence an n and if we measure the size of n by the number of bit b log n in it binary representation the efficiency class will be even worse namely doubly exponential ab the poor efficiency class of the algorithm could be anticipated by the nature of recurrence indeed it contains two recursive call with the size of smaller instance only slightly smaller than size have you encountered such a situation we can also see the reason behind the algorithm inefficiency by looking at a recursive tree of call tracing the algorithm an example of such a tree for n is given in figure note that the same value of the function are being evaluated here again and again which is clearly extremely we can obtain a much faster algorithm by simply computing the successive element of the fibonacci sequence iteratively a is done in the following algo algorithm fibn computes the nth fibonacci number iteratively by using it definition input a nonnegative integer n output the nth fibonacci number f f for i to n do f i f i f i return f n f f f f f f f f f f f f f f f figure tree of recursive call for computing the th fibonacci number by the definition based this algorithm clearly make n hence it is linear a a function of n and only exponential a a function of the number of bit b in n binary note that using an extra array for storing all the preceding element of the fibonacci sequence can be avoided storing just two value is necessary to accomplish the task see problem in this section the third alternative for computing the nth fibonacci number lie in using formula the efficiency of the algorithm will obviously be determined by the efficiency of an exponentiation algorithm used for computing if it is done by simply multiplying by itself n time the algorithm will be in n there are faster algorithm for the exponentiation for example we will discus log n b algorithm for this problem in chapter and note also that special care should be exercised in implementing this approach to computing the nth fibonacci since all it intermediate result are irrational number we would have to make sure that their approximation in the computer are accurate enough so that the final round off yield a correct finally there exists a log n algorithm for computing the nth fibonacci number that manipulates only it is based on the equality f n f n n f n f n for n and an efficient way of computing matrix exercise find a web site dedicated to application of the fibonacci number and study fibonaccis rabbit problem a man put a pair of rabbit in a place sur rounded by a how many pair of rabbit will be there in a year if the initial pair of rabbit male and female are newborn and all rabbit pair are not fertile during their first month of life but thereafter give birth to one new malefemale pair at the end of every climbing stair find the number of different way to climb an n stair stair case if each step is either one or two for example a stair staircase can be climbed three way and how many even number are there among the first n fibonacci number among the number f f f n give a closed form formula valid for every n check by direct substitution that the function n n indeed satisfies recurrence and initial condition the maximum value of the java primitive type int and long are and find the smallest n for which the nth fibonacci number is not going to fit in a memory allocated for the type the type consider the recursive definition based algorithm for computing the nth fi bonacci number f let cn and zn be the number of time f and f are computed prove that cn f zn f n improve algorithm f ib of the text so that it requires only prove the equality f n f n n f n f n for n how many modulo division are made by euclid algorithm on two consec utive fibonacci number f n and f n a the algorithm dissecting a fibonacci rectangle given a rectangle whose side are two con secutive fibonacci number design an algorithm to dissect it into square with no more than two square being the same what is the time efficiency class of your in the language of your choice implement two algorithm for computing the last five digit of the nth fibonacci number that are based on a the recursive definition based algorithm fn b the iterative definition based algorithm perform an experiment to find the largest value of n for which your program run under minute on your empirical analysis of algorithm in section and we saw how algorithm both nonrecursive and recursive can be analyzed though these technique can be applied success fully to many simple algorithm the power of mathematics even when enhanced with more advanced technique see sed pur gra and gre is far from in fact even some seemingly simple algorithm have proved to be very difficult to analyze with mathematical precision and a we pointed out in section this is especially true for the average case the principal alternative to the mathematical analysis of an algorithm ef ficiency is it empirical this approach implies step spelled out in the following general plan for the empirical analysis of algorithm time efficiency understand the experiment decide on the efficiency metric m to be measured and the measurement unit an operation count a time decide on characteristic of the input sample it range size and so prepare a program implementing the algorithm or algorithm for the exper generate a sample of run the algorithm or algorithm on the sample input and record the data analyze the data let u discus these step one at a there are several different goal one can pursue in analyzing algorithm they include checking the accuracy of a theoretical assertion about the algorithm efficiency comparing the efficiency of several algorithm for solving the same problem or different imple mentation of the same algorithm developing a hypothesis about the algorithm efficiency class and ascertaining the efficiency of the program implementing the algorithm on a particular obviously an experiment design should de pend on the question the experimenter seek to in particular the goal of the experiment should influence if not dictate how the algorithm efficiency is to be the first alternative is to insert a counter or counter into a program implementing the algorithm to count the number of time the algorithm basic operation is this is usually a straightforward operation you should only be mindful of the possibility that the basic operation is executed in several place in the program and that all it execution need to be accounted a straightforward a this task usually is you should always test the modified program to ensure that it work correctly in term of both the problem it solves and the count it the second alternative is to time the program implementing the algorithm in the easiest way to do this is to use a system command such a the time command in alternatively one can measure the running time of a code fragment by asking for the system time right before the fragment start tstart and just after it completion tfinish and then computing the difference between the two tfinish in c and c you can use the function clock for this purpose in java the method currenttimemillis in the system class is it is important to keep several fact in mind first a system time is typically not very accurate and you might get somewhat different result on repeated run of the same program on the same an obvious remedy is to make several such measurement and then take their average or the median a the sample observation second given the high speed of modern com puters the running time may fail to register at all and be reported a the standard trick to overcome this obstacle is to run the program in an extra loop many time measure the total running time and then divide it by the number of the loop third on a computer running under a time sharing system such a unix the reported time may include the time spent by the cpu on other program which obviously defeat the purpose of the therefore you should take care to ask the system for the time devoted specifically to execution of if the system time is given in unit called tick the difference should be divided by a constant indicating the number of tick per time your in unix this time is called the user time and it is automatically provided by the time thus measuring the physical running time ha several disadvantage both principal dependence on a particular machine being the most important of them and technical not shared by counting the execution of a basic on the other hand the physical running time provides very specific information about an algorithm performance in a particular computing environment which can be of more importance to the experimenter than say the algorithm asymptotic efficiency in addition measuring time spent on different segment of a program can pinpoint a bottleneck in the program performance that can be missed by an abstract deliberation about the algorithm basic getting such data called profiling is an important resource in the empirical analysis of an algorithm running time the data in question can usually be obtained from the system tool available in most computing whether you decide to measure the efficiency by basic operation counting or by time clocking you will need to decide on a sample of input for the often the goal is to use a sample representing a typical input so the challenge is to understand what a typical input for some class of algorithm for algorithm for the traveling salesman problem that we are going to discus later in the book researcher have developed a set of instance they use for benchmark but much more often than not an input sample ha to be developed by the typically you will have to make decision about the sample size it is sensible to start with a relatively small sample and increase it later if necessary the range of instance size typically neither trivially small nor excessively large and a procedure for generating instance in the range the instance size can either adhere to some pattern or or be generated randomly within the range the principal advantage of size changing according to a pattern is that it impact is easier to for example if a sample size are generated by doubling you can compute the ratio mnmn of the observed metric m the count or the time to see whether the ratio exhibit a behavior typical of algorithm in one of the basic efficiency class discussed in section the major disadvantage of nonrandom size is the possibility that the algorithm under investigation exhibit atypical behavior on the sample for example if all the size in a sample are even and your algorithm run much more slowly on odd size input the empirical result will be quite another important issue concerning size in an experiment sample is whether several instance of the same size should be if you expect the observed metric to vary considerably on instance of the same size it would be probably wise to include several instance for every size in the there are well developed method in statistic to help the experimenter make such de cisions you will find no shortage of book on this of course if several instance of the same size are included in the sample the average or median of the observed value for each size should be computed and investigated instead of or in addition to individual sample much more often than not an empirical analysis requires generating random even if you decide to use a pattern for input size you will typically want instance themselves generated generating random number on a digital computer is known to present a difficult problem because in principle the problem can be solved only this is the reason computer scientist prefer to call such number a a practical matter the easiest and most natural way of getting such number is to take advantage of a random number generator available in computer language typically it output will be a value of a pseudorandom variable uniformly distributed in the interval between and if a different pseudorandom variable is desired an appropriate transformation need to be for example if x is a continuous random variable uniformly distributed on the interval x the variable y l xr l will be uniformly distributed among the integer value between integer l and r l alternatively you can implement one of several known algorithm for generating pseudorandom the most widely used and thoroughly studied of such algorithm is the linear congruential algorithm randomn m seed a b generates a sequence of n pseudorandom number according to the linear congruential method input a positive integer n and positive integer parameter m seed a b output a sequence r rn of n pseudorandom integer uniformly distributed among integer value between and m note pseudorandom number between and can be obtained by treating the integer generated a digit after the decimal point r seed for i to n do ri a ri b mod m the simplicity of this pseudocode is misleading because the devil lie in the detail of choosing the algorithm here is a partial list of recommendation based on the result of a sophisticated mathematical analysis see knuii for detail seed may be chosen arbitrarily and is often set to the current date and time m should be large and may be conveniently taken a w where w is the computer word size a should be selected a an integer between and with no particular pattern in it digit but such that a mod and the value of b can be chosen a the empirical data obtained a the result of an experiment need to be recorded and then presented for an data can be presented numerically in a table or graphically in a scatterplot by point in a cartesian coordinate it is a good idea to use both these option whenever it is feasible because both method have their unique strength and the principal advantage of tabulated data lie in the opportunity to manip ulate it for example one can compute the ratio mngn where gn is a candidate to represent the efficiency class of the algorithm in if the algorithm is indeed in gn most likely these ratio will converge to some po itive constant a n get note that careless novice sometimes assume that this constant must be which is of course incorrect according to the definition of or one can compute the ratio mnmn and see how the running time reacts to doubling of it input a we discussed in section such ratio should change only slightly for logarithmic algorithm and most likely converge to and for linear quadratic and cubic algorithm respectively to name the most obvious and convenient on the other hand the form of a scatterplot may also help in ascertaining the algorithm probable efficiency for a logarithmic algorithm the scat terplot will have a concave shape figure this fact distinguishes it from all the other basic efficiency for a linear algorithm the point will tend to aggregate around a straight line or more generally to be contained between two straight line figure scatterplots of function in n lg n and n will have a convex shape figure making them difficult to a scatterplot of a cubic algorithm will also have a convex shape but it will show a much more rapid increase in the metric an exponential algorithm will most probably require a logarithmic scale for the vertical axis in which the val ues of loga mn rather than those of mn are the commonly used logarithm base is or in such a coordinate system a scatterplot of a truly exponential algorithm should resemble a linear function because mn can im ply logb mn logb c n logb a and vice one of the possible application of the empirical analysis is to predict the al gorithms performance on an instance not included in the experiment for example if you observe that the ratio mngn are close to some constant c for the sample instance it could be sensible to approximate mn by the prod uct cgn for other instance this approach should be used with caution especially for value of n outside the sample mathematician call such prediction extrapolation a opposed to interpolation which deal with value within the sample of course you can try unleashing the standard tech niques of statistical data analysis and note however that the majority of such technique are based on specific probabilistic assumption that may or may not be valid for the experimental data in it seems appropriate to end this section by pointing out the basic differ ences between mathematical and empirical analysis of the princi pal strength of the mathematical analysis is it independence of specific input it principal weakness is it limited applicability especially for investigating the average case the principal strength of the empirical analysis lie in it applicability to any algorithm but it result can depend on the particular sample of instance and the computer used in the count or time count or time n n a b count or time n c figure typical scatter a b c one of the convex exercise consider the following well known sorting algorithm which is studied later in the book with a counter inserted to count the number of key algorithm input an array of n orderable element output the total number of key comparison made count for i to n do v ai j i while j and aj v do count count aj aj j j aj v return count is the comparison counter inserted in the right if you believe it is prove it if you believe it is not make an appropriate run the program of problem with a properly inserted counter or coun ters for the number of key comparison on random array of size analyze the data obtained to form a hypothesis about the algorithm average case estimate the number of key comparison we should expect for a randomly generated array of size sorted by the same repeat problem by measuring the program running time in hypothesize a likely efficiency class of an algorithm based on the following empirical observation of it basic operation count size count what scale transformation will make a logarithmic scatterplot look like a linear how can one distinguish a scatterplot for an algorithm in lg lg n from a scatterplot for an algorithm in lg find empirically the largest number of division made by euclid algo rithm for computing gcdm n for n m for each positive integer k find empirically the smallest pair of integer n m for which euclid algorithm need to make k division in order to find gcdm the average case efficiency of euclid algorithm on input of size n can be measured by the average number of division davgn made by the algorithm in computing gcdn gcdn gcdn for example davg produce a scatterplot of davgn and indicate the algorithm likely average case efficiency run an experiment to ascertain the efficiency class of the sieve of erato thenes see section run a timing experiment for the three algorithm for computing gcdm n presented in section fundamental of algorithmic problem solving let u start by reiterating an important point made in the introduction to this chapter we can consider algorithm to be procedural solution to these solution are not answer but specific instruction for getting it is this emphasis on precisely defined constructive procedure that make computer science distinct from other in particular this distinguishes it from the oretical mathematics whose practitioner are typically satisfied with just proving the existence of a solution to a problem and possibly investigating the solution we now list and briefly discus a sequence of step one typically go through in designing and analyzing an algorithm figure understanding the problem from a practical perspective the first thing you need to do before designing an algorithm is to understand completely the problem read the problem description carefully and ask question if you have any doubt about the problem do a few small example by hand think about special case and ask question again if there are a few type of problem that arise in computing application quite we review them in the next if the problem in question is one of them you might be able to use a known algorithm for solving of course it help to understand how such an algorithm work and to know it strength and weakness especially if you have to choose among several available but often you will not find a readily available algorithm and will have to design your the sequence of step outlined in this section should help you in this exciting but not always easy an input to an algorithm specifies an instance of the problem the algorithm it is very important to specify exactly the set of instance the algorithm need to a an example recall the variation in the set of instance for the three greatest common divisor algorithm discussed in the previous if you fail to do this your algorithm may work correctly for a majority of input but crash on some boundary remember that a correct algorithm is not one that work most of the time but one that work correctly for all legitimate do not skimp on this first step of the algorithmic problem solving process otherwise you will run the risk of unnecessary ascertaining the capability of the computational device once you completely understand a problem you need to ascertain the capability of the computational device the algorithm is intended the vast majority of understand the problem decide on computational mean exact approximate solving algorithm design technique design an algorithm prove correctness analyze the algorithm code the algorithm figure algorithm design and analysis algorithm in use today are still destined to be programmed for a computer closely resembling the von neumann machine a computer architecture outlined by the prominent hungarian american mathematician john von neumann in collaboration with burk and goldstine in the essence of this architecture is captured by the so called random access machine it central assumption is that instruction are executed one after another one operation at a accordingly algorithm designed to be executed on such machine are called sequential the central assumption of the ram model doe not hold for some newer computer that can execute operation concurrently in algorithm that take advantage of this capability are called parallel still studying the classic technique for design and analysis of algorithm under the ram model remains the cornerstone of algorithmics for the foreseeable should you worry about the speed and amount of memory of a computer at your if you are designing an algorithm a a scientific exercise the answer is a qualified a you will see in section most computer scientist prefer to study algorithm in term independent of specification parameter for a particular if you are designing an algorithm a a practical tool the answer may depend on a problem you need to even the slow computer of today are almost unimaginably consequently in many situation you need not worry about a computer being too slow for the there are important problem however that are very complex by their nature or have to process huge volume of data or deal with application where the time is in such situation it is imperative to be aware of the speed and memory available on a particular computer choosing between exact and approximate problem solving the next principal decision is to choose between solving the problem exactly or solving it in the former case an algorithm is called an exact algorithm in the latter case an algorithm is called an approximation why would one opt for an approximation first there are important problem that simply cannot be solved exactly for most of their instance example include extracting square root solving nonlinear equation and evaluating definite second available algorithm for solving a problem exactly can be unacceptably slow because of the problem intrinsic this happens in particular for many problem involving a very large number of choice you will see example of such difficult problem in chapter and third an approximation algorithm can be a part of a more sophisticated algorithm that solves a problem algorithm design technique now with all the component of the algorithmic problem solving in place how do you design an algorithm to solve a given this is the main question this book seek to answer by teaching you several general design what is an algorithm design an algorithm design technique or strategy or paradigm is a general approach to solving problem algorithmically that is applicable to a variety of problem from different area of check this book table of content and you will see that a majority of it chapter are devoted to individual design they distill a few key idea that have proven to be useful in designing learning these technique is of utmost importance for the following first they provide guidance for designing algorithm for new problem problem for which there is no known satisfactory therefore to use the language of a famous proverb learning such technique is akin to learning to fish a opposed to being given a fish caught by somebody it is not true of course that each of these general technique will be necessarily applicable to every problem you may but taken together they do constitute a powerful collection of tool that you will find quite handy in your study and second algorithm are the cornerstone of computer every science is interested in classifying it principal subject and computer science is no algorithm design technique make it possible to classify algorithm according to an underlying design idea therefore they can serve a a natural way to both categorize and study designing an algorithm and data structure while the algorithm design technique do provide a powerful set of general ap proaches to algorithmic problem solving designing an algorithm for a particular problem may still be a challenging some design technique can be simply inapplicable to the problem in sometimes several technique need to be combined and there are algorithm that are hard to pinpoint a application of the known design even when a particular design technique is ap plicable getting an algorithm often requires a nontrivial ingenuity on the part of the algorithm with practice both task choosing among the general technique and applying them get easier but they are rarely of course one should pay close attention to choosing data structure appro priate for the operation performed by the for example the sieve of eratosthenes introduced in section would run longer if we used a linked list instead of an array in it implementation also note that some of the al gorithm design technique discussed in chapter and depend intimately on structuring or restructuring data specifying a problem many year ago an influential textbook proclaimed the fundamental importance of both algo rithms and data structure for computer programming by it very title algorithm data structure program in the new world of object oriented pro gramming data structure remain crucially important for both design and analysis of we review basic data structure in section method of specifying an algorithm once you have designed an algorithm you need to specify it in some in section to give you an example euclid algorithm is described in word in a free and also a step by step form and in these are the two option that are most widely used nowadays for specifying using a natural language ha an obvious appeal however the inherent ambi guity of any natural language make a succinct and clear description of algorithm surprisingly nevertheless being able to do this is an important skill that you should strive to develop in the process of learning pseudocode is a mixture of a natural language and programming language like pseudocode is usually more precise than natural language and it usage often yield more succinct algorithm surprisingly computer scientist have never agreed on a single form of pseudocode leaving textbook author with a need to design their own fortunately these dialect are so close to each other that anyone familiar with a modern programming language should be able to understand them this book dialect wa selected to cause minimal difficulty for a for the sake of simplicity we omit declaration of variable and use indentation to show the scope of such statement a for if and a you saw in the previous section we use an arrow for the assignment operation and two slash for in the earlier day of computing the dominant vehicle for specifying algorithm wa a flowchart a method of expressing an algorithm by a collection of connected geometric shape containing description of the algorithm this representation technique ha proved to be inconvenient for all but very simple algorithm nowadays it can be found only in old algorithm the state of the art of computing ha not yet reached a point where an algorithm description be it in a natural language or pseudocode can be fed into an electronic computer instead it need to be converted into a computer program written in a particular computer we can look at such a program a yet another way of specifying the algorithm although it is preferable to consider it a the algorithm proving an algorithm correctness once an algorithm ha been specified you have to prove it that is you have to prove that the algorithm yield a required result for every legitimate input in a finite amount of for example the correctness of euclid algorithm for computing the greatest common divisor stem from the correctness of the equality gcdm n gcdn m mod n which in turn need a proof see problem in exercise the simple observation that the second integer get smaller on every iteration of the algorithm and the fact that the algorithm stop when the second integer becomes for some algorithm a proof of correctness is quite easy for others it can be quite a common technique for proving correctness is to use mathematical induction because an algorithm iteration provide a natural sequence of step needed for such it might be worth mentioning that although tracing the algorithm performance for a few specific input can be a very worthwhile activity it cannot prove the algorithm correctness but in order to show that an algorithm is incorrect you need just one instance of it input for which the algorithm the notion of correctness for approximation algorithm is le straightforward than it is for exact for an approximation algorithm we usually would like to be able to show that the error produced by the algorithm doe not exceed a predefined you can find example of such investigation in chapter analyzing an algorithm we usually want our algorithm to posse several after correctness by far the most important is in fact there are two kind of algorithm efficiency time efficiency indicating how fast the algorithm run and space ef ficiency indicating how much extra memory it a general framework and specific technique for analyzing an algorithm efficiency appear in chapter another desirable characteristic of an algorithm is unlike effi ciency which can be precisely defined and investigated with mathematical rigor simplicity like beauty is to a considerable degree in the eye of the for example most people would agree that euclid algorithm is simpler than the middle school procedure for computing gcdm n but it is not clear whether eu clids algorithm is simpler than the consecutive integer checking still simplicity is an important algorithm characteristic to strive because sim pler algorithm are easier to understand and easier to program consequently the resulting program usually contain fewer there is also the undeniable aes thetic appeal of sometimes simpler algorithm are also more efficient than more complicated unfortunately it is not always true in which case a judicious compromise need to be yet another desirable characteristic of an algorithm is there are in fact two issue here generality of the problem the algorithm solves and the set of input it on the first issue note that it is sometimes easier to design an algorithm for a problem posed in more general consider for example the problem of determining whether two integer are relatively prime whether their only common divisor is equal to it is easier to design an algorithm for a more general problem of computing the greatest common divisor of two integer and to solve the former problem check whether the gcd is or there are situation however where designing a more general algorithm is unnecessary or difficult or even for example it is unnecessary to sort a list of n number to find it median which is it n th smallest to give another example the standard formula for root of a quadratic equation cannot be generalized to handle polynomial of arbitrary a to the set of input your main concern should be designing an algorithm that can handle a set of input that is natural for the problem at for example excluding integer equal to a possible input for a greatest common divisor algorithm would be quite on the other hand although the standard formula for the root of a quadratic equation hold for complex coefficient we would normally not implement it on this level of generality unless this capability is explicitly if you are not satisfied with the algorithm efficiency simplicity or generality you must return to the drawing board and redesign the in fact even if your evaluation is positive it is still worth searching for other algorithmic recall the three different algorithm in the previous section for computing the greatest common divisor generally you should not expect to get the best algorithm on the first at the very least you should try to fine tune the algorithm you already for example we made several improvement in our implementation of the sieve of eratosthenes compared with it initial outline in section can you identify you will do well if you keep in mind the following observation of antoine de saint exupe ry the french writer pilot and aircraft designer a designer know he ha arrived at perfection not when there is no longer anything to add but when there is no longer anything to take coding an algorithm most algorithm are destined to be ultimately implemented a computer pro programming an algorithm present both a peril and an the peril lie in the possibility of making the transition from an algorithm to a pro gram either incorrectly or very some influential computer scientist strongly believe that unless the correctness of a computer program is proven with full mathematical rigor the program cannot be considered they have developed special technique for doing such proof see gri but the power of these technique of formal verification is limited so far to very small a a practical matter the validity of program is still established by testing of computer program is an art rather than a science but that doe not mean that there is nothing in it to look up book devoted to testing and debugging even more important test and debug your program thoroughly whenever you implement an also note that throughout the book we assume that input to algorithm belong to the specified set and hence require no when implementing algorithm a program to be used in actual application you should provide such of course implementing an algorithm correctly is necessary but not sufficient you would not like to diminish your algorithm power by an inefficient implemen modern compiler do provide a certain safety net in this regard especially when they are used in their code optimization still you need to be aware of such standard trick a computing a loop invariant an expression that doe not change it value outside the loop collecting common subexpressions replac ing expensive operation by cheap one and so see ker and ben for a good discussion of code tuning and other issue related to algorithm program typically such improvement can speed up a program only by a constant factor whereas a better algorithm can make a difference in running time by order of but once an algorithm is selected a speedup may be worth an i found this call for design simplicity in an essay collection by jon bentley ben the essay deal with a variety of issue in algorithm design and implementation and are justifiably titled programming i wholeheartedly recommend the writing of both jon bentley and antoine de saint exupe a working program provides an additional opportunity in allowing an em pirical analysis of the underlying such an analysis is based on timing the program on several input and then analyzing the result we dis cuss the advantage and disadvantage of this approach to analyzing algorithm in section in conclusion let u emphasize again the main lesson of the process depicted in figure a a rule a good algorithm is a result of repeated effort and even if you have been fortunate enough to get an algorithmic idea that seems perfect you should still try to see whether it can be actually this is good news since it make the ultimate result so much more yes i did think of naming this book the joy of on the other hand how doe one know when to in the real world more often than not a project schedule or the impatience of your bos will stop and so it should be perfection is expensive and in fact not always called designing an algorithm is an engineering like activity that call for compromise among competing goal under the constraint of available resource with the designer time being one of the in the academic world the question lead to an interesting but usually difficult investigation of an algorithm actually this question is not about the efficiency of an algorithm but about the complexity of the problem it solves what is the minimum amount of effort any algorithm will need to exert to solve the for some problem the answer to this question is for example any algorithm that sort an array by comparing value of it element need about n log n comparison for some array of size n see section but for many seemingly easy problem such a integer multiplication computer scientist do not yet have a final another important issue of algorithmic problem solving is the question of whether or not every problem can be solved by an we are not talking here about problem that do not have a solution such a finding real root of a quadratic equation with a negative for such case an output indicating that the problem doe not have a solution is all we can and should expect from an nor are we talking about ambiguously stated even some unambiguous problem that must have a simple yes or no answer are undecidable unsolvable by any an important example of such a problem appears in section fortunately a vast majority of problem in practical computing can be solved by an before leaving this section let u be sure that you do not have the misconception possibly caused by the somewhat mechanical nature of the diagram of figure that designing an algorithm is a dull there is nothing further from the truth inventing or algorithm is a very creative and rewarding this book is designed to convince you that this is the exercise old world puzzle a peasant find himself on a riverbank with a wolf a goat and a head of he need to transport all three to the other side of the river in his however the boat ha room for only the peasant himself and one other item either the wolf the goat or the in his absence the wolf would eat the goat and the goat would eat the solve this problem for the peasant or prove it ha no note the peasant is a vegetarian but doe not like cabbage and hence can eat neither the goat nor the cabbage to help him solve the and it go without saying that the wolf is a protected new world puzzle there are four people who want to cross a rickety bridge they all begin on the same you have minute to get them all across to the other it is night and they have one a maximum of two people can cross the bridge at one any party that cross either one or two people must have the flashlight with the flashlight must be walked back and forth it cannot be thrown for person take minute to cross the bridge person take minute person take minute and person take a pair must walk together at the rate of the slower person note according to a rumor on the internet interviewer at a well known software company located near seattle have given this problem to which of the following formula can be considered an algorithm for comput ing the area of a triangle whose side length are given positive number a b and s pp ap bp c where p a b c s bc sin a where a is the angle between side b and c s aha where ha is the height to base a write pseudocode for an algorithm for finding real root of equation ax bx c for arbitrary real coefficient a b and you may assume the availability of the square root function sqrt describe the standard algorithm for finding the binary representation of a positive decimal integer in in describe the algorithm used by your favorite atm machine in dispensing you may give your description in either english or pseudocode which ever you find more can the problem of computing the number be solved how many instance doe this problem look up an algorithm for this problem on the give an example of a problem other than computing the greatest common divisor for which you know more than one which of them is which is more consider the following algorithm for finding the distance between the two closest element in an array of algorithm input array of number output minimum distance between two of it element dmin for i to n do for j to n do if i j and ai aj dmin dmin ai aj return dmin make a many improvement a you can in this algorithmic solution to the if you need to you may change the algorithm altogether if not improve the implementation one of the most influential book on problem solving titled how to solve it pol wa written by the hungarian american mathematician george po lya po lya summarized his idea in a four point find this summary on the internet or better yet in his book and compare it with the plan outlined in section what do they have in how are they algorithm visualization in addition to the mathematical and empirical analysis of algorithm there is yet a third way to study it is called algorithm visualization and can be defined a the use of image to convey some useful information about that information can be a visual illustration of an algorithm operation of it per formance on different kind of input or of it execution speed versus that of other algorithm for the same to accomplish this goal an algorithm visualiza tion us graphic element point line segment two or three dimensional bar and so on to represent some interesting event in the algorithm there are two principal variation of algorithm visualization static algorithm visualization dynamic algorithm visualization also called algorithm animation static algorithm visualization show an algorithm progress through a series of still algorithm animation on the other hand show a continuous movie like presentation of an algorithm animation is an arguably more sophisticated option which of course is much more difficult to early effort in the area of algorithm visualization go back to the the watershed event happened in with the appearance of a minute color sound film titled sorting out this algorithm visualization classic wa produced at the university of toronto by ronald baecker with the assistance of sherman bae it contained visualization of nine well known sorting algorithm more than half of them are discussed later in the book and provided quite a convincing demonstration of their relative the success of sorting out sorting made sorting algorithm a perennial fa vorite for algorithm indeed the sorting problem lends itself quite naturally to visual presentation via vertical or horizontal bar or stick of different height or length which need to be rearranged according to their size figure this presentation is convenient however only for illustrating action of a typical sorting algorithm on small for larger file sorting out sorting used the ingenious idea of presenting data by a scatterplot of point on a coordinate plane with the first coordinate representing an item position in the file and the second one representing the item value with such a representation the process of sorting look like a transformation of a random scatterplot of point into the point along a frame diagonal figure in addition most sorting algorithm figure initial and final screen of a typical visualization of a sorting algorithm using the bar work by comparing and exchanging two given item at a time an event that can be animated relatively since the appearance of sorting out sorting a great number of algorithm animation have been created especially after the appearance of java and the figure initial and final screen of a typical visualization of a sorting algorithm using the scatterplot world wide web in the they range in scope from one particular algorithm to a group of algorithm for the same problem sorting or the same application area geometric algorithm to general purpose animation at the end of a catalog of link to existing visualization maintained under the nsf supported algovizproject contained over unfortunately a survey of existing visualization found most of them to be of low quality with the content heavily skewed toward easier topic such a sorting there are two principal application of algorithm visualization research and potential benefit for researcher are based on expectation that algo rithm visualization may help uncover some unknown feature of for example one researcher used a visualization of the recursive tower of hanoi algo rithm in which odd and even numbered disk were colored in two different he noticed that two disk of the same color never came in direct contact during the algorithm this observation helped him in developing a better non recursive version of the classic to give another example bentley and mcilroy ben mentioned using an algorithm animation system in their work on improving a library implementation of a leading sorting the application of algorithm visualization to education seek to help student learning the available evidence of it effectiveness is decisively although some experiment did register positive learning outcome others failed to do the increasing body of evidence indicates that creating sophisticated software system is not going to be in fact it appears that the level of student involvement with visualization might be more important than specific feature of visualization in some experiment low tech visualization prepared by student were more effective than passive exposure to sophisticated software to summarize although some success in both research and education have been reported in the literature they are not a impressive a one might a deeper understanding of human perception of image will be required before the true potential of algorithm visualization is summary there are two kind of algorithm efficiency time efficiency and space time efficiency indicates how fast the algorithm run space efficiency deal with the extra space it an algorithm time efficiency is principally measured a a function of it input size by counting the number of time it basic operation is a basic operation is the operation that contributes the most to running typically it is the most time consuming operation in the algorithm innermost for some algorithm the running time can differ considerably for input of the same size leading to worst case efficiency average case efficiency and best case the established framework for analyzing time efficiency is primarily grounded in the order of growth of the algorithm running time a it input size go to the notation o and are used to indicate and compare the asymptotic order of growth of function expressing algorithm the efficiency of a large number of algorithm fall into the following few class constant logarithmic linear linearithmic quadratic cubic and the main tool for analyzing the time efficiency of a nonrecursive algorithm is to set up a sum expressing the number of execution of it basic operation and ascertain the sum order of the main tool for analyzing the time efficiency of a recursive algorithm is to set up a recurrence relation expressing the number of execution of it basic operation and ascertain the solution order of succinctness of a recursive algorithm may mask it the fibonacci number are an important sequence of integer in which every element is equal to the sum of it two immediate there are several algorithm for computing the fibonacci number with drastically different empirical analysis of an algorithm is performed by running a program implementing the algorithm on a sample of input and analyzing the data observed the basic operation count or physical running this often involves generating pseudorandom the applicability to any algorithm is the principal strength of this approach the dependence of result on the particular computer and instance sample is it main algorithm visualization is the use of image to convey useful information about the two principal variation of algorithm visualization are static algorithm visualization and dynamic algorithm visualization also called algorithm brute force and exhaustive search science is a far removed from brute force a this sword from a edward lytton leila book ii chapter i doing a thing well is often a waste of robert byrne a master pool and billiards player and a writer after introducing the framework and method for algorithm analysis in the preceding chapter we are ready to embark on a discussion of algorithm design each of the next eight chapter is devoted to a particular design the subject of this chapter is brute force and it important special case exhaustive brute force can be described a follows brute force is a straightforward approach to solving a problem usually directly based on the problem statement and definition of the concept the force implied by the strategy definition is that of a computer and not that of one just do would be another way to describe the prescription of the brute force and often the brute force strategy is indeed the one that is easiest to a an example consider the exponentiation problem compute an for a nonzero number a and a nonnegative integer although this problem might seem trivial it provides a useful vehicle for illustrating several algorithm design strategy including the brute also note that computing an mod m for some large integer is a principal component of a leading encryption by the definition of exponentiation an a a n time this suggests simply computing an by multiplying by a n we have already encountered at least two brute force algorithm in the book the consecutive integer checking algorithm for computing gcdm n in section and the definition based algorithm for matrix multiplication in section many other example are given later in this can you identify a few algorithm you already know a being based on the brute force though rarely a source of clever or efficient algorithm the brute force ap proach should not be overlooked a an important algorithm design first unlike some of the other strategy brute force is applicable to a very wide va riety of in fact it seems to be the only general approach for which it is more difficult to point out problem it cannot second for some impor tant problem sorting searching matrix multiplication string matching the brute force approach yield reasonable algorithm of at least some practi cal value with no limitation on instance third the expense of designing a more efficient algorithm may be unjustifiable if only a few instance of a prob lem need to be solved and a brute force algorithm can solve those instance with acceptable fourth even if too inefficient in general a brute force algo rithm can still be useful for solving small size instance of a finally a brute force algorithm can serve an important theoretical or educational pur pose a a yardstick with which to judge more efficient alternative for solving a selection sort and bubble sort in this section we consider the application of the brute force approach to the problem of sorting given a list of n orderable item number character from some alphabet character string rearrange them in nondecreasing a we mentioned in section dozen of algorithm have been developed for solving this very important you might have learned several of them in the if you have try to forget them for the time being and look at the problem now after your mind is unburdened of previous knowledge of sorting algo rithms ask yourself a question what would be the most straightforward method for solving the sorting reasonable people may disagree on the answer to this the two algorithm discussed here selection sort and bubble sort seem to be the two prime selection sort we start selection sort by scanning the entire given list to find it smallest element and exchange it with the first element putting the smallest element in it final position in the sorted then we scan the list starting with the second element to find the smallest among the last n element and exchange it with the second element putting the second smallest element in it final generally on the ith pas through the list which we number from to n the algorithm search for the smallest item among the last n i element and swap it with ai a a ai ai amin an in their final position the last n i element after n pass the list is here is pseudocode of this algorithm which for simplicity assumes that the list is implemented a an array algorithm sort a given array by selection sort input an array of orderable element output array sorted in nondecreasing order for i to n do min i for j i to n do if aj amin min j swap ai and amin a an example the action of the algorithm on the list is illustrated in figure the analysis of selection sort is the input size is given by the number of element n the basic operation is the key comparison aj the number of time it is executed depends only on the array size and is given by the following sum n n n n cn n i n i j i i i figure example of sorting with selection each line corresponds to one iteration of the algorithm a pas through the list tail to the right of the vertical bar an element in bold indicates the smallest element element to the left of the vertical bar are in their final position and are not considered in this and subsequent since we have already encountered the last sum in analyzing the algorithm of example in section you should be able to compute it now on your whether you compute this sum by distributing the summation symbol or by immediately getting the sum of decreasing integer the answer of course must be the same n n n n n cn n i i j i i thus selection sort is a n algorithm on all note however that the number of key swap is only n or more precisely n one for each repetition of the i this property distinguishes selection sort positively from many other sorting bubble sort another brute force application to the sorting problem is to compare adjacent element of the list and exchange them if they are out of by doing it repeatedly we end up bubbling up the largest element to the last position on the the next pas bubble up the second largest element and so on until after n pass the list is pas i i n of bubble sort can be represented by the following diagram a aj aj an i an i an in their final position here is pseudocode of this algorithm sort a given array by bubble sort input an array of orderable element output array sorted in nondecreasing order for i to n do for j to n i do if aj aj swap aj and aj the action of the algorithm on the list is illustrated a an example in figure the number of key comparison for the bubble sort version given above is the same for all array of size n it is obtained by a sum that is almost identical to the sum for selection sort figure first two pass of bubble sort on the list a new line is shown after a swap of two element is the element to the right of the vertical bar are in their final position and are not considered in subsequent iteration of the n n i n cn n i i j i n n n n i i the number of key swap however depends on the in the worst case of decreasing array it is the same a the number of key comparison sworst n cn n n a is often the case with an application of the brute force strategy the first version of an algorithm obtained can often be improved upon with a modest amount of specifically we can improve the crude version of bubble sort given above by exploiting the following observation if a pas through the list make no exchange the list ha been sorted and we can stop the algorithm problem a in this section though the new version run faster on some input it is still in n in the worst and average in fact even among elementary sorting method bubble sort is an inferior choice and if it were not for it catchy name you would probably have never heard of however the general lesson you just learned is important and worth repeating a first application of the brute force approach often result in an algorithm that can be improved with a modest amount of exercise give an example of an algorithm that should not be considered an appli cation of the brute force give an example of a problem that cannot be solved by a brute force what is the time efficiency of the brute force algorithm for computing an a a function of a a function of the number of bit in the binary representation of if you are to compute an mod m where a and n is a large positive integer how would you circumvent the problem of a very large magnitude of for each of the algorithm in problem and of exercise tell whether or not the algorithm is based on the brute force design a brute force algorithm for computing the value of a polynomial px anxn an xn ax a at a given point x and determine it worst case efficiency n design a linear algorithm for this if the algorithm you designed is in is it possible to design an algorithm with a better than linear efficiency for this a network topology specifies how computer printer and other device are connected over a the figure below illustrates three common topology of network the ring the star and the fully connected ring star fully connected mesh you are given a boolean matrix where n which is supposed to be the adjacency matrix of a graph modeling a network with one of these your task is to determine which of these three topology if any the matrix design a brute force algorithm for this task and indicate it time efficiency tetromino tiling tetrominoes are tile made of four there are five type of tetrominoes shown below straight tetromino square tetromino l tetromino t tetromino z tetromino is it possible to tile cover exactly without overlap an chessboard with straight square l t z a stack of fake coin there are n stack of n identical looking all of the coin in one of these stack are counterfeit while all the coin in the other stack are every genuine coin weighs gram every fake weighs you have an analytical scale that can determine the exact weight of any number of devise a brute force algorithm to identify the stack with the fake coin and determine it worst case efficiency what is the minimum number of weighing needed to identify the stack with the fake sort the list e x a m p l e in alphabetical order by selection is selection sort the definition of a stable sorting algorithm wa given in section is it possible to implement selection sort for linked list with the same n efficiency a the array sort the list e x a m p l e in alphabetical order by bubble prove that if bubble sort make no exchange on it pas through a list the list is sorted and the algorithm can be write pseudocode of the method that incorporates this prove that the worst case efficiency of the improved version is is bubble sort alternating disk you have a row of n disk of two color n dark and n they alternate dark light dark light and so you want to get all the dark disk to the right hand end and all the light disk to the left hand the only move you are allowed to make are those that interchange the position of two neighboring design an algorithm for solving this puzzle and determine the number of move it gar