introduction two ideas lie gleaming on the jewelers velvet the first is the calculus the second the algorithm the calculus and the rich body of mathematical analysis to which it gave rise made modern science possible but it has been the algorithm that has made possible the modern world david berlinski the advent of the algorithm why do you need to study algorithms if you are going to be a computer professional there are both practical and theoretical reasons to study algorithms from a practical standpoint you have to know a standard set of important algorithms from different areas of computing in addition you should be able to design new algorithms and analyze their efficiency from the theoretical standpoint the study of algorithms sometimes called algorithmics has come to be recognized as the cornerstone of computer science david harel in his delightful book pointedly titled algorithmics the spirit of computing put it as follows algorithmics is more than a branch of computer science it is the core of computer science and in all fairness can be said to be relevant to most of science business and technology har p but even if you are not a student in a computingrelated program there are compelling reasons to study algorithms to put it bluntly computer programs would not exist without algorithms and with computer applications becoming indispensable in almost all aspects of our professional and personal lives studying algorithms becomes a necessity for more and more people another reason for studying algorithms is their usefulness in developing analytical skills after all algorithms can be seen as special kinds of solutions to problems not just answers but precisely defined procedures for getting answers consequently specific algorithm design techniques can be interpreted as problemsolving strategies that can be useful regardless of whether a computer is involved of course the precision inherently imposed by algorithmic thinking limits the kinds of problems that can be solved with an algorithm you will not find for example an algorithm for living a happy life or becoming rich and famous on the other hand this required precision has an important educational advantage donald knuth one of the most prominent computer scientists in the history of algorithmics put it as follows a person welltrained in computer science knows how to deal with algorithms how to construct them manipulate them understand them analyze them this knowledge is preparation for much more than writing good computer programs it is a generalpurpose mental tool that will be a definite aid to the understanding of other subjects whether they be chemistry linguistics or music etc the reason for this may be understood in the following way it has often been said that a person does not really understand something until after teaching it to someone else actually a person does not really understand something until after teaching it to a computer ie expressing it as an algorithm an attempt to formalize things as algorithms leads to a much deeper understanding than if we simply try to comprehend things in the traditional way knu p we take up the notion of algorithm in section as examples we use three algorithms for the same problem computing the greatest common divisor there are several reasons for this choice first it deals with a problem familiar to everybody from their middleschool days second it makes the important point that the same problem can often be solved by several algorithms quite typically these algorithms differ in their idea level of sophistication and efficiency third one of these algorithms deserves to be introduced first both because of its age it appeared in euclids famous treatise more than two thousand years ago and its enduring power and importance finally investigation of these three algorithms leads to some general observations about several important properties of algorithms in general section deals with algorithmic problem solving there we discuss several important issues related to the design and analysis of algorithms the different aspects of algorithmic problem solving range from analysis of the problem and the means of expressing an algorithm to establishing its correctness and analyzing its efficiency the section does not contain a magic recipe for designing an algorithm for an arbitrary problem it is a wellestablished fact that such a recipe does not exist still the material of section should be useful for organizing your work on designing and analyzing algorithms section is devoted to a few problem types that have proven to be particularly important to the study of algorithms and their application in fact there are textbooks eg sed organized around such problem types i hold the view shared by many others that an organization based on algorithm design techniques is superior in any case it is very important to be aware of the principal problem types not only are they the most commonly encountered problem types in reallife applications they are used throughout the book to demonstrate particular algorithm design techniques section contains a review of fundamental data structures it is meant to serve as a reference rather than a deliberate discussion of this topic if you need a more detailed exposition there is a wealth of good books on the subject most of them tailored to a particular programming language sequential search and bruteforce string matching we saw in the previous section two applications of the bruteforce approach to the sorting porblem here we discuss two applications of this strategy to the problem of searching the first deals with the canonical problem of searching for an item of a given value in a given list the second is different in that it deals with the stringmatching problem sequential search we have already encountered a bruteforce algorithm for the general searching problem it is called sequential search see section to repeat the algorithm simply compares successive elements of a given list with a given search key until either a match is encountered successful search or the list is exhausted without finding a match unsuccessful search a simple extra trick is often employed in implementing sequential search if we append the search key to the end of the list the search for the key will have to be successful and therefore we can eliminate the end of list check altogether here is pseudocode of this enhanced version algorithm sequentialsearchan k implements sequential search with a search key as a sentinel input an array a of n elements and a search key k output the index of the first element in an whose value is equal to k or if no such element is found an k i while ai k do ii if i n return i else return another straightforward improvement can be incorporated in sequential search if a given list is known to be sorted searching in such a list can be stopped as soon as an element greater than or equal to the search key is encountered sequential search provides an excellent illustration of the bruteforce approach with its characteristic strength simplicity and weakness inferior efficiency the efficiency results obtained in section for the standard version of sequential search change for the enhanced version only very slightly so that the algorithm remains linear in both the worst and average cases we discuss later in the book several searching algorithms with a better time efficiency bruteforce string matching recall the stringmatching problem introduced in section given a string of n characters called the text and a string of m characters m n called the pattern find a substring of the text that matches the pattern to put it more precisely we want to find i the index of the leftmost character of the first matching substring in the text such that ti p tij pj tim pm t ti tij tim tn text t p pj pm pattern p if matches other than the first one need to be found a stringmatching algorithm can simply continue working until the entire text is exhausted a bruteforce algorithm for the stringmatching problem is quite obvious align the pattern against the first m characters of the text and start matching the corresponding pairs of characters from left to right until either all the m pairs of the characters match then the algorithm can stop or a mismatching pair is encountered in the latter case shift the pattern one position to the right and resume the character comparisons starting again with the first character of the pattern and its counterpart in the text note that the last position in the text that can still be a beginning of a matching substring is n m provided the text positions are indexed from to n beyond that position there are not enough characters to match the entire pattern hence the algorithm need not make any comparisons there algorithm bruteforcestringmatcht n p m implements bruteforce string matching input an array t n of n characters representing a text and an array p m of m characters representing a pattern output the index of the first character in the text that starts a matching substring or if the search is unsuccessful for i to n m do j while j m and p j t i j do j j if j m return i return an operation of the algorithm is illustrated in figure note that for this example the algorithm shifts the pattern almost always after a single character comparison the worst case is much worse the algorithm may have to make all m comparisons before shifting the pattern and this can happen for each of the n m tries problem in this sections exercises asks you to give a specific example of such a situation thus in the worst case the algorithm makes n o b o d y n o t i c e d h i m n o t n o t n o t n o t n o t n o t n o t n o t figure example of bruteforce string matching the patterns characters that are compared with their text counterparts are in bold type mn m character comparisons which puts it in the onm class for a typical word search in a natural language text however we should expect that most shifts would happen after very few comparisons check the example again therefore the averagecase efficiency should be considerably better than the worstcase efficiency indeed it is for searching in random texts it has been shown to be linear ie n there are several more sophisticated and more efficient algorithms for string searching the most widely known of them by r boyer and j moore is outlined in section along with its simplification suggested by r horspool exercises find the number of comparisons made by the sentinel version of sequential search a in the worst case b in the average case if the probability of a successful search is p p as shown in section the average number of key comparisons made by sequential search without a sentinel under standard assumptions about its inputs is given by the formula cavgn pn n p where p is the probability of a successful search determine for a fixed n the values of p p for which this formula yields the maximum value of cavgn and the minimum value of cavgn gadget testing a firm wants to determine the highest floor of its nstory headquarters from which a gadget can fall without breaking the firm has two identical gadgets to experiment with if one of them gets broken it can not be repaired and the experiment will have to be completed with the remaining gadget design an algorithm in the best efficiency class you can to solve this problem determine the number of character comparisons made by the bruteforce algorithm in searching for the pattern gandhi in the text thereismoretolifethanincreasingitsspeed assume that the length of the text it is characters long is known before the search starts how many comparisons both successful and unsuccessful will be made by the bruteforce algorithm in searching for each of the following patterns in the binary text of one thousand zeros a b c give an example of a text of length n and a pattern of length m that constitutes a worstcase input for the bruteforce stringmatching algorithm exactly how many character comparisons will be made for such input in solving the stringmatching problem would there be any advantage in comparing pattern and text characters righttoleft instead of lefttoright consider the problem of counting in a given text the number of substrings that start with an a and end with a b for example there are four such substrings in cabaaxbya a design a bruteforce algorithm for this problem and determine its efficiency class b design a more efficient algorithm for this problem gin write a visualization program for the bruteforce stringmatching algorithm word find a popular diversion in the united states word find or word search puzzles ask the player to find each of a given set of words in a square table filled with single letters a word can read horizontally left or right vertically up or down or along a degree diagonal in any of the four directions formed by consecutively adjacent cells of the table it may wrap around the tables boundaries but it must read in the same direction with no zigzagging the same cell of the table may be used in different words but in a given word the same cell may be used no more than once write a computer program for solving this puzzle battleship game write a program based on a version of bruteforce pattern matching for playing the game battleship on the computer the rules of the game are as follows there are two opponents in the game in this case a human player and the computer the game is played on two identical boards tables of squares on which each opponent places his or her ships not seen by the opponent each player has five ships each of which occupies a certain number of squares on the board a destroyer two squares a submarine three squares a cruiser three squares a battleship four squares and an aircraft carrier five squares each ship is placed either horizontally or vertically with no two ships touching each other the game is played by the opponents taking turns shooting at each others ships the result of every shot is displayed as either a hit or a miss in case of a hit the player gets to go again and keeps playing until missing the goal is to sink all the opponents ships before the opponent succeeds in doing it first to sink a ship all squares occupied by the ship must be hit closestpair and convexhull problems by brute force in this section we consider a straightforward approach to two wellknown problems dealing with a finite set of points in the plane these problems aside from their theoretical interest arise in two important applied areas computational geometry and operations research closestpair problem the closestpair problem calls for finding the two closest points in a set of n points it is the simplest of a variety of problems in computational geometry that deals with proximity of points in the plane or higherdimensional spaces points in question can represent such physical objects as airplanes or post offices as well as database records statistical samples dna sequences and so on an airtraffic controller might be interested in two closest planes as the most probable collision candidates a regional postal service manager might need a solution to the closestpair problem to find candidate postoffice locations to be closed one of the important applications of the closestpair problem is cluster analysis in statistics based on n data points hierarchical cluster analysis seeks to organize them in a hierarchy of clusters based on some similarity metric for numerical data this metric is usually the euclidean distance for text and other nonnumerical data metrics such as the hamming distance see problem in this sections exercises are used a bottomup algorithm begins with each element as a separate cluster and merges them into successively larger clusters by combining the closest pair of clusters for simplicity we consider the twodimensional case of the closestpair problem we assume that the points in question are specified in a standard fashion by their x y cartesian coordinates and that the distance between two points pixi yi and pj xj yj is the standard euclidean distance dpi pj xi xj yi yj the bruteforce approach to solving this problem leads to the following obvious algorithm compute the distance between each pair of distinct points and find a pair with the smallest distance of course we do not want to compute the distance between the same pair of points twice to avoid doing so we consider only the pairs of points pi pj for which i j pseudocode below computes the distance between the two closest points getting the closest points themselves requires just a trivial modification algorithm bruteforceclosestpairp finds distance between two closest points in the plane by brute force input a list p of n n points px y pnxn yn output the distance between the closest pair of points d for i to n do for j i to n do d mind sqrtxi xj yi yj sqrt is square root return d the basic operation of the algorithm is computing the square root in the age of electronic calculators with a squareroot button one might be led to believe that computing the square root is as simple an operation as say addition or multiplication of course it is not for starters even for most integers square roots are irrational numbers that therefore can be found only approximately moreover computing such approximations is not a trivial matter but in fact computing square roots in the loop can be avoided can you think how the trick is to realize that we can simply ignore the squareroot function and compare the values xi xj yi yj themselves we can do this because the smaller a number of which we take the square root the smaller its square root or as mathematicians say the squareroot function is strictly increasing then the basic operation of the algorithm will be squaring a number the number of times it will be executed can be computed as follows n n n cn n i i j i i n n n n n of course speeding up the innermost loop of the algorithm could only decrease the algorithms running time by a constant factor see problem in this sections exercises but it can not improve its asymptotic efficiency class in chapter we discuss a linearithmic algorithm for this problem which is based on a more sophisticated design technique convexhull problem on to the other problem that of computing the convex hull finding the convex hull for a given set of points in the plane or a higher dimensional space is one of the most important some people believe the most important problems in computational geometry this prominence is due to a variety of applications in which this problem needs to be solved either by itself or as a part of a larger task several such applications are based on the fact that convex hulls provide convenient approximations of object shapes and data sets given for example in computer animation replacing objects by their convex hulls speeds up collision detection the same idea is used in path planning for mars mission rovers convex hulls are used in computing accessibility maps produced from satellite images by geographic information systems they are also used for detecting outliers by some statistical techniques an efficient algorithm for computing a diameter of a set of points which is the largest distance between two of the points needs the sets convex hull to find the largest distance between two of its extreme points see below finally convex hulls are important for solving many optimization problems because their extreme points provide a limited set of solution candidates we start with a definition of a convex set definition a set of points finite or infinite in the plane is called convex if for any two points p and q in the set the entire line segment with the endpoints at p and q belongs to the set all the sets depicted in figure a are convex and so are a straight line a triangle a rectangle and more generally any convex polygon a circle and the entire plane on the other hand the sets depicted in figure b any finite set of two or more distinct points the boundary of any convex polygon and a circumference are examples of sets that are not convex now we are ready for the notion of the convex hull intuitively the convex hull of a set of n points in the plane is the smallest convex polygon that contains all of them either inside or on its boundary if this formulation does not fire up your enthusiasm consider the problem as one of barricading n sleeping tigers by a fence of the shortest length this interpretation is due to d harel har it is somewhat lively however because the fenceposts have to be erected right at the spots where some of the tigers sleep there is another much tamer interpretation of this notion imagine that the points in question are represented by nails driven into a large sheet of plywood representing the plane take a rubber band and stretch it to include all the nails then let it snap into place the convex hull is the area bounded by the snapped rubber band figure a formal definition of the convex hull that is applicable to arbitrary sets including sets of points that happen to lie on the same line follows definition the convex hull of a set s of points is the smallest convex set containing s the smallest requirement means that the convex hull of s must be a subset of any convex set containing s if s is convex its convex hull is obviously s itself if s is a set of two points its convex hull is the line segment connecting these points if s is a set of three by a triangle a rectangle and more generally any convex polygon we mean here a region ie the set of points both inside and on the boundary of the shape in question a b figure a convex sets b sets that are not convex figure rubberband interpretation of the convex hull points not on the same line its convex hull is the triangle with the vertices at the three points given if the three points do lie on the same line the convex hull is the line segment with its endpoints at the two points that are farthest apart for an example of the convex hull for a larger set see figure a study of the examples makes the following theorem an expected result theorem the convex hull of any set s of n points not all on the same line is a convex polygon with the vertices at some of the points of s if all the points do lie on the same line the polygon degenerates to a line segment but still with the endpoints at two points of s p p p p p p p p figure the convex hull for this set of eight points is the convex polygon with vertices at p p p p and p the convexhull problem is the problem of constructing the convex hull for a given set s of n points to solve it we need to find the points that will serve as the vertices of the polygon in question mathematicians call the vertices of such a polygon extreme points by definition an extreme point of a convex set is a point of this set that is not a middle point of any line segment with endpoints in the set for example the extreme points of a triangle are its three vertices the extreme points of a circle are all the points of its circumference and the extreme points of the convex hull of the set of eight points in figure are p p p p and p extreme points have several special properties other points of a convex set do not have one of them is exploited by the simplex method a very important algorithm discussed in section this algorithm solves linear programming problems which are problems of finding a minimum or a maximum of a linear function of n variables subject to linear constraints see problem in this sections exercises for an example and sections and for a general discussion here however we are interested in extreme points because their identification solves the convexhull problem actually to solve this problem completely we need to know a bit more than just which of n points of a given set are extreme points of the sets convex hull we need to know which pairs of points need to be connected to form the boundary of the convex hull note that this issue can also be addressed by listing the extreme points in a clockwise or a counterclockwise order so how can we solve the convexhull problem in a bruteforce manner if you do not see an immediate plan for a frontal attack do not be dismayed the convexhull problem is one with no obvious algorithmic solution nevertheless there is a simple but inefficient algorithm that is based on the following observation about line segments making up the boundary of a convex hull a line segment connecting two points pi and pj of a set of n points is a part of the convex hulls boundary if and only if all the other points of the set lie on the same side of the straight line through these two points verify this property for the set in figure repeating this test for every pair of points yields a list of line segments that make up the convex hulls boundary a few elementary facts from analytical geometry are needed to implement this algorithm first the straight line through two points x y x y in the coordinate plane can be defined by the equation ax by c where a y y b x x c xy yx second such a line divides the plane into two halfplanes for all the points in one of them ax by c while for all the points in the other ax by c for the points on the line itself of course ax by c thus to check whether certain points lie on the same side of the line we can simply check whether the expression ax by c has the same sign for each of these points we leave the implementation details as an exercise what is the time efficiency of this algorithm it is in on for each of nn pairs of distinct points we may need to find the sign of ax by c for each of the other n points there are much more efficient algorithms for this important problem and we discuss one of them later in the book exercises assuming that sqrt takes about times longer than each of the other operations in the innermost loop of bruteforceclosestpoints which are assumed to take the same amount of time estimate how much faster the algorithm will run after the improvement discussed in section can you design a more efficient algorithm than the one based on the bruteforce strategy to solve the closestpair problem for n points x x xn on the real line let x x xn be real numbers representing coordinates of n villages located along a straight road a post office needs to be built in one of these villages a design an efficient algorithm to find the postoffice location minimizing the average distance between the villages and the post office b design an efficient algorithm to find the postoffice location minimizing the maximum distance from a village to the post office for the sake of simplicity we assume here that no three points of a given set lie on the same line a modification needed for the general case is left for the exercises a there are several alternative ways to define a distance between two points px y and px y in the cartesian plane in particular the manhattan distance is defined as dmp p x x y y prove that dm satisfies the following axioms which every distance function must satisfy i dmp p for any two points p and p and dmp p if and only if p p ii dmp p dmp p iii dmp p dmp p dmp p for any p p and p b sketch all the points in the cartesian plane whose manhattan distance to the origin is equal to do the same for the euclidean distance c true or false a solution to the closestpair problem does not depend on which of the two metrics de euclidean or dm manhattan is used the hamming distance between two strings of equal length is defined as the number of positions at which the corresponding symbols are different it is named after richard hamming a prominent american scientist and engineer who introduced it in his seminal paper on errordetecting and errorcorrecting codes a does the hamming distance satisfy the three axioms of a distance metric listed in problem b what is the time efficiency class of the bruteforce algorithm for the closestpair problem if the points in question are strings of m symbols long and the distance between two of them is measured by the hamming distance odd pie fight there are n people positioned on a field euclidean plane so that each has a unique nearest neighbor each person has a cream pie at a signal everybody hurls his or her pie at the nearest neighbor assuming that n is odd and that nobody can miss his or her target true or false there always remains at least one person not hit by a pie car the closestpair problem can be posed in the kdimensional space in which the euclidean distance between two points p x xk and p x xk is defined as dp p k sxs xs what is the timeefficiency class of the bruteforce algorithm for the kdimensional closestpair problem find the convex hulls of the following sets and identify their extreme points if they have any a a line segment b a square c the boundary of a square d a straight line design a lineartime algorithm to determine two extreme points of the convex hull of a given set of n points in the plane what modification needs to be made in the bruteforce algorithm for the convexhull problem to handle more than two points on the same straight line write a program implementing the bruteforce algorithm for the convexhull problem consider the following small instance of the linear programming problem maximize x y subject to x y x y x y a sketch in the cartesian plane the problems feasible region defined as the set of points satisfying all the problems constraints b identify the regions extreme points c solve this optimization problem by using the following theorem a linear programming problem with a nonempty bounded feasible region always has a solution which can be found at one of the extreme points of its feasible region exhaustive search many important problems require finding an element with a special property in a domain that grows exponentially or faster with an instance size typically such problems arise in situations that involve explicitly or implicitly combinatorial objects such as permutations combinations and subsets of a given set many such problems are optimization problems they ask to find an element that maximizes or minimizes some desired characteristic such as a path length or an assignment cost exhaustive search is simply a bruteforce approach to combinatorial problems it suggests generating each and every element of the problem domain selecting those of them that satisfy all the constraints and then finding a desired element eg the one that optimizes some objective function note that although the idea of exhaustive search is quite straightforward its implementation typically requires an algorithm for generating certain combinatorial objects we delay a discussion of such algorithms until the next chapter and assume here that they exist we illustrate exhaustive search by applying it to three important problems the traveling salesman problem the knapsack problem and the assignment problem traveling salesman problem the traveling salesman problem tsp has been intriguing researchers for the last years by its seemingly simple formulation important applications and interesting connections to other combinatorial problems in laymans terms the problem asks to find the shortest tour through a given set of n cities that visits each city exactly once before returning to the city where it started the problem can be conveniently modeled by a weighted graph with the graphs vertices representing the cities and the edge weights specifying the distances then the problem can be stated as the problem of finding the shortest hamiltonian circuit of the graph a hamiltonian circuit is defined as a cycle that passes through all the vertices of the graph exactly once it is named after the irish mathematician sir william rowan hamilton who became interested in such cycles as an application of his algebraic discoveries it is easy to see that a hamiltonian circuit can also be defined as a sequence of n adjacent vertices vi vi vin vi where the first vertex of the sequence is the same as the last one and all the other n vertices are distinct further we can assume with no loss of generality that all circuits start and end at one particular vertex they are cycles after all are they not thus we can get all the tours by generating all the permutations of n intermediate cities compute the tour lengths and find the shortest among them figure presents a small instance of the problem and its solution by this method an inspection of figure reveals three pairs of tours that differ only by their direction hence we could cut the number of vertex permutations by half we could for example choose any two intermediate vertices say b and c and then consider only permutations in which b precedes c this trick implicitly defines a tours direction this improvement can not brighten the efficiency picture much however the total number of permutations needed is still n which makes the exhaustivesearch approach impractical for all but very small values of n on the other hand if you always see your glass as halffull you can claim that cutting the work by half is nothing to sneeze at even if you solve a small instance of the problem especially by hand also note that had we not limited our investigation to the circuits starting at the same vertex the number of permutations would have been even larger by a factor of n knapsack problem here is another wellknown problem in algorithmics given n items of known weights w w wn and values v v vn and a knapsack of capacity w find the most valuable subset of the items that fit into the knapsack if you do not like the idea of putting yourself in the shoes of a thief who wants to steal the most a b c d to ur l e ng th a b c d a i a b d c a i optimal a c b d a i a c d b a i optimal a d b c a i a d c b a i figure solution to a small instance of the traveling salesman problem by exhaustive search valuable loot that fits into his knapsack think about a transport plane that has to deliver the most valuable set of items to a remote location without exceeding the planes capacity figure a presents a small instance of the knapsack problem the exhaustivesearch approach to this problem leads to generating all the subsets of the set of n items given computing the total weight of each subset in order to identify feasible subsets ie the ones with the total weight not exceeding the knapsack capacity and finding a subset of the largest value among them as an example the solution to the instance of figure a is given in figure b since the number of subsets of an nelement set is n the exhaustive search leads to a n algorithm no matter how efficiently individual subsets are generated thus for both the traveling salesman and knapsack problems considered above exhaustive search leads to algorithms that are extremely inefficient on every input in fact these two problems are the bestknown examples of socalled nphard problems no polynomialtime algorithm is known for any nphard problem moreover most computer scientists believe that such algorithms do not exist although this very important conjecture has never been proven moresophisticated approaches backtracking and branchandbound see sections and enable us to solve some but not all instances of these and w w w w v v v v knapsack item item item item a subset total weight total value not feasible not feasible not feasible not feasible not feasible not feasible not feasible b figure a instance of the knapsack problem b its solution by exhaustive search the information about the optimal selection is in bold similar problems in less than exponential time alternatively we can use one of many approximation algorithms such as those described in section assignment problem in our third example of a problem that can be solved by exhaustive search there are n people who need to be assigned to execute n jobs one person per job that is each person is assigned to exactly one job and each job is assigned to exactly one person the cost that would accrue if the ith person is assigned to the j th job is a known quantity ci j for each pair i j n the problem is to find an assignment with the minimum total cost a small instance of this problem follows with the table entries representing the assignment costs ci j job job job job person person person person it is easy to see that an instance of the assignment problem is completely specified by its cost matrix c in terms of this matrix the problem is to select one element in each row of the matrix so that all selected elements are in different columns and the total sum of the selected elements is the smallest possible note that no obvious strategy for finding a solution works here for example we can not select the smallest element in each row because the smallest elements may happen to be in the same column in fact the smallest element in the entire matrix need not be a component of an optimal solution thus opting for the exhaustive search may appear as an unavoidable evil we can describe feasible solutions to the assignment problem as ntuples j jn in which the ith component i n indicates the column of the element selected in the ith row ie the job number assigned to the ith person for example for the cost matrix above indicates the assignment of person to job person to job person to job and person to job the requirements of the assignment problem imply that there is a onetoone correspondence between feasible assignments and permutations of the first n integers therefore the exhaustivesearch approach to the assignment problem would require generating all the permutations of integers n computing the total cost of each assignment by summing up the corresponding elements of the cost matrix and finally selecting the one with the smallest sum a few first iterations of applying this algorithm to the instance given above are shown in figure you are asked to complete it in the exercises cost cost c cost cost etc cost cost figure first few iterations of solving a small instance of the assignment problem by exhaustive search since the number of permutations to be considered for the general case of the assignment problem is n exhaustive search is impractical for all but very small instances of the problem fortunately there is a much more efficient algorithm for this problem called the hungarian method after the hungarian mathematicians ko nig and egerva ry whose work underlies the method see eg kol this is good news the fact that a problem domain grows exponentially or faster does not necessarily imply that there can be no efficient algorithm for solving it in fact we present several other examples of such problems later in the book however such examples are more of an exception to the rule more often than not there are no known polynomialtime algorithms for problems whose domain grows exponentially with instance size provided we want to solve them exactly and as we mentioned above such algorithms quite possibly do not exist exercises a assuming that each tour can be generated in constant time what will be the efficiency class of the exhaustivesearch algorithm outlined in the text for the traveling salesman problem b if this algorithm is programmed on a computer that makes ten billion additions per second estimate the maximum number of cities for which the problem can be solved in i hour ii hours iii year iv century outline an exhaustivesearch algorithm for the hamiltonian circuit problem outline an algorithm to determine whether a connected graph represented by its adjacency matrix has an eulerian circuit what is the efficiency class of your algorithm complete the application of exhaustive search to the instance of the assignment problem started in the text give an example of the assignment problem whose optimal solution does not include the smallest element of its cost matrix consider the partition problem given n positive integers partition them into two disjoint subsets with the same sum of their elements of course the problem does not always have a solution design an exhaustivesearch algorithm for this problem try to minimize the number of subsets the algorithm needs to generate consider the clique problem given a graph g and a positive integer k determine whether the graph contains a clique of size k ie a complete subgraph of k vertices design an exhaustivesearch algorithm for this problem explain how exhaustive search can be applied to the sorting problem and determine the efficiency class of such an algorithm eightqueens problem consider the classic puzzle of placing eight queens on an chessboard so that no two queens are in the same row or in the same column or on the same diagonal how many different positions are there so that a no two queens are on the same square b no two queens are in the same row c no two queens are in the same row or in the same column also estimate how long it would take to find all the solutions to the problem by exhaustive search based on each of these approaches on a computer capable of checking billion positions per second magic squares a magic square of order n is an arrangement of the integers from to n in an n n matrix with each number occurring exactly once so that each row each column and each main diagonal has the same sum a prove that if a magic square of order n exists the sum in question must be equal to nn b design an exhaustivesearch algorithm for generating all magic squares of order n c go to the internet or your library and find a better algorithm for generating magic squares d implement the two algorithms the exhaustive search and the one you have found and run an experiment to determine the largest value of n for which each of the algorithms is able to find a magic square of order n in less than minute on your computer famous alphametic a puzzle in which the digits in a correct mathematical expression such as a sum are replaced by letters is called cryptarithm if in addition the puzzles words make sense it is said to be an alphametic the most wellknown alphametic was published by the renowned british puzzlist henry e dudeney s end more money two conditions are assumed first the correspondence between letters and decimal digits is onetoone ie each letter represents one digit only and different letters represent different digits second the digit zero does not appear as the leftmost digit in any of the numbers to solve an alphametic means to find which digit each letter represents note that a solutions uniqueness can not be assumed and has to be verified by the solver a write a program for solving cryptarithms by exhaustive search assume that a given cryptarithm is a sum of two words b solve dudeneys puzzle the way it was expected to be solved when it was first published in depthfirst search and breadthfirst search the term exhaustive search can also be applied to two very important algorithms that systematically process all vertices and edges of a graph these two traversal algorithms are depthfirst search dfs and breadthfirst search bfs these algorithms have proved to be very useful for many applications involving graphs in artificial intelligence and operations research in addition they are indispensable for efficient investigation of fundamental properties of graphs such as connectivity and cycle presence depthfirst search depthfirst search starts a graphs traversal at an arbitrary vertex by marking it as visited on each iteration the algorithm proceeds to an unvisited vertex that is adjacent to the one it is currently in if there are several such vertices a tie can be resolved arbitrarily as a practical matter which of the adjacent unvisited candidates is chosen is dictated by the data structure representing the graph in our examples we always break ties by the alphabetical order of the vertices this process continues until a dead end a vertex with no adjacent unvisited vertices is encountered at a dead end the algorithm backs up one edge to the vertex it came from and tries to continue visiting unvisited vertices from there the algorithm eventually halts after backing up to the starting vertex with the latter being a dead end by then all the vertices in the same connected component as the starting vertex have been visited if unvisited vertices still remain the depthfirst search must be restarted at any one of them it is convenient to use a stack to trace the operation of depthfirst search we push a vertex onto the stack when the vertex is reached for the first time ie the a g g h e c h a e b j c f d f i d f i c h d b a g j b j i e a b c figure example of a dfs traversal a graph b traversals stack the first subscript number indicates the order in which a vertex is visited ie pushed onto the stack the second one indicates the order in which it becomes a deadend ie popped off the stack c dfs forest with the tree and back edges shown with solid and dashed lines respectively visit of the vertex starts and we pop a vertex off the stack when it becomes a dead end ie the visit of the vertex ends it is also very useful to accompany a depthfirst search traversal by constructing the socalled depthfirst search forest the starting vertex of the traversal serves as the root of the first tree in such a forest whenever a new unvisited vertex is reached for the first time it is attached as a child to the vertex from which it is being reached such an edge is called a tree edge because the set of all such edges forms a forest the algorithm may also encounter an edge leading to a previously visited vertex other than its immediate predecessor ie its parent in the tree such an edge is called a back edge because it connects a vertex to its ancestor other than the parent in the depthfirst search forest figure provides an example of a depthfirst search traversal with the traversal stack and corresponding depthfirst search forest shown as well here is pseudocode of the depthfirst search algorithm dfsg implements a depthfirst search traversal of a given graph input graph g v e output graph g with its vertices marked with consecutive integers in the order they are first encountered by the dfs traversal mark each vertex in v with as a mark of being unvisited count for each vertex v in v do if v is marked with dfsv dfsv visits recursively all the unvisited vertices connected to vertex v by a path and numbers them in the order they are encountered via global variable count count count mark v with count for each vertex w in v adjacent to v do if w is marked with dfsw the brevity of the dfs pseudocode and the ease with which it can be performed by hand may create a wrong impression about the level of sophistication of this algorithm to appreciate its true power and depth you should trace the algorithms action by looking not at a graphs diagram but at its adjacency matrix or adjacency lists try it for the graph in figure or a smaller example how efficient is depthfirst search it is not difficult to see that this algorithm is in fact quite efficient since it takes just the time proportional to the size of the data structure used for representing the graph in question thus for the adjacency matrix representation the traversal time is in v and for the adjacency list representation it is in v e where v and e are the number of the graphs vertices and edges respectively a dfs forest which is obtained as a byproduct of a dfs traversal deserves a few comments too to begin with it is not actually a forest rather we can look at it as the given graph with its edges classified by the dfs traversal into two disjoint classes tree edges and back edges no other types are possible for a dfs forest of an undirected graph again tree edges are edges used by the dfs traversal to reach previously unvisited vertices if we consider only the edges in this class we will indeed get a forest back edges connect vertices to previously visited vertices other than their immediate predecessors in the traversal they connect vertices to their ancestors in the forest other than their parents a dfs traversal itself and the forestlike representation of the graph it provides have proved to be extremely helpful for the development of efficient algorithms for checking many important properties of graphs note that the dfs yields two orderings of vertices the order in which the vertices are reached for the first time pushed onto the stack and the order in which the vertices become dead ends popped off the stack these orders are qualitatively different and various applications can take advantage of either of them important elementary applications of dfs include checking connectivity and checking acyclicity of a graph since dfs halts after visiting all the vertices con the discovery of several such applications was an important breakthrough achieved by the two american computer scientists john hopcroft and robert tarjan in the s for this and other contributions they were given the turing award the most prestigious prize in the computing field hop tar nected by a path to the starting vertex checking a graphs connectivity can be done as follows start a dfs traversal at an arbitrary vertex and check after the algorithm halts whether all the vertices of the graph will have been visited if they have the graph is connected otherwise it is not connected more generally we can use dfs for identifying connected components of a graph how as for checking for a cycle presence in a graph we can take advantage of the graphs representation in the form of a dfs forest if the latter does not have back edges the graph is clearly acyclic if there is a back edge from some vertex u to its ancestor v eg the back edge from d to a in figure c the graph has a cycle that comprises the path from v to u via a sequence of tree edges in the dfs forest followed by the back edge from u to v you will find a few other applications of dfs later in the book although more sophisticated applications such as finding articulation points of a graph are not included a vertex of a connected graph is said to be its articulation point if its removal with all edges incident to it breaks the graph into disjoint pieces breadthfirst search if depthfirst search is a traversal for the brave the algorithm goes as far from home as it can breadthfirst search is a traversal for the cautious it proceeds in a concentric manner by visiting first all the vertices that are adjacent to a starting vertex then all unvisited vertices two edges apart from it and so on until all the vertices in the same connected component as the starting vertex are visited if there still remain unvisited vertices the algorithm has to be restarted at an arbitrary vertex of another connected component of the graph it is convenient to use a queue note the difference from depthfirst search to trace the operation of breadthfirst search the queue is initialized with the traversals starting vertex which is marked as visited on each iteration the algorithm identifies all unvisited vertices that are adjacent to the front vertex marks them as visited and adds them to the queue after that the front vertex is removed from the queue similarly to a dfs traversal it is useful to accompany a bfs traversal by constructing the socalled breadthfirst search forest the traversals starting vertex serves as the root of the first tree in such a forest whenever a new unvisited vertex is reached for the first time the vertex is attached as a child to the vertex it is being reached from with an edge called a tree edge if an edge leading to a previously visited vertex other than its immediate predecessor ie its parent in the tree is encountered the edge is noted as a cross edge figure provides an example of a breadthfirst search traversal with the traversal queue and corresponding breadthfirst search forest shown g h a g a e c f a c d e f b c d e h j g h j i d b f b i j i a b c figure example of a bfs traversal a graph b traversal queue with the numbers indicating the order in which the vertices are visited ie added to and removed from the queue c bfs forest with the tree and cross edges shown with solid and dotted lines respectively here is pseudocode of the breadthfirst search algorithm bfsg implements a breadthfirst search traversal of a given graph input graph g v e output graph g with its vertices marked with consecutive integers in the order they are visited by the bfs traversal mark each vertex in v with as a mark of being unvisited count for each vertex v in v do if v is marked with bfsv bfsv visits all the unvisited vertices connected to vertex v by a path and numbers them in the order they are visited via global variable count count count mark v with count and initialize a queue with v while the queue is not empty do for each vertex w in v adjacent to the front vertex do if w is marked with count count mark w with count add w to the queue remove the front vertex from the queue a a b c d b e e f g h c f d g a b figure illustration of the bfsbased algorithm for finding a minimumedge path a graph b part of its bfs tree that identifies the minimumedge path from a to g breadthfirst search has the same efficiency as depthfirst search it is in v for the adjacency matrix representation and in v e for the adjacency list representation unlike depthfirst search it yields a single ordering of vertices because the queue is a fifo firstin firstout structure and hence the order in which vertices are added to the queue is the same order in which they are removed from it as to the structure of a bfs forest of an undirected graph it can also have two kinds of edges tree edges and cross edges tree edges are the ones used to reach previously unvisited vertices cross edges connect vertices to those visited before but unlike back edges in a dfs tree they connect vertices either on the same or adjacent levels of a bfs tree bfs can be used to check connectivity and acyclicity of a graph essentially in the same manner as dfs can it is not applicable however for several less straightforward applications such as finding articulation points on the other hand it can be helpful in some situations where dfs can not for example bfs can be used for finding a path with the fewest number of edges between two given vertices to do this we start a bfs traversal at one of the two vertices and stop it as soon as the other vertex is reached the simple path from the root of the bfs tree to the second vertex is the path sought for example path a b c g in the graph in figure has the fewest number of edges among all the paths between vertices a and g although the correctness of this application appears to stem immediately from the way bfs operates a mathematical proof of its validity is not quite elementary see eg cor section table summarizes the main facts about depthfirst search and breadthfirst search table main facts about depthfirst search dfs and breadthfirst search bfs dfs bfs data structure a stack a queue number of vertex orderings two orderings one ordering edge types undirected graphs tree and back edges tree and cross edges applications connectivity connectivity acyclicity acyclicity articulation points minimumedge paths efficiency for adjacency matrix v v efficiency for adjacency lists v e v e exercises consider the following graph f b c g d a e a write down the adjacency matrix and adjacency lists specifying this graph assume that the matrix rows and columns and vertices in the adjacency lists follow in the alphabetical order of the vertex labels b starting at vertex a and resolving ties by the vertex alphabetical order traverse the graph by depthfirst search and construct the corresponding depthfirst search tree give the order in which the vertices were reached for the first time pushed onto the traversal stack and the order in which the vertices became dead ends popped off the stack if we define sparse graphs as graphs for which e ov which implementation of dfs will have a better time efficiency for such graphs the one that uses the adjacency matrix or the one that uses the adjacency lists let g be a graph with n vertices and m edges a true or false all its dfs forests for traversals starting at different vertices will have the same number of trees b true or false all its dfs forests will have the same number of tree edges and the same number of back edges traverse the graph of problem by breadthfirst search and construct the corresponding breadthfirst search tree start the traversal at vertex a and resolve ties by the vertex alphabetical order prove that a cross edge in a bfs tree of an undirected graph can connect vertices only on either the same level or on two adjacent levels of a bfs tree a explain how one can check a graphs acyclicity by using breadthfirst search b does either of the two traversals dfs or bfs always find a cycle faster than the other if you answer yes indicate which of them is better and explain why it is the case if you answer no give two examples supporting your answer explain how one can identify connected components of a graph by using a a depthfirst search b a breadthfirst search a graph is said to be bipartite if all its vertices can be partitioned into two disjoint subsets x and y so that every edge connects a vertex in x with a vertex in y one can also say that a graph is bipartite if its vertices can be colored in two colors so that every edge has its vertices colored in different colors such graphs are also called colorable for example graph i is bipartite while graph ii is not x y x a b y x y c d i ii a design a dfsbased algorithm for checking whether a graph is bipartite b design a bfsbased algorithm for checking whether a graph is bipartite write a program that for a given graph outputs a vertices of each connected component b its cycle or a message that the graph is acyclic one can model a maze by having a vertex for a starting point a finishing point dead ends and all the points in the maze where more than one path can be taken and then connecting the vertices according to the paths in the maze a construct such a graph for the following maze b which traversal dfs or bfs would you use if you found yourself in a maze and why three jugs sime on denis poisson a famous french mathematician and physicist is said to have become interested in mathematics after encountering some version of the following old puzzle given an pint jug full of water and two empty jugs of and pint capacity get exactly pints of water in one of the jugs by completely filling up andor emptying jugs into others solve this puzzle by using breadthfirst search summary brute force is a straightforward approach to solving a problem usually directly based on the problem statement and definitions of the concepts involved the principal strengths of the bruteforce approach are wide applicability and simplicity its principal weakness is the subpar efficiency of most bruteforce algorithms a first application of the bruteforce approach often results in an algorithm that can be improved with a modest amount of effort the following noted algorithms can be considered as examples of the bruteforce approach definitionbased algorithm for matrix multiplication selection sort sequential search straightforward stringmatching algorithm exhaustive search is a bruteforce approach to combinatorial problems it suggests generating each and every combinatorial object of the problem selecting those of them that satisfy all the constraints and then finding a desired object the traveling salesman problem the knapsack problem and the assignment problem are typical examples of problems that can be solved at least theoretically by exhaustivesearch algorithms exhaustive search is impractical for all but very small instances of problems it can be applied to depthfirst search dfs and breadthfirst search bfs are two principal graphtraversal algorithms by representing a graph in a form of a depthfirst or breadthfirst search forest they help in the investigation of many important properties of the graph both algorithms have the same time efficiency v for the adjacency matrix representation and v e for the adjacency list representation decreaseandconquer plutarch says that sertorius in order to teach his soldiers that perseverance and wit are better than brute force had two horses brought before them and set two men to pull out their tails one of the men was a burly hercules who tugged and tugged but all to no purpose the other was a sharp weaselfaced tailor who plucked one hair at a time amidst roars of laughter and soon left the tail quite bare e cobham brewer dictionary of phrase and fable the decreaseandconquer technique is based on exploiting the relationship between a solution to a given instance of a problem and a solution to its smaller instance once such a relationship is established it can be exploited either top down or bottom up the former leads naturally to a recursive implementation although as one can see from several examples in this chapter an ultimate implementation may well be nonrecursive the bottomup variation is usually implemented iteratively starting with a solution to the smallest instance of the problem it is called sometimes the incremental approach there are three major variations of decreaseandconquer decrease by a constant decrease by a constant factor variable size decrease in the decreasebyaconstant variation the size of an instance is reduced by the same constant on each iteration of the algorithm typically this constant is equal to one figure although other constant size reductions do happen occasionally consider as an example the exponentiation problem of computing an where a and n is a nonnegative integer the relationship between a solution to an instance of size n and an instance of size n is obtained by the obvious formula an an a so the function f n an can be computed either top down by using its recursive definition problem of size n subproblem of size n solution to the subproblem solution to the original problem figure decreaseby oneandconquer technique f n f n a if n if n or bottom up by multiplying by a n times yes it is the same as the bruteforce algorithm but we have come to it by a different thought process more interesting examples of decreasebyone algorithms appear in sections the decreasebyaconstantfactor technique suggests reducing a problem instance by the same constant factor on each iteration of the algorithm in most applications this constant factor is equal to two can you give an example of such an algorithm the decreasebyhalf idea is illustrated in figure for an example let us revisit the exponentiation problem if the instance of size n is to compute an the instance of half its size is to compute an with the obvious relationship between the two an an but since we consider here instances with integer exponents only the former does not work for odd n if n is odd we have to compute an by using the rule for evenvalued exponents and then multiply the result by a to summarize we have the following formula problem of size n subproblem of size n solution to the subproblem solution to the original problem figure decreaseby halfandconquer technique an an an a if n is even and positive if n is odd if n if we compute an recursively according to formula and measure the algorithms efficiency by the number of multiplications we should expect the algorithm to be in log n because on each iteration the size is reduced by about a half at the expense of one or two multiplications a few other examples of decreasebyaconstantfactor algorithms are given in section and its exercises such algorithms are so efficient however that there are few examples of this kind finally in the variablesizedecrease variety of decreaseandconquer the sizereduction pattern varies from one iteration of an algorithm to another euclids algorithm for computing the greatest common divisor provides a good example of such a situation recall that this algorithm is based on the formula gcdm n gcdn m mod n though the value of the second argument is always smaller on the righthand side than on the lefthand side it decreases neither by a constant nor by a constant factor a few other examples of such algorithms appear in section insertion sort in this section we consider an application of the decreasebyone technique to sorting an array an following the techniques idea we assume that the smaller problem of sorting the array an has already been solved to give us a sorted array of size n a an how can we take advantage of this solution to the smaller problem to get a solution to the original problem by taking into account the element an obviously all we need is to find an appropriate position for an among the sorted elements and insert it there this is usually done by scanning the sorted subarray from right to left until the first element smaller than or equal to an is encountered to insert an right after that element the resulting algorithm is called straight insertion sort or simply insertion sort though insertion sort is clearly based on a recursive idea it is more efficient to implement this algorithm bottom up ie iteratively as shown in figure starting with a and ending with an ai is inserted in its appropriate place among the first i elements of the array that have been already sorted but unlike selection sort are generally not in their final positions here is pseudocode of this algorithm algorithm insertionsortan sorts a given array by insertion sort input an array an of n orderable elements output array an sorted in nondecreasing order for i to n do v ai j i while j and aj v do aj aj j j aj v a a j a j ai ai an smaller than or equal to ai greater than ai figure iteration of insertion sort ai is inserted in its proper position among the preceding elements previously sorted figure example of sorting with insertion sort a vertical bar separates the sorted part of the array from the remaining elements the element being inserted is in bold the operation of the algorithm is illustrated in figure the basic operation of the algorithm is the key comparison aj v why not j because it is almost certainly faster than the former in an actual computer implementation moreover it is not germane to the algorithm a better implementation with a sentinel see problem in this sections exercises eliminates it altogether the number of key comparisons in this algorithm obviously depends on the nature of the input in the worst case aj v is executed the largest number of times ie for every j i since v ai it happens if and only if aj ai for j i note that we are using the fact that on the ith iteration of insertion sort all the elements preceding ai are the first i elements in the input albeit in the sorted order thus for the worstcase input we get a a for i a a for i an an for i n in other words the worstcase input is an array of strictly decreasing values the number of key comparisons for such an input is n i n n n cworst n i n i j i thus in the worst case insertion sort makes exactly the same number of comparisons as selection sort see section in the best case the comparison aj v is executed only once on every iteration of the outer loop it happens if and only if ai ai for every i n ie if the input array is already sorted in nondecreasing order though it makes sense that the best case of an algorithm happens when the problem is already solved it is not always the case as you are going to see in our discussion of quicksort in chapter thus for sorted arrays the number of key comparisons is n cbest n n n i this very good performance in the best case of sorted arrays is not very useful by itself because we can not expect such convenient inputs however almostsorted files do arise in a variety of applications and insertion sort preserves its excellent performance on such inputs a rigorous analysis of the algorithms averagecase efficiency is based on investigating the number of element pairs that are out of order see problem in this sections exercises it shows that on randomly ordered arrays insertion sort makes on average half as many comparisons as on decreasing arrays ie cavgn n n this twiceasfast averagecase performance coupled with an excellent efficiency on almostsorted arrays makes insertion sort stand out among its principal competitors among elementary sorting algorithms selection sort and bubble sort in addition its extension named shellsort after its inventor d l shell she gives us an even better algorithm for sorting moderately large files see problem in this sections exercises exercises ferrying soldiers a detachment of n soldiers must cross a wide and deep river with no bridge in sight they notice two yearold boys playing in a rowboat by the shore the boat is so tiny however that it can only hold two boys or one soldier how can the soldiers get across the river and leave the boys in joint possession of the boat how many times need the boat pass from shore to shore alternating glasses a there are n glasses standing next to each other in a row the first n of them filled with a soda drink and the remaining n glasses empty make the glasses alternate in a filledemptyfilledempty pattern in the minimum number of glass moves gar b solve the same problem if n glasses n with a drink and n empty are initially in a random order marking cells design an algorithm for the following task for any even n mark n cells on an infinite sheet of graph paper so that each marked cell has an odd number of marked neighbors two cells are considered neighbors if they are next to each other either horizontally or vertically but not diagonally the marked cells must form a contiguous region ie a region in which there is a path between any pair of marked cells that goes through a sequence of marked neighbors kor design a decreasebyone algorithm for generating the power set of a set of n elements the power set of a set s is the set of all the subsets of s including the empty set and s itself consider the following algorithm to check connectivity of a graph defined by its adjacency matrix algorithm connectedan n input adjacency matrix an n of an undirected graph g output true if g is connected and false if it is not if n return onevertex graph is connected by definition else if not connectedan n return else for j to n do if an j return return does this algorithm work correctly for every undirected graph with n vertices if you answer yes indicate the algorithms efficiency class in the worst case if you answer no explain why team ordering you have the results of a completed roundrobin tournament in which n teams played each other once each game ended either with a victory for one of the teams or with a tie design an algorithm that lists the teams in a sequence so that every team did not lose the game with the team listed immediately after it what is the time efficiency class of your algorithm apply insertion sort to sort the list e x a m p l e in alphabetical order a what sentinel should be put before the first element of an array being sorted in order to avoid checking the inbound condition j on each iteration of the inner loop of insertion sort b is the sentinel version in the same efficiency class as the original version is it possible to implement insertion sort for sorting linked lists will it have the same on time efficiency as the array version compare the texts implementation of insertion sort with the following version algorithm insertsortan for i to n do j i while j and aj aj do swapaj aj j j what is the time efficiency of this algorithm how is it compared to that of the version given in section let an be an array of n sortable elements for simplicity you may assume that all the elements are distinct a pair ai aj is called an inversion if i j and ai aj a what arrays of size n have the largest number of inversions and what is this number answer the same questions for the smallest number of inversions b show that the averagecase number of key comparisons in insertion sort is given by the formula cavgn n shellsort more accurately shells sort is an important sorting algorithm that works by applying insertion sort to each of several interleaving sublists of a given list on each pass through the list the sublists in question are formed by stepping through the list with an increment hi taken from some predefined decreasing sequence of step sizes h hi which must end with the algorithm works for any such sequence though some sequences are known to yield a better efficiency than others for example the sequence used of course in reverse is known to be among the best for this purpose a apply shellsort to the list s h e l l s o r t i s u s e f u l b is shellsort a stable sorting algorithm c implement shellsort straight insertion sort selection sort and bubble sort in the language of your choice and compare their performance on random arrays of sizes n for n and as well as on increasing and decreasing arrays of these sizes topological sorting in this section we discuss an important problem for directed graphs with a variety of applications involving prerequisiterestricted tasks before we pose this problem though let us review a few basic facts about directed graphs themselves a directed graph or digraph for short is a graph with directions specified for all its edges figure a is an example the adjacency matrix and adjacency lists are still two principal means of representing a digraph there are only two notable differences between undirected and directed graphs in representing them the adjacency matrix of a directed graph does not have to be symmetric an edge in a directed graph has just one not two corresponding nodes in the digraphs adjacency lists a b a d c b e d c e a b figure a digraph b dfs forest of the digraph for the dfs traversal started at a depthfirst search and breadthfirst search are principal traversal algorithms for traversing digraphs as well but the structure of corresponding forests can be more complex than for undirected graphs thus even for the simple example of figure a the depthfirst search forest figure b exhibits all four types of edges possible in a dfs forest of a directed graph tree edges ab bc de back edges ba from vertices to their ancestors forward edges ac from vertices to their descendants in the tree other than their children and cross edges dc which are none of the aforementioned types note that a back edge in a dfs forest of a directed graph can connect a vertex to its parent whether or not it is the case the presence of a back edge indicates that the digraph has a directed cycle a directed cycle in a digraph is a sequence of three or more of its vertices that starts and ends with the same vertex and in which every vertex is connected to its immediate predecessor by an edge directed from the predecessor to the successor for example a b a is a directed cycle in the digraph in figure a conversely if a dfs forest of a digraph has no back edges the digraph is a dag an acronym for directed acyclic graph edge directions lead to new questions about digraphs that are either meaningless or trivial for undirected graphs in this section we discuss one such question as a motivating example consider a set of five required courses c c c c c a parttime student has to take in some degree program the courses can be taken in any order as long as the following course prerequisites are met c and c have no prerequisites c requires c and c c requires c and c requires c and c the student can take only one course per term in which order should the student take the courses the situation can be modeled by a digraph in which vertices represent courses and directed edges indicate prerequisite requirements figure in terms of this digraph the question is whether we can list its vertices in such an order that for every edge in the graph the vertex where the edge starts is listed before the vertex where the edge ends can you find such an ordering of this digraphs vertices this problem is called topological sorting it can be posed for an c c c c c figure digraph representing the prerequisite structure of five courses c c c the poppingoff order c c c c c c c c the topologically sorted list c c c c c c c c c a b c figure a digraph for which the topological sorting problem needs to be solved b dfs traversal stack with the subscript numbers indicating the poppingoff order c solution to the problem arbitrary digraph but it is easy to see that the problem can not have a solution if a digraph has a directed cycle thus for topological sorting to be possible a digraph in question must be a dag it turns out that being a dag is not only necessary but also sufficient for topological sorting to be possible ie if a digraph has no directed cycles the topological sorting problem for it has a solution moreover there are two efficient algorithms that both verify whether a digraph is a dag and if it is produce an ordering of vertices that solves the topological sorting problem the first algorithm is a simple application of depthfirst search perform a dfs traversal and note the order in which vertices become deadends ie popped off the traversal stack reversing this order yields a solution to the topological sorting problem provided of course no back edge has been encountered during the traversal if a back edge has been encountered the digraph is not a dag and topological sorting of its vertices is impossible why does the algorithm work when a vertex v is popped off a dfs stack no vertex u with an edge from u to v can be among the vertices popped off before v otherwise u v would have been a back edge hence any such vertex u will be listed after v in the poppedoff order list and before v in the reversed list figure illustrates an application of this algorithm to the digraph in figure note that in figure c we have drawn the edges of the digraph and they all point from left to right as the problems statement requires it is a convenient way to check visually the correctness of a solution to an instance of the topological sorting problem c c c c c delete c c delete c c c c c c c delete c c delete c delete c c c the solution obtained is c c c c c figure illustration of the sourceremoval algorithm for the topological sorting problem on each iteration a vertex with no incoming edges is deleted from the digraph the second algorithm is based on a direct implementation of the decreaseby oneandconquer technique repeatedly identify in a remaining digraph a source which is a vertex with no incoming edges and delete it along with all the edges outgoing from it if there are several sources break the tie arbitrarily if there are none stop because the problem can not be solved see problem a in this sections exercises the order in which the vertices are deleted yields a solution to the topological sorting problem the application of this algorithm to the same digraph representing the five courses is given in figure note that the solution obtained by the sourceremoval algorithm is different from the one obtained by the dfsbased algorithm both of them are correct of course the topological sorting problem may have several alternative solutions the tiny size of the example we used might create a wrong impression about the topological sorting problem but imagine a large project eg in construction research or software development that involves a multitude of interrelated tasks with known prerequisites the first thing to do in such a situation is to make sure that the set of given prerequisites is not contradictory the convenient way of doing this is to solve the topological sorting problem for the projects digraph only then can one start thinking about scheduling tasks to say minimize the total completion time of the project this would require of course other algorithms that you can find in general books on operations research or in special ones on cpm critical path method and pert program evaluation and review technique methodologies as to applications of topological sorting in computer science they include instruction scheduling in program compilation cell evaluation ordering in spreadsheet formulas and resolving symbol dependencies in linkers exercises apply the dfsbased algorithm to solve the topological sorting problem for the following digraphs a b a b c d c d e g e f g f a b a prove that the topological sorting problem has a solution if and only if it is a dag b for a digraph with n vertices what is the largest number of distinct solutions the topological sorting problem can have a what is the time efficiency of the dfsbased algorithm for topological sorting b how can one modify the dfsbased algorithm to avoid reversing the vertex ordering generated by dfs can one use the order in which vertices are pushed onto the dfs stack instead of the order they are popped off it to solve the topological sorting problem apply the sourceremoval algorithm to the digraphs of problem above a prove that a nonempty dag must have at least one source b how would you find a source or determine that such a vertex does not exist in a digraph represented by its adjacency matrix what is the time efficiency of this operation c how would you find a source or determine that such a vertex does not exist in a digraph represented by its adjacency lists what is the time efficiency of this operation can you implement the sourceremoval algorithm for a digraph represented by its adjacency lists so that its running time is in ov e implement the two topological sorting algorithms in the language of your choice run an experiment to compare their running times a digraph is called strongly connected if for any pair of two distinct vertices u and v there exists a directed path from u to v and a directed path from v to u in general a digraphs vertices can be partitioned into disjoint maximal subsets of vertices that are mutually accessible via directed paths these subsets are called strongly connected components of the digraph there are two dfsbased algorithms for identifying strongly connected components here is the simpler but somewhat less efficient one of the two step perform a dfs traversal of the digraph given and number its vertices in the order they become dead ends step reverse the directions of all the edges of the digraph step perform a dfs traversal of the new digraph by starting and if necessary restarting the traversal at the highest numbered vertex among still unvisited vertices the strongly connected components are exactly the vertices of the dfs trees obtained during the last traversal a apply this algorithm to the following digraph to determine its strongly connected components a b c d e f g h b what is the time efficiency class of this algorithm give separate answers for the adjacency matrix representation and adjacency list representation of an input digraph c how many strongly connected components does a dag have spiders web a spider sits at the bottom point s of its web and a fly sits at the top f how many different ways can the spider reach the fly by moving along the webs lines in the directions indicated by the arrows kor f s algorithms for generating combinatorial objects in this section we keep our promise to discuss algorithms for generating combinatorial objects the most important types of combinatorial objects are permutations combinations and subsets of a given set they typically arise in problems that require a consideration of different choices we already encountered them in chapter when we discussed exhaustive search combinatorial objects are studied in a branch of discrete mathematics called combinatorics mathematicians of course are primarily interested in different counting formulas we should be grateful for such formulas because they tell us how many items need to be generated in particular they warn us that the number of combinatorial objects typically grows exponentially or even faster as a function of the problem size but our primary interest here lies in algorithms for generating combinatorial objects not just in counting them generating permutations we start with permutations for simplicity we assume that the underlying set whose elements need to be permuted is simply the set of integers from to n more generally they can be interpreted as indices of elements in an nelement set a an what would the decreasebyone technique suggest for the problem of generating all n permutations of n the smallerbyone problem is to generate all n permutations assuming that the smaller problem is solved we can get a solution to the larger one by inserting n in each of the n possible positions among elements of every permutation of n elements all the permutations obtained in this fashion will be distinct why and their total number will be nn n hence we will obtain all the permutations of n we can insert n in the previously generated permutations either left to right or right to left it turns out that it is beneficial to start with inserting n into n by moving right to left and then switch direction every time a new permutation of n needs to be processed an example of applying this approach bottom up for n is given in figure the advantage of this order of generating permutations stems from the fact that it satisfies the minimalchange requirement each permutation can be obtained from its immediate predecessor by exchanging just two elements in it for the method being discussed these two elements are always adjacent to each other start insert into right to left insert into right to left insert into left to right figure generating permutations bottom up check this for the permutations generated in figure the minimalchange requirement is beneficial both for the algorithms speed and for applications using the permutations for example in section we needed permutations of cities to solve the traveling salesman problem by exhaustive search if such permutations are generated by a minimalchange algorithm we can compute the length of a new tour from the length of its predecessor in constant rather than linear time how it is possible to get the same ordering of permutations of n elements without explicitly generating permutations for smaller values of n it can be done by associating a direction with each element k in a permutation we indicate such a direction by a small arrow written above the element in question eg the element k is said to be mobile in such an arrowmarked permutation if its arrow points to a smaller number adjacent to it for example for the permutation and are mobile while and are not using the notion of a mobile element we can give the following description of the johnsontrotter algorithm for generating permutations algorithm johnsontrottern implements johnsontrotter algorithm for generating permutations input a positive integer n output a list of all permutations of n initialize the first permutation with n while the last permutation has a mobile element do find its largest mobile element k swap k with the adjacent element ks arrow points to reverse the direction of all the elements that are larger than k add the new permutation to the list here is an application of this algorithm for n with the largest mobile element shown in bold this algorithm is one of the most efficient for generating permutations it can be implemented to run in time proportional to the number of permutations ie in n of course it is horribly slow for all but very small values of n however this is not the algorithms fault but rather the fault of the problem it simply asks to generate too many items one can argue that the permutation ordering generated by the johnsontrotter algorithm is not quite natural for example the natural place for permutation nn seems to be the last one on the list this would be the case if permutations were listed in increasing order also called the lexicographic order which is the order in which they would be listed in a dictionary if the numbers were interpreted as letters of an alphabet for example for n so how can we generate the permutation following aa anan in lexicographic order if an an which is the case for exactly one half of all the permutations we can simply transpose these last two elements for example is followed by if an an we find the permutations longest decreasing suffix ai ai an but ai ai increase ai by exchanging it with the smallest element of the suffix that is greater than ai and reverse the new suffix to put it in increasing order for example is followed by here is pseudocode of this simple algorithm whose origins go as far back as thcentury india algorithm lexicographicpermuten generates permutations in lexicographic order input a positive integer n output a list of all permutations of n in lexicographic order initialize the first permutation with n while last permutation has two consecutive elements in increasing order do let i be its largest index such that ai ai ai ai an find the largest index j such that ai aj j i since ai ai swap ai with aj aiai an will remain in decreasing order reverse the order of the elements from ai to an inclusive add the new permutation to the list generating subsets recall that in section we examined the knapsack problem which asks to find the most valuable subset of items that fits a knapsack of a given capacity the exhaustivesearch approach to solving this problem discussed there was based on generating all subsets of a given set of items in this section we discuss algorithms for generating all n subsets of an abstract set a a an mathematicians call the set of all subsets of a set its power set the decreasebyone idea is immediately applicable to this problem too all subsets of a a an can be divided into two groups those that do not contain an and those that do the former group is nothing but all the subsets of a an while each and every element of the latter can be obtained by adding an to a subset of a an thus once we have a list of all subsets of a an we can get all the subsets of a an by adding to the list all its elements with an put into each of them an application of this algorithm to generate all subsets of a a a is illustrated in figure similarly to generating permutations we do not have to generate power sets of smaller sets a convenient way of solving the problem directly is based on a onetoone correspondence between all n subsets of an n element set a a an n subsets a a a a a a a a a a a a a a a a a figure generating subsets bottom up and all n bit strings b bn of length n the easiest way to establish such a correspondence is to assign to a subset the bit string in which bi if ai belongs to the subset and bi if ai does not belong to it we mentioned this idea of bit vectors in section for example the bit string will correspond to the empty subset of a threeelement set will correspond to the set itself ie a a a and will represent a a with this correspondence in place we can generate all the bit strings of length n by generating successive binary numbers from to n padded when necessary with an appropriate number of leading s for example for the case of n we obtain bit strings subsets a a a a a a a a a a a a note that although the bit strings are generated by this algorithm in lexicographic order in the twosymbol alphabet of and the order of the subsets looks anything but natural for example we might want to have the socalled squashed order in which any subset involving aj can be listed only after all the subsets involving a aj as was the case for the list of the threeelement set in figure it is easy to adjust the bit stringbased algorithm above to yield a squashed ordering of the subsets involved see problem in this sections exercises a more challenging question is whether there exists a minimalchange algorithm for generating bit strings so that every one of them differs from its immediate predecessor by only a single bit in the language of subsets we want every subset to differ from its immediate predecessor by either an addition or a deletion but not both of a single element the answer to this question is yes for example for n we can get such a sequence of bit strings is called the binary reflected gray code frank gray a researcher at att bell laboratories reinvented it in the s to minimize the effect of errors in transmitting digital signals see eg ros pp seventy years earlier the french engineer e mile baudot used such codes in telegraphy here is pseudocode that generates the binary reflected gray code recursively algorithm brgcn generates recursively the binary reflected gray code of order n input a positive integer n output a list of all bit strings of length n composing the gray code if n make list l containing bit strings and in this order else generate list l of bit strings of size n by calling brgcn copy list l to list l in reversed order add in front of each bit string in list l add in front of each bit string in list l append l to l to get list l return l the correctness of the algorithm stems from the fact that it generates n bit strings and all of them are distinct both these assertions are easy to check by mathematical induction note that the binary reflected gray code is cyclic its last bit string differs from the first one by a single bit for a nonrecursive algorithm for generating the binary reflected gray code see problem in this sections exercises exercises is it realistic to implement an algorithm that requires generating all permutations of a element set on your computer what about all the subsets of such a set generate all permutations of by a the bottomup minimalchange algorithm b the johnsontrotter algorithm c the lexicographicorder algorithm apply lexicographicpermute to multiset does it generate correctly all the permutations in lexicographic order consider the following implementation of the algorithm for generating permutations discovered by b heap hea algorithm heappermuten implements heaps algorithm for generating permutations input a positive integer n and a global array an output all permutations of elements of a if n write a else for i to n do heappermuten if n is odd swap a and an else swap ai and an a trace the algorithm by hand for n and b prove the correctness of heaps algorithm c what is the time efficiency of heappermute generate all the subsets of a fourelement set a a a a a by each of the two algorithms outlined in this section what simple trick would make the bit stringbased algorithm generate subsets in squashed order write pseudocode for a recursive algorithm for generating all n bit strings of length n write a nonrecursive algorithm for generating n bit strings of length n that implements bit strings as arrays and does not use binary additions a generate the binary reflexive gray code of order b trace the following nonrecursive algorithm to generate the binary reflexive gray code of order start with the nbit string of all s for i n generate the ith bit string by flipping bit b in the previous bit string where b is the position of the least significant in the binary representation of i design a decreaseandconquer algorithm for generating all combinations of k items chosen from n ie all kelement subsets of a given nelement set is your algorithm a minimalchange algorithm gray code and the tower of hanoi a show that the disk moves made in the classic recursive algorithm for the tower of hanoi puzzle can be used for generating the binary reflected gray code b show how the binary reflected gray code can be used for solving the tower of hanoi puzzle fair attraction in olden days one could encounter the following attraction at a fair a light bulb was connected to several switches in such a way that it lighted up only when all the switches were closed each switch was controlled by a push button pressing the button toggled the switch but there was no way to know the state of the switch the object was to turn the light bulb on design an algorithm to turn on the light bulb with the minimum number of button pushes needed in the worst case for n switches decreasebyaconstantfactor algorithms you may recall from the introduction to this chapter that decreasebyaconstantfactor is the second major variety of decreaseandconquer as an example of an algorithm based on this technique we mentioned there exponentiation by squaring defined by formula in this section you will find a few other examples of such algorithms the most important and wellknown of them is binary search decreasebyaconstantfactor algorithms usually run in logarithmic time and being very efficient do not happen often a reduction by a factor other than two is especially rare binary search binary search is a remarkably efficient algorithm for searching in a sorted array it works by comparing a search key k with the arrays middle element am if they match the algorithm stops otherwise the same operation is repeated recursively for the first half of the array if k am and for the second half if k am k a am am am an search here if search here if k am k am as an example let us apply binary search to searching for k in the array the iterations of the algorithm are given in the following table index value iteration l m r iteration l m r iteration lm r though binary search is clearly based on a recursive idea it can be easily implemented as a nonrecursive algorithm too here is pseudocode of this nonrecursive version algorithm binarysearchan k implements nonrecursive binary search input an array an sorted in ascending order and a search key k output an index of the arrays element that is equal to k or if there is no such element l r n while l r do m l r if k am return m else if k am r m else l m return the standard way to analyze the efficiency of binary search is to count the number of times the search key is compared with an element of the array moreover for the sake of simplicity we will count the socalled threeway comparisons this assumes that after one comparison of k with am the algorithm can determine whether k is smaller equal to or larger than am how many such comparisons does the algorithm make on an array of n elements the answer obviously depends not only on n but also on the specifics of a particular instance of the problem let us find the number of key comparisons in the worst case cworstn the worstcase inputs include all arrays that do not contain a given search key as well as some successful searches since after one comparison the algorithm faces the same situation but for an array half the size we get the following recurrence relation for cworstn cworst n cworst n for n cworst stop and convince yourself that n must be indeed rounded down and that the initial condition must be written as specified we already encountered recurrence with a different initial condition in section see recurrence and its solution there for n k for the initial condition cworst we obtain cworst k k log n further similarly to the case of recurrence problem in exercises the solution given by formula for n k can be tweaked to get a solution valid for an arbitrary positive integer n cworst n log n logn formula deserves attention first it implies that the worstcase time efficiency of binary search is in log n second it is the answer we should have fully expected since the algorithm simply reduces the size of the remaining array by about half on each iteration the number of such iterations needed to reduce the initial size n to the final size has to be about log n third to reiterate the point made in section the logarithmic function grows so slowly that its values remain small even for very large values of n in particular according to formula it will take no more than log threeway comparisons to find an element of a given value or establish that there is no such element in any sorted array of one thousand elements and it will take no more than log comparisons to do it for any sorted array of size one million what can we say about the averagecase efficiency of binary search a sophisticated analysis shows that the average number of key comparisons made by binary search is only slightly smaller than that in the worst case cavgn log n more accurate formulas for the average number of comparisons in a successful and an unsuccessful search are cayvegs n log n and canvogn logn respectively though binary search is an optimal searching algorithm if we restrict our operations only to comparisons between keys see section there are searching algorithms see interpolation search in section and hashing in section with a better averagecase time efficiency and one of them hashing does not even require the array to be sorted these algorithms do require some special calculations in addition to key comparisons however finally the idea behind binary search has several applications beyond searching see eg ben in addition it can be applied to solving nonlinear equations in one unknown we discuss this continuous analogue of binary search called the method of bisection in section fakecoin problem of several versions of the fakecoin identification problem we consider here the one that best illustrates the decreasebyaconstantfactor strategy among n identicallooking coins one is fake with a balance scale we can compare any two sets of coins that is by tipping to the left to the right or staying even the balance scale will tell whether the sets weigh the same or which of the sets is heavier than the other but not by how much the problem is to design an efficient algorithm for detecting the fake coin an easier version of the problem the one we discuss here assumes that the fake coin is known to be say lighter than the genuine one the most natural idea for solving this problem is to divide n coins into two piles of n coins each leaving one extra coin aside if n is odd and put the two a much more challenging version assumes no additional information about the relative weights of the fake and genuine coins or even the presence of the fake coin among n given coins we pursue this more difficult version in the exercises for section piles on the scale if the piles weigh the same the coin put aside must be fake otherwise we can proceed in the same manner with the lighter pile which must be the one with the fake coin we can easily set up a recurrence relation for the number of weighings w n needed by this algorithm in the worst case w n w n for n w this recurrence should look familiar to you indeed it is almost identical to the one for the worstcase number of comparisons in binary search the difference is in the initial condition this similarity is not really surprising since both algorithms are based on the same technique of halving an instance size the solution to the recurrence for the number of weighings is also very similar to the one we had for binary search w n log n this stuff should look elementary by now if not outright boring but wait the interesting point here is the fact that the above algorithm is not the most efficient solution it would be more efficient to divide the coins not into two but into three piles of about n coins each details of a precise formulation are developed in this sections exercises do not miss it if your instructor forgets demand the instructor to assign problem after weighing two of the piles we can reduce the instance size by a factor of three accordingly we should expect the number of weighings to be about log n which is smaller than log n russian peasant multiplication now we consider a nonorthodox algorithm for multiplying two positive integers called multiplication a la russe or the russian peasant method let n and m be positive integers whose product we want to compute and let us measure the instance size by the value of n now if n is even an instance of half the size has to deal with n and we have an obvious formula relating the solution to the problems larger instance to the solution to the smaller one n m n m if n is odd we need only a slight adjustment of this formula n m n m m using these formulas and the trivial case of m m to stop we can compute product n m either recursively or iteratively an example of computing with this algorithm is given in figure note that all the extra addends shown in parentheses in figure a are in the rows that have odd values in the first column therefore we can find the product by simply adding all the elements in the m column that have an odd number in the n column figure b also note that the algorithm involves just the simple operations of halving doubling and adding a feature that might be attractive for example to those n m n m a b figure computing by the russian peasant method who do not want to memorize the table of multiplications it is this feature of the algorithm that most probably made it attractive to russian peasants who according to western visitors used it widely in the nineteenth century and for whom the method is named in fact the method was known to egyptian mathematicians as early as bc cha p it also leads to very fast hardware implementation since doubling and halving of binary numbers can be performed using shifts which are among the most basic operations at the machine level josephus problem our last example is the josephus problem named for flavius josephus a famous jewish historian who participated in and chronicled the jewish revolt of ce against the romans josephus as a general managed to hold the fortress of jotapata for days but after the fall of the city he took refuge with diehards in a nearby cave there the rebels voted to perish rather than surrender josephus proposed that each man in turn should dispatch his neighbor the order to be determined by casting lots josephus contrived to draw the last lot and as one of the two surviving men in the cave he prevailed upon his intended victim to surrender to the romans so let n people numbered to n stand in a circle starting the grim count with person number we eliminate every second person until only one survivor is left the problem is to determine the survivors number j n for example figure if n is people in positions and will be eliminated on the first pass through the circle and people in initial positions and will be eliminated on the second pass leaving a sole survivor in initial position thus j to give another example if n is people in positions and will be eliminated on the first pass it is more convenient to include in the first pass and people in positions and for convenience on the second thus j a b figure instances of the josephus problem for a n and b n subscript numbers indicate the pass on which the person in that position is eliminated the solutions are j and j respectively it is convenient to consider the cases of even and odd ns separately if n is even ie n k the first pass through the circle yields an instance of exactly the same problem but half its initial size the only difference is in position numbering for example a person in initial position will be in position for the second pass a person in initial position will be in position and so on check figure a it is easy to see that to get the initial position of a person we simply need to multiply his new position by and subtract this relationship will hold in particular for the survivor ie j k j k let us now consider the case of an odd n n ie n k the first pass eliminates people in all even positions if we add to this the elimination of the person in position right after that we are left with an instance of size k here to get the initial position that corresponds to the new position numbering we have to multiply the new position number by and add check figure b thus for odd values of n we get j k j k can we get a closedform solution to the twocase recurrence subject to the initial condition j the answer is yes though getting it requires more ingenuity than just applying backward substitutions in fact one way to find a solution is to apply forward substitutions to get say the first values of j n discern a pattern and then prove its general validity by mathematical induction we leave the execution of this plan to the exercises alternatively you can look it up in gkp whose exposition of the josephus problem we have been following interestingly the most elegant form of the closedform answer involves the binary representation of size n j n can be obtained by a bit cyclic shift left of n itself for example j j and j j exercises cutting a stick a stick n inches long needs to be cut into n inch pieces outline an algorithm that performs this task with the minimum number of cuts if several pieces of the stick can be cut at the same time also give a formula for the minimum number of cuts design a decreasebyhalf algorithm for computing log n and determine its time efficiency a what is the largest number of key comparisons made by binary search in searching for a key in the following array b list all the keys of this array that will require the largest number of key comparisons when searched for by binary search c find the average number of key comparisons made by binary search in a successful search in this array assume that each key is searched for with the same probability d find the average number of key comparisons made by binary search in an unsuccessful search in this array assume that searches for keys in each of the intervals formed by the arrays elements are equally likely estimate how many times faster an average successful search will be in a sorted array of one million elements if it is done by binary search versus sequential search the time efficiency of sequential search does not depend on whether a list is implemented as an array or as a linked list is it also true for searching a sorted list by binary search a design a version of binary search that uses only twoway comparisons such as and implement your algorithm in the language of your choice and carefully debug it such programs are notorious for being prone to bugs b analyze the time efficiency of the twoway comparison version designed in part a picture guessing a version of the popular problemsolving task involves presenting people with an array of pictures seven rows of six pictures each and asking them to identify the target picture by asking questions that can be answered yes or no further people are then required to identify the picture with as few questions as possible suggest the most efficient algorithm for this problem and indicate the largest number of questions that may be necessary consider ternary search the following algorithm for searching in a sorted array an if n simply compare the search key k with the single element of the array otherwise search recursively by comparing k with a n and if k is larger compare it with a n to determine in which third of the array to continue the search a what design technique is this algorithm based on b set up a recurrence for the number of key comparisons in the worst case you may assume that n k c solve the recurrence for n k d compare this algorithms efficiency with that of binary search an array an contains n integers from to n in increasing order thus one integer in this range is missing design the most efficient algorithm you can to find the missing integer and indicate its time efficiency a write pseudocode for the divideintothree algorithm for the fakecoin problem make sure that your algorithm handles properly all values of n not only those that are multiples of b set up a recurrence relation for the number of weighings in the divideintothree algorithm for the fakecoin problem and solve it for n k c for large values of n about how many times faster is this algorithm than the one based on dividing coins into two piles your answer should not depend on n a apply the russian peasant algorithm to compute b from the standpoint of time efficiency does it matter whether we multiply n by m or m by n by the russian peasant algorithm a write pseudocode for the russian peasant multiplication algorithm b what is the time efficiency class of russian peasant multiplication find j the solution to the josephus problem for n prove that the solution to the josephus problem is for every n that is a power of for the josephus problem a compute j n for n b discern a pattern in the solutions for the first fifteen values of n and prove its general validity c prove the validity of getting j n by a bit cyclic shift left of the binary representation of n variablesizedecrease algorithms in the third principal variety of decreaseandconquer the size reduction pattern varies from one iteration of the algorithm to another euclids algorithm for computing the greatest common divisor section provides a good example of this kind of algorithm in this section we encounter a few more examples of this variety computing a median and the selection problem the selection problem is the problem of finding the kth smallest element in a list of n numbers this number is called the kth order statistic of course for k or k n we can simply scan the list in question to find the smallest or largest element respectively a more interesting case of this problem is for k n which asks to find an element that is not larger than one half of the lists elements and not smaller than the other half this middle value is called the median and it is one of the most important notions in mathematical statistics obviously we can find the kth smallest element in a list by sorting the list first and then selecting the kth element in the output of a sorting algorithm the time of such an algorithm is determined by the efficiency of the sorting algorithm used thus with a fast sorting algorithm such as mergesort discussed in the next chapter the algorithms efficiency is in on log n you should immediately suspect however that sorting the entire list is most likely overkill since the problem asks not to order the entire list but just to find its kth smallest element indeed we can take advantage of the idea of partitioning a given list around some value p of say its first element in general this is a rearrangement of the lists elements so that the left part contains all the elements smaller than or equal to p followed by the pivot p itself followed by all the elements greater than or equal to p p all are p p all are p of the two principal algorithmic alternatives to partition an array here we discuss the lomuto partitioning ben p we introduce the better known hoares algorithm in the next chapter to get the idea behind the lomuto partitioning it is helpful to think of an array or more generally a subarray alr l r n under consideration as composed of three contiguous segments listed in the order they follow pivot p they are as follows a segment with elements known to be smaller than p the segment of elements known to be greater than or equal to p and the segment of elements yet to be compared to p see figure a note that the segments can be empty for example it is always the case for the first two segments before the algorithm starts starting with i l the algorithm scans the subarray alr left to right maintaining this structure until a partition is achieved on each iteration it compares the first element in the unknown segment pointed to by the scanning index i in figure a with the pivot p if ai p i is simply incremented to expand the segment of the elements greater than or equal to p while shrinking the unprocessed segment if ai p it is the segment of the elements smaller than p that needs to be expanded this is done by incrementing s the index of the last l s i r p p p a l s r p p p b l s r p p p c figure illustration of the lomuto partitioning element in the first segment swapping ai and as and then incrementing i to point to the new first element of the shrunk unprocessed segment after no unprocessed elements remain figure b the algorithm swaps the pivot with as to achieve a partition being sought figure c here is pseudocode implementing this partitioning procedure algorithm lomutopartitional r partitions subarray by lomutos algorithm using first element as pivot input a subarray alr of array an defined by its left and right indices l and r l r output partition of alr and the new position of the pivot p al sl for i l to r do if ai p s s swapas ai swapal as return s how can we take advantage of a list partition to find the kth smallest element in it let us assume that the list is implemented as an array whose elements are indexed starting with a and let s be the partitions split position ie the index of the arrays element occupied by the pivot after partitioning if s k pivot p itself is obviously the kth smallest element which solves the problem if s k the kth smallest element in the entire array can be found as the kth smallest element in the left part of the partitioned array and if s k it can be found as the k sth smallest element in its right part thus if we do not solve the problem outright we reduce its instance to a smaller one which can be solved by the same approach ie recursively this algorithm is called quickselect to find the kth smallest element in array an by this algorithm call quickselectan k where algorithm quickselectalr k solves the selection problem by recursive partitionbased algorithm input subarray alr of array an of orderable elements and integer k k r l output the value of the kth smallest element in alr s lomutopartitionalr or another partition algorithm if s k return as else if s l k quickselectals k else quickselectas r k s in fact the same idea can be implemented without recursion as well for the nonrecursive version we need not even adjust the value of k but just continue until s k example apply the partitionbased algorithm to find the median of the following list of nine numbers here k and our task is to find the th smallest element in the array we use the above version of array partitioning showing the pivots in bold s i s i s i s i s i since s is smaller than k we proceed with the right part of the array s i s i s i now s k and hence we can stop the found median is which is greater than and but smaller than and how efficient is quickselect partitioning an nelement array always requires n key comparisons if it produces the split that solves the selection problem without requiring more iterations then for this best case we obtain cbestn n n unfortunately the algorithm can produce an extremely unbalanced partition of a given array with one part being empty and the other containing n elements in the worst case this can happen on each of the n iterations for a specific example of the worstcase input consider say the case of k n and a strictly increasing array this implies that cworstn n n n n n which compares poorly with the straightforward sortingbased approach mentioned in the beginning of our selection problem discussion thus the usefulness of the partitionbased algorithm depends on the algorithms efficiency in the average case fortunately a careful mathematical analysis has shown that the averagecase efficiency is linear in fact computer scientists have discovered a more sophisticated way of choosing a pivot in quickselect that guarantees linear time even in the worst case blo but it is too complicated to be recommended for practical applications it is also worth noting that the partitionbased algorithm solves a somewhat more general problem of identifying the k smallest and n k largest elements of a given list not just the value of its kth smallest element interpolation search as the next example of a variablesizedecrease algorithm we consider an algorithm for searching in a sorted array called interpolation search unlike binary search which always compares a search key with the middle value of a given sorted array and hence reduces the problems instance size by half interpolation search takes into account the value of the search key in order to find the arrays element to be compared with the search key in a sense the algorithm mimics the way we value ar v al l x r index figure index computation in interpolation search search for a name in a telephone book if we are searching for someone named brown we open the book not in the middle but very close to the beginning unlike our action when searching for someone named say smith more precisely on the iteration dealing with the arrays portion between the leftmost element al and the rightmost element ar the algorithm assumes that the array values increase linearly ie along the straight line through the points l al and r ar the accuracy of this assumption can influence the algorithms efficiency but not its correctness accordingly the search keys value v is compared with the element whose index is computed as the roundoff of the x coordinate of the point on the straight line through the points l al and r ar whose y coordinate is equal to the search value v figure writing down a standard equation for the straight line passing through the points l al and r ar substituting v for y and solving it for x leads to the following formula xl v alr l ar al the logic behind this approach is quite straightforward we know that the array values are increasing more accurately not decreasing from al to ar but we do not know how they do it had these values increased linearly which is the simplest manner possible the index computed by formula would be the expected location of the arrays element with the value equal to v of course if v is not between al and ar formula need not be applied why after comparing v with ax the algorithm either stops if they are equal or proceeds by searching in the same manner among the elements indexed either between l and x or between x and r depending on whether ax is smaller or larger than v thus the size of the problems instance is reduced but we can not tell a priori by how much the analysis of the algorithms efficiency shows that interpolation search uses fewer than log log n key comparisons on the average when searching in a list of n random keys this function grows so slowly that the number of comparisons is a very small constant for all practically feasible inputs see problem in this sections exercises but in the worst case interpolation search is only linear which must be considered a bad performance why assessing the worthiness of interpolation search versus that of binary search robert sedgewick wrote in the second edition of his algorithms that binary search is probably better for smaller files but interpolation search is worth considering for large files and for applications where comparisons are particularly expensive or access costs are very high note that in section we discuss a continuous counterpart of interpolation search which can be seen as one more example of a variablesizedecrease algorithm searching and insertion in a binary search tree let us revisit the binary search tree recall that this is a binary tree whose nodes contain elements of a set of orderable items one element per node so that for every node all elements in the left subtree are smaller and all the elements in the right subtree are greater than the element in the subtrees root when we need to search for an element of a given value v in such a tree we do it recursively in the following manner if the tree is empty the search ends in failure if the tree is not empty we compare v with the trees root kr if they match a desired element is found and the search can be stopped if they do not match we continue with the search in the left subtree of the root if v kr and in the right subtree if v kr thus on each iteration of the algorithm the problem of searching in a binary search tree is reduced to searching in a smaller binary search tree the most sensible measure of the size of a search tree is its height obviously the decrease in a trees height normally changes from one iteration to another of the binary tree search thus giving us an excellent example of a variablesizedecrease algorithm in the worst case of the binary tree search the tree is severely skewed this happens in particular if a tree is constructed by successive insertions of an increasing or decreasing sequence of keys figure obviously the search for an in such a tree requires n comparisons making the worstcase efficiency of the search operation fall into n fortunately the averagecase efficiency turns out to be in log n more precisely the number of key comparisons needed for a search in a binary search tree built from n random keys is about ln n log n since insertion of a new key into a binary search tree is almost identical to that of searching there it also exemplifies the variablesizedecrease technique and has the same efficiency characteristics as the search operation a a a a an an an an a b figure binary search trees for a an increasing sequence of keys and b a decreasing sequence of keys the game of nim there are several wellknown games that share the following features there are two players who move in turn no randomness or hidden information is permitted all players know all information about gameplay a game is impartial each player has the same moves available from the same game position each of a finite number of available moves leads to a smaller instance of the same game the game ends with a win by one of the players there are no ties the winner is the last player who is able to move a prototypical example of such games is nim generally the game is played with several piles of chips but we consider the onepile version first thus there is a single pile of n chips two players take turns by removing from the pile at least one and at most m chips the number of chips taken may vary from one move to another but both the lower and upper limits stay the same who wins the game by taking the last chip the player moving first or second if both players make the best moves possible let us call an instance of the game a winning position for the player to move next if that player has a winning strategy ie a sequence of moves that results in a victory no matter what moves the opponent makes let us call an instance of the game a losing position for the player to move next if every move available for that player leads to a winning position for the opponent the standard approach to determining which positions are winning and which are losing is to investigate small values of n first it is logical to consider the instance of n as a losing one for the player to move next because this player is the first one who can not make a move any instance with n m chips is obviously a winning position for the player to move next why the instance with n m chips is a losing one because taking any allowed number of chips puts the opponent in a winning position see an illustration for m in figure any instance with m n m chips is a winning position for the player to move next because there is a move that leaves the opponent with m chips which is a losing figure illustration of onepile nim with the maximum number of chips that may be taken on each move m the numbers indicate n the number of chips in the pile the losing positions for the player to move are circled only winning moves from the winning positions are shown in bold position m m chips is the next losing position and so on it is not difficult to see the pattern that can be formally proved by mathematical induction an instance with n chips is a winning position for the player to move next if and only if n is not a multiple of m the winning strategy is to take n modm chips on every move any deviation from this strategy puts the opponent in a winning position onepile nim has been known for a very long time it appeared in particular as the summation game in the first published book on recreational mathematics authored by claudegaspar bachet a french aristocrat and mathematician in a player picks a positive integer less than say and then his opponent and he take turns adding any integer less than the first player to reach exactly is the winner dud in general nim is played with i piles of chips of sizes n n ni on each move a player can take any available number of chips including all of them from any single pile the goal is the same to be the last player able to make a move note that for i it is easy to figure out who wins this game and how here is a hint the answer for the games instances with n n differs from the answer for those with n n a solution to the general case of nim is quite unexpected because it is based on the binary representation of the pile sizes let b b bi be the pile sizes in binary compute their binary digital sum also known as the nim sum defined as the sum of binary digits discarding any carry in other words a binary digit si in the sum is if the number of s in the ith position in the addends is even and it is if the number of s is odd it turns out that an instance of nim is a winning one for the player to move next if and only if its nim sum contains at least one consequently nims instance is a losing instance if and only if its nim sum contains only zeros for example for the commonly played instance with n n n the nim sum is since this sum contains a the instance is a winning one for the player moving first to find a winning move from this position the player needs to change one of the three bit strings so that the new nim sum contains only s it is not difficult to see that the only way to accomplish this is to remove two chips from the first pile this ingenious solution to the game of nim was discovered by harvard mathematics professor c l bouton more than years ago since then mathematicians have developed a much more general theory of such games an excellent account of this theory with applications to many specific games is given in the monograph by e r berlekamp j h conway and r k guy ber exercises a if we measure an instance size of computing the greatest common divisor of m and n by the size of the second number n by how much can the size decrease after one iteration of euclids algorithm b prove that an instance size will always decrease at least by a factor of two after two successive iterations of euclids algorithm apply quickselect to find the median of the list of numbers write pseudocode for a nonrecursive implementation of quickselect derive the formula underlying interpolation search give an example of the worstcase input for interpolation search and show that the algorithm is linear in the worst case a find the smallest value of n for which log log n is greater than b determine which if any of the following assertions are true i log log n olog n ii log log n log n iii log log n log n a outline an algorithm for finding the largest key in a binary search tree would you classify your algorithm as a variablesizedecrease algorithm b what is the time efficiency class of your algorithm in the worst case a outline an algorithm for deleting a key from a binary search tree would you classify this algorithm as a variablesizedecrease algorithm b what is the time efficiency class of your algorithm in the worst case outline a variablesizedecrease algorithm for constructing an eulerian circuit in a connected graph with all vertices of even degrees misere onepile nim consider the socalled misere version of the onepile nim in which the player taking the last chip loses the game all the other conditions of the game remain the same ie the pile contains n chips and on each move a player takes at least one but no more than m chips identify the winning and losing positions for the player to move next in this game a moldy chocolate two players take turns by breaking an m n chocolate bar which has one spoiled square each break must be a single straight line cutting all the way across the bar along the boundaries between the squares after each break the player who broke the bar last eats the piece that does not contain the spoiled square the player left with the spoiled square loses the game is it better to go first or second in this game b write an interactive program to play this game with the computer your program should make a winning move in a winning position and a random legitimate move in a losing position flipping pancakes there are n pancakes all of different sizes that are stacked on top of each other you are allowed to slip a flipper under one of the pancakes and flip over the whole stack above the flipper the purpose is to arrange pancakes according to their size with the biggest at the bottom you can see a visualization of this puzzle on the interactive mathematics miscellany and puzzles site bog design an algorithm for solving this puzzle you need to search for a given number in an n n matrix in which every row and every column is sorted in increasing order can you design a on algorithm for this problem laa summary decreaseandconquer is a general algorithm design technique based on exploiting a relationship between a solution to a given instance of a problem and a solution to a smaller instance of the same problem once such a relationship is established it can be exploited either top down usually recursively or bottom up there are three major variations of decreaseandconquer decreasebyaconstant most often by one eg insertion sort decreasebyaconstantfactor most often by the factor of two eg binary search variablesizedecrease eg euclids algorithm insertion sort is a direct application of the decreaseby oneandconquer technique to the sorting problem it is a n algorithm both in the worst and average cases but it is about twice as fast on average than in the worst case the algorithms notable advantage is a good performance on almostsorted arrays a digraph is a graph with directions on its edges the topological sorting problem asks to list vertices of a digraph in an order such that for every edge of the digraph the vertex it starts at is listed before the vertex it points to this problem has a solution if and only if a digraph is a dag directed acyclic graph ie it has no directed cycles there are two algorithms for solving the topological sorting problem the first one is based on depthfirst search the second is based on a direct application of the decreasebyone technique the decreasebyone technique is a natural approach to developing algorithms for generating elementary combinatorial objects the most efficient class of such algorithms are minimalchange algorithms however the number of combinatorial objects grows so fast that even the best algorithms are of practical interest only for very small instances of such problems binary search is a very efficient algorithm for searching in a sorted array it is a principal example of a decreasebyaconstantfactor algorithm other examples include exponentiation by squaring identifying a fake coin with a balance scale russian peasant multiplication and the josephus problem for some decreaseandconquer algorithms the size reduction varies from one iteration of the algorithm to another examples of such variablesizedecrease algorithms include euclids algorithm the partitionbased algorithm for the selection problem interpolation search and searching and insertion in a binary search tree nim exemplifies games that proceed through a series of diminishing instances of the same game divideandconquer whatever man prays for he prays for a miracle every prayer reduces itself to this great god grant that twice two be not four ivan turgenev russian novelist and shortstory writer divideandconquer is probably the bestknown general algorithm design technique though its fame may have something to do with its catchy name it is well deserved quite a few very efficient algorithms are specific implementations of this general strategy divideandconquer algorithms work according to the following general plan a problem is divided into several subproblems of the same type ideally of about equal size the subproblems are solved typically recursively though sometimes a different algorithm is employed especially when subproblems become small enough if necessary the solutions to the subproblems are combined to get a solution to the original problem the divideandconquer technique is diagrammed in figure which depicts the case of dividing a problem into two smaller subproblems by far the most widely occurring case at least for divideandconquer algorithms designed to be executed on a singleprocessor computer as an example let us consider the problem of computing the sum of n numbers a an if n we can divide the problem into two instances of the same problem to compute the sum of the first n numbers and to compute the sum of the remaining n numbers of course if n we simply return a as the answer once each of these two sums is computed by applying the same method recursively we can add their values to get the sum in question a an a a n a n an is this an efficient way to compute the sum of n numbers a moment of reflection why could it be more efficient than the bruteforce summation a problem of size n subproblem subproblem of size n of size n solution to solution to subproblem subproblem solution to the original problem figure divideandconquer technique typical case small example of summing say four numbers by this algorithm a formal analysis which follows and common sense we do not normally compute sums this way do we all lead to a negative answer to this question thus not every divideandconquer algorithm is necessarily more efficient than even a bruteforce solution but often our prayers to the goddess of algorithmics see the chapters epigraph are answered and the time spent on executing the divideandconquer plan turns out to be significantly smaller than solving a problem by a different method in fact the divideandconquer approach yields some of the most important and efficient algorithms in computer science we discuss a few classic examples of such algorithms in this chapter though we consider only sequential algorithms here it is worth keeping in mind that the divideandconquer technique is ideally suited for parallel computations in which each subproblem can be solved simultaneously by its own processor actually the divideandconquer algorithm called the pairwise summation may substantially reduce the accumulated roundoff error of the sum of numbers that can be represented only approximately in a digital computer hig as mentioned above in the most typical case of divideandconquer a problems instance of size n is divided into two instances of size n more generally an instance of size n can be divided into b instances of size nb with a of them needing to be solved here a and b are constants a and b assuming that size n is a power of b to simplify our analysis we get the following recurrence for the running time t n t n at nb f n where f n is a function that accounts for the time spent on dividing an instance of size n into instances of size nb and combining their solutions for the sum example above a b and f n recurrence is called the general divideandconquer recurrence obviously the order of growth of its solution t n depends on the values of the constants a and b and the order of growth of the function f n the efficiency analysis of many divideandconquer algorithms is greatly simplified by the following theorem see appendix b master theorem if f n nd where d in recurrence then nd if a bd t n nd log n if a bd nlogb a if a bd analogous results hold for the o and notations too for example the recurrence for the number of additions an made by the divideandconquer sumcomputation algorithm see above on inputs of size n k is an an thus for this example a b and d hence since a bd an nlogb a nlog n note that we were able to find the solutions efficiency class without going through the drudgery of solving the recurrence but of course this approach can only establish a solutions order of growth to within an unknown multiplicative constant whereas solving a recurrence equation with a specific initial condition yields an exact answer at least for ns that are powers of b it is also worth pointing out that if a recurrence covers decreasebyaconstantfactor algorithms discussed in the previous chapter in fact some people consider such algorithms as binary search degenerate cases of divideandconquer where just one of two subproblems of half the size needs to be solved it is better not to do this and consider decreasebyaconstantfactor and divideandconquer as different design paradigms mergesort mergesort is a perfect example of a successful application of the divideandconquer technique it sorts a given array an by dividing it into two halves a n and a n n sorting each of them recursively and then merging the two smaller sorted arrays into a single sorted one algorithm mergesortan sorts array an by recursive mergesort input an array an of orderable elements output array an sorted in nondecreasing order if n copy a n to b n copy a n n to c n mergesortb n mergesortc n mergeb c a see below the merging of two sorted arrays can be done as follows two pointers array indices are initialized to point to the first elements of the arrays being merged the elements pointed to are compared and the smaller of them is added to a new array being constructed after that the index of the smaller element is incremented to point to its immediate successor in the array it was copied from this operation is repeated until one of the two given arrays is exhausted and then the remaining elements of the other array are copied to the end of the new array algorithm mergebp cq ap q merges two sorted arrays into one sorted array input arrays bp and cq both sorted output sorted array ap q of the elements of b and c i j k while i p and j q do if bi cj ak bi i i else ak cj j j kk if i p copy cjq to akp q else copy bip to akp q the operation of the algorithm on the list is illustrated in figure figure example of mergesort operation how efficient is mergesort assuming for simplicity that n is a power of the recurrence relation for the number of key comparisons cn is cn cn cmergen for n c let us analyze cmergen the number of key comparisons performed during the merging stage at each step exactly one comparison is made after which the total number of elements in the two arrays still needing to be processed is reduced by in the worst case neither of the two arrays becomes empty before the other one contains just one element eg smaller elements may come from the alternating arrays therefore for the worst case cmergen n and we have the recurrence cworst n cworst n n for n cworst hence according to the master theorem cworstn n log n why in fact it is easy to find the exact solution to the worstcase recurrence for n k cworstn n log n n the number of key comparisons made by mergesort in the worst case comes very close to the theoretical minimum that any general comparisonbased sorting algorithm can have for large n the number of comparisons made by this algorithm in the average case turns out to be about n less see gon p and hence is also in n log n a noteworthy advantage of mergesort over quicksort and heapsort the two important advanced sorting algorithms to be discussed later is its stability see problem in this sections exercises the principal shortcoming of mergesort is the linear amount of extra storage the algorithm requires though merging can be done inplace the resulting algorithm is quite complicated and of theoretical interest only there are two main ideas leading to several variations of mergesort first the algorithm can be implemented bottom up by merging pairs of the arrays elements then merging the sorted pairs and so on if n is not a power of only slight bookkeeping complications arise this avoids the time and space overhead of using a stack to handle recursive calls second we can divide a list to be sorted in more than two parts sort each recursively and then merge them together this scheme which is particularly useful for sorting files residing on secondary memory devices is called multiway mergesort exercises a write pseudocode for a divideandconquer algorithm for finding the position of the largest element in an array of n numbers b what will be your algorithms output for arrays with several elements of the largest value c set up and solve a recurrence relation for the number of key comparisons made by your algorithm d how does this algorithm compare with the bruteforce algorithm for this problem a write pseudocode for a divideandconquer algorithm for finding values of both the largest and smallest elements in an array of n numbers b set up and solve for n k a recurrence relation for the number of key comparisons made by your algorithm c how does this algorithm compare with the bruteforce algorithm for this problem a write pseudocode for a divideandconquer algorithm for the exponentiation problem of computing an where n is a positive integer b set up and solve a recurrence relation for the number of multiplications made by this algorithm as we shall see in section this theoretical minimum is log n n log n n c how does this algorithm compare with the bruteforce algorithm for this problem as mentioned in chapter logarithm bases are irrelevant in most contexts arising in analyzing an algorithms efficiency class is this true for both assertions of the master theorem that include logarithms find the order of growth for solutions of the following recurrences a t n t n n t b t n t n n t c t n t n n t apply mergesort to sort the list e x a m p l e in alphabetical order is mergesort a stable sorting algorithm a solve the recurrence relation for the number of key comparisons made by mergesort in the worst case you may assume that n k b set up a recurrence relation for the number of key comparisons made by mergesort on bestcase inputs and solve it for n k c set up a recurrence relation for the number of key moves made by the version of mergesort given in section does taking the number of key moves into account change the algorithms efficiency class let an be an array of n real numbers a pair ai aj is said to be an inversion if these numbers are out of order ie i j but ai aj design an on log n algorithm for counting the number of inversions implement the bottomup version of mergesort in the language of your choice tromino puzzle a tromino more accurately a right tromino is an lshaped tile formed by three squares the problem is to cover any n n chessboard with a missing square with trominoes trominoes can be oriented in an arbitrary way but they should cover all the squares of the board except the missing one exactly and with no overlaps gol design a divideandconquer algorithm for this problem quicksort quicksort is the other important sorting algorithm that is based on the divideandconquer approach unlike mergesort which divides its input elements according to their position in the array quicksort divides them according to their value we already encountered this idea of an array partition in section where we discussed the selection problem a partition is an arrangement of the arrays elements so that all the elements to the left of some element as are less than or equal to as and all the elements to the right of as are greater than or equal to it a as as as an all are as all are as obviously after a partition is achieved as will be in its final position in the sorted array and we can continue sorting the two subarrays to the left and to the right of as independently eg by the same method note the difference with mergesort there the division of the problem into two subproblems is immediate and the entire work happens in combining their solutions here the entire work happens in the division stage with no work required to combine the solutions to the subproblems here is pseudocode of quicksort call quicksortan where algorithm quicksortal r sorts a subarray by quicksort input subarray of array an defined by its left and right indices l and r output subarray alr sorted in nondecreasing order if l r s partitionalr s is a split position quicksortals quicksortas r as a partition algorithm we can certainly use the lomuto partition discussed in section alternatively we can partition an and more generally its subarray alr l r n by the more sophisticated method suggested by car hoare the prominent british computer scientist who invented quicksort car hoare at age invented his algorithm in while trying to sort words for a machine translation project from russian to english says hoare my first thought on how to do this was bubblesort and by an amazing stroke of luck my second thought was quicksort it is hard to disagree with his overall assessment i have been very lucky what a wonderful way to start a career in computing by discovering a new sorting algorithm hoa twenty years later he received the turing award for fundamental contributions to the definition and design of programming languages in he was also knighted for services to education and computer science as before we start by selecting a pivot an element with respect to whose value we are going to divide the subarray there are several different strategies for selecting a pivot we will return to this issue when we analyze the algorithms efficiency for now we use the simplest strategy of selecting the subarrays first element p al unlike the lomuto algorithm we will now scan the subarray from both ends comparing the subarrays elements to the pivot the lefttoright scan denoted below by index pointer i starts with the second element since we want elements smaller than the pivot to be in the left part of the subarray this scan skips over elements that are smaller than the pivot and stops upon encountering the first element greater than or equal to the pivot the righttoleft scan denoted below by index pointer j starts with the last element of the subarray since we want elements larger than the pivot to be in the right part of the subarray this scan skips over elements that are larger than the pivot and stops on encountering the first element smaller than or equal to the pivot why is it worth stopping the scans after encountering an element equal to the pivot because doing this tends to yield more even splits for arrays with a lot of duplicates which makes the algorithm run faster for example if we did otherwise for an array of n equal elements we would have gotten a split into subarrays of sizes n and reducing the problem size just by after scanning the entire array after both scans stop three situations may arise depending on whether or not the scanning indices have crossed if scanning indices i and j have not crossed ie i j we simply exchange ai and aj and resume the scans by incrementing i and decrementing j respectively i j p all are p p p all are p if the scanning indices have crossed over ie i j we will have partitioned the subarray after exchanging the pivot with aj j i p all are p p p all are p finally if the scanning indices stop while pointing to the same element ie i j the value they are pointing to must be equal to p why thus we have the subarray partitioned with the split position s i j ji p all are p p all are p we can combine the last case with the case of crossedover indices i j by exchanging the pivot with aj whenever i j here is pseudocode implementing this partitioning procedure algorithm hoarepartitional r partitions a subarray by hoares algorithm using the first element as a pivot input subarray of array an defined by its left and right indices l and r l r output partition of alr with the split position returned as this functions value p al i l j r repeat repeat i i until ai p repeat j j until aj p swapai aj until i j swapai aj undo last swap when i j swapal aj return j note that index i can go out of the subarrays bounds in this pseudocode rather than checking for this possibility every time index i is incremented we can append to array an a sentinel that would prevent index i from advancing beyond position n note that the more sophisticated method of pivot selection mentioned at the end of the section makes such a sentinel unnecessary an example of sorting an array by quicksort is given in figure we start our discussion of quicksorts efficiency by noting that the number of key comparisons made before a partition is achieved is n if the scanning indices cross over and n if they coincide why if all the splits happen in the middle of corresponding subarrays we will have the best case the number of key comparisons in the best case satisfies the recurrence cbest n cbest n n for n cbest according to the master theorem cbestn n log n solving it exactly for n k yields cbestn n log n in the worst case all the splits will be skewed to the extreme one of the two subarrays will be empty and the size of the other will be just less than the size of the subarray being partitioned this unfortunate situation will happen in particular for increasing arrays ie for inputs for which the problem is already solved indeed if an is a strictly increasing array and we use a as the pivot the lefttoright scan will stop on a while the righttoleft scan will go all the way to reach a indicating the split at position i j i j i j i j i j j i i j i r s i j i j i r i r s s j i i r i r i r i r ij s j i i r i r i j b i j j i a figure example of quicksort operation a arrays transformations with pivots shown in bold b tree of recursive calls to quicksort with input values l and r of subarray bounds and split position s of a partition obtained j i a a an so after making n comparisons to get to this partition and exchanging the pivot a with itself the algorithm will be left with the strictly increasing array an to sort this sorting of strictly increasing arrays of diminishing sizes will continue until the last one an n has been processed the total number of key comparisons made will be equal to cworst n n n n n n thus the question about the utility of quicksort comes down to its averagecase behavior let cavgn be the average number of key comparisons made by quicksort on a randomly ordered array of size n a partition can happen in any position s s n after n comparisons are made to achieve the partition after the partition the left and right subarrays will have s and n s elements respectively assuming that the partition split can happen in each position s with the same probability n we get the following recurrence relation n cavgn n n cavgs cavgn s for n s cavg cavg its solution which is much trickier than the worstand bestcase analyses turns out to be cavgn n ln n n log n thus on the average quicksort makes only more comparisons than in the best case moreover its innermost loop is so efficient that it usually runs faster than mergesort and heapsort another n log n algorithm that we discuss in chapter on randomly ordered arrays of nontrivial sizes this certainly justifies the name given to the algorithm by its inventor because of quicksorts importance there have been persistent efforts over the years to refine the basic algorithm among several improvements discovered by researchers are better pivot selection methods such as randomized quicksort that uses a random element or the medianofthree method that uses the median of the leftmost rightmost and the middle element of the array switching to insertion sort on very small subarrays between and elements for most computer systems or not sorting small subarrays at all and finishing the algorithm with insertion sort applied to the entire nearly sorted array modifications of the partitioning algorithm such as the threeway partition into segments smaller than equal to and larger than the pivot see problem in this sections exercises according to robert sedgewick sed p the worlds leading expert on quicksort such improvements in combination can cut the running time of the algorithm by like any sorting algorithm quicksort has weaknesses it is not stable it requires a stack to store parameters of subarrays that are yet to be sorted while the size of this stack can be made to be in olog n by always sorting first the smaller of two subarrays obtained by partitioning it is worse than the o space efficiency of heapsort although more sophisticated ways of choosing a pivot make the quadratic running time of the worst case very unlikely they do not eliminate it completely and even the performance on randomly ordered arrays is known to be sensitive not only to implementation details of the algorithm but also to both computer architecture and data type still the januaryfebruary issue of computing in science engineering a joint publication of the american institute of physics and the ieee computer society selected quicksort as one of the algorithms with the greatest influence on the development and practice of science and engineering in the th century exercises apply quicksort to sort the list e x a m p l e in alphabetical order draw the tree of the recursive calls made for the partitioning procedure outlined in this section a prove that if the scanning indices stop while pointing to the same element ie i j the value they are pointing to must be equal to p b prove that when the scanning indices stop j can not point to an element more than one position to the left of the one pointed to by i give an example showing that quicksort is not a stable sorting algorithm give an example of an array of n elements for which the sentinel mentioned in the text is actually needed what should be its value also explain why a single sentinel suffices for any input for the version of quicksort given in this section a are arrays made up of all equal elements the worstcase input the bestcase input or neither b are strictly decreasing arrays the worstcase input the bestcase input or neither a for quicksort with the medianofthree pivot selection are strictly increasing arrays the worstcase input the bestcase input or neither b answer the same question for strictly decreasing arrays a estimate how many times faster quicksort will sort an array of one million random numbers than insertion sort b true or false for every n there are nelement arrays that are sorted faster by insertion sort than by quicksort design an algorithm to rearrange elements of a given array of n real numbers so that all its negative elements precede all its positive elements your algorithm should be both time efficient and space efficient a the dutch national flag problem is to rearrange an array of characters r w and b red white and blue are the colors of the dutch national flag so that all the rs come first the ws come next and the bs come last dij design a linear inplace algorithm for this problem b explain how a solution to the dutch national flag problem can be used in quicksort implement quicksort in the language of your choice run your program on a sample of inputs to verify the theoretical assertions about the algorithms efficiency nuts and bolts you are given a collection of n bolts of different widths and n corresponding nuts you are allowed to try a nut and bolt together from which you can determine whether the nut is larger than the bolt smaller than the bolt or matches the bolt exactly however there is no way to compare two nuts together or two bolts together the problem is to match each bolt to its nut design an algorithm for this problem with averagecase efficiency in n log n raw important problem types in the limitless sea of problems one encounters in computing there are a few areas that have attracted particular attention from researchers by and large their interest has been driven either by the problems practical importance or by some specific characteristics making the problem an interesting research subject fortunately these two motivating forces reinforce each other in most cases in this section we are going to introduce the most important problem types sorting searching string processing graph problems combinatorial problems geometric problems numerical problems these problems are used in subsequent chapters of the book to illustrate different algorithm design techniques and methods of algorithm analysis sorting the sorting problem is to rearrange the items of a given list in nondecreasing order of course for this problem to be meaningful the nature of the list items must allow such an ordering mathematicians would say that there must exist a relation of total ordering as a practical matter we usually need to sort lists of numbers characters from an alphabet character strings and most important records similar to those maintained by schools about their students libraries about their holdings and companies about their employees in the case of records we need to choose a piece of information to guide sorting for example we can choose to sort student records in alphabetical order of names or by student number or by student gradepoint average such a specially chosen piece of information is called a key computer scientists often talk about sorting a list of keys even when the lists items are not records but say just integers why would we want a sorted list to begin with a sorted list can be a required output of a task such as ranking internet search results or ranking students by their gpa scores further sorting makes many questions about the list easier to answer the most important of them is searching it is why dictionaries telephone books class lists and so on are sorted you will see other examples of the usefulness of list presorting in section in a similar vein sorting is used as an auxiliary step in several important algorithms in other areas eg geometric algorithms and data compression the greedy approach an important algorithm design technique discussed later in the book requires a sorted input by now computer scientists have discovered dozens of different sorting algorithms in fact inventing a new sorting algorithm has been likened to designing the proverbial mousetrap and i am happy to report that the hunt for a better sorting mousetrap continues this perseverance is admirable in view of the following facts on the one hand there are a few good sorting algorithms that sort an arbitrary array of size n using about n log n comparisons on the other hand no algorithm that sorts by key comparisons as opposed to say comparing small pieces of keys can do substantially better than that there is a reason for this embarrassment of algorithmic riches in the land of sorting although some algorithms are indeed better than others there is no algorithm that would be the best solution in all situations some of the algorithms are simple but relatively slow while others are faster but more complex some work better on randomly ordered inputs while others do better on almostsorted lists some are suitable only for lists residing in the fast memory while others can be adapted for sorting large files stored on a disk and so on two properties of sorting algorithms deserve special mention a sorting algorithm is called stable if it preserves the relative order of any two equal elements in its input in other words if an input list contains two equal elements in positions i and j where i j then in the sorted list they have to be in positions i and j respectively such that i j this property can be desirable if for example we have a list of students sorted alphabetically and we want to sort it according to student gpa a stable algorithm will yield a list in which students with the same gpa will still be sorted alphabetically generally speaking algorithms that can exchange keys located far apart are not stable but they usually work faster you will see how this general comment applies to important sorting algorithms later in the book the second notable feature of a sorting algorithm is the amount of extra memory the algorithm requires an algorithm is said to be inplace if it does not require extra memory except possibly for a few memory units there are important sorting algorithms that are inplace and those that are not searching the searching problem deals with finding a given value called a search key in a given set or a multiset which permits several elements to have the same value there are plenty of searching algorithms to choose from they range from the straightforward sequential search to a spectacularly efficient but limited binary search and algorithms based on representing the underlying set in a different form more conducive to searching the latter algorithms are of particular importance for realworld applications because they are indispensable for storing and retrieving information from large databases for searching too there is no single algorithm that fits all situations best some algorithms work faster than others but require more memory some are very fast but applicable only to sorted arrays and so on unlike with sorting algorithms there is no stability problem but different issues arise specifically in applications where the underlying data may change frequently relative to the number of searches searching has to be considered in conjunction with two other operations an addition to and deletion from the data set of an item in such situations data structures and algorithms should be chosen to strike a balance among the requirements of each operation also organizing very large data sets for efficient searching poses special challenges with important implications for realworld applications string processing in recent decades the rapid proliferation of applications dealing with nonnumerical data has intensified the interest of researchers and computing practitioners in stringhandling algorithms a string is a sequence of characters from an alphabet strings of particular interest are text strings which comprise letters numbers and special characters bit strings which comprise zeros and ones and gene sequences which can be modeled by strings of characters from the fourcharacter alphabet a c g t it should be pointed out however that stringprocessing algorithms have been important for computer science for a long time in conjunction with computer languages and compiling issues one particular problem that of searching for a given word in a text has attracted special attention from researchers they call it string matching several algorithms that exploit the special nature of this type of searching have been invented we introduce one very simple algorithm in chapter and discuss two algorithms based on a remarkable idea by r boyer and j moore in chapter graph problems one of the oldest and most interesting areas in algorithmics is graph algorithms informally a graph can be thought of as a collection of points called vertices some of which are connected by line segments called edges a more formal definition is given in the next section graphs are an interesting subject to study for both theoretical and practical reasons graphs can be used for modeling a wide variety of applications including transportation communication social and economic networks project scheduling and games studying different technical and social aspects of the internet in particular is one of the active areas of current research involving computer scientists economists and social scientists see eg eas basic graph algorithms include graphtraversal algorithms how can one reach all the points in a network shortestpath algorithms what is the best route between two cities and topological sorting for graphs with directed edges is a set of courses with their prerequisites consistent or selfcontradictory fortunately these algorithms can be considered illustrations of general design techniques accordingly you will find them in corresponding chapters of the book some graph problems are computationally very hard the most wellknown examples are the traveling salesman problem and the graphcoloring problem the traveling salesman problem tsp is the problem of finding the shortest tour through n cities that visits every city exactly once in addition to obvious applications involving route planning it arises in such modern applications as circuit board and vlsi chip fabrication xray crystallography and genetic engineering the graphcoloring problem seeks to assign the smallest number of colors to the vertices of a graph so that no two adjacent vertices are the same color this problem arises in several applications such as event scheduling if the events are represented by vertices that are connected by an edge if and only if the corresponding events can not be scheduled at the same time a solution to the graphcoloring problem yields an optimal schedule combinatorial problems from a more abstract perspective the traveling salesman problem and the graphcoloring problem are examples of combinatorial problems these are problems that ask explicitly or implicitly to find a combinatorial object such as a permutation a combination or a subset that satisfies certain constraints a desired combinatorial object may also be required to have some additional property such as a maximum value or a minimum cost generally speaking combinatorial problems are the most difficult problems in computing from both a theoretical and practical standpoint their difficulty stems from the following facts first the number of combinatorial objects typically grows extremely fast with a problems size reaching unimaginable magnitudes even for moderatesized instances second there are no known algorithms for solving most such problems exactly in an acceptable amount of time moreover most computer scientists believe that such algorithms do not exist this conjecture has been neither proved nor disproved and it remains the most important unresolved issue in theoretical computer science we discuss this topic in more detail in section some combinatorial problems can be solved by efficient algorithms but they should be considered fortunate exceptions to the rule the shortestpath problem mentioned earlier is among such exceptions geometric problems geometric algorithms deal with geometric objects such as points lines and polygons the ancient greeks were very much interested in developing procedures they did not call them algorithms of course for solving a variety of geometric problems including problems of constructing simple geometric shapes triangles circles and so on with an unmarked ruler and a compass then for about years intense interest in geometric algorithms disappeared to be resurrected in the age of computers no more rulers and compasses just bits bytes and good old human ingenuity of course today people are interested in geometric algorithms with quite different applications in mind such as computer graphics robotics and tomography we will discuss algorithms for only two classic problems of computational geometry the closestpair problem and the convexhull problem the closestpair problem is selfexplanatory given n points in the plane find the closest pair among them the convexhull problem asks to find the smallest convex polygon that would include all the points of a given set if you are interested in other geometric algorithms you will find a wealth of material in such specialized monographs as deb oro and pre numerical problems numerical problems another large special area of applications are problems that involve mathematical objects of continuous nature solving equations and systems of equations computing definite integrals evaluating functions and so on the majority of such mathematical problems can be solved only approximately another principal difficulty stems from the fact that such problems typically require manipulating real numbers which can be represented in a computer only approximately moreover a large number of arithmetic operations performed on approximately represented numbers can lead to an accumulation of the roundoff error to a point where it can drastically distort an output produced by a seemingly sound algorithm many sophisticated algorithms have been developed over the years in this area and they continue to play a critical role in many scientific and engineering applications but in the last years or so the computing industry has shifted its focus to business applications these new applications require primarily algorithms for information storage retrieval transportation through networks and presentation to users as a result of this revolutionary change numerical analysis has lost its formerly dominating position in both industry and computer science programs still it is important for any computerliterate person to have at least a rudimentary idea about numerical algorithms we discuss several classical numerical algorithms in sections and exercises consider the algorithm for the sorting problem that sorts an array by counting for each of its elements the number of smaller elements and then uses this information to put the element in its appropriate position in the sorted array algorithm comparisoncountingsortan sorts an array by comparison counting input array an of orderable values output array sn of as elements sorted in nondecreasing order for i to n do counti for i to n do for j i to n do if ai aj countj countj else counti counti for i to n do scounti ai return s a apply this algorithm to sorting the list b is this algorithm stable c is it inplace name the algorithms for the searching problem that you already know give a good succinct description of each algorithm in english if you know no such algorithms use this opportunity to design one design a simple algorithm for the stringmatching problem ko nigsberg bridges the ko nigsberg bridge puzzle is universally accepted as the problem that gave birth to graph theory it was solved by the great swissborn mathematician leonhard euler the problem asked whether one could in a single stroll cross all seven bridges of the city of ko nigsberg exactly once and return to a starting point following is a sketch of the river with its two islands and seven bridges a state the problem as a graph problem b does this problem have a solution if you believe it does draw such a stroll if you believe it does not explain why and indicate the smallest number of new bridges that would be required to make such a stroll possible icosian game a century after eulers discovery see problem another famous puzzle this one invented by the renowned irish mathematician sir william hamilton was presented to the world under the name of the icosian game the games board was a circular wooden board on which the following graph was carved find a hamiltonian circuit a path that visits all the graphs vertices exactly once before returning to the starting vertex for this graph consider the following problem design an algorithm to determine the best route for a subway passenger to take from one designated station to another in a welldeveloped subway system similar to those in such cities as washington dc and london uk a the problems statement is somewhat vague which is typical of reallife problems in particular what reasonable criterion can be used for defining the best route b how would you model this problem by a graph a rephrase the travelingsalesman problem in combinatorial object terms b rephrase the graphcoloring problem in combinatorial object terms consider the following map b a d c e f a explain how we can use the graphcoloring problem to color the map so that no two neighboring regions are colored the same b use your answer to part a to color the map with the smallest number of colors design an algorithm for the following problem given a set of n points in the cartesian plane determine whether all of them lie on the same circumference write a program that reads as its inputs the x y coordinates of the endpoints of two line segments pq and pq and determines whether the segments have a common point binary tree traversals and related properties in this section we see how the divideandconquer technique can be applied to binary trees a binary tree t is defined as a finite set of nodes that is either empty or consists of a root and two disjoint binary trees tl and tr called respectively the left and right subtree of the root we usually think of a binary tree as a special case of an ordered tree figure this standard interpretation was an alternative definition of a binary tree in section since the definition itself divides a binary tree into two smaller structures of the same type the left subtree and the right subtree many problems about binary trees can be solved by applying the divideandconquer technique as an example let us consider a recursive algorithm for computing the height of a binary tree recall that the height is defined as the length of the longest path from the root to a leaf hence it can be computed as the maximum of the heights of the roots left tleft tright figure standard representation of a binary tree and right subtrees plus we have to add to account for the extra level of the root also note that it is convenient to define the height of the empty tree as thus we have the following recursive algorithm algorithm heightt computes recursively the height of a binary tree input a binary tree t output the height of t if t return else return maxheighttlef t heighttright we measure the problems instance size by the number of nodes nt in a given binary tree t obviously the number of comparisons made to compute the maximum of two numbers and the number of additions ant made by the algorithm are the same we have the following recurrence relation for ant ant antlef t antright for nt a before we solve this recurrence can you tell what its solution is let us note that addition is not the most frequently executed operation of this algorithm what is checking and this is very typical for binary tree algorithms that the tree is not empty for example for the empty tree the comparison t is executed once but there are no additions and for a singlenode tree the comparison and addition numbers are and respectively it helps in the analysis of tree algorithms to draw the trees extension by replacing the empty subtrees by special nodes the extra nodes shown by little squares in figure are called external the original nodes shown by little circles are called internal by definition the extension of the empty binary tree is a single external node it is easy to see that the height algorithm makes exactly one addition for every internal node of the extended tree and it makes one comparison to check whether a b figure binary tree on the left and its extension on the right internal nodes are shown as circles external nodes are shown as squares the tree is empty for every internal and external node therefore to ascertain the algorithms efficiency we need to know how many external nodes an extended binary tree with n internal nodes can have after checking figure and a few similar examples it is easy to hypothesize that the number of external nodes x is always more than the number of internal nodes n x n to prove this equality consider the total number of nodes both internal and external since every node except the root is one of the two children of an internal node we have the equation n x n which immediately implies equality note that equality also applies to any nonempty full binary tree in which by definition every node has either zero or two children for a full binary tree n and x denote the numbers of parental nodes and leaves respectively returning to algorithm height the number of comparisons to check whether the tree is empty is cn n x n and the number of additions is an n the most important divideandconquer algorithms for binary trees are the three classic traversals preorder inorder and postorder all three traversals visit nodes of a binary tree recursively ie by visiting the trees root and its left and right subtrees they differ only by the timing of the roots visit in the preorder traversal the root is visited before the left and right subtrees are visited in that order in the inorder traversal the root is visited after visiting its left subtree but before visiting the right subtree in the postorder traversal the root is visited after visiting the left and right subtrees in that order these traversals are illustrated in figure their pseudocodes are quite straightforward repeating the descriptions given above these traversals are also a standard feature of data structures textbooks as to their efficiency analysis it is identical to the above analysis of the height algorithm because a recursive call is made for each node of an extended binary tree finally we should note that obviously not all questions about binary trees require traversals of both left and right subtrees for example the search and insert operations for a binary search tree require processing only one of the two subtrees accordingly we considered them in section not as applications of divideandconquer but rather as examples of the variablesizedecrease technique a b c preorder a b d g e c f inorder d g b e a f c d e f postorder g d e b f c a g figure binary tree and its traversals exercises design a divideandconquer algorithm for computing the number of levels in a binary tree in particular the algorithm must return and for the empty and singlenode trees respectively what is the time efficiency class of your algorithm the following algorithm seeks to compute the number of leaves in a binary tree algorithm leafcountert computes recursively the number of leaves in a binary tree input a binary tree t output the number of leaves in t if t return else return leafcountertlef t leafcountertright is this algorithm correct if it is prove it if it is not make an appropriate correction can you compute the height of a binary tree with the same asymptotic efficiency as the sections divideandconquer algorithm but without using a stack explicitly or implicitly of course you may use a different algorithm altogether prove equality by mathematical induction traverse the following binary tree a in preorder b in inorder c in postorder a b c d e f write pseudocode for one of the classic traversal algorithms preorder inorder and postorder for binary trees assuming that your algorithm is recursive find the number of recursive calls made which of the three classic traversal algorithms yields a sorted list if applied to a binary search tree prove this property a draw a binary tree with nodes labeled in such a way that the inorder and postorder traversals of the tree yield the following lists inorder and postorder b give an example of two permutations of the same n labels n that can not be inorder and postorder traversal lists of the same binary tree c design an algorithm that constructs a binary tree for which two given lists of n labels n are generated by the inorder and postorder traversals of the tree your algorithm should also identify inputs for which the problem has no solution the internal path length i of an extended binary tree is defined as the sum of the lengths of the paths taken over all internal nodes from the root to each internal node similarly the external path length e of an extended binary tree is defined as the sum of the lengths of the paths taken over all external nodes from the root to each external node prove that e i n where n is the number of internal nodes in the tree write a program for computing the internal path length of an extended binary tree use it to investigate empirically the average number of key comparisons for searching in a randomly generated binary search tree chocolate bar puzzle given an n m chocolate bar you need to break it into nm pieces you can break a bar only in a straight line and only one bar can be broken at a time design an algorithm that solves the problem with the minimum number of bar breaks what is this minimum number justify your answer by using properties of a binary tree multiplication of large integers and strassens matrix multiplication in this section we examine two surprising algorithms for seemingly straightforward tasks multiplying two integers and multiplying two square matrices both achieve a better asymptotic efficiency by ingenious application of the divideandconquer technique multiplication of large integers some applications notably modern cryptography require manipulation of integers that are over decimal digits long since such integers are too long to fit in a single word of a modern computer they require special treatment this practical need supports investigations of algorithms for efficient manipulation of large integers in this section we outline an interesting algorithm for multiplying such numbers obviously if we use the conventional penandpencil algorithm for multiplying two ndigit integers each of the n digits of the first number is multiplied by each of the n digits of the second number for the total of n digit multiplications if one of the numbers has fewer digits than the other we can pad the shorter number with leading zeros to equalize their lengths though it might appear that it would be impossible to design an algorithm with fewer than n digit multiplications this turns out not to be the case the miracle of divideandconquer comes to the rescue to accomplish this feat to demonstrate the basic idea of the algorithm let us start with a case of twodigit integers say and these numbers can be represented as follows and now let us multiply them the last formula yields the correct answer of of course but it uses the same four digit multiplications as the penandpencil algorithm fortunately we can compute the middle term with just one digit multiplication by taking advantage of the products and that need to be computed anyway of course there is nothing special about the numbers we just multiplied for any pair of twodigit numbers a aa and b bb their product c can be computed by the formula c a b c c c where c a b is the product of their first digits c a b is the product of their second digits c a a b b c c is the product of the sum of the as digits and the sum of the bs digits minus the sum of c and c now we apply this trick to multiplying two ndigit integers a and b where n is a positive even number let us divide both numbers in the middle after all we promised to take advantage of the divideandconquer technique we denote the first half of the as digits by a and the second half by a for b the notations are b and b respectively in these notations a aa implies that a an a and b bb implies that b bn b therefore taking advantage of the same trick we used for twodigit numbers we get c a b an a bn b a bn a b a bn a b cn cn c where c a b is the product of their first halves c a b is the product of their second halves c a a b b c c is the product of the sum of the as halves and the sum of the bs halves minus the sum of c and c if n is even we can apply the same method for computing the products c c and c thus if n is a power of we have a recursive algorithm for computing the product of two ndigit integers in its pure form the recursion is stopped when n becomes it can also be stopped when we deem n small enough to multiply the numbers of that size directly how many digit multiplications does this algorithm make since multiplication of ndigit numbers requires three multiplications of ndigit numbers the recurrence for the number of multiplications mn is mn mn for n m solving it by backward substitutions for n k yields mk mk mk mk imki kmkk k since k log n mn log n nlog n on the last step we took advantage of the following property of logarithms alogb c clogb a but what about additions and subtractions have we not decreased the number of multiplications by requiring more of those operations let an be the number of digit additions and subtractions executed by the above algorithm in multiplying two ndigit decimal integers besides an of these operations needed to compute the three products of ndigit numbers the above formulas require five additions and one subtraction hence we have the recurrence an an cn for n a applying the master theorem which was stated in the beginning of the chapter we obtain an nlog which means that the total number of additions and subtractions have the same asymptotic order of growth as the number of multiplications the asymptotic advantage of this algorithm notwithstanding how practical is it the answer depends of course on the computer system and program quality implementing the algorithm which might explain the rather wide disparity of reported results on some machines the divideandconquer algorithm has been reported to outperform the conventional method on numbers only decimal digits long and to run more than twice faster with numbers over decimal digits long the area of particular importance for modern cryptography whatever this outperformance crossover point happens to be on a particular machine it is worth switching to the conventional algorithm after the multiplicands become smaller than the crossover point finally if you program in an objectoriented language such as java c or smalltalk you should also be aware that these languages have special classes for dealing with large integers discovered by yearold russian mathematician anatoly karatsuba in the divideandconquer algorithm proved wrong the thenprevailing opinion that the time efficiency of any integer multiplication algorithm must be in n the discovery encouraged researchers to look for even asymptotically faster algorithms for this and other algebraic problems we will see such an algorithm in the next section strassens matrix multiplication now that we have seen that the divideandconquer approach can reduce the number of onedigit multiplications in multiplying two integers we should not be surprised that a similar feat can be accomplished for multiplying matrices such an algorithm was published by v strassen in str the principal insight of the algorithm lies in the discovery that we can find the product c of two matrices a and b with just seven multiplications as opposed to the eight required by the bruteforce algorithm see example in section this is accomplished by using the following formulas c c a a b b c c a a b b m m m m m m m m m m m m where m a a b b m a a b m a b b m a b b m a a b m a a b b m a a b b thus to multiply two matrices strassens algorithm makes seven multiplications and additionssubtractions whereas the bruteforce algorithm requires eight multiplications and four additions these numbers should not lead us to multiplying matrices by strassens algorithm its importance stems from its asymptotic superiority as matrix order n goes to infinity let a and b be two n n matrices where n is a power of if n is not a power of matrices can be padded with rows and columns of zeros we can divide a b and their product c into four n n submatrices each as follows c c a a b b c c a a b b it is not difficult to verify that one can treat these submatrices as numbers to get the correct product for example c can be computed either as a b a b or as m m m m where m m m and m are found by strassens formulas with the numbers replaced by the corresponding submatrices if the seven products of n n matrices are computed recursively by the same method we have strassens algorithm for matrix multiplication let us evaluate the asymptotic efficiency of this algorithm if mn is the number of multiplications made by strassens algorithm in multiplying two n n matrices where n is a power of we get the following recurrence relation for it mn mn for n m since n k mk mk mk mk imki kmkk k since k log n mn log n nlog n which is smaller than n required by the bruteforce algorithm since this savings in the number of multiplications was achieved at the expense of making extra additions we must check the number of additions an made by strassens algorithm to multiply two matrices of order n the algorithm needs to multiply seven matrices of order n and make additionssubtractions of matrices of size n when n no additions are made since two numbers are simply multiplied these observations yield the following recurrence relation an an n for n a though one can obtain a closedform solution to this recurrence see problem in this sections exercises here we simply establish the solutions order of growth according to the master theorem an nlog in other words the number of additions has the same order of growth as the number of multiplications this puts strassens algorithm in nlog which is a better efficiency class than n of the bruteforce method since the time of strassens discovery several other algorithms for multiplying two n n matrices of real numbers in on time with progressively smaller constants have been invented the fastest algorithm so far is that of coopersmith and winograd coo with its efficiency in on the decreasing values of the exponents have been obtained at the expense of the increasing complexity of these algorithms because of large multiplicative constants none of them is of practical value however they are interesting from a theoretical point of view on one hand they get closer and closer to the best theoretical lower bound known for matrix multiplication which is n multiplications though the gap between this bound and the best available algorithm remains unresolved on the other hand matrix multiplication is known to be computationally equivalent to some other important problems such as solving systems of linear equations discussed in the next chapter exercises what are the smallest and largest numbers of digits the product of two decimal ndigit integers can have compute by applying the divideandconquer algorithm outlined in the text a prove the equality alogb c clogb a which was used in section b why is nlog better than log n as a closedform formula for mn a why did we not include multiplications by n in the multiplication count mn of the largeinteger multiplication algorithm b in addition to assuming that n is a power of we made for the sake of simplicity another more subtle assumption in setting up the recurrences for mn and an which is not always true it does not change the final answers however what is this assumption how many onedigit additions are made by the penandpencil algorithm in multiplying two ndigit integers you may disregard potential carries verify the formulas underlying strassens algorithm for multiplying matrices apply strassens algorithm to compute exiting the recursion when n ie computing the products of matrices by the bruteforce algorithm solve the recurrence for the number of additions required by strassens algorithm assume that n is a power of v pan pan has discovered a divideandconquer matrix multiplication algorithm that is based on multiplying two matrices using multiplications find the asymptotic efficiency of pans algorithm you may ignore additions and compare it with that of strassens algorithm practical implementations of strassens algorithm usually switch to the bruteforce method after matrix sizes become smaller than some crossover point run an experiment to determine such a crossover point on your computer system the closestpair and convexhull problems by divideandconquer in section we discussed the bruteforce approach to solving two classic problems of computational geometry the closestpair problem and the convexhull problem we saw that the twodimensional versions of these problems can be solved by bruteforce algorithms in n and on time respectively in this section we discuss more sophisticated and asymptotically more efficient algorithms for these problems which are based on the divideandconquer technique the closestpair problem let p be a set of n points in the cartesian plane for the sake of simplicity we assume that the points are distinct we can also assume that the points are ordered in nondecreasing order of their x coordinate if they were not we could sort them first by an efficeint sorting algorithm such as mergesort it will also be convenient to have the points sorted in a separate list in nondecreasing order of the y coordinate we will denote such a list q if n the problem can be solved by the obvious bruteforce algorithm if n we can divide the points into two subsets pl and pr of n and n points respectively by drawing a vertical line through the median m of their x coordinates so that n points lie to the left of or on the line itself and n points lie to the right of or on the line then we can solve the closestpair problem xm dl dr xm d d d min p d d a b figure a idea of the divideandconquer algorithm for the closestpair problem b rectangle that may contain points closer than dmin to point p recursively for subsets pl and pr let dl and dr be the smallest distances between pairs of points in pl and pr respectively and let d mindl dr note that d is not necessarily the smallest distance between all the point pairs because points of a closer pair can lie on the opposite sides of the separating line therefore as a step combining the solutions to the smaller subproblems we need to examine such points obviously we can limit our attention to the points inside the symmetric vertical strip of width d around the separating line since the distance between any other pair of points is at least d figure a let s be the list of points inside the strip of width d around the separating line obtained from q and hence ordered in nondecreasing order of their y coordinate we will scan this list updating the information about dmin the minimum distance seen so far if we encounter a closer pair of points initially dmin d and subsequently dmin d let px y be a point on this list for a point p x y to have a chance to be closer to p than dmin the point must follow p on list s and the difference between their y coordinates must be less than dmin why geometrically this means that p must belong to the rectangle shown in figure b the principal insight exploited by the algorithm is the observation that the rectangle can contain just a few such points because the points in each half left and right of the rectangle must be at least distance d apart it is easy to prove that the total number of such points in the rectangle including p does not exceed eight problem in this sections exercises a more careful analysis reduces this number to six see joh p thus the algorithm can consider no more than five next points following p on the list s before moving up to the next point here is pseudocode of the algorithm we follow the advice given in section to avoid computing square roots inside the innermost loop of the algorithm algorithm efficientclosestpairp q solves the closestpair problem by divideandconquer input an array p of n points in the cartesian plane sorted in nondecreasing order of their x coordinates and an array q of the same points sorted in nondecreasing order of the y coordinates output euclidean distance between the closest pair of points if n return the minimal distance found by the bruteforce algorithm else copy the first n points of p to array pl copy the same n points from q to array ql copy the remaining n points of p to array pr copy the same n points from q to array qr dl efficientclosestpairpl ql dr efficientclosestpairpr qr d mindl dr m p n x copy all the points of q for which x m d into array snum dminsq d for i to num do ki while k num and sky siy dminsq dminsq minskx six sky siy dminsq kk return sqrtdminsq the algorithm spends linear time both for dividing the problem into two problems half the size and combining the obtained solutions therefore assuming as usual that n is a power of we have the following recurrence for the running time of the algorithm t n t n f n where f n n applying the master theorem with a b and d we get t n n log n the necessity to presort input points does not change the overall efficiency class if sorting is done by a on log n algorithm such as mergesort in fact this is the best efficiency class one can achieve because it has been proved that any algorithm for this problem must be in n log n under some natural assumptions about operations an algorithm can perform see pre p convexhull problem let us revisit the convexhull problem introduced in section find the smallest convex polygon that contains n given points in the plane we consider here a divideandconquer algorithm called quickhull because of its resemblance to quicksort let s be a set of n points px y pnxn yn in the cartesian plane we assume that the points are sorted in nondecreasing order of their x coordinates with ties resolved by increasing order of the y coordinates of the points involved it is not difficult to prove the geometrically obvious fact that the leftmost point p and the rightmost point pn are two distinct extreme points of the sets convex hull figure let p pn be the straight line through points p and pn directed from p to pn this line separates the points of s into two sets s is the set of points to the left of this line and s is the set of points to the right of this line we say that point q is to the left of the line q q directed from point q to point q if qqq forms a counterclockwise cycle later we cite an analytical way to check this condition based on checking the sign of a determinant formed by the coordinates of the three points the points of s on the line ppn other than p and pn can not be extreme points of the convex hull and hence are excluded from further consideration the boundary of the convex hull of s is made up of two polygonal chains an upper boundary and a lower boundary the upper boundary called the upper hull is a sequence of line segments with vertices at p some of the points in s if s is not empty and pn the lower boundary called the lower hull is a sequence of line segments with vertices at p some of the points in s if s is not empty and pn the fact that the convex hull of the entire set s is composed of the upper and lower hulls which can be constructed independently and in a similar fashion is a very useful observation exploited by several algorithms for this problem for concreteness let us discuss how quickhull proceeds to construct the upper hull the lower hull can be constructed in the same manner if s is empty the pn p figure upper and lower hulls of a set of points pmax pn p figure the idea of quickhull upper hull is simply the line segment with the endpoints at p and pn if s is not empty the algorithm identifies point pmax in s which is the farthest from the line p pn figure if there is a tie the point that maximizes the angle pmaxppn can be selected note that point pmax maximizes the area of the triangle with two vertices at p and pn and the third one at some other point of s then the algorithm identifies all the points of set s that are to the left of the line ppmax these are the points that will make up the set s the points of s to the left of the line pm axpn will make up the set s it is not difficult to prove the following pmax is a vertex of the upper hull the points inside ppmaxpn can not be vertices of the upper hull and hence can be eliminated from further consideration there are no points to the left of both lines p pmax and p m axpn therefore the algorithm can continue constructing the upper hulls of p s pmax and pmax s pn recursively and then simply concatenate them to get the upper hull of the entire set p s pn now we have to figure out how the algorithms geometric operations can be actually implemented fortunately we can take advantage of the following very useful fact from analytical geometry if qx y qx y and qx y are three arbitrary points in the cartesian plane then the area of the triangle qqq is equal to onehalf of the magnitude of the determinant x y x y xy xy xy xy xy xy x y while the sign of this expression is positive if and only if the point q x y is to the left of the line q q using this formula we can check in constant time whether a point lies to the left of the line determined by two other points as well as find the distance from the point to the line quickhull has the same n worstcase efficiency as quicksort problem in this sections exercises in the average case however we should expect a much better performance first the algorithm should benefit from the quicksortlike savings from the onaverage balanced split of the problem into two smaller subproblems second a significant fraction of the points namely those inside ppmaxpn see figure are eliminated from further processing under a natural assumption that points given are chosen randomly from a uniform distribution over some convex region eg a circle or a rectangle the averagecase efficiency of quickhull turns out to be linear ove exercises a for the onedimensional version of the closestpair problem ie for the problem of finding two closest numbers among a given set of n real numbers design an algorithm that is directly based on the divideandconquer technique and determine its efficiency class b is it a good algorithm for this problem prove that the divideandconquer algorithm for the closestpair problem examines for every point p in the vertical strip see figures a and b no more than seven other points that can be closer to p than dmin the minimum distance between two points encountered by the algorithm up to that point consider the version of the divideandconquer twodimensional closestpair algorithm in which instead of presorting input set p we simply sort each of the two sets pl and pr in nondecreasing order of their y coordinates on each recursive call assuming that sorting is done by mergesort set up a recurrence relation for the running time in the worst case and solve it for n k implement the divideandconquer closestpair algorithm outlined in this section in the language of your choice find on the web a visualization of an algorithm for the closestpair problem what algorithm does this visualization represent the voronoi polygon for a point p of a set s of points in the plane is defined to be the perimeter of the set of all points in the plane closer to p than to any other point in s the union of all the voronoi polygons of the points in s is called the voronoi diagram of s a what is the voronoi diagram for a set of three points b find a visualization of an algorithm for generating the voronoi diagram on the web and study a few examples of such diagrams based on your observations can you tell how the solution to the previous question is generalized to the general case explain how one can find point pmax in the quickhull algorithm analytically what is the bestcase efficiency of quickhull give a specific example of inputs that make quickhull run in quadratic time implement quickhull in the language of your choice creating decagons there are points in the plane no three of them on the same line devise an algorithm to construct decagons with their vertices at these points the decagons need not be convex but each of them has to be simple ie its boundary should not cross itself and no two decagons may have a common point shortest path around there is a fenced area in the twodimensional euclidean plane in the shape of a convex polygon with vertices at points px y px y pnxn yn not necessarily in this order there are two more points axa ya and bxb yb such that xa minx x xn and xb maxx x xn design a reasonably efficient algorithm for computing the length of the shortest path between a and b oro summary divideandconquer is a general algorithm design technique that solves a problem by dividing it into several smaller subproblems of the same type ideally of about equal size solving each of them recursively and then combining their solutions to get a solution to the original problem many efficient algorithms are based on this technique although it can be both inapplicable and inferior to simpler algorithmic solutions running time t n of many divideandconquer algorithms satisfies the recurrence t n at nb f n the master theorem establishes the order of growth of its solutions mergesort is a divideandconquer sorting algorithm it works by dividing an input array into two halves sorting them recursively and then merging the two sorted halves to get the original array sorted the algorithms time efficiency is in n log n in all cases with the number of key comparisons being very close to the theoretical minimum its principal drawback is a significant extra storage requirement quicksort is a divideandconquer sorting algorithm that works by partitioning its input elements according to their value relative to some preselected element quicksort is noted for its superior efficiency among n log n algorithms for sorting randomly ordered arrays but also for the quadratic worstcase efficiency the classic traversals of a binary tree preorder inorder and postorder and similar algorithms that require recursive processing of both left and right subtrees can be considered examples of the divideandconquer technique their analysis is helped by replacing all the empty subtrees of a given tree by special external nodes there is a divideandconquer algorithm for multiplying two ndigit integers that requires about n onedigit multiplications strassens algorithm needs only seven multiplications to multiply two matrices by exploiting the divideandconquer technique this algorithm can multiply two n n matrices with about n multiplications the divideandconquer technique can be successfully applied to two important problems of computational geometry the closestpair problem and the convexhull problem transformandconquer thats the secret to life replace one worry with another charles m schulz american cartoonist the creator of peanuts this chapter deals with a group of design methods that are based on the idea of transformation we call this general technique transformandconquer because these methods work as twostage procedures first in the transformation stage the problems instance is modified to be for one reason or another more amenable to solution then in the second or conquering stage it is solved there are three major variations of this idea that differ by what we transform a given instance to figure transformation to a simpler or more convenient instance of the same problem we call it instance simplification transformation to a different representation of the same instance we call it representation change transformation to an instance of a different problem for which an algorithm is already available we call it problem reduction in the first three sections of this chapter we encounter examples of the instancesimplification variety section deals with the simple but fruitful idea of presorting many algorithmic problems are easier to solve if their input is sorted of course the benefits of sorting should more than compensate for the simpler instance or problems another representation solution instance or another problems instance figure transformandconquer strategy time spent on it otherwise we would be better off dealing with an unsorted input directly section introduces one of the most important algorithms in applied mathematics gaussian elimination this algorithm solves a system of linear equations by first transforming it to another system with a special property that makes finding a solution quite easy in section the ideas of instance simplification and representation change are applied to search trees the results are avl trees and multiway balanced search trees of the latter we consider the simplest case trees section presents heaps and heapsort even if you are already familiar with this important data structure and its application to sorting you can still benefit from looking at them in this new light of transformandconquer design in section we discuss horners rule a remarkable algorithm for evaluating polynomials if there were an algorithm hall of fame horners rule would be a serious candidate for induction based on the algorithms elegance and efficiency we also consider there two interesting algorithms for the exponentiation problem both based on the representationchange idea the chapter concludes with a review of several applications of the third variety of transformandconquer problem reduction this variety should be considered the most radical of the three one problem is reduced to another ie transformed into an entirely different problem this is a very powerful idea and it is extensively used in the complexity theory chapter its application to designing practical algorithms is not trivial however first we need to identify a new problem into which the given problem should be transformed then we must make sure that the transformation algorithm followed by the algorithm for solving the new problem is time efficient compared to other algorithmic alternatives among several examples we discuss an important special case of mathematical modeling or expressing a problem in terms of purely mathematical objects such as variables functions and equations presorting presorting is an old idea in computer science in fact interest in sorting algorithms is due to a significant degree to the fact that many questions about a list are easier to answer if the list is sorted obviously the time efficiency of algorithms that involve sorting may depend on the efficiency of the sorting algorithm being used for the sake of simplicity we assume throughout this section that lists are implemented as arrays because some sorting algorithms are easier to implement for the array representation so far we have discussed three elementary sorting algorithms selection sort bubble sort and insertion sort that are quadratic in the worst and average cases and two advanced algorithms mergesort which is always in n log n and quicksort whose efficiency is also n log n in the average case but is quadratic in the worst case are there faster sorting algorithms as we have already stated in section see also section no general comparisonbased sorting algorithm can have a better efficiency than n log n in the worst case and the same result holds for the averagecase efficiency following are three examples that illustrate the idea of presorting more examples can be found in this sections exercises example checking element uniqueness in an array if this element uniqueness problem looks familiar to you it should we considered a bruteforce algorithm for the problem in section see example the bruteforce algorithm compared pairs of the arrays elements until either two equal elements were found or no more pairs were left its worstcase efficiency was in n alternatively we can sort the array first and then check only its consecutive elements if the array has equal elements a pair of them must be next to each other and vice versa algorithm presortelementuniquenessan solves the element uniqueness problem by sorting the array first input an array an of orderable elements output returns true if a has no equal elements false otherwise sort the array a for i to n do if ai ai return false return true the running time of this algorithm is the sum of the time spent on sorting and the time spent on checking consecutive elements since the former requires at least n log n comparisons and the latter needs no more than n comparisons it is the sorting part that will determine the overall efficiency of the algorithm so if we use a quadratic sorting algorithm here the entire algorithm will not be more efficient than the bruteforce one but if we use a good sorting algorithm such as mergesort with worstcase efficiency in n log n the worstcase efficiency of the entire presortingbased algorithm will be also in n log n t n tsort n tscann n log n n n log n example computing a mode a mode is a value that occurs most often in a given list of numbers for example for the mode is if several different values occur most often any of them can be considered a mode the bruteforce approach to computing a mode would scan the list and compute the frequencies of all its distinct values then find the value with the largest frequency sorting algorithms called radix sorts are linear but in terms of the total number of input bits these algorithms work by comparing individual bits or pieces of keys rather than keys in their entirety although the running time of these algorithms is proportional to the number of input bits they are still essentially n log n algorithms because the number of bits per key must be at least log n in order to accommodate n distinct keys of input in order to implement this idea we can store the values already encountered along with their frequencies in a separate list on each iteration the ith element of the original list is compared with the values already encountered by traversing this auxiliary list if a matching value is found its frequency is incremented otherwise the current element is added to the list of distinct values seen so far with a frequency of it is not difficult to see that the worstcase input for this algorithm is a list with no equal elements for such a list its ith element is compared with i elements of the auxiliary list of distinct values seen so far before being added to the list with a frequency of as a result the worstcase number of comparisons made by this algorithm in creating the frequency list is n i n n n cn n i the additional n comparisons needed to find the largest frequency in the auxiliary list do not change the quadratic worstcase efficiency class of the algorithm as an alternative let us first sort the input then all equal values will be adjacent to each other to compute the mode all we need to do is to find the longest run of adjacent equal values in the sorted array algorithm presortmodean computes the mode of an array by sorting it first input an array an of orderable elements output the arrays mode sort the array a i current run begins at position i modef requency highest frequency seen so far while i n do runlength runvalue ai while i runlength n and ai runlength runvalue runlength runlength if runlength modef requency modef requency runlength modevalue runvalue i i runlength return modevalue the analysis here is similar to the analysis of example the running time of the algorithm will be dominated by the time spent on sorting since the remainder of the algorithm takes linear time why consequently with an n log n sort this methods worstcase efficiency will be in a better asymptotic class than the worstcase efficiency of the bruteforce algorithm example searching problem consider the problem of searching for a given value v in a given array of n sortable items the bruteforce solution here is sequential search section which needs n comparisons in the worst case if the array is sorted first we can then apply binary search which requires only log n comparisons in the worst case assuming the most efficient n log n sort the total running time of such a searching algorithm in the worst case will be t n tsort n tsearchn n log n log n n log n which is inferior to sequential search the same will also be true for the averagecase efficiency of course if we are to search in the same list more than once the time spent on sorting might well be justified problem in this sections exercises asks to estimate the minimum number of searches needed to justify presorting before we finish our discussion of presorting we should mention that many if not most geometric algorithms dealing with sets of points use presorting in one way or another points can be sorted by one of their coordinates or by their distance from a particular line or by some angle and so on for example presorting was used in the divideandconquer algorithms for the closestpair problem and for the convexhull problem which were discussed in section further some problems for directed acyclic graphs can be solved more easily after topologically sorting the digraph in question the problems of finding the longest and shortest paths in such digraphs see the exercises for sections and illustrate this point finally most algorithms based on the greedy technique which is the subject of chapter require presorting of their inputs as an intrinsic part of their operations exercises consider the problem of finding the distance between the two closest numbers in an array of n numbers the distance between two numbers x and y is computed as x y a design a presortingbased algorithm for solving this problem and determine its efficiency class b compare the efficiency of this algorithm with that of the bruteforce algorithm see problem in exercises let a a an and b b bm be two sets of numbers consider the problem of finding their intersection ie the set c of all the numbers that are in both a and b a design a bruteforce algorithm for solving this problem and determine its efficiency class b design a presortingbased algorithm for solving this problem and determine its efficiency class consider the problem of finding the smallest and largest elements in an array of n numbers a design a presortingbased algorithm for solving this problem and determine its efficiency class b compare the efficiency of the three algorithms i the bruteforce algorithm ii this presortingbased algorithm and iii the divideandconquer algorithm see problem in exercises estimate how many searches will be needed to justify time spent on presorting an array of elements if sorting is done by mergesort and searching is done by binary search you may assume that all searches are for elements known to be in the array what about an array of elements to sort or not to sort design a reasonably efficient algorithm for solving each of the following problems and determine its efficiency class a you are given n telephone bills and m checks sent to pay the bills n m assuming that telephone numbers are written on the checks find out who failed to pay for simplicity you may also assume that only one check is written for a particular bill and that it covers the bill in full b you have a file of n student records indicating each students number name home address and date of birth find out the number of students from each of the us states given a set of n points in the cartesian plane connect them in a simple polygon ie a closed path through all the points so that its line segments the polygons edges do not intersect except for neighboring edges at their common vertex for example p p p p p p p p p p p p a does the problem always have a solution does it always have a unique solution b design a reasonably efficient algorithm for solving this problem and indicate its efficiency class you have an array of n real numbers and another integer s find out whether the array contains two elements whose sum is s for example for the array and s the answer is yes but for the same array and s the answer is no design an algorithm for this problem with a better than quadratic time efficiency you have a list of n open intervals a b a b an bn on the real line an open interval a b comprises all the points strictly between its endpoints a and b ie a b x a x b find the maximum number of these intervals that have a common point for example for the intervals this maximum number is design an algorithm for this problem with a better than quadratic time efficiency number placement given a list of n distinct integers and a sequence of n boxes with preset inequality signs inserted between them design an algorithm that places the numbers into the boxes to satisfy those inequalities for example the numbers can be placed in the five boxes as shown below maxima search a a point xi yi in the cartesian plane is said to be dominated by point xj yj if xi xj and yi yj with at least one of the two inequalities being strict given a set of n points one of them is said to be a maximum of the set if it is not dominated by any other point in the set for example in the figure below all the maximum points of the set of points are circled y x design an efficient algorithm for finding all the maximum points of a given set of n points in the cartesian plane what is the time efficiency class of your algorithm b give a few realworld applications of this algorithm anagram detection a design an efficient algorithm for finding all sets of anagrams in a large file such as a dictionary of english words ben for example eat ate and tea belong to one such set b write a program implementing the algorithm gaussian elimination you are certainly familiar with systems of two linear equations in two unknowns ax ay b ax ay b recall that unless the coefficients of one equation are proportional to the coefficients of the other the system has a unique solution the standard method for finding this solution is to use either equation to express one of the variables as a function of the other and then substitute the result into the other equation yielding a linear equation whose solution is then used to find the value of the second variable in many applications we need to solve a system of n equations in n unknowns ax ax anxn b ax ax anxn b anx anx annxn bn where n is a large number theoretically we can solve such a system by generalizing the substitution method for solving systems of two linear equations what general design technique would such a method be based upon however the resulting algorithm would be extremely cumbersome fortunately there is a much more elegant algorithm for solving systems of linear equations called gaussian elimination the idea of gaussian elimination is to transform a system of n linear equations in n unknowns to an equivalent system ie a system with the same solution as the original one with an uppertriangular coefficient matrix a matrix with all zeros below its main diagonal the method is named after carl friedrich gauss who like other giants in the history of mathematics such as isaac newton and leonhard euler made numerous fundamental contributions to both theoretical and computational mathematics the method was known to the chinese years before the europeans rediscovered it ax ax anxn b ax ax anxn b ax ax anxn b ax anxn b anx anx annxn bn annxn bn in matrix notations we can write this as ax b axb where a a an b a a an b a a a an b b a a an b b an an ann bn ann bn we added primes to the matrix elements and righthand sides of the new system to stress the point that their values differ from their counterparts in the original system why is the system with the uppertriangular coefficient matrix better than a system with an arbitrary coefficient matrix because we can easily solve the system with an uppertriangular coefficient matrix by back substitutions as follows first we can immediately find the value of xn from the last equation then we can substitute this value into the next to last equation to get xn and so on until we substitute the known values of the last n variables into the first equation from which we find the value of x so how can we get from a system with an arbitrary coefficient matrix a to an equivalent system with an uppertriangular coefficient matrix a we can do that through a series of the socalled elementary operations exchanging two equations of the system replacing an equation with its nonzero multiple replacing an equation with a sum or difference of this equation and some multiple of another equation since no elementary operation can change a solution to a system any system that is obtained through a series of such operations will have the same solution as the original one let us see how we can get to a system with an uppertriangular matrix first we use a as a pivot to make all x coefficients zeros in the equations below the first one specifically we replace the second equation with the difference between it and the first equation multiplied by aa to get an equation with a zero coefficient for x doing the same for the third fourth and finally nth equation with the multiples aa aa ana of the first equation respectively makes all the coefficients of x below the first equation zero then we get rid of all the coefficients of x by subtracting an appropriate multiple of the second equation from each of the equations below the second one repeating this elimination for each of the first n variables ultimately yields a system with an uppertriangular coefficient matrix before we look at an example of gaussian elimination let us note that we can operate with just a systems coefficient matrix augmented as its n st column with the equations righthand side values in other words we need to write explicitly neither the variable names nor the plus and equality signs example solve the system by gaussian elimination x x x x x x x x x row row row row row row now we can obtain the solution by back substitutions x x x and x x x here is pseudocode of the first stage called forward elimination of the algorithm algorithm forwardeliminationan n bn applies gaussian elimination to matrix a of a systems coefficients augmented with vector b of the systems righthand side values input matrix an n and columnvector bn output an equivalent uppertriangular matrix in place of a with the corresponding righthand side values in the n st column for i to n do ai n bi augments the matrix for i to n do for j i to n do for k i to n do aj k aj k ai k aj i ai i there are two important observations to make about this pseudocode first it is not always correct if ai i we can not divide by it and hence can not use the ith row as a pivot for the ith iteration of the algorithm in such a case we should take advantage of the first elementary operation and exchange the ith row with some row below it that has a nonzero coefficient in the ith column if the system has a unique solution which is the normal case for systems under consideration such a row must exist since we have to be prepared for the possibility of row exchanges anyway we can take care of another potential difficulty the possibility that ai i is so small and consequently the scaling factor aj iai i so large that the new value of aj k might become distorted by a roundoff error caused by a subtraction of two numbers of greatly different magnitudes to avoid this problem we can always look for a row with the largest absolute value of the coefficient in the ith column exchange it with the ith row and then use the new ai i as the ith iterations pivot this modification called partial pivoting guarantees that the magnitude of the scaling factor will never exceed the second observation is the fact that the innermost loop is written with a glaring inefficiency can you find it before checking the following pseudocode which both incorporates partial pivoting and eliminates this inefficiency algorithm betterforwardeliminationan n bn implements gaussian elimination with partial pivoting input matrix an n and columnvector bn output an equivalent uppertriangular matrix in place of a and the corresponding righthand side values in place of the n st column for i to n do ai n bi appends b to a as the last column for i to n do pivotrow i for j i to n do if aj i apivotrow i pivotrow j for k i to n do swapai k apivotrow k for j i to n do temp aj i ai i for k i to n do aj k aj k ai k temp let us find the time efficiency of this algorithm its innermost loop consists of a single line aj k aj k ai k temp we discuss roundoff errors in more detail in section which contains one multiplication and one subtraction on most computers multiplication is unquestionably more expensive than additionsubtraction and hence it is multiplication that is usually quoted as the algorithms basic operation the standard summation formulas and rules reviewed in section see also appendix a are very helpful in the following derivation n n n n n n n cn n i n i i j i ki i j i i j i n n n in i n in i i i n n nn n n n n nn n n j j j j j j j nn n n n since the second back substitution stage of gaussian elimination is in n as you are asked to show in the exercises the running time is dominated by the cubic elimination stage making the entire algorithm cubic as well theoretically gaussian elimination always either yields an exact solution to a system of linear equations when the system has a unique solution or discovers that no such solution exists in the latter case the system will have either no solutions or infinitely many of them in practice solving systems of significant size on a computer by this method is not nearly so straightforward as the method would lead us to believe the principal difficulty lies in preventing an accumulation of roundoff errors see section consult textbooks on numerical analysis that analyze this and other implementation issues in great detail lu decomposition gaussian elimination has an interesting and very useful byproduct called lu decomposition of the coefficient matrix in fact modern commercial implementations of gaussian elimination are based on such a decomposition rather than on the basic algorithm outlined above example let us return to the example in the beginning of this section where we applied gaussian elimination to the matrix as we mentioned in section on some computers multiplication is not necessarily more expensive than additionsubtraction for this algorithm this point is moot since we can simply count the number of times the innermost loop is executed which is of course exactly the same number as the number of multiplications and the number of subtractions there a consider the lowertriangular matrix l made up of s on its main diagonal and the row multiples used in the forward elimination process l and the uppertriangular matrix u that was the result of this elimination u it turns out that the product lu of these matrices is equal to matrix a for this particular pair of l and u you can verify this fact by direct multiplication but as a general proposition it needs of course a proof which we omit here therefore solving the system ax b is equivalent to solving the system lu x b the latter system can be solved as follows denote y u x then ly b solve the system ly b first which is easy to do because l is a lowertriangular matrix then solve the system u x y with the uppertriangular matrix u to find x thus for the system at the beginning of this section we first solve ly b y y y its solution is y y y y y y solving u x y means solving x x x and the solution is x x x x x x note that once we have the lu decomposition of matrix a we can solve systems ax b with as many righthand side vectors b as we want to one at a time this is a distinct advantage over the classic gaussian elimination discussed earlier also note that the lu decomposition does not actually require extra memory because we can store the nonzero part of u in the uppertriangular part of a including the main diagonal and store the nontrivial part of l below the main diagonal of a computing a matrix inverse gaussian elimination is a very useful algorithm that tackles one of the most important problems of applied mathematics solving systems of linear equations in fact gaussian elimination can also be applied to several other problems of linear algebra such as computing a matrix inverse the inverse of an n n matrix a is an n n matrix denoted a such that aa i where i is the n n identity matrix the matrix with all zero elements except the main diagonal elements which are all ones not every square matrix has an inverse but when it exists the inverse is unique if a matrix a does not have an inverse it is called singular one can prove that a matrix is singular if and only if one of its rows is a linear combination a sum of some multiples of the other rows a convenient way to check whether a matrix is nonsingular is to apply gaussian elimination if it yields an uppertriangular matrix with no zeros on the main diagonal the matrix is nonsingular otherwise it is singular so being singular is a very special situation and most square matrices do have their inverses theoretically inverse matrices are very important because they play the role of reciprocals in matrix algebra overcoming the absence of the explicit division operation for matrices for example in a complete analogy with a linear equation in one unknown ax b whose solution can be written as x ab if a is not zero we can express a solution to a system of n equations in n unknowns ax b as x ab if a is nonsingular where b is of course a vector not a number according to the definition of the inverse matrix for a nonsingular n n matrix a to compute it we need to find n numbers xij i j n such that a a an x x xn a a an x x xn an an ann xn xn xnn we can find the unknowns by solving n systems of linear equations that have the same coefficient matrix a the vector of unknowns xj is the j th column of the inverse and the righthand side vector ej is the j th column of the identity matrix j n axj ej we can solve these systems by applying gaussian elimination to matrix a augmented by the n n identity matrix better yet we can use forward elimination to find the lu decomposition of a and then solve the systems lu xj ej j n as explained earlier computing a determinant another problem that can be solved by gaussian elimination is computing a determinant the determinant of an n n matrix a denoted det a or a is a number whose value can be defined recursively as follows if n ie if a consists of a single element a det a is equal to a for n det a is computed by the recursive formula n det a sj aj det aj j where sj is if j is odd and if j is even aj is the element in row and column j and aj is the n n matrix obtained from matrix a by deleting its row and column j in particular for a matrix the definition implies a formula that is easy to remember det a a a det a a det a aa aa a a in other words the determinant of a matrix is simply equal to the difference between the products of its diagonal elements for a matrix we get a a a det a a a a a a a det a a a det a a a det a a a a a a a a aaa aaa aaa aaa aaa aaa incidentally this formula is very handy in a variety of applications in particular we used it twice already in section as a part of the quickhull algorithm but what if we need to compute a determinant of a large matrix although this is a task that is rarely needed in practice it is worth discussing nevertheless using the recursive definition can be of little help because it implies computing the sum of nterms here gaussian elimination comes to the rescue again the central point is the fact that the determinant of an uppertriangular matrix is equal to the product of elements on its main diagonal and it is easy to see how elementary operations employed by the elimination algorithm influence the determinants value basically it either remains unchanged or changes a sign or is multiplied by the constant used by the elimination algorithm as a result we can compute the determinant of an n n matrix in cubic time determinants play an important role in the theory of systems of linear equations specifically a system of n linear equations in n unknowns ax b has a unique solution if and only if the determinant of its coefficient matrix det a is not equal to zero moreover this solution can be found by the formulas called cramers rule x det a xj det aj xn det an det a det a det a where det aj is the determinant of the matrix obtained by replacing the j th column of a by the column b you are asked to investigate in the exercises whether using cramers rule is a good algorithm for solving systems of linear equations exercises solve the following system by gaussian elimination x x x x x x x x x a solve the system of the previous question by the lu decomposition method b from the standpoint of general algorithm design techniques how would you classify the lu decomposition method solve the system of problem by computing the inverse of its coefficient matrix and then multiplying it by the vector on the righthand side would it be correct to get the efficiency class of the forward elimination stage of gaussian elimination as follows n n n n cn n in i i j i ki i n n n in i i n n n n n n i i i i i since sn nin n n sn inn i n and sn n i n sn sn sn n i write pseudocode for the backsubstitution stage of gaussian elimination and show that its running time is in n assuming that division of two numbers takes three times longer than their multiplication estimate how much faster betterforwardelimination is than forwardelimination of course you should also assume that a compiler is not going to eliminate the inefficiency in forwardelimination a give an example of a system of two linear equations in two unknowns that has a unique solution and solve it by gaussian elimination b give an example of a system of two linear equations in two unknowns that has no solution and apply gaussian elimination to it c give an example of a system of two linear equations in two unknowns that has infinitely many solutions and apply gaussian elimination to it the gaussjordan elimination method differs from gaussian elimination in that the elements above the main diagonal of the coefficient matrix are made zero at the same time and by the same use of a pivot row as the elements below the main diagonal a apply the gaussjordan method to the system of problem of these exercises b what general design strategy is this algorithm based on c in general how many multiplications are made by this method in solving a system of n equations in n unknowns how does this compare with the number of multiplications made by the gaussian elimination method in both its elimination and backsubstitution stages a system ax b of n linear equations in n unknowns has a unique solution if and only if det a is it a good idea to check this condition before applying gaussian elimination to the system a apply cramers rule to solve the system of problem of these exercises b estimate how many times longer it will take to solve a system of n linear equations in n unknowns by cramers rule than by gaussian elimination assume that all the determinants in cramers rule formulas are computed independently by gaussian elimination lights out this oneperson game is played on an n n board composed of light panels each panel has a switch that can be turned on and off thereby toggling the onoff state of this and four vertically and horizontally adjacent panels of course toggling a corner square affects a total of three panels and toggling a noncorner panel on the boards border affects a total of four squares given an initial subset of lighted squares the goal is to turn all the lights off a show that an answer can be found by solving a system of linear equations with coefficients and righthand sides using the modulo arithmetic b use gaussian elimination to solve the allones instance of this problem where all the panels of the board are initially lit c use gaussian elimination to solve the allones instance of this problem where all the panels of the board are initially lit balanced search trees in sections and we discussed the binary search tree one of the principal data structures for implementing dictionaries it is a binary tree whose nodes contain elements of a set of orderable items one element per node so that all elements in the left subtree are smaller than the element in the subtrees root and all the elements in the right subtree are greater than it note that this transformation from a set to a binary search tree is an example of the representationchange technique what do we gain by such transformation compared to the straightforward implementation of a dictionary by say an array we gain in the time efficiency of searching insertion and deletion which are all in log n but only in the average case in the worst case these operations are in n because the tree can degenerate into a severely unbalanced one with its height equal to n computer scientists have expended a lot of effort in trying to find a structure that preserves the good properties of the classical binary search tree principally the logarithmic efficiency of the dictionary operations and having the sets elements sorted but avoids its worstcase degeneracy they have come up with two approaches the first approach is of the instancesimplification variety an unbalanced binary search tree is transformed into a balanced one because of this such trees are called selfbalancing specific implementations of this idea differ by their definition of balance an avl tree requires the difference between the heights of the left and right subtrees of every node never exceed a redblack tree tolerates the height of one subtree being twice as large as the other subtree of the same node if an insertion or deletion of a new node creates a tree with a violated balance requirement the tree is restructured by one of a family of special transformations called rotations that restore the balance required in this section we will discuss only avl trees information about other types of binary search trees that utilize the idea of rebalancing via rotations including redblack trees and splay trees can be found in the references cor sed and tar the second approach is of the representationchange variety allow more than one element in a node of a search tree specific cases of such trees are trees trees and more general and important btrees they differ in the number of elements admissible in a single node of a search tree but all are perfectly balanced we discuss the simplest case of such trees the tree in this section leaving the discussion of btrees for chapter avl trees avl trees were invented in by two russian scientists g m adelsonvelsky and e m landis ade after whom this data structure is named a b figure a avl tree b binary search tree that is not an avl tree the numbers above the nodes indicate the nodes balance factors definition an avl tree is a binary search tree in which the balance factor of every node which is defined as the difference between the heights of the nodes left and right subtrees is either or or the height of the empty tree is defined as of course the balance factor can also be computed as the difference between the numbers of levels rather than the height difference of the nodes left and right subtrees for example the binary search tree in figure a is an avl tree but the one in figure b is not if an insertion of a new node makes an avl tree unbalanced we transform the tree by a rotation a rotation in an avl tree is a local transformation of its subtree rooted at a node whose balance has become either or if there are several such nodes we rotate the tree rooted at the unbalanced node that is the closest to the newly inserted leaf there are only four types of rotations in fact two of them are mirror images of the other two in their simplest form the four rotations are shown in figure the first rotation type is called the single right rotation or rrotation imagine rotating the edge connecting the root and its left child in the binary tree in figure a to the right figure presents the single rrotation in its most general form note that this rotation is performed after a new key is inserted into the left subtree of the left child of a tree whose root had the balance of before the insertion the symmetric single left rotation or lrotation is the mirror image of the single rrotation it is performed after a new key is inserted into the right subtree of the right child of a tree whose root had the balance of before the insertion you are asked to draw a diagram of the general case of the single lrotation in the exercises r a l b lr c rl d figure four rotation types for avl trees with three nodes a single rrotation b single lrotation c double lrrotation d double rlrotation the second rotation type is called the double leftright rotation lrrotation it is in fact a combination of two rotations we perform the lrotation of the left subtree of root r followed by the rrotation of the new tree rooted at r figure it is performed after a new key is inserted into the right subtree of the left child of a tree whose root had the balance of before the insertion single rrotation r c c r t t t t t t figure general form of the rrotation in the avl tree a shaded node is the last one inserted double lrrotation r g c c r g t t t t t t t t or or figure general form of the double lrrotation in the avl tree a shaded node is the last one inserted it can be either in the left subtree or in the right subtree of the roots grandchild the double rightleft rotation rlrotation is the mirror image of the double lrrotation and is left for the exercises note that the rotations are not trivial transformations though fortunately they can be done in constant time not only should they guarantee that a resulting tree is balanced but they should also preserve the basic requirements of a binary search tree for example in the initial tree of figure all the keys of subtree t are smaller than c which is smaller than all the keys of subtree t which are smaller than r which is smaller than all the keys of subtree t and the same relationships among the key values hold as they must for the balanced tree after the rotation l r lr rl figure construction of an avl tree for the list by successive insertions the parenthesized number of a rotations abbreviation indicates the root of the tree being reorganized an example of constructing an avl tree for a given list of numbers is shown in figure as you trace the algorithms operations keep in mind that if there are several nodes with the balance the rotation is done for the tree rooted at the unbalanced node that is the closest to the newly inserted leaf how efficient are avl trees as with any search tree the critical characteristic is the trees height it turns out that it is bounded both above and below by logarithmic functions specifically the height h of any avl tree with n nodes satisfies the inequalities log n h logn these weirdlooking constants are roundoffs of some irrational numbers related to fibonacci numbers and the golden ratio see section the inequalities immediately imply that the operations of searching and insertion are log n in the worst case getting an exact formula for the average height of an avl tree constructed for random lists of keys has proved to be difficult but it is known from extensive experiments that it is about log n except when n is small knuiii p thus searching in an avl tree requires on average almost the same number of comparisons as searching in a sorted array by binary search the operation of key deletion in an avl tree is considerably more difficult than insertion but fortunately it turns out to be in the same efficiency class as insertion ie logarithmic these impressive efficiency characteristics come at a price however the drawbacks of avl trees are frequent rotations and the need to maintain balances for its nodes these drawbacks have prevented avl trees from becoming the standard structure for implementing dictionaries at the same time their underlying idea that of rebalancing a binary search tree via rotations has proved to be very fruitful and has led to discoveries of other interesting variations of the classical binary search tree trees as mentioned at the beginning of this section the second idea of balancing a search tree is to allow more than one key in the same node of such a tree the simplest implementation of this idea is trees introduced by the us computer scientist john hopcroft in a tree is a tree that can have nodes of two kinds nodes and nodes a node contains a single key k and has two children the left child serves as the root of a subtree whose keys are less than k and the right child serves as the root of a subtree whose keys are greater than k in other words a node is the same kind of node we have in the classical binary search tree a node contains two ordered keys k and k k k and has three children the leftmost child serves as the root of a subtree with keys less than k the middle child serves as the root of a subtree with keys between k and k and the rightmost child serves as the root of a subtree with keys greater than k figure the last requirement of the tree is that all its leaves must be on the same level in other words a tree is always perfectly heightbalanced the length of a path from the root to a leaf is the same for every leaf it is this property that we buy by allowing more than one key in the same node of a search tree searching for a given key k in a tree is quite straightforward we start at the root if the root is a node we act as if it were a binary search tree we either stop if k is equal to the roots key or continue the search in the left or right node node k k k k k k k k k figure two kinds of nodes of a tree subtree if k is respectively smaller or larger than the roots key if the root is a node we know after no more than two key comparisons whether the search can be stopped if k is equal to one of the roots keys or in which of the roots three subtrees it needs to be continued inserting a new key in a tree is done as follows first of all we always insert a new key k in a leaf except for the empty tree the appropriate leaf is found by performing a search for k if the leaf in question is a node we insert k there as either the first or the second key depending on whether k is smaller or larger than the nodes old key if the leaf is a node we split the leaf in two the smallest of the three keys two old ones and the new key is put in the first leaf the largest key is put in the second leaf and the middle key is promoted to the old leafs parent if the leaf happens to be the trees root a new root is created to accept the middle key note that promotion of a middle key to its parent can cause the parents overflow if it was a node and hence can lead to several node splits along the chain of the leafs ancestors an example of a tree construction is given in figure as for any search tree the efficiency of the dictionary operations depends on the trees height so let us first find an upper bound for it a tree of height h with the smallest number of keys is a full tree of nodes such as the final tree in figure for h therefore for any tree of height h with n nodes we get the inequality n h h and hence h logn on the other hand a tree of height h with the largest number of keys is a full tree of nodes each with two keys and three children therefore for any tree with n nodes n h h h figure construction of a tree for the list and hence h logn these lower and upper bounds on height h logn h logn imply that the time efficiencies of searching insertion and deletion are all in log n in both the worst and average case we consider a very important generalization of trees called btrees in section exercises which of the following binary trees are avl trees a b c a for n and draw all the binary trees with n nodes that satisfy the balance requirement of avl trees b draw a binary tree of height that can be an avl tree and has the smallest number of nodes among all such trees draw diagrams of the single lrotation and of the double rlrotation in their general form for each of the following lists construct an avl tree by inserting their elements successively starting with the empty tree a b c a for an avl tree containing real numbers design an algorithm for computing the range ie the difference between the largest and smallest numbers in the tree and determine its worstcase efficiency b true or false the smallest and the largest keys in an avl tree can always be found on either the last level or the nexttolast level write a program for constructing an avl tree for a given list of n distinct integers a construct a tree for the list c o m p u t i n g use the alphabetical order of the letters and insert them successively starting with the empty tree b assuming that the probabilities of searching for each of the keys ie the letters are the same find the largest number and the average number of key comparisons for successful searches in this tree let tb and t be respectively a classical binary search tree and a tree constructed for the same list of keys inserted in the corresponding trees in the same order true or false searching for the same key in t always takes fewer or the same number of key comparisons as searching in tb for a tree containing real numbers design an algorithm for computing the range ie the difference between the largest and smallest numbers in the tree and determine its worstcase efficiency write a program for constructing a tree for a given list of n integers heaps and heapsort the data structure called the heap is definitely not a disordered pile of items as the words definition in a standard dictionary might suggest rather it is a clever partially ordered data structure that is especially suitable for implementing priority queues recall that a priority queue is a multiset of items with an orderable characteristic called an items priority with the following operations figure illustration of the definition of heap only the leftmost tree is a heap finding an item with the highest ie largest priority deleting an item with the highest priority adding a new item to the multiset it is primarily an efficient implementation of these operations that makes the heap both interesting and useful priority queues arise naturally in such applications as scheduling job executions by computer operating systems and traffic management by communication networks they also arise in several important algorithms eg prims algorithm section dijkstras algorithm section huffman encoding section and branchandbound applications section the heap is also the data structure that serves as a cornerstone of a theoretically important sorting algorithm called heapsort we discuss this algorithm after we define the heap and investigate its basic properties notion of the heap definition a heap can be defined as a binary tree with keys assigned to its nodes one key per node provided the following two conditions are met the shape property the binary tree is essentially complete or simply complete ie all its levels are full except possibly the last level where only some rightmost leaves may be missing the parental dominance or heap property the key in each node is greater than or equal to the keys in its children this condition is considered automatically satisfied for all leaves for example consider the trees of figure the first tree is a heap the second one is not a heap because the trees shape property is violated and the third one is not a heap because the parental dominance fails for the node with key note that key values in a heap are ordered top down ie a sequence of values on any path from the root to a leaf is decreasing nonincreasing if equal keys are allowed however there is no lefttoright order in key values ie there is no some authors require the key at each node to be less than or equal to the keys at its children we call this variation a minheap the array representation index value parents leaves figure heap and its array representation relationship among key values for nodes either on the same level of the tree or more generally in the left and right subtrees of the same node here is a list of important properties of heaps which are not difficult to prove check these properties for the heap of figure as an example there exists exactly one essentially complete binary tree with n nodes its height is equal to log n the root of a heap always contains its largest element a node of a heap considered with all its descendants is also a heap a heap can be implemented as an array by recording its elements in the topdown lefttoright fashion it is convenient to store the heaps elements in positions through n of such an array leaving h either unused or putting there a sentinel whose value is greater than every element in the heap in such a representation a the parental node keys will be in the first n positions of the array while the leaf keys will occupy the last n positions b the children of a key in the arrays parental position i i n will be in positions i and i and correspondingly the parent of a key in position i i n will be in position i thus we could also define a heap as an array h n in which every element in position i in the first half of the array is greater than or equal to the elements in positions i and i ie h i maxh i h i for i n of course if i n just h i h i needs to be satisfied while the ideas behind the majority of algorithms dealing with heaps are easier to understand if we think of heaps as binary trees their actual implementations are usually much simpler and more efficient with arrays how can we construct a heap for a given list of keys there are two principal alternatives for doing this the first is the bottomup heap construction algorithm illustrated in figure it initializes the essentially complete binary tree with n nodes by placing keys in the order given and then heapifies the tree as follows starting with the last parental node the algorithm checks whether the parental figure bottomup construction of a heap for the list the doubleheaded arrows show key comparisons verifying the parental dominance dominance holds for the key in this node if it does not the algorithm exchanges the nodes key k with the larger key of its children and checks whether the parental dominance holds for k in its new position this process continues until the parental dominance for k is satisfied eventually it has to because it holds automatically for any key in a leaf after completing the heapification of the subtree rooted at the current parental node the algorithm proceeds to do the same for the nodes immediate predecessor the algorithm stops after this is done for the root of the tree algorithm heapbottomuph n constructs a heap from elements of a given array by the bottomup algorithm input an array h n of orderable items output a heap h n for i n downto do k i v h k heap false while not heap and k n do j k if j n there are two children if h j h j j j if v h j heap true else h k h j kj h k v how efficient is this algorithm in the worst case assume for simplicity that n k so that a heaps tree is full ie the largest possible number of nodes occurs on each level let h be the height of the tree according to the first property of heaps in the list at the beginning of the section h log n or just log n k for the specific values of n we are considering each key on level i of the tree will travel to the leaf level h in the worst case of the heap construction algorithm since moving to the next level down requires two comparisons one to find the larger child and the other to determine whether the exchange is required the total number of key comparisons involving a key on level i will be h i therefore the total number of key comparisons in the worst case will be h h cworst n h i h ii n logn i level i keys i where the validity of the last equality can be proved either by using the closedform formula for the sum h ii see appendix a or by mathematical induction on i h thus with this bottomup algorithm a heap of size n can be constructed with fewer than n comparisons the alternative and less efficient algorithm constructs a heap by successive insertions of a new key into a previously constructed heap some people call it the topdown heap construction algorithm so how can we insert a new key k into a heap first attach a new node with key k in it after the last leaf of the existing heap then sift k up to its appropriate place in the new heap as follows compare k with its parents key if the latter is greater than or equal to k stop the structure is a heap otherwise swap these two keys and compare k with its new parent this swapping continues until k is not greater than its last parent or it reaches the root illustrated in figure obviously this insertion operation can not require more key comparisons than the heaps height since the height of a heap with n nodes is about log n the time efficiency of insertion is in olog n how can we delete an item from a heap we consider here only the most important case of deleting the roots key leaving the question about deleting an arbitrary key in a heap for the exercises authors of textbooks like to do such things to their readers do they not deleting the roots key from a heap can be done with the following algorithm illustrated in figure figure inserting a key into the heap constructed in figure the new key is sifted up via a swap with its parent until it is not larger than its parent or is in the root step step step figure deleting the roots key from a heap the key to be deleted is swapped with the last key after which the smaller tree is heapified by exchanging the new key in its root with the larger key in its children until the parental dominance requirement is satisfied maximum key deletion from a heap step exchange the roots key with the last key k of the heap step decrease the heaps size by step heapify the smaller tree by sifting k down the tree exactly in the same way we did it in the bottomup heap construction algorithm that is verify the parental dominance for k if it holds we are done if not swap k with the larger of its children and repeat this operation until the parental dominance condition holds for k in its new position the efficiency of deletion is determined by the number of key comparisons needed to heapify the tree after the swap has been made and the size of the tree is decreased by since this can not require more key comparisons than twice the heaps height the time efficiency of deletion is in olog n as well heapsort now we can describe heapsort an interesting sorting algorithm discovered by j w j williams wil this is a twostage algorithm that works as follows stage heap construction construct a heap for a given array stage maximum deletions apply the rootdeletion operation n times to the remaining heap as a result the array elements are eliminated in decreasing order but since under the array implementation of heaps an element being deleted is placed last the resulting array will be exactly the original array sorted in increasing order heapsort is traced on a specific input in figure the same input as the one stage heap construction stage maximum deletions figure sorting the array by heapsort in figure is intentionally used so that you can compare the tree and array implementations of the bottomup heap construction algorithm since we already know that the heap construction stage of the algorithm is in on we have to investigate just the time efficiency of the second stage for the number of key comparisons cn needed for eliminating the root keys from the heaps of diminishing sizes from n to we get the following inequality n cn logn logn log log i i n logn n logn n log n i this means that cn on log n for the second stage of heapsort for both stages we get on on log n on log n a more detailed analysis shows that the time efficiency of heapsort is in fact in n log n in both the worst and average cases thus heapsorts time efficiency falls in the same class as that of mergesort unlike the latter heapsort is inplace ie it does not require any extra storage timing experiments on random files show that heapsort runs more slowly than quicksort but can be competitive with mergesort exercises a construct a heap for the list by the bottomup algorithm b construct a heap for the list by successive key insertions topdown algorithm c is it always true that the bottomup and topdown algorithms yield the same heap for the same input outline an algorithm for checking whether an array h n is a heap and determine its time efficiency a find the smallest and the largest number of keys that a heap of height h can contain b prove that the height of a heap with n nodes is equal to log n prove the following equality used in section h h ii n logn where n h i a design an efficient algorithm for finding and deleting an element of the smallest value in a heap and determine its time efficiency b design an efficient algorithm for finding and deleting an element of a given value v in a heap h and determine its time efficiency indicate the time efficiency classes of the three main operations of the priority queue implemented as a an unsorted array b a sorted array c a binary search tree d an avl tree e a heap sort the following lists by heapsort by using the array representation of heaps a in increasing order b in increasing order c s o r t i n g in alphabetical order is heapsort a stable sorting algorithm what variety of the transformandconquer technique does heapsort represent which sorting algorithm other than heapsort uses a priority queue implement three advanced sorting algorithms mergesort quicksort and heapsort in the language of your choice and investigate their performance on arrays of sizes n and for each of these sizes consider a randomly generated files of integers in the range n b increasing files of integers n c decreasing files of integers n n spaghetti sort imagine a handful of uncooked spaghetti individual rods whose lengths represent numbers that need to be sorted a outline a spaghetti sort a sorting algorithm that takes advantage of this unorthodox representation b what does this example of computer science folklore see dew have to do with the topic of this chapter in general and heapsort in particular horners rule and binary exponentiation in this section we discuss the problem of computing the value of a polynomial px anxn anxn ax a at a given point x and its important special case of computing xn polynomials constitute the most important class of functions because they possess a wealth of good properties on the one hand and can be used for approximating other types of functions on the other the problem of manipulating polynomials efficiently has been important for several centuries new discoveries were still being made the last years by far the most important of them was the fast fourier transform fft the practical importance of this remarkable algorithm which is based on representing a polynomial by its values at specially chosen points was such that some people consider it one of the most important algorithmic discoveries of all time because of its relative complexity we do not discuss the fft algorithm in this book an interested reader will find a wealth of literature on the subject including reasonably accessible treatments in such textbooks as kle and cor horners rule horners rule is an old but very elegant and efficient algorithm for evaluating a polynomial it is named after the british mathematician w g horner who published it in the early th century but according to knuth knuii p the method was used by isaac newton years before horner you will appreciate this method much more if you first design an algorithm for the polynomial evaluation problem by yourself and investigate its efficiency see problems and in this sections exercises horners rule is a good example of the representationchange technique since it is based on representing px by a formula different from this new formula is obtained from by successively taking x as a common factor in the remaining polynomials of diminishing degrees px anx anx x a for example for the polynomial px x x x x we get px x x x x xx x x xxx x xxxx it is in formula that we will substitute a value of x at which the polynomial needs to be evaluated it is hard to believe that this is a way to an efficient algorithm but the unpleasant appearance of formula is just that an appearance as we shall see there is no need to go explicitly through the transformation leading to it all we need is an original list of the polynomials coefficients the penandpencil calculation can be conveniently organized with a tworow table the first row contains the polynomials coefficients including all the coefficients equal to zero if any listed from the highest an to the lowest a except for its first entry which is an the second row is filled left to right as follows the next entry is computed as the xs value times the last entry in the second row plus the next coefficient from the first row the final entry computed in this fashion is the value being sought example evaluate px x x x x at x coefficients x thus p on comparing the tables entries with formula you will see that is the value of x at x is the value of xx at x is the value of xxx at x and finally is the value of xxxx px at x pseudocode of this algorithm is the shortest one imaginable for a nontrivial algorithm algorithm hornerp n x evaluates a polynomial at a given point by horners rule input an array p n of coefficients of a polynomial of degree n stored from the lowest to the highest and a number x output the value of the polynomial at x p p n for i n downto do p x p p i return p the number of multiplications and the number of additions are given by the same sum n mn an n i to appreciate how efficient horners rule is consider only the first term of a polynomial of degree n anxn just computing this single term by the bruteforce algorithm would require n multiplications whereas horners rule computes in addition to this term n other terms and it still uses the same number of multiplications it is not surprising that horners rule is an optimal algorithm for polynomial evaluation without preprocessing the polynomials coefficients but it took scientists years after horners publication to come to the realization that such a question was worth investigating horners rule also has some useful byproducts the intermediate numbers generated by the algorithm in the process of evaluating px at some point x turn out to be the coefficients of the quotient of the division of px by x x and the final result in addition to being px is equal to the remainder of this division thus according to example the quotient and the remainder of the division of x x x x by x are x x x and respectively this division algorithm known as synthetic division is more convenient than socalled long division binary exponentiation the amazing efficiency of horners rule fades if the method is applied to computing an which is the value of xn at x a in fact it degenerates to the bruteforce multiplication of a by itself with wasteful additions of zeros in between since computing an actually an mod m is an essential operation in several important primalitytesting and encryption methods we consider now two algorithms for computing an that are based on the representationchange idea they both exploit the binary representation of exponent n but one of them processes this binary string left to right whereas the second does it right to left let n bi bi b be the bit string representing a positive integer n in the binary number system this means that the value of n can be computed as the value of the polynomial px bi xi bixi b at x for example if n its binary representation is and let us now compute the value of this polynomial by applying horners rule and see what the methods operations imply for computing the power an ap abi i biib horners rule for the binary polynomial p implications for an ap p the leading digit is always for n ap a for i i downto do for i i downto do p p bi ap apbi but apbi ap abi ap abi ap if bi ap a if bi thus after initializing the accumulators value to a we can scan the bit string representing the exponent n to always square the last value of the accumulator and if the current binary digit is also to multiply it by a these observations lead to the following lefttoright binary exponentiation method of computing an algorithm leftrightbinaryexponentiationa bn computes an by the lefttoright binary exponentiation algorithm input a number a and a list bn of binary digits bi b in the binary expansion of a positive integer n output the value of an product a for i i downto do product product product if bi product product a return product example compute a by the lefttoright binary exponentiation algorithm here n so we have binary digits of n product accumulator a a a a a a a a a since the algorithm makes one or two multiplications on each repetition of its only loop the total number of multiplications mn made by it in computing an is b mn b where b is the length of the bit string representing the exponent n taking into account that b log n we can conclude that the efficiency of the lefttoright binary exponentiation is logarithmic thus this algorithm is in a better efficiency class than the bruteforce exponentiation which always requires n multiplications the righttoleft binary exponentiation uses the same binary polynomial p see yielding the value of n but rather than applying horners rule to it as the previous method did this one exploits it differently an abi i biib abi i abii ab thus an can be computed as the product of the terms abii ai if bi if bi ie the product of consecutive terms ai skipping those for which the binary digit bi is zero in addition we can compute ai by simply squaring the same term we computed for the previous value of i since ai ai so we compute all such powers of a from the smallest to the largest from right to left but we include in the product accumulator only those whose corresponding binary digit is here is pseudocode of this algorithm algorithm rightleftbinaryexponentiationa bn computes an by the righttoleft binary exponentiation algorithm input a number a and a list bn of binary digits bi b in the binary expansion of a nonnegative integer n output the value of an term a initializes ai if b product a else product for i to i do term term term if bi product product term return product example compute a by the righttoleft binary exponentiation method here n so we have the following table filled in from right to left binary digits of n a a a a terms ai a a a a a a a product accumulator obviously the algorithms efficiency is also logarithmic for the same reason the lefttoright binary multiplication is the usefulness of both binary exponentiation algorithms is reduced somewhat by their reliance on availability of the explicit binary expansion of exponent n problem in this sections exercises asks you to design an algorithm that does not have this shortcoming exercises consider the following bruteforce algorithm for evaluating a polynomial algorithm bruteforcepolynomialevaluationp n x computes the value of polynomial p at a given point x by the highest to lowest term bruteforce algorithm input an array p n of the coefficients of a polynomial of degree n stored from the lowest to the highest and a number x output the value of the polynomial at the point x p for i n downto do power for j to i do power power x p p p i power return p find the total number of multiplications and the total number of additions made by this algorithm write pseudocode for the bruteforce polynomial evaluation that stems from substituting a given value of the variable into the polynomials formula and evaluating it from the lowest term to the highest one determine the number of multiplications and the number of additions made by this algorithm a estimate how much faster horners rule is compared to the lowesttohighest term bruteforce algorithm of problem if i the time of one multiplication is significantly larger than the time of one addition ii the time of one multiplication is about the same as the time of one addition b is horners rule more time efficient at the expense of being less space efficient than the bruteforce algorithm a apply horners rule to evaluate the polynomial px x x x at x b use the results of the above application of horners rule to find the quotient and remainder of the division of px by x apply horners rule to convert from binary to decimal compare the number of multiplications and additionssubtractions needed by the long division of a polynomial px anxn anxn a by x c where c is some constant with the number of these operations in the synthetic division a apply the lefttoright binary exponentiation algorithm to compute a b is it possible to extend the lefttoright binary exponentiation algorithm to work for every nonnegative integer exponent apply the righttoleft binary exponentiation algorithm to compute a design a nonrecursive algorithm for computing an that mimics the righttoleft binary exponentiation but does not explicitly use the binary representation of n is it a good idea to use a generalpurpose polynomialevaluation algorithm such as horners rule to evaluate the polynomial px xn xn x according to the corollary of the fundamental theorem of algebra every polynomial px anxn anxn a can be represented in the form px anx xx x x xn where x x xn are the roots of the polynomial generally complex and not necessarily distinct discuss which of the two representations is more convenient for each of the following operations a polynomial evaluation at a given point b addition of two polynomials c multiplication of two polynomials polynomial interpolation given a set of n data points xi yi where no two xi are the same find a polynomial px of degree at most n such that pxi yi for every i n problem reduction here is my version of a wellknown joke about mathematicians professor x a noted mathematician noticed that when his wife wanted to boil water for their tea she took their kettle from their cupboard filled it with water and put it on the stove once when his wife was away if you have to know she was signing her bestseller in a local bookstore the professor had to boil water by himself he saw that the kettle was sitting on the kitchen counter what did professor x do he put the kettle in the cupboard first and then proceeded to follow his wifes routine reduction alg a problem problem solution to be solved solvable by alg a to problem figure problem reduction strategy the way professor x approached his task is an example of an important problemsolving strategy called problem reduction if you need to solve a problem reduce it to another problem that you know how to solve figure the joke about the professor notwithstanding the idea of problem reduction plays a central role in theoretical computer science where it is used to classify problems according to their complexity we will touch on this classification in chapter but the strategy can be used for actual problem solving too the practical difficulty in applying it lies of course in finding a problem to which the problem at hand should be reduced moreover if we want our efforts to be of practical value we need our reductionbased algorithm to be more efficient than solving the original problem directly note that we have already encountered this technique earlier in the book in section for example we mentioned the socalled synthetic division done by applying horners rule for polynomial evaluation in section we used the following fact from analytical geometry if px y px y and px y are three arbitrary points in the plane then the determinant x y x y xy xy xy xy xy xy x y is positive if and only if the point p is to the left of the directed line p pthrough points p and p in other words we reduced a geometric question about the relative locations of three points to a question about the sign of a determinant in fact the entire idea of analytical geometry is based on reducing geometric problems to algebraic ones and the vast majority of geometric algorithms take advantage of this historic insight by rene descartes in this section we give a few more examples of algorithms based on the strategy of problem reduction computing the least common multiple recall that the least common multiple of two positive integers m and n denoted lcmm n is defined as the smallest integer that is divisible by both m and n for example lcm and lcm the least common multiple is one of the most important notions in elementary arithmetic and algebra perhaps you remember the following middleschool method for computing it given the prime factorizations of m and n compute the product of all the common prime factors of m and n all the prime factors of m that are not in n and all the prime factors of n that are not in m for example lcm as a computational procedure this algorithm has the same drawbacks as the middleschool algorithm for computing the greatest common divisor discussed in section it is inefficient and requires a list of consecutive primes a much more efficient algorithm for computing the least common multiple can be devised by using problem reduction after all there is a very efficient algorithm euclids algorithm for finding the greatest common divisor which is a product of all the common prime factors of m and n can we find a formula relating lcmm n and gcdm n it is not difficult to see that the product of lcmm n and gcdm n includes every factor of m and n exactly once and hence is simply equal to the product of m and n this observation leads to the formula lcmm n mn gcdm n where gcdm n can be computed very efficiently by euclids algorithm counting paths in a graph as our next example we consider the problem of counting paths between two vertices in a graph it is not difficult to prove by mathematical induction that the number of different paths of length k from the ith vertex to the j th vertex of a graph undirected or directed equals the i j th element of ak where a is the adjacency matrix of the graph therefore the problem of counting a graphs paths can be solved with an algorithm for computing an appropriate power of its adjacency matrix note that the exponentiation algorithms we discussed before for computing powers of numbers are applicable to matrices as well as a specific example consider the graph of figure its adjacency matrix a and its square a indicate the numbers of paths of length and respectively between the corresponding vertices of the graph in particular there are three a b a b c d a b c d a a a b a b c c c d d d figure a graph its adjacency matrix a and its square a the elements of a and a indicate the numbers of paths of lengths and respectively paths of length that start and end at vertex a a b a a c a and a d a but there is only one path of length from a to c a d c reduction of optimization problems our next example deals with solving optimization problems if a problem asks to find a maximum of some function it is said to be a maximization problem if it asks to find a functions minimum it is called a minimization problem suppose now that you need to find a minimum of some function f x and you have an algorithm for function maximization how can you take advantage of the latter the answer lies in the simple formula min f x maxf x in other words to minimize a function we can maximize its negative instead and to get a correct minimal value of the function itself change the sign of the answer this property is illustrated for a function of one real variable in figure of course the formula max f x minf x is valid as well it shows how a maximization problem can be reduced to an equivalent minimization problem this relationship between minimization and maximization problems is very general it holds for functions defined on any domain d in particular we can y f x f x x x f x f x figure relationship between minimization and maximization problems min f x maxf x apply it to functions of several variables subject to additional constraints a very important class of such problems is introduced below in this section now that we are on the topic of function optimization it is worth pointing out that the standard calculus procedure for finding extremum points of a function is in fact also based on problem reduction indeed it suggests finding the functions derivative f x and then solving the equation f x to find the functions critical points in other words the optimization problem is reduced to the problem of solving an equation as the principal part of finding extremum points note that we are not calling the calculus procedure an algorithm since it is not clearly defined in fact there is no general method for solving equations a little secret of calculus textbooks is that problems are carefully selected so that critical points can always be found without difficulty this makes the lives of both students and instructors easier but in the process may unintentionally create a wrong impression in students minds linear programming many problems of optimal decision making can be reduced to an instance of the linear programming problem a problem of optimizing a linear function of several variables subject to constraints in the form of linear equations and linear inequalities example consider a university endowment that needs to invest million this sum has to be split between three types of investments stocks bonds and cash the endowment managers expect an annual return of and for their stock bond and cash investments respectively since stocks are more risky than bonds the endowment rules require the amount invested in stocks to be no more than onethird of the moneys invested in bonds in addition at least of the total amount invested in stocks and bonds must be invested in cash how should the managers invest the money to maximize the return let us create a mathematical model of this problem let x y and z be the amounts in millions of dollars invested in stocks bonds and cash respectively by using these variables we can pose the following optimization problem maximize x y z subject to x y z x y z x y x y z although this example is both small and simple it does show how a problem of optimal decision making can be reduced to an instance of the general linear programming problem maximize or minimize cx cnxn subject to aix ainxn or or bi for i m x xn the last group of constraints called the nonnegativity constraints are strictly speaking unnecessary because they are special cases of more general constraints aix ainxn bi but it is convenient to treat them separately linear programming has proved to be flexible enough to model a wide variety of important applications such as airline crew scheduling transportation and communication network planning oil exploration and refining and industrial production optimization in fact linear programming is considered by many as one of the most important achievements in the history of applied mathematics the classic algorithm for this problem is called the simplex method section it was discovered by the us mathematician george dantzig in the s dan although the worstcase efficiency of this algorithm is known to be exponential it performs very well on typical inputs moreover a more recent algorithm by narendra karmarkar kar not only has a proven polynomial worstcase efficiency but has also performed competitively with the simplex method in empirical tests it is important to stress however that the simplex method and karmarkars algorithm can successfully handle only linear programming problems that do not limit its variables to integer values when variables of a linear programming problem are required to be integers the linear programming problem is said to be an integer linear programming problem except for some special cases eg the assignment problem and the problems discussed in sections integer linear programming problems are much more difficult there is no known polynomialtime algorithm for solving an arbitrary instance of the general integer linear programming problem and as we see in chapter such an algorithm quite possibly does not exist other approaches such as the branchandbound technique discussed in section are typically used for solving integer linear programming problems example let us see how the knapsack problem can be reduced to a linear programming problem recall from section that the knapsack problem can be posed as follows given a knapsack of capacity w and n items of weights w wn and values v vn find the most valuable subset of the items that fits into the knapsack we consider first the continuous or fractional version of the problem in which any fraction of any item given can be taken into the knapsack let xj j n be a variable representing a fraction of item j taken into the knapsack obviously xj must satisfy the inequality xj then the total weight of the selected items can be expressed by the sum n wj xj and their n j total value by the sum j vj xj thus the continuous version of the knapsack problem can be posed as the following linear programming problem n maximize vj xj j n subject to wj xj w j xj for j n there is no need to apply a general method for solving linear programming problems here this particular problem can be solved by a simple special algorithm that is introduced in section but why wait try to discover it on your own now this reduction of the knapsack problem to an instance of the linear programming problem is still useful though to prove the correctness of the algorithm in question in the discrete or version of the knapsack problem we are only allowed either to take a whole item or not to take it at all hence we have the following integer linear programming problem for this version n maximize vj xj j n subject to wj xj w j xj for j n this seemingly minor modification makes a drastic difference for the complexity of this and similar problems constrained to take only discrete values in their potential ranges despite the fact that the version might seem to be easier because it can ignore any subset of the continuous version that has a fractional value of an item the version is in fact much more complicated than its continuous counterpart the reader interested in specific algorithms for solving this problem will find a wealth of literature on the subject including the monographs mar and kel reduction to graph problems as we pointed out in section many problems can be solved by a reduction to one of the standard graph problems this is true in particular for a variety of puzzles and games in these applications vertices of a graph typically represent possible states of the problem in question and edges indicate permitted transitions among such states one of the graphs vertices represents an initial state and another represents a goal state of the problem there might be several vertices of the latter kind such a graph is called a statespace graph thus the transformation just described reduces the problem to the question about a path from the initialstate vertex to a goalstate vertex pwgc pg wc pg p pwc g pw c pwg pc pg pgc w w pgc pc pg pwg c pw g pwc p pg wc pg pwgc figure statespace graph for the peasant wolf goat and cabbage puzzle example let us revisit the classic rivercrossing puzzle that was included in the exercises for section a peasant finds himself on a river bank with a wolf a goat and a head of cabbage he needs to transport all three to the other side of the river in his boat however the boat has room only for the peasant himself and one other item either the wolf the goat or the cabbage in his absence the wolf would eat the goat and the goat would eat the cabbage find a way for the peasant to solve his problem or prove that it has no solution the statespace graph for this problem is given in figure its vertices are labeled to indicate the states they represent p w g c stand for the peasant the wolf the goat and the cabbage respectively the two bars denote the river for convenience we also label the edges by indicating the boats occupants for each crossing in terms of this graph we are interested in finding a path from the initialstate vertex labeled pwgc to the finalstate vertex labeled pwgc it is easy to see that there exist two distinct simple paths from the initialstate vertex to the final state vertex what are they if we find them by applying breadthfirst search we get a formal proof that these paths have the smallest number of edges possible hence this puzzle has two solutions requiring seven river crossings which is the minimum number of crossings needed our success in solving this simple puzzle should not lead you to believe that generating and investigating statespace graphs is always a straightforward task to get a better appreciation of them consult books on artificial intelligence ai the branch of computer science in which statespace graphs are a principal subject in this book we deal with an important special case of statespace graphs in sections and exercises a prove the equality lcmm n mn gcdm n that underlies the algorithm for computing lcmm n b euclids algorithm is known to be in olog n if it is the algorithm that is used for computing gcdm n what is the efficiency of the algorithm for computing lcmm n you are given a list of numbers for which you need to construct a minheap a minheap is a complete binary tree in which every key is less than or equal to the keys in its children how would you use an algorithm for constructing a maxheap a heap as defined in section to construct a minheap prove that the number of different paths of length k from the ith vertex to the j th vertex in a graph undirected or directed equals the i j th element of ak where a is the adjacency matrix of the graph a design an algorithm with a time efficiency better than cubic for checking whether a graph with n vertices contains a cycle of length man b consider the following algorithm for the same problem starting at an arbitrary vertex traverse the graph by depthfirst search and check whether its depthfirst search forest has a vertex with a back edge leading to its grandparent if it does the graph contains a triangle if it does not the graph does not contain a triangle as its subgraph is this algorithm correct given n points p x y pn xn yn in the coordinate plane design an algorithm to check whether all the points lie within a triangle with its vertices at three of the points given you can either design an algorithm from scratch or reduce the problem to another one with a known algorithm consider the problem of finding for a given positive integer n the pair of integers whose sum is n and whose product is as large as possible design an efficient algorithm for this problem and indicate its efficiency class the assignment problem introduced in section can be stated as follows there are n people who need to be assigned to execute n jobs one person per job that is each person is assigned to exactly one job and each job is assigned to exactly one person the cost that would accrue if the ith person is assigned to the j th job is a known quantity ci j for each pair i j n the problem is to assign the people to the jobs to minimize the total cost of the assignment express the assignment problem as a linear programming problem solve the instance of the linear programming problem given in section maximize x y z subject to x y z x y z x y x y z the graphcoloring problem is usually stated as the vertexcoloring problem assign the smallest number of colors to vertices of a given graph so that no two adjacent vertices are the same color consider the edgecoloring problem assign the smallest number of colors possible to edges of a given graph so that no two edges with the same endpoint are the same color explain how the edgecoloring problem can be reduced to a vertexcoloring problem consider the twodimensional post office location problem given n points x y xn yn in the cartesian plane find a location x y for a post nixi x yi y the average manhattan disoffice that minimizes n tance from the post office to these points explain how this problem can be efficiently solved by the problem reduction technique provided the post office does not have to be located at one of the input points jealous husbands there are n married couples who need to cross a river they have a boat that can hold no more than two people at a time to complicate matters all the husbands are jealous and will not agree on any crossing procedure that would put a wife on the same bank of the river with another womans husband without the wifes husband being there too even if there are other people on the same bank can they cross the river under such constraints a solve the problem for n b solve the problem for n which is the classical version of this problem c does the problem have a solution for n if it does indicate how many river crossings it will take if it does not explain why doublen dominoes dominoes are small rectangular tiles with dots called spots or pips embossed at both halves of the tiles a standard doublesix domino set has tiles one for each unordered pair of integers from to in general a doublen domino set would consist of domino tiles for each unordered pair of integers from to n n determine all values of n for which one constructs a ring made up of all the tiles in a doublen domino set summary transformandconquer is the fourth general algorithm design and problemsolving strategy discussed in the book it is in fact a group of techniques based on the idea of transformation to a problem that is easier to solve there are three principal varieties of the transformandconquer strategy instance simplification representation change and problem reduction instance simplification is transforming an instance of a problem to an instance of the same problem with some special property that makes the problem easier to solve list presorting gaussian elimination and rotations in avl trees are good examples of this strategy representation change implies changing one representation of a problems instance to another representation of the same instance examples discussed in this chapter include representation of a set by a tree heaps and heapsort horners rule for polynomial evaluation and two binary exponentiation algorithms problem reduction calls for transforming a given problem to another problem that can be solved by a known algorithm among examples of applying this idea to algorithmic problem solving see section reductions to linear programming and reductions to graph problems are especially important some examples used to illustrate transformandconquer happen to be very important data structures and algorithms they are heaps and heapsort avl and trees gaussian elimination and horners rule a heap is an essentially complete binary tree with keys one per node satisfying the parental dominance requirement though defined as binary trees heaps are normally implemented as arrays heaps are most important for the efficient implementation of priority queues they also underlie heapsort heapsort is a theoretically important sorting algorithm based on arranging elements of an array in a heap and then successively removing the largest element from a remaining heap the algorithms running time is in n log n both in the worst case and in the average case in addition it is inplace avl trees are binary search trees that are always balanced to the extent possible for a binary tree the balance is maintained by transformations of four types called rotations all basic operations on avl trees are in olog n it eliminates the bad worstcase efficiency of classic binary search trees trees achieve a perfect balance in a search tree by allowing a node to contain up to two ordered keys and have up to three children this idea can be generalized to yield very important btrees discussed later in the book gaussian elimination an algorithm for solving systems of linear equations is a principal algorithm in linear algebra it solves a system by transforming it to an equivalent system with an uppertriangular coefficient matrix which is easy to solve by back substitutions gaussian elimination requires about n multiplications horners rule is an optimal algorithm for polynomial evaluation without coefficient preprocessing it requires only n multiplications and n additions to evaluate an ndegree polynomial at a given point horners rule also has a few useful byproducts such as the synthetic division algorithm two binary exponentiation algorithms for computing an are introduced in section both of them exploit the binary representation of the exponent n but they process it in the opposite directions left to right and right to left linear programming concerns optimizing a linear function of several variables subject to constraints in the form of linear equations and linear inequalities there are efficient algorithms capable of solving very large instances of this problem with many thousands of variables and constraints provided the variables are not required to be integers the latter called integer linear programming constitute a much more difficult class of problems fundamental data structures since the vast majority of algorithms of interest operate on data particular ways of organizing data play a critical role in the design and analysis of algorithms a data structure can be defined as a particular scheme of organizing related data items the nature of the data items is dictated by the problem at hand they can range from elementary data types eg integers or characters to data structures eg a onedimensional array of onedimensional arrays is often used for implementing matrices there are a few data structures that have proved to be particularly important for computer algorithms since you are undoubtedly familiar with most if not all of them just a quick review is provided here linear data structures the two most important elementary data structures are the array and the linked list a onedimensional array is a sequence of n items of the same data type that are stored contiguously in computer memory and made accessible by specifying a value of the arrays index figure in the majority of cases the index is an integer either between and n as shown in figure or between and n some computer languages allow an array index to range between any two integer bounds low and high and some even permit nonnumerical indices to specify for example data items corresponding to the months of the year by the month names each and every element of an array can be accessed in the same constant amount of time regardless of where in the array the element in question is located this feature positively distinguishes arrays from linked lists discussed below arrays are used for implementing a variety of other data structures prominent among them is the string a sequence of characters from an alphabet terminated by a special character indicating the strings end strings composed of zeros and ones are called binary strings or bit strings strings are indispensable for processing textual data defining computer languages and compiling programs written in them and studying abstract computational models operations we usually perform on strings differ from those we typically perform on other arrays say arrays of numbers they include computing the string length comparing two strings to determine which one precedes the other in lexicographic ie alphabetical order and concatenating two strings forming one string from two given strings by appending the second to the end of the first a linked list is a sequence of zero or more elements called nodes each containing two kinds of information some data and one or more links called pointers to other nodes of the linked list a special pointer called null is used to indicate the absence of a nodes successor in a singly linked list each node except the last one contains a single pointer to the next element figure to access a particular node of a linked list one starts with the lists first node and traverses the pointer chain until the particular node is reached thus the time needed to access an element of a singly linked list unlike that of an array depends on where in the list the element is located on the positive side linked lists do item item item n figure array of n elements item item item n null figure singly linked list of n elements null item item item n null figure doubly linked list of n elements not require any preliminary reservation of the computer memory and insertions and deletions can be made quite efficiently in a linked list by reconnecting a few appropriate pointers we can exploit flexibility of the linked list structure in a variety of ways for example it is often convenient to start a linked list with a special node called the header this node may contain information about the linked list itself such as its current length it may also contain in addition to a pointer to the first element a pointer to the linked lists last element another extension is the structure called the doubly linked list in which every node except the first and the last contains pointers to both its successor and its predecessor figure the array and linked list are two principal choices in representing a more abstract data structure called a linear list or simply a list a list is a finite sequence of data items ie a collection of data items arranged in a certain linear order the basic operations performed on this data structure are searching for inserting and deleting an element two special types of lists stacks and queues are particularly important a stack is a list in which insertions and deletions can be done only at the end this end is called the top because a stack is usually visualized not horizontally but vertically akin to a stack of plates whose operations it mimics very closely as a result when elements are added to pushed onto a stack and deleted from popped off it the structure operates in a lastinfirstout lifo fashion exactly like a stack of plates if we can add or remove a plate only from the top stacks have a multitude of applications in particular they are indispensable for implementing recursive algorithms a queue on the other hand is a list from which elements are deleted from one end of the structure called the front this operation is called dequeue and new elements are added to the other end called the rear this operation is called enqueue consequently a queue operates in a firstinfirstout fifo fashion akin to a queue of customers served by a single teller in a bank queues also have many important applications including several algorithms for graph problems many important applications require selection of an item of the highest priority among a dynamically changing set of candidates a data structure that seeks to satisfy the needs of such applications is called a priority queue a priority queue is a collection of data items from a totally ordered universe most often integer or real numbers the principal operations on a priority queue are finding its largest element deleting its largest element and adding a new element of course a priority queue must be implemented so that the last two operations yield another priority queue straightforward implementations of this data structure can be based on either an array or a sorted array but neither of these options yields the most efficient solution possible a better implementation of a priority queue is based on an ingenious data structure called the heap we discuss heaps and an important sorting algorithm based on them in section graphs as we mentioned in the previous section a graph is informally thought of as a collection of points in the plane called vertices or nodes some of them connected by line segments called edges or arcs formally a graph g v e is defined by a pair of two sets a finite nonempty set v of items called vertices and a set e of pairs of these items called edges if these pairs of vertices are unordered ie a pair of vertices u v is the same as the pair v u we say that the vertices u and v are adjacent to each other and that they are connected by the undirected edge u v we call the vertices u and v endpoints of the edge u v and say that u and v are incident to this edge we also say that the edge u v is incident to its endpoints u and v a graph g is called undirected if every edge in it is undirected if a pair of vertices u v is not the same as the pair v u we say that the edge u v is directed from the vertex u called the edges tail to the vertex v called the edges head we also say that the edge u v leaves u and enters v a graph whose every edge is directed is called directed directed graphs are also called digraphs it is normally convenient to label vertices of a graph or a digraph with letters integer numbers or if an application calls for it character strings figure the graph depicted in figure a has six vertices and seven undirected edges v a b c d e f e a c a d b c b f c e d e e f the digraph depicted in figure b has six vertices and eight directed edges v a b c d e f e a c b c b f c e d a d e e c e f a c b a c b d e f d e f a b figure a undirected graph b digraph our definition of a graph does not forbid loops or edges connecting vertices to themselves unless explicitly stated otherwise we will consider graphs without loops since our definition disallows multiple edges between the same vertices of an undirected graph we have the following inequality for the number of edges e possible in an undirected graph with v vertices and no loops e v v we get the largest number of edges in a graph if there is an edge connecting each of its v vertices with all v other vertices we have to divide product v v by however because it includes every edge twice a graph with every pair of its vertices connected by an edge is called complete a standard notation for the complete graph with v vertices is kv a graph with relatively few possible edges missing is called dense a graph with few edges relative to the number of its vertices is called sparse whether we are dealing with a dense or sparse graph may influence how we choose to represent the graph and consequently the running time of an algorithm being designed or used graph representations graphs for computer algorithms are usually represented in one of two ways the adjacency matrix and adjacency lists the adjacency matrix of a graph with n vertices is an n n boolean matrix with one row and one column for each of the graphs vertices in which the element in the ith row and the j th column is equal to if there is an edge from the ith vertex to the j th vertex and equal to if there is no such edge for example the adjacency matrix for the graph of figure a is given in figure a note that the adjacency matrix of an undirected graph is always symmetric ie ai j aj i for every i j n why the adjacency lists of a graph or a digraph is a collection of linked lists one for each vertex that contain all the vertices adjacent to the lists vertex ie all the vertices connected to it by an edge usually such lists start with a header identifying a vertex for which the list is compiled for example figure b represents the graph in figure a via its adjacency lists to put it another way a b c d e f a a c d b b c f c c a b e d d a e e e c d f f f b e a b figure a adjacency matrix and b adjacency lists of the graph in figure a adjacency lists indicate columns of the adjacency matrix that for a given vertex contain s if a graph is sparse the adjacency list representation may use less space than the corresponding adjacency matrix despite the extra storage consumed by pointers of the linked lists the situation is exactly opposite for dense graphs in general which of the two representations is more convenient depends on the nature of the problem on the algorithm used for solving it and possibly on the type of input graph sparse or dense weighted graphs a weighted graph or weighted digraph is a graph or digraph with numbers assigned to its edges these numbers are called weights or costs an interest in such graphs is motivated by numerous realworld applications such as finding the shortest path between two points in a transportation or communication network or the traveling salesman problem mentioned earlier both principal representations of a graph can be easily adopted to accommodate weighted graphs if a weighted graph is represented by its adjacency matrix then its element ai j will simply contain the weight of the edge from the ith to the j th vertex if there is such an edge and a special symbol eg if there is no such edge such a matrix is called the weight matrix or cost matrix this approach is illustrated in figure b for the weighted graph in figure a for some applications it is more convenient to put s on the main diagonal of the adjacency matrix adjacency lists for a weighted graph have to include in their nodes not only the name of an adjacent vertex but also the weight of the corresponding edge figure c paths and cycles among the many properties of graphs two are important for a great number of applications connectivity and acyclicity both are based on the notion of a path a path from vertex u to vertex v of a graph g can be defined as a sequence of adjacent connected by an edge vertices that starts with u and ends with v if all vertices of a path are distinct the path is said to be simple the length of a path is the total number of vertices in the vertex sequence defining the path minus which is the same as the number of edges in the path for example a c b f is a simple path of length from a to f in the graph in figure a whereas a c e c b f is a path not simple of length from a to f a b c d a b a a b c b b a c d c c a b d c d d d b c a b c figure a weighted graph b its weight matrix c its adjacency lists a f b c e g h d i figure graph that is not connected in the case of a directed graph we are usually interested in directed paths a directed path is a sequence of vertices in which every consecutive pair of the vertices is connected by an edge directed from the vertex listed first to the vertex listed next for example a c e f is a directed path from a to f in the graph in figure b a graph is said to be connected if for every pair of its vertices u and v there is a path from u to v if we make a model of a connected graph by connecting some balls representing the graphs vertices with strings representing the edges it will be a single piece if a graph is not connected such a model will consist of several connected pieces that are called connected components of the graph formally a connected component is a maximal not expandable by including another vertex and an edge connected subgraph of a given graph for example the graphs in figures a and a are connected whereas the graph in figure is not because there is no path for example from a to f the graph in figure has two connected components with vertices a b c d e and f g h i respectively graphs with several connected components do happen in realworld applications a graph representing the interstate highway system of the united states would be an example why it is important to know for many applications whether or not a graph under consideration has cycles a cycle is a path of a positive length that starts and ends at the same vertex and does not traverse the same edge more than once for example f h i g f is a cycle in the graph in figure a graph with no cycles is said to be acyclic we discuss acyclic graphs in the next subsection trees a tree more accurately a free tree is a connected acyclic graph figure a a graph that has no cycles but is not necessarily connected is called a forest each of its connected components is a tree figure b a subgraph of a given graph g v e is a graph g v e such that v v and e e a b a b h c d c d e i f g f g j a b figure a tree b forest i d a c a e b d e b c g f h g f h i a b figure a free tree b its transformation into a rooted tree trees have several important properties other graphs do not have in particular the number of edges in a tree is always one less than the number of its vertices e v as the graph in figure demonstrates this property is necessary but not sufficient for a graph to be a tree however for connected graphs it is sufficient and hence provides a convenient way of checking whether a connected graph has a cycle rooted trees another very important property of trees is the fact that for every two vertices in a tree there always exists exactly one simple path from one of these vertices to the other this property makes it possible to select an arbitrary vertex in a free tree and consider it as the root of the socalled rooted tree a rooted tree is usually depicted by placing its root on the top level of the tree the vertices adjacent to the root below it level the vertices two edges apart from the root still below level and so on figure presents such a transformation from a free tree to a rooted tree rooted trees play a very important role in computer science a much more important one than free trees do in fact for the sake of brevity they are often referred to as simply trees an obvious application of trees is for describing hierarchies from file directories to organizational charts of enterprises there are many less obvious applications such as implementing dictionaries see below efficient access to very large data sets section and data encoding section as we discuss in chapter trees also are helpful in analysis of recursive algorithms to finish this farfromcomplete list of tree applications we should mention the socalled statespace trees that underline two important algorithm design techniques backtracking and branchandbound sections and for any vertex v in a tree t all the vertices on the simple path from the root to that vertex are called ancestors of v the vertex itself is usually considered its own ancestor the set of ancestors that excludes the vertex itself is referred to as the set of proper ancestors if u v is the last edge of the simple path from the root to vertex v and u v u is said to be the parent of v and v is called a child of u vertices that have the same parent are said to be siblings a vertex with no children is called a leaf a vertex with at least one child is called parental all the vertices for which a vertex v is an ancestor are said to be descendants of v the proper descendants exclude the vertex v itself all the descendants of a vertex v with all the edges connecting them form the subtree of t rooted at that vertex thus for the tree in figure b the root of the tree is a vertices d g f h and i are leaves and vertices a b e and c are parental the parent of b is a the children of b are c and g the siblings of b are d and e and the vertices of the subtree rooted at b are b c g h i the depth of a vertex v is the length of the simple path from the root to v the height of a tree is the length of the longest simple path from the root to a leaf for example the depth of vertex c of the tree in figure b is and the height of the tree is thus if we count tree levels top down starting with for the roots level the depth of a vertex is simply its level in the tree and the trees height is the maximum level of its vertices you should be alert to the fact that some authors define the height of a tree as the number of levels in it this makes the height of a tree larger by than the height defined as the length of the longest simple path from the root to a leaf ordered trees an ordered tree is a rooted tree in which all the children of each vertex are ordered it is convenient to assume that in a trees diagram all the children are ordered left to right a binary tree can be defined as an ordered tree in which every vertex has no more than two children and each child is designated as either a left child or a right child of its parent a binary tree may also be empty an example of a binary tree is given in figure a the binary tree with its root at the left right child of a vertex in a binary tree is called the left right subtree of that vertex since left and right subtrees are binary trees as well a binary tree can also be defined recursively this makes it possible to solve many problems involving binary trees by recursive algorithms a b figure a binary tree b binary search tree null null null null null null null null figure standard implementation of the binary search tree in figure b in figure b some numbers are assigned to vertices of the binary tree in figure a note that a number assigned to each parental vertex is larger than all the numbers in its left subtree and smaller than all the numbers in its right subtree such trees are called binary search trees binary trees and binary search trees have a wide variety of applications in computer science you will encounter some of them throughout the book in particular binary search trees can be generalized to more general types of search trees called multiway search trees which are indispensable for efficient access to very large data sets as you will see later in the book the efficiency of most important algorithms for binary search trees and their extensions depends on the trees height therefore the following inequalities for the height h of a binary tree with n nodes are especially important for analysis of such algorithms log n h n a binary tree is usually implemented for computing purposes by a collection of nodes corresponding to vertices of the tree each node contains some information associated with the vertex its name or some value assigned to it and two pointers to the nodes representing the left child and right child of the vertex respectively figure illustrates such an implementation for the binary search tree in figure b a computer representation of an arbitrary ordered tree can be done by simply providing a parental vertex with the number of pointers equal to the number of its children this representation may prove to be inconvenient if the number of children varies widely among the nodes we can avoid this inconvenience by using nodes with just two pointers as we did for binary trees here however the left pointer will point to the first child of the vertex and the right pointer will point to its next sibling accordingly this representation is called the first childnext sibling representation thus all the siblings of a vertex are linked via the nodes right pointers in a singly linked list with the first element of the list pointed to by the left pointer of their parent figure a illustrates this representation for the tree in figure b it is not difficult to see that this representation effectively transforms an ordered tree into a binary tree said to be associated with the ordered tree we get this representation by rotating the pointers about degrees clockwise see figure b sets and dictionaries the notion of a set plays a central role in mathematics a set can be described as an unordered collection possibly empty of distinct items called elements of the a null a b null d e null b c null g null null f null c d null h null i null h g e i f a b figure a first childnext sibling representation of the tree in figure b b its binary tree representation set a specific set is defined either by an explicit listing of its elements eg s or by specifying a property that all the sets elements and only they must satisfy eg s n n is a prime number smaller than the most important set operations are checking membership of a given item in a given set finding the union of two sets which comprises all the elements in either or both of them and finding the intersection of two sets which comprises all the common elements in the sets sets can be implemented in computer applications in two ways the first considers only sets that are subsets of some large set u called the universal set if set u has n elements then any subset s of u can be represented by a bit string of size n called a bit vector in which the ith element is if and only if the ith element of u is included in set s thus to continue with our example if u then s is represented by the bit string this way of representing sets makes it possible to implement the standard set operations very fast but at the expense of potentially using a large amount of storage the second and more common way to represent a set for computing purposes is to use the list structure to indicate the sets elements of course this option too is feasible only for finite sets fortunately unlike mathematics this is the kind of sets most computer applications need note however the two principal points of distinction between sets and lists first a set can not contain identical elements a list can this requirement for uniqueness is sometimes circumvented by the introduction of a multiset or bag an unordered collection of items that are not necessarily distinct second a set is an unordered collection of items therefore changing the order of its elements does not change the set a list defined as an ordered collection of items is exactly the opposite this is an important theoretical distinction but fortunately it is not important for many applications it is also worth mentioning that if a set is represented by a list depending on the application at hand it might be worth maintaining the list in a sorted order in computing the operations we need to perform for a set or a multiset most often are searching for a given item adding a new item and deleting an item from the collection a data structure that implements these three operations is called the dictionary note the relationship between this data structure and the problem of searching mentioned in section obviously we are dealing here with searching in a dynamic context consequently an efficient implementation of a dictionary has to strike a compromise between the efficiency of searching and the efficiencies of the other two operations there are quite a few ways a dictionary can be implemented they range from an unsophisticated use of arrays sorted or not to much more sophisticated techniques such as hashing and balanced search trees which we discuss later in the book a number of applications in computing require a dynamic partition of some nelement set into a collection of disjoint subsets after being initialized as a collection of n oneelement subsets the collection is subjected to a sequence of intermixed union and search operations this problem is called the set union problem we discuss efficient algorithmic solutions to this problem in section in conjunction with one of its important applications you may have noticed that in our review of basic data structures we almost always mentioned specific operations that are typically performed for the structure in question this intimate relationship between the data and operations has been recognized by computer scientists for a long time it has led them in particular to the idea of an abstract data type adt a set of abstract objects representing data items with a collection of operations that can be performed on them as illustrations of this notion reread say our definitions of the priority queue and dictionary although abstract data types could be implemented in older procedural languages such as pascal see eg aho it is much more convenient to do this in objectoriented languages such as c and java which support abstract data types by means of classes exercises describe how one can implement each of the following operations on an array so that the time it takes does not depend on the arrays size n a delete the ith element of an array i n b delete the ith element of a sorted array the remaining array has to stay sorted of course if you have to solve the searching problem for a list of n numbers how can you take advantage of the fact that the list is known to be sorted give separate answers for a lists represented as arrays b lists represented as linked lists a show the stack after each operation of the following sequence that starts with the empty stack pusha pushb pop pushc pushd pop b show the queue after each operation of the following sequence that starts with the empty queue enqueuea enqueueb dequeue enqueuec enqueued dequeue a let a be the adjacency matrix of an undirected graph explain what property of the matrix indicates that i the graph is complete ii the graph has a loop ie an edge connecting a vertex to itself iii the graph has an isolated vertex ie a vertex with no edges incident to it b answer the same questions for the adjacency list representation give a detailed description of an algorithm for transforming a free tree into a tree rooted at a given vertex of the free tree prove the inequalities that bracket the height of a binary tree with n vertices log n h n indicate how the adt priority queue can be implemented as a an unsorted array b a sorted array c a binary search tree how would you implement a dictionary of a reasonably small size n if you knew that all its elements are distinct eg names of the states of the united states specify an implementation of each dictionary operation for each of the following applications indicate the most appropriate data structure a answering telephone calls in the order of their known priorities b sending backlog orders to customers in the order they have been received c implementing a calculator for computing simple arithmetical expressions anagram checking design an algorithm for checking whether two given words are anagrams ie whether one word can be obtained by permuting the letters of the other for example the words tea and eat are anagrams summary an algorithm is a sequence of nonambiguous instructions for solving a problem in a finite amount of time an input to an algorithm specifies an instance of the problem the algorithm solves algorithms can be specified in a natural language or pseudocode they can also be implemented as computer programs among several ways to classify algorithms the two principal alternatives are to group algorithms according to types of problems they solve to group algorithms according to underlying design techniques they are based upon the important problem types are sorting searching string processing graph problems combinatorial problems geometric problems and numerical problems algorithm design techniques or strategies or paradigms are general approaches to solving problems algorithmically applicable to a variety of problems from different areas of computing although designing an algorithm is undoubtedly a creative activity one can identify a sequence of interrelated actions involved in such a process they are summarized in figure a good algorithm is usually the result of repeated efforts and rework the same problem can often be solved by several algorithms for example three algorithms were given for computing the greatest common divisor of two integers euclids algorithm the consecutive integer checking algorithm and the middleschool method enhanced by the sieve of eratosthenes for generating a list of primes algorithms operate on data this makes the issue of data structuring critical for efficient algorithmic problem solving the most important elementary data structures are the array and the linked list they are used for representing more abstract data structures such as the list the stack the queue the graph via its adjacency matrix or adjacency lists the binary tree and the set an abstract collection of objects with several operations that can be performed on them is called an abstract data type adt the list the stack the queue the priority queue and the dictionary are important examples of abstract data types modern objectoriented languages support implementation of adts by means of classes space and time tradeoffs things which matter most must never be at the mercy of things which matter less johann wolfgang von go ethe space and time tradeoffs in algorithm design are a wellknown issue for both theoreticians and practitioners of computing consider as an example the problem of computing values of a function at many points in its domain if it is time that is at a premium we can precompute the functions values and store them in a table this is exactly what human computers had to do before the advent of electronic computers in the process burdening libraries with thick volumes of mathematical tables though such tables have lost much of their appeal with the widespread use of electronic computers the underlying idea has proven to be quite useful in the development of several important algorithms for other problems in somewhat more general terms the idea is to preprocess the problems input in whole or in part and store the additional information obtained to accelerate solving the problem afterward we call this approach input enhancement and discuss the following algorithms based on it counting methods for sorting section boyermoore algorithm for string matching and its simplified version suggested by horspool section the other type of technique that exploits spacefortime tradeoffs simply uses extra space to facilitate faster andor more flexible access to the data we call this approach prestructuring this name highlights two facets of this variation of the spacefortime tradeoff some processing is done before a problem in question the standard terms used synonymously for this technique are preprocessing and preconditioning confusingly these terms can also be applied to methods that use the idea of preprocessing but do not use extra space see chapter thus in order to avoid confusion we use input enhancement as a special name for the spacefortime tradeoff technique being discussed here is actually solved but unlike the inputenhancement variety it deals with access structuring we illustrate this approach by hashing section indexing with btrees section there is one more algorithm design technique related to the spacefortime tradeoff idea dynamic programming this strategy is based on recording solutions to overlapping subproblems of a given problem in a table from which a solution to the problem in question is then obtained we discuss this welldeveloped technique separately in the next chapter of the book two final comments about the interplay between time and space in algorithm design need to be made first the two resources time and space do not have to compete with each other in all design situations in fact they can align to bring an algorithmic solution that minimizes both the running time and the space consumed such a situation arises in particular when an algorithm uses a spaceefficient data structure to represent a problems input which leads in turn to a faster algorithm consider as an example the problem of traversing graphs recall that the time efficiency of the two principal traversal algorithms depthfirst search and breadthfirst search depends on the data structure used for representing graphs it is n for the adjacency matrix representation and n m for the adjacency list representation where n and m are the numbers of vertices and edges respectively if input graphs are sparse ie have few edges relative to the number of vertices say m on the adjacency list representation may well be more efficient from both the space and the runningtime points of view the same situation arises in the manipulation of sparse matrices and sparse polynomials if the percentage of zeros in such objects is sufficiently high we can save both space and time by ignoring zeros in the objects representation and processing second one can not discuss spacetime tradeoffs without mentioning the hugely important area of data compression note however that in data compression size reduction is the goal rather than a technique for solving another problem we discuss just one data compression algorithm in the next chapter the reader interested in this topic will find a wealth of algorithms in such books as say sorting by counting as a first example of applying the inputenhancement technique we discuss its application to the sorting problem one rather obvious idea is to count for each element of a list to be sorted the total number of elements smaller than this element and record the results in a table these numbers will indicate the positions of the elements in the sorted list eg if the count is for some element it should be in the th position with index if we start counting with in the sorted array thus we will be able to sort the list by simply copying its elements to their appropriate positions in a new sorted list this algorithm is called comparisoncounting sort figure array a initially count after pass i count after pass i count after pass i count after pass i count after pass i count final state count array s figure example of sorting by comparison counting algorithm comparisoncountingsortan sorts an array by comparison counting input an array an of orderable elements output array sn of as elements sorted in nondecreasing order for i to n do counti for i to n do for j i to n do if ai aj countj countj else counti counti for i to n do scounti ai return s what is the time efficiency of this algorithm it should be quadratic because the algorithm considers all the different pairs of an nelement array more formally the number of times its basic operation the comparison ai aj is executed is equal to the sum we have encountered several times already n n n n nn cn n i n i i j i i i thus the algorithm makes the same number of key comparisons as selection sort and in addition uses a linear amount of extra space on the positive side the algorithm makes the minimum number of key moves possible placing each of them directly in their final position in a sorted array the counting idea does work productively in a situation in which elements to be sorted belong to a known small set of values assume for example that we have to sort a list whose values can be either or rather than applying a general sorting algorithm we should be able to take advantage of this additional information about values to be sorted indeed we can scan the list to compute the number of s and the number of s in it and then on the second pass simply make the appropriate number of the first elements equal to and the remaining elements equal to more generally if element values are integers between some lower bound l and upper bound u we can compute the frequency of each of those values and store them in array f u l then the first f positions in the sorted list must be filled with l the next f positions with l and so on all this can be done of course only if we can overwrite the given elements let us consider a more realistic situation of sorting a list of items with some other information associated with their keys so that we can not overwrite the lists elements then we can copy elements into a new array sn to hold the sorted list as follows the elements of a whose values are equal to the lowest possible value l are copied into the first f elements of s ie positions through f the elements of value l are copied to positions from f to f f and so on since such accumulated sums of frequencies are called a distribution in statistics the method itself is known as distribution counting example consider sorting the array whose values are known to come from the set and should not be overwritten in the process of sorting the frequency and distribution arrays are as follows array values frequencies distribution values note that the distribution values indicate the proper positions for the last occurrences of their elements in the final sorted array if we index array positions from to n the distribution values must be reduced by to get corresponding element positions it is more convenient to process the input array right to left for the example the last element is and since its distribution value is we place this in position of the array s that will hold the sorted list then we decrease the s distribution value by and proceed to the next from the right element in the given array the entire processing of this example is depicted in figure d s a a a a a a figure example of sorting by distribution counting the distribution values being decremented are shown in bold here is pseudocode of this algorithm algorithm distributioncountingsortan l u sorts an array of integers from a limited range by distribution counting input an array an of integers between l and u l u output array sn of as elements sorted in nondecreasing order for j to u l do dj initialize frequencies for i to n do dai l dai l compute frequencies for j to u l do dj dj dj reuse for distribution for i n downto do j ai l sdj ai dj dj return s assuming that the range of array values is fixed this is obviously a linear algorithm because it makes just two consecutive passes through its input array a this is a better timeefficiency class than that of the most efficient sorting algorithms mergesort quicksort and heapsort we have encountered it is important to remember however that this efficiency is obtained by exploiting the specific nature of inputs for which sorting by distribution counting works in addition to trading space for time exercises is it possible to exchange numeric values of two variables say u and v without using any extra storage will the comparisoncounting algorithm work correctly for arrays with equal values assuming that the set of possible list values is a b c d sort the following list in alphabetical order by the distributioncounting algorithm b c d c b a a b is the distributioncounting algorithm stable design a oneline algorithm for sorting any array of size n whose values are n distinct integers from to n the ancestry problem asks to determine whether a vertex u is an ancestor of vertex v in a given binary or more generally rooted ordered tree of n vertices design a on inputenhancement algorithm that provides sufficient information to solve this problem for any pair of the trees vertices in constant time the following technique known as virtual initialization provides a timeefficient way to initialize just some elements of a given array an so that for each of its elements we can say in constant time whether it has been initialized and if it has been with which value this is done by utilizing a variable counter for the number of initialized elements in a and two auxiliary arrays of the same size say bn and cn defined as follows b bcounter contain the indices of the elements of a that were initialized b contains the index of the element initialized first b contains the index of the element initialized second etc furthermore if ai was the kth element k counter to be initialized ci contains k a sketch the state of arrays a b and c after the three assignments a x a z a y b in general how can we check with this scheme whether ai has been initialized and if it has been with which value least distance sorting there are egyptian stone statues standing in a row in an art gallery hall a new curator wants to move them so that the statues are ordered by their height how should this be done to minimize the total distance that the statues are moved you may assume for simplicity that all the statues have different heights azi a write a program for multiplying two sparse matrices a p q matrix a and a q r matrix b b write a program for multiplying two sparse polynomials px and qx of degrees m and n respectively is it a good idea to write a program that plays the classic game of tictactoe with the human user by storing all possible positions on the games board along with the best move for each of them input enhancement in string matching in this section we see how the technique of input enhancement can be applied to the problem of string matching recall that the problem of string matching requires finding an occurrence of a given string of m characters called the pattern in a longer string of n characters called the text we discussed the bruteforce algorithm for this problem in section it simply matches corresponding pairs of characters in the pattern and the text left to right and if a mismatch occurs shifts the pattern one position to the right for the next trial since the maximum number of such trials is n m and in the worst case m comparisons need to be made on each of them the worstcase efficiency of the bruteforce algorithm is in the onm class on average however we should expect just a few comparisons before a patterns shift and for random naturallanguage texts the averagecase efficiency indeed turns out to be in on m several faster algorithms have been discovered most of them exploit the inputenhancement idea preprocess the pattern to get some information about it store this information in a table and then use this information during an actual search for the pattern in a given text this is exactly the idea behind the two bestknown algorithms of this type the knuthmorrispratt algorithm knu and the boyermoore algorithm boy the principal difference between these two algorithms lies in the way they compare characters of a pattern with their counterparts in a text the knuthmorrispratt algorithm does it left to right whereas the boyermoore algorithm does it right to left since the latter idea leads to simpler algorithms it is the only one that we will pursue here note that the boyermoore algorithm starts by aligning the pattern against the beginning characters of the text if the first trial fails it shifts the pattern to the right it is comparisons within a trial that the algorithm does right to left starting with the last character in the pattern although the underlying idea of the boyermoore algorithm is simple its actual implementation in a working method is less so therefore we start our discussion with a simplified version of the boyermoore algorithm suggested by r horspool hor in addition to being simpler horspools algorithm is not necessarily less efficient than the boyermoore algorithm on random strings horspools algorithm consider as an example searching for the pattern barber in some text s c sn b a r b e r starting with the last r of the pattern and moving right to left we compare the corresponding pairs of characters in the pattern and the text if all the patterns characters match successfully a matching substring is found then the search can be either stopped altogether or continued if another occurrence of the same pattern is desired if a mismatch occurs we need to shift the pattern to the right clearly we would like to make as large a shift as possible without risking the possibility of missing a matching substring in the text horspools algorithm determines the size of such a shift by looking at the character c of the text that is aligned against the last character of the pattern this is the case even if character c itself matches its counterpart in the pattern in general the following four possibilities can occur case if there are no cs in the pattern eg c is letter s in our example we can safely shift the pattern by its entire length if we shift less some character of the pattern would be aligned against the texts character c that is known not to be in the pattern s s sn b a r b e r b a r b e r case if there are occurrences of character c in the pattern but it is not the last one there eg c is letter b in our example the shift should align the rightmost occurrence of c in the pattern with the c in the text s b sn b a r b e r b a r b e r case if c happens to be the last character in the pattern but there are no cs among its other m characters eg c is letter r in our example the situation is similar to that of case and the pattern should be shifted by the entire patterns length m s m e r sn l e a d e r l e a d e r case finally if c happens to be the last character in the pattern and there are other cs among its first m characters eg c is letter r in our example the situation is similar to that of case and the rightmost occurrence of c among the first m characters in the pattern should be aligned with the texts c s a r sn r e o r d e r r e o r d e r these examples clearly demonstrate that righttoleft character comparisons can lead to farther shifts of the pattern than the shifts by only one position always made by the bruteforce algorithm however if such an algorithm had to check all the characters of the pattern on every trial it would lose much of this superiority fortunately the idea of input enhancement makes repetitive comparisons unnecessary we can precompute shift sizes and store them in a table the table will be indexed by all possible characters that can be encountered in a text including for natural language texts the space punctuation symbols and other special characters note that no other information about the text in which eventual searching will be done is required the tables entries will indicate the shift sizes computed by the formula the patterns length m if c is not among the first m characters of the pattern t c the distance from the rightmost c among the first m characters of the pattern to its last character otherwise for example for the pattern barber all the tables entries will be equal to except for the entries for e b r and a which will be and respectively here is a simple algorithm for computing the shift table entries initialize all the entries to the patterns length m and scan the pattern left to right repeating the following step m times for the j th character of the pattern j m overwrite its entry in the table with m j which is the characters distance to the last character of the pattern note that since the algorithm scans the pattern from left to right the last overwrite will happen for the characters rightmost occurrence exactly as we would like it to be algorithm shifttablep m fills the shift table used by horspools and boyermoore algorithms input pattern p m and an alphabet of possible characters output tablesize indexed by the alphabets characters and filled with shift sizes computed by formula for i to size do tablei m for j to m do tablep j m j return table now we can summarize the algorithm as follows horspools algorithm step for a given pattern of length m and the alphabet used in both the pattern and text construct the shift table as described above step align the pattern against the beginning of the text step repeat the following until either a matching substring is found or the pattern reaches beyond the last character of the text starting with the last character in the pattern compare the corresponding characters in the pattern and text until either all m characters are matched then stop or a mismatching pair is encountered in the latter case retrieve the entry t c from the cs column of the shift table where c is the texts character currently aligned against the last character of the pattern and shift the pattern by t c characters to the right along the text here is pseudocode of horspools algorithm algorithm horspoolmatchingp m t n implements horspools algorithm for string matching input pattern p m and text t n output the index of the left end of the first matching substring or if there are no matches shifttablep m generate table of shifts i m position of the patterns right end while i n do k number of matched characters while k m and p m k t i k do kk if k m return i m else i i tablet i return example as an example of a complete application of horspools algorithm consider searching for the pattern barber in a text that comprises english letters and spaces denoted by underscores the shift table as we mentioned is filled as follows character c a b c d e f r z shift t c the actual search in a particular text proceeds as follows j i m s a w m e i n a b a r b e r s h o p b a r b e r b a r b e r b a r b e r b a r b e r b a r b e r b a r b e r a simple example can demonstrate that the worstcase efficiency of horspools algorithm is in onm problem in this sections exercises but for random texts it is in n and although in the same efficiency class horspools algorithm is obviously faster on average than the bruteforce algorithm in fact as mentioned it is often at least as efficient as its more sophisticated predecessor discovered by r boyer and j moore boyermoore algorithm now we outline the boyermoore algorithm itself if the first comparison of the rightmost character in the pattern with the corresponding character c in the text fails the algorithm does exactly the same thing as horspools algorithm namely it shifts the pattern to the right by the number of characters retrieved from the table precomputed as explained earlier the two algorithms act differently however after some positive number k k m of the patterns characters are matched successfully before a mismatch is encountered s c sik si sn text p pmk pmk pm pattern in this situation the boyermoore algorithm determines the shift size by considering two quantities the first one is guided by the texts character c that caused a mismatch with its counterpart in the pattern accordingly it is called the badsymbol shift the reasoning behind this shift is the reasoning we used in horspools algorithm if c is not in the pattern we shift the pattern to just pass this c in the text conveniently the size of this shift can be computed by the formula tc k where tc is the entry in the precomputed table used by horspools algorithm see above and k is the number of matched characters s c sik si sn text p pmk pmk pm pattern p pm for example if we search for the pattern barber in some text and match the last two characters before failing on letter s in the text we can shift the pattern by ts positions s s e r sn b a r b e r b a r b e r the same formula can also be used when the mismatching character c of the text occurs in the pattern provided tc k for example if we search for the pattern barber in some text and match the last two characters before failing on letter a we can shift the pattern by ta positions s a e r sn b a r b e r b a r b e r if tc k we obviously do not want to shift the pattern by or a negative number of positions rather we can fall back on the bruteforce thinking and simply shift the pattern by one position to the right to summarize the badsymbol shift d is computed by the boyermoore algorithm either as tc k if this quantity is positive and as if it is negative or zero this can be expressed by the following compact formula d maxtc k the second type of shift is guided by a successful match of the last k characters of the pattern we refer to the ending portion of the pattern as its suffix of size k and denote it suff k accordingly we call this type of shift the goodsuffix shift we now apply the reasoning that guided us in filling the badsymbol shift table which was based on a single alphabet character c to the patterns suffixes of sizes m to fill in the goodsuffix shift table let us first consider the case when there is another occurrence of suff k in the pattern or to be more accurate there is another occurrence of suff k not preceded by the same character as in its rightmost occurrence it would be useless to shift the pattern to match another occurrence of suff k preceded by the same character because this would simply repeat a failed trial in this case we can shift the pattern by the distance d between such a second rightmost occurrence not preceded by the same character as in the rightmost occurrence of suff k and its rightmost occurrence for example for the pattern abcbab these distances for k and will be and respectively k pattern d abcbab abcbab what is to be done if there is no other occurrence of suff k not preceded by the same character as in its rightmost occurrence in most cases we can shift the pattern by its entire length m for example for the pattern dbcbab and k we can shift the pattern by its entire length of characters s c b a b sn d b c b a b d b c b a b unfortunately shifting the pattern by its entire length when there is no other occurrence of suff k not preceded by the same character as in its rightmost occurrence is not always correct for example for the pattern abcbab and k shifting by could miss a matching substring that starts with the texts ab aligned with the last two characters of the pattern s c b a b c b a b sn a b c b a b a b c b a b note that the shift by is correct for the pattern dbcbab but not for abcbab because the latter pattern has the same substring ab as its prefix beginning part of the pattern and as its suffix ending part of the pattern to avoid such an erroneous shift based on a suffix of size k for which there is no other occurrence in the pattern not preceded by the same character as in its rightmost occurrence we need to find the longest prefix of size l k that matches the suffix of the same size l if such a prefix exists the shift size d is computed as the distance between this prefix and the corresponding suffix otherwise d is set to the patterns length m as an example here is the complete list of the d values the goodsuffix table of the boyermoore algorithm for the pattern abcbab k pattern d abcbab abcbab abcbab abcbab abcbab now we are prepared to summarize the boyermoore algorithm in its entirety the boyermoore algorithm step for a given pattern and the alphabet used in both the pattern and the text construct the badsymbol shift table as described earlier step using the pattern construct the goodsuffix shift table as described earlier step align the pattern against the beginning of the text step repeat the following step until either a matching substring is found or the pattern reaches beyond the last character of the text starting with the last character in the pattern compare the corresponding characters in the pattern and the text until either all m character pairs are matched then stop or a mismatching pair is encountered after k character pairs are matched successfully in the latter case retrieve the entry tc from the cs column of the badsymbol table where c is the texts mismatched character if k also retrieve the corresponding d entry from the goodsuffix table shift the pattern to the right by the number of positions computed by the formula d d if k maxd d if k where d maxtc k shifting by the maximum of the two available shifts when k is quite logical the two shifts are based on the observations the first one about a texts mismatched character and the second one about a matched group of the patterns rightmost characters that imply that shifting by less than d and d characters respectively can not lead to aligning the pattern with a matching substring in the text since we are interested in shifting the pattern as far as possible without missing a possible matching substring we take the maximum of these two numbers example as a complete example let us consider searching for the pattern baobab in a text made of english letters and spaces the badsymbol table looks as follows c a b c d o z tc the goodsuffix table is filled as follows k pattern d baobab baobab baobab baobab baobab the actual search for this pattern in the text given in figure proceeds as follows after the last b of the pattern fails to match its counterpart k in the text the algorithm retrieves tk from the badsymbol table and shifts the pattern by d maxtk positions to the right the new try successfully matches two pairs of characters after the failure of the third comparison on the space character in the text the algorithm retrieves t from the badsymbol table and d from the goodsuffix table to shift the pattern by maxd d max note that on this iteration it is the goodsuffix rule that leads to a farther shift of the pattern the next try successfully matches just one pair of bs after the failure of the next comparison on the space character in the text the algorithm retrieves t from the badsymbol table and d from the goodsuffix table to shift b e s s k n e w a b o u t b a o b a b s b a o b a b d tk b a o b a b d t b a o b a b d d t d max d d max b a o b a b figure example of string matching with the boyermoore algorithm the pattern by maxdd max note that on this iteration it is the badsymbol rule that leads to a farther shift of the pattern the next try finds a matching substring in the text after successfully matching all six characters of the pattern with their counterparts in the text when searching for the first occurrence of the pattern the worstcase efficiency of the boyermoore algorithm is known to be linear though this algorithm runs very fast especially on large alphabets relative to the length of the pattern many people prefer its simplified versions such as horspools algorithm when dealing with naturallanguagelike strings exercises apply horspools algorithm to search for the pattern baobab in the text bess knew about baobabs consider the problem of searching for genes in dna sequences using horspools algorithm a dna sequence is represented by a text on the alphabet a c g t and the gene or gene segment is the pattern a construct the shift table for the following gene segment of your chromosome tcctattctt b apply horspools algorithm to locate the above pattern in the following dna sequence ttatagatctcgtattcttttatagatctcctattctt how many character comparisons will be made by horspools algorithm in searching for each of the following patterns in the binary text of zeros a b c for searching in a text of length n for a pattern of length m n m with horspools algorithm give an example of a worstcase input b bestcase input is it possible for horspools algorithm to make more character comparisons than the bruteforce algorithm would make in searching for the same pattern in the same text if horspools algorithm discovers a matching substring how large a shift should it make to search for a next possible match how many character comparisons will the boyermoore algorithm make in searching for each of the following patterns in the binary text of zeros a b c a would the boyermoore algorithm work correctly with just the badsymbol table to guide pattern shifts b would the boyermoore algorithm work correctly with just the goodsuffix table to guide pattern shifts a if the last characters of a pattern and its counterpart in the text do match does horspools algorithm have to check other characters right to left or can it check them left to right too b answer the same question for the boyermoore algorithm implement horspools algorithm the boyermoore algorithm and the bruteforce algorithm of section in the language of your choice and run an experiment to compare their efficiencies for matching a random binary patterns in random binary texts b random naturallanguage patterns in naturallanguage texts you are given two strings s and t each n characters long you have to establish whether one of them is a right cyclic shift of the other for example plea is a right cyclic shift of leap and vice versa formally t is a right cyclic shift of s if t can be obtained by concatenating the n icharacter suffix of s and the icharacter prefix of s for some i n a design a spaceefficient algorithm for the task indicate the space and time efficiencies of your algorithm b design a timeefficient algorithm for the task indicate the time and space efficiencies of your algorithm hashing in this section we consider a very efficient way to implement dictionaries recall that a dictionary is an abstract data type namely a set with the operations of searching lookup insertion and deletion defined on its elements the elements of this set can be of an arbitrary nature numbers characters of some alphabet character strings and so on in practice the most important case is that of records student records in a school citizen records in a governmental office book records in a library typically records comprise several fields each responsible for keeping a particular type of information about an entity the record represents for example a student record may contain fields for the students id name date of birth sex home address major and so on among record fields there is usually at least one called a key that is used for identifying entities represented by the records eg the students id in the discussion below we assume that we have to implement a dictionary of n records with keys k k kn hashing is based on the idea of distributing keys among a onedimensional array h m called a hash table the distribution is done by computing for each of the keys the value of some predefined function h called the hash function this function assigns an integer between and m called the hash address to a key for example if keys are nonnegative integers a hash function can be of the form hk k mod m obviously the remainder of division by m is always between and m if keys are letters of some alphabet we can first assign a letter its position in the alphabet denoted here ordk and then apply the same kind of a function used for integers finally if k is a character string cc cs we s can use as a very unsophisticated option or d ci mod m a better option i is to compute hk as follows h for i to s do h h c ordci mod m where c is a constant larger than every ordci in general a hash function needs to satisfy somewhat conflicting requirements a hash tables size should not be excessively large compared to the number of keys but it should be sufficient to not jeopardize the implementations time efficiency see below a hash function needs to distribute keys among the cells of the hash table as evenly as possible this requirement makes it desirable for most applications to have a hash function dependent on all bits of a key not just some of them a hash function has to be easy to compute this can be obtained by treating ordci as digits of a number in the cbased system computing its decimal value by horners rule and finding the remainder of the number after dividing it by m ki kj b m figure collision of two keys in hashing hki hkj obviously if we choose a hash tables size m to be smaller than the number of keys n we will get collisions a phenomenon of two or more keys being hashed into the same cell of the hash table figure but collisions should be expected even if m is considerably larger than n see problem in this sections exercises in fact in the worst case all the keys could be hashed to the same cell of the hash table fortunately with an appropriately chosen hash table size and a good hash function this situation happens very rarely still every hashing scheme must have a collision resolution mechanism this mechanism is different in the two principal versions of hashing open hashing also called separate chaining and closed hashing also called open addressing open hashing separate chaining in open hashing keys are stored in linked lists attached to cells of a hash table each list contains all the keys hashed to its cell consider as an example the following list of words a fool and his money are soon parted as a hash function we will use the simple function for strings mentioned above ie we will add the positions of a words letters in the alphabet and compute the sums remainder after division by we start with the empty table the first key is the word a its hash value is ha mod the second key the word fool is installed in the ninth cell since mod and so on the final result of this process is given in figure note a collision of the keys are and soon because hare mod and hsoon mod how do we search in a dictionary implemented as such a table of linked lists we do this by simply applying to a search key the same procedure that was used for creating the table to illustrate if we want to search for the key kid in the hash table of figure we first compute the value of the same hash function for the key hkid since the list attached to cell is not empty its linked list may contain the search key but because of possible collisions we can not tell whether this is the case until we traverse this linked list after comparing the string kid first with the string are and then with the string soon we end up with an unsuccessful search in general the efficiency of searching depends on the lengths of the linked lists which in turn depend on the dictionary and table sizes as well as the quality keys a fool and his money are soon parted hash addresses a and money fool his are parted soon figure example of a hash table construction with separate chaining of the hash function if the hash function distributes n keys among m cells of the hash table about evenly each list will be about nm keys long the ratio nm called the load factor of the hash table plays a crucial role in the efficiency of hashing in particular the average number of pointers chain links inspected in successful searches s and unsuccessful searches u turns out to be s and u respectively under the standard assumptions of searching for a randomly selected element and a hash function distributing keys uniformly among the tables cells these results are quite natural indeed they are almost identical to searching sequentially in a linked list what we have gained by hashing is a reduction in average list size by a factor of m the size of the hash table normally we want the load factor to be not far from having it too small would imply a lot of empty lists and hence inefficient use of space having it too large would mean longer linked lists and hence longer search times but if we do have the load factor around we have an amazingly efficient scheme that makes it possible to search for a given key for on average the price of one or two comparisons true in addition to comparisons we need to spend time on computing the value of the hash function for a search key but it is a constanttime operation independent from n and m note that we are getting this remarkable efficiency not only as a result of the methods ingenuity but also at the expense of extra space the two other dictionary operations insertion and deletion are almost identical to searching insertions are normally done at the end of a list but see problem in this sections exercises for a possible modification of this rule deletion is performed by searching for a key to be deleted and then removing it from its list hence the efficiency of these operations is identical to that of searching and they are all in the average case if the number of keys n is about equal to the hash tables size m closed hashing open addressing in closed hashing all keys are stored in the hash table itself without the use of linked lists of course this implies that the table size m must be at least as large as the number of keys n different strategies can be employed for collision resolution the simplest one called linear probing checks the cell following the one where the collision occurs if that cell is empty the new key is installed there if the next cell is already occupied the availability of that cells immediate successor is checked and so on note that if the end of the hash table is reached the search is wrapped to the beginning of the table ie it is treated as a circular array this method is illustrated in figure with the same word list and hash function used above to illustrate separate chaining to search for a given key k we start by computing hk where h is the hash function used in the table construction if the cell hk is empty the search is unsuccessful if the cell is not empty we must compare k with the cells occupant if they are equal we have found a matching key if they are not we compare k with a key in the next cell and continue in this manner until we encounter either a matching key a successful search or an empty cell unsuccessful search for example if we search for the word lit in the table of figure we will get hlit mod and since cell is empty we can stop immediately however if we search for kid with hkid mod we will have to compare kid with are soon parted and a before we can declare the search unsuccessful although the search and insertion operations are straightforward for this version of hashing deletion is not for example if we simply delete the key are from the last state of the hash table in figure we will be unable to find the key soon afterward indeed after computing hsoon the algorithm would find this location empty and report the unsuccessful search result a simple solution keys a fool and his money are soon parted hash addresses a a fool a and fool a and fool his a and money fool his a and money fool his are a and money fool his are soon parted a and money fool his are soon figure example of a hash table construction with linear probing is to use lazy deletion ie to mark previously occupied locations by a special symbol to distinguish them from locations that have not been occupied the mathematical analysis of linear probing is a much more difficult problem than that of separate chaining the simplified versions of these results state that the average number of times the algorithm must access the hash table with the load factor in successful and unsuccessful searches is respectively s and u and the accuracy of these approximations increases with larger sizes of the hash table these numbers are surprisingly small even for densely populated tables ie for large percentage values of still as the hash table gets closer to being full the performance of linear probing deteriorates because of a phenomenon called clustering a cluster in linear probing is a sequence of contiguously occupied cells with a possible wrapping for example the final state of the hash table of figure has two clusters clusters are bad news in hashing because they make the dictionary operations less efficient as clusters become larger the probability that a new element will be attached to a cluster increases in addition large clusters increase the probability that two clusters will coalesce after a new keys insertion causing even more clustering several other collision resolution strategies have been suggested to alleviate this problem one of the most important is double hashing under this scheme we use another hash function sk to determine a fixed increment for the probing sequence to be used after a collision at location l hk l sk mod m l sk mod m to guarantee that every location in the table is probed by sequence the increment sk and the table size m must be relatively prime ie their only common divisor must be this condition is satisfied automatically if m itself is prime some functions recommended in the literature are sk m k mod m and sk k mod for small tables and sk k mod for larger ones this problem was solved in by a young graduate student in mathematics named donald e knuth knuth went on to become one of the most important computer scientists of our time his multivolume treatise the art of computer programming knui knuii knuiii knuiv remains the most comprehensive and influential book on algorithmics ever published mathematical analysis of double hashing has proved to be quite difficult some partial results and considerable practical experience with the method suggest that with good hashing functions both primary and secondary double hashing is superior to linear probing but its performance also deteriorates when the table gets close to being full a natural solution in such a situation is rehashing the current table is scanned and all its keys are relocated into a larger table it is worthwhile to compare the main properties of hashing with balanced search trees its principal competitor for implementing dictionaries asymptotic time efficiency with hashing searching insertion and deletion can be implemented to take time on the average but n time in the very unlikely worst case for balanced search trees the average time efficiencies are log n for both the average and worst cases ordering preservation unlike balanced search trees hashing does not assume existence of key ordering and usually does not preserve it this makes hashing less suitable for applications that need to iterate over the keys in order or require range queries such as counting the number of keys between some lower and upper bounds since its discovery in the s by ibm researchers hashing has found many important applications in particular it has become a standard technique for storing a symbol table a table of a computer programs symbols generated during compilation hashing is quite handy for such ai applications as checking whether positions generated by a chessplaying computer program have already been considered with some modifications it has also proved to be useful for storing very large dictionaries on disks this variation of hashing is called extendible hashing since disk access is expensive compared with probes performed in the main memory it is preferable to make many more probes than disk accesses accordingly a location computed by a hash function in extendible hashing indicates a disk address of a bucket that can hold up to b keys when a keys bucket is identified all its keys are read into main memory and then searched for the key in question in the next section we discuss btrees a principal alternative for storing large dictionaries exercises for the input and hash function hk k mod a construct the open hash table b find the largest number of key comparisons in a successful search in this table c find the average number of key comparisons in a successful search in this table for the input and hash function hk k mod a construct the closed hash table b find the largest number of key comparisons in a successful search in this table c find the average number of key comparisons in a successful search in this table why is it not a good idea for a hash function to depend on just one letter say the first one of a naturallanguage word find the probability of all n keys being hashed to the same cell of a hash table of size m if the hash function distributes keys evenly among all the cells of the table birthday paradox the birthday paradox asks how many people should be in a room so that the chances are better than even that two of them will have the same birthday month and day find the quite unexpected answer to this problem what implication for hashing does this result have answer the following questions for the separatechaining version of hashing a where would you insert keys if you knew that all the keys in the dictionary are distinct which dictionary operations if any would benefit from this modification b we could keep keys of the same linked list sorted which of the dictionary operations would benefit from this modification how could we take advantage of this if all the keys stored in the entire table need to be sorted explain how to use hashing to check whether all elements of a list are distinct what is the time efficiency of this application compare its efficiency with that of the bruteforce algorithm section and of the presortingbased algorithm section fill in the following table with the averagecase as the first entry and worstcase as the second entry efficiency classes for the five implementations of the adt dictionary unordered ordered binary balanced array array search tree search tree hashing search insertion deletion we have discussed hashing in the context of techniques based on spacetime tradeoffs but it also takes advantage of another general strategy which one write a computer program that uses hashing for the following problem given a naturallanguage text generate a list of distinct words with the number of occurrences of each word in the text insert appropriate counters in the program to compare the empirical efficiency of hashing with the corresponding theoretical results p k p pi ki pi pn kn pn t t ti ti tn tn figure parental node of a btree btrees the idea of using extra space to facilitate faster access to a given data set is particularly important if the data set in question contains a very large number of records that need to be stored on a disk a principal device in organizing such data sets is an index which provides some information about the location of records with indicated key values for data sets of structured records as opposed to unstructured data such as text images sound and video the most important index organization is the btree introduced by r bayer and e mcgreight bay it extends the idea of the tree see section by permitting more than a single key in the same node of a search tree in the btree version we consider here all data records or record keys are stored at the leaves in increasing order of the keys the parental nodes are used for indexing specifically each parental node contains n ordered keys k kn assumed for the sake of simplicity to be distinct the keys are interposed with n pointers to the nodes children so that all the keys in subtree t are smaller than k all the keys in subtree t are greater than or equal to k and smaller than k with k being equal to the smallest key in t and so on through the last subtree tn whose keys are greater than or equal to kn with kn being equal to the smallest key in tn see figure in addition a btree of order m must satisfy the following structural properties the root is either a leaf or has between and m children each node except for the root and the leaves has between m and m children and hence between m and m keys the tree is perfectly balanced ie all its leaves are at the same level the node depicted in figure is called the nnode thus all the nodes in a classic binary search tree are nodes a tree introduced in section comprises nodes and nodes figure example of a btree of order an example of a btree of order is given in figure searching in a btree is very similar to searching in the binary search tree and even more so in the tree starting with the root we follow a chain of pointers to the leaf that may contain the search key then we search for the search key among the keys of that leaf note that since keys are stored in sorted order at both parental nodes and leaves we can use binary search if the number of keys at a node is large enough to make it worthwhile it is not the number of key comparisons however that we should be concerned about in a typical application of this data structure when used for storing a large data file on a disk the nodes of a btree normally correspond to the disk pages since the time needed to access a disk page is typically several orders of magnitude larger than the time needed to compare keys in the fast computer memory it is the number of disk accesses that becomes the principal indicator of the efficiency of this and similar data structures how many nodes of a btree do we need to access during a search for a record with a given key value this number is obviously equal to the height of the tree plus to estimate the height let us find the smallest number of keys a btree of order m and positive height h can have the root of the tree will contain at least one key level will have at least two nodes with at least m keys in each of them for the total minimum number of keys m level will have at least m nodes the children of the nodes on level with at least m in each of them for the total minimum number of keys m m in general the nodes of level i i h will contain at least m i m keys finally level h the leaf level will have at least m h nodes with at least one key in each thus for any btree of order m with n nodes and height h we have the following inequality h n m i m m h i after a series of standard simplifications see problem in this sections exercises this inequality reduces to n m h which in turn yields the following upper bound on the height h of the btree of order m with n nodes h log m n inequality immediately implies that searching in a btree is a olog n operation but it is important to ascertain here not just the efficiency class but the actual number of disk accesses implied by this formula the following table contains the values of the righthandside estimates for a file of million records and a few typical values of the trees order m order m hs upper bound keep in mind that the tables entries are upper estimates for the number of disk accesses in actual applications this number rarely exceeds with the btrees root and sometimes firstlevel nodes stored in the fast memory to minimize the number of disk accesses the operations of insertion and deletion are less straightforward than searching but both can also be done in olog n time here we outline an insertion algorithm only a deletion algorithm can be found in the references eg aho cor the most straightforward algorithm for inserting a new record into a btree is quite similar to the algorithm for insertion into a tree outlined in section first we apply the search procedure to the new records key k to find the appropriate leaf for the new record if there is room for the record in that leaf we place it there in an appropriate position so that the keys remain sorted and we are done if there is no room for the record the leaf is split in half by sending the second half of the records to a new node after that the smallest key k in the new node and the pointer to it are inserted into the old leafs parent immediately after the key and pointer to the old leaf this recursive procedure may percolate up to the trees root if the root is already full too a new root is created with the two halves of the old roots keys split between two children of the new root as an example figure shows the result of inserting into the btree in figure under the restriction that the leaves can not contain more than three items you should be aware that there are other algorithms for implementing insertions into a btree for example to avoid the possibility of recursive node splits we can split full nodes encountered in searching for an appropriate leaf for the new record another possibility is to avoid some node splits by moving a key to the nodes sibling for example inserting into the btree in figure can be done by moving the smallest key of the full leaf to its sibling with keys and and replacing the key value of their parent by the new smallest value in figure btree obtained after inserting into the btree in figure the second child this modification tends to save some space at the expense of a slightly more complicated algorithm a btree does not have to be always associated with the indexing of a large file and it can be considered as one of several search tree varieties as with other types of search trees such as binary search trees avl trees and trees a btree can be constructed by successive insertions of data records into the initially empty tree the empty tree is considered to be a btree too when all keys reside in the leaves and the upper levels are organized as a btree comprising an index the entire structure is usually called in fact a btree exercises give examples of using an index in reallife applications that do not involve computers a prove the equality h m i m m h m h i which was used in the derivation of upper bound for the height of a btree b complete the derivation of inequality find the minimum order of the btree that guarantees that the number of disk accesses in searching in a file of million records does not exceed assume that the roots page is stored in main memory draw the btree obtained after inserting and then in the btree in figure assume that a leaf can not contain more than three items outline an algorithm for finding the largest key in a btree a a topdown tree is a btree of order with the following modification of the insert operation whenever a search for a leaf for a new key encounters a full node ie a node with three keys the node is split into two nodes by sending its middle key to the nodes parent or if the full node happens to be the root the new root for the middle key is created construct a topdown tree by inserting the following list of keys in the initially empty tree b what is the principal advantage of this insertion procedure compared with the one used for trees in section what is its disadvantage a write a program implementing a key insertion algorithm in a btree b write a program for visualization of a key insertion algorithm in a btree summary space and time tradeoffs in algorithm design are a wellknown issue for both theoreticians and practitioners of computing as an algorithm design technique trading space for time is much more prevalent than trading time for space input enhancement is one of the two principal varieties of trading space for time in algorithm design its idea is to preprocess the problems input in whole or in part and store the additional information obtained in order to accelerate solving the problem afterward sorting by distribution counting and several important algorithms for string matching are examples of algorithms based on this technique distribution counting is a special method for sorting lists of elements from a small set of possible values horspools algorithm for string matching can be considered a simplified version of the boyermoore algorithmboth algorithms are based on the ideas of input enhancement and righttoleft comparisons of a patterns characters both algorithms use the same badsymbol shift table the boyermoore also uses a second table called the goodsuffix shift table prestructuring the second type of technique that exploits spacefortime tradeoffs uses extra space to facilitate a faster andor more flexible access to the data hashing and btrees are important examples of prestructuring hashing is a very efficient approach to implementing dictionaries it is based on the idea of mapping keys into a onedimensional table the size limitations of such a table make it necessary to employ a collision resolution mechanism the two principal varieties of hashing are open hashing or separate chaining with keys stored in linked lists outside of the hash table and closed hashing or open addressing with keys stored inside the table both enable searching insertion and deletion in time on average the btree is a balanced search tree that generalizes the idea of the tree by allowing multiple keys at the same node its principal application called the btree is for keeping indexlike information about data stored on a disk by choosing the order of the tree appropriately one can implement the operations of searching insertion and deletion with just a few disk accesses even for extremely large files dynamic programming an idea like a ghost must be spoken to a little before it will explain itself charles dickens dynamic programming is an algorithm design technique with a rather interesting history it was invented by a prominent us mathematician richard bellman in the s as a general method for optimizing multistage decision processes thus the word programming in the name of this technique stands for planning and does not refer to computer programming after proving its worth as an important tool of applied mathematics dynamic programming has eventually come to be considered at least in computer science circles as a general algorithm design technique that does not have to be limited to special types of optimization problems it is from this point of view that we will consider this technique here dynamic programming is a technique for solving problems with overlapping subproblems typically these subproblems arise from a recurrence relating a given problems solution to solutions of its smaller subproblems rather than solving overlapping subproblems again and again dynamic programming suggests solving each of the smaller subproblems only once and recording the results in a table from which a solution to the original problem can then be obtained this technique can be illustrated by revisiting the fibonacci numbers discussed in section if you have not read that section you will be able to follow the discussion anyway but it is a beautiful topic so if you feel a temptation to read it do succumb to it the fibonacci numbers are the elements of the sequence which can be defined by the simple recurrence f n f n f n for n and two initial conditions f f if we try to use recurrence directly to compute the nth fibonacci number f n we would have to recompute the same values of this function many times see figure for an example note that the problem of computing f n is expressed in terms of its smaller and overlapping subproblems of computing f n and f n so we can simply fill elements of a onedimensional array with the n consecutive values of f n by starting in view of initial conditions with and and using equation as the rule for producing all the other elements obviously the last element of this array will contain f n singleloop pseudocode of this very simple algorithm can be found in section note that we can in fact avoid using an extra array to accomplish this task by recording the values of just the last two elements of the fibonacci sequence problem in exercises this phenomenon is not unusual and we shall encounter it in a few more examples in this chapter thus although a straightforward application of dynamic programming can be interpreted as a special variety of spacefortime tradeoff a dynamic programming algorithm can sometimes be refined to avoid using extra space certain algorithms compute the nth fibonacci number without computing all the preceding elements of this sequence see section it is typical of an algorithm based on the classic bottomup dynamic programming approach however to solve all smaller subproblems of a given problem one variation of the dynamic programming approach seeks to avoid solving unnecessary subproblems this technique illustrated in section exploits socalled memory functions and can be considered a topdown variation of dynamic programming whether one uses the classical bottomup version of dynamic programming or its topdown variation the crucial step in designing such an algorithm remains the same deriving a recurrence relating a solution to the problem to solutions to its smaller subproblems the immediate availability of equation for computing the nth fibonacci number is one of the few exceptions to this rule since a majority of dynamic programming applications deal with optimization problems we also need to mention a general principle that underlines such applications richard bellman called it the principle of optimality in terms somewhat different from its original formulation it says that an optimal solution to any instance of an optimization problem is composed of optimal solutions to its subinstances the principle of optimality holds much more often than not to give a rather rare example it fails for finding the longest simple path in a graph although its applicability to a particular problem needs to be checked of course such a check is usually not a principal difficulty in developing a dynamic programming algorithm in the sections and exercises of this chapter are a few standard examples of dynamic programming algorithms the algorithms in section were in fact invented independently of the discovery of dynamic programming and only later came to be viewed as examples of this techniques applications numerous other applications range from the optimal way of breaking text into lines eg baa to image resizing avi to a variety of applications to sophisticated engineering problems eg ber three basic examples the goal of this section is to introduce dynamic programming via three typical examples example coinrow problem there is a row of n coins whose values are some positive integers c c cn not necessarily distinct the goal is to pick up the maximum amount of money subject to the constraint that no two coins adjacent in the initial row can be picked up let f n be the maximum amount that can be picked up from the row of n coins to derive a recurrence for f n we partition all the allowed coin selections into two groups those that include the last coin and those without it the largest amount we can get from the first group is equal to cn f n the value of the nth coin plus the maximum amount we can pick up from the first n coins the maximum amount we can get from the second group is equal to f n by the definition of f n thus we have the following recurrence subject to the obvious initial conditions f n maxcn f n f n for n f f c we can compute f n by filling the onerow table left to right in the manner similar to the way it was done for the nth fibonacci number by algorithm fibn in section algorithm coinrowc n applies formula bottom up to find the maximum amount of money that can be picked up from a coin row without picking two adjacent coins input array cn of positive integers indicating the coin values output the maximum amount of money that can be picked up f f c for i to n do f i maxci f i f i return f n the application of the algorithm to the coin row of denominations is shown in figure it yields the maximum amount of it is worth pointing index c f f c f index c f max f index c f max f index c f max f index c f max f index c f max f figure solving the coinrow problem by dynamic programming for the coin row out that in fact we also solved the problem for the first i coins in the row given for every i for example for i the maximum amount is f to find the coins with the maximum total value found we need to backtrace the computations to see which of the two possibilities cn f n or f n produced the maxima in formula in the last application of the formula it was the sum c f which means that the coin c is a part of an optimal solution moving to computing f the maximum was produced by the sum c f which means that the coin c is a part of an optimal solution as well finally the maximum in computing f was produced by f implying that the coin c is not the part of an optimal solution and the coin c is thus the optimal solution is c c c to avoid repeating the same computations during the backtracing the information about which of the two terms in was larger can be recorded in an extra array when the values of f are computed using the coinrow to find f n the largest amount of money that can be picked up as well as the coins composing an optimal set clearly takes n time and n space this is by far superior to the alternatives the straightforward topdown application of recurrence and solving the problem by exhaustive search problem in this sections exercises example changemaking problem consider the general instance of the following wellknown problem give change for amount n using the minimum number of coins of denominations d d dm for the coin denominations used in the united states as for those used in most if not all other countries there is a very simple and efficient algorithm discussed in the next chapter here we consider a dynamic programming algorithm for the general case assuming availability of unlimited quantities of coins for each of the m denominations d d dm where d let f n be the minimum number of coins whose values add up to n it is convenient to define f the amount n can only be obtained by adding one coin of denomination dj to the amount n dj for j m such that n dj therefore we can consider all such denominations and select the one minimizing f n dj since is a constant we can of course find the smallest f n dj first and then add to it hence we have the following recurrence for f n f n min f n dj for n j ndj f we can compute f n by filling a onerow table left to right in the manner similar to the way it was done above for the coinrow problem but computing a table entry here requires finding the minimum of up to m numbers algorithm changemakingdm n applies dynamic programming to find the minimum number of coins of denominations d d dm where d that add up to a given amount n input positive integer n and array dm of increasing positive integers indicating the coin denominations where d output the minimum number of coins that add up to n f for i to n do temp j while j m and i dj do temp minf i dj temp j j f i temp return f n the application of the algorithm to amount n and denominations is shown in figure the answer it yields is two coins the time and space efficiencies of the algorithm are obviously onm and n respectively n f f n f minf f n f minf f n f minf f f n f minf f f f n f minf f f f n f minf f f f figure application of algorithm mincoinchange to amount n and coin denominations and to find the coins of an optimal solution we need to backtrace the computations to see which of the denominations produced the minima in formula for the instance considered the last application of the formula for n the minimum was produced by d the second minimum for n was also produced for a coin of that denomination thus the minimumcoin set for n is two s example coincollecting problem several coins are placed in cells of an n m board no more than one coin per cell a robot located in the upper left cell of the board needs to collect as many of the coins as possible and bring them to the bottom right cell on each step the robot can move either one cell to the right or one cell down from its current location when the robot visits a cell with a coin it always picks up that coin design an algorithm to find the maximum number of coins the robot can collect and a path it needs to follow to do this let f i j be the largest number of coins the robot can collect and bring to the cell i j in the ith row and j th column of the board it can reach this cell either from the adjacent cell i j above it or from the adjacent cell i j to the left of it the largest numbers of coins that can be brought to these cells are f i j and f i j respectively of course there are no adjacent cells above the cells in the first row and there are no adjacent cells to the left of the cells in the first column for those cells we assume that f i j and f i j are equal to for their nonexistent neighbors therefore the largest number of coins the robot can bring to cell i j is the maximum of these two numbers plus one possible coin at cell i j itself in other words we have the following formula for f i j f i j maxf i j f i j cij for i n j m f j for j m and f i for i n where cij if there is a coin in cell i j and cij otherwise using these formulas we can fill in the n m table of f i j values either row by row or column by column as is typical for dynamic programming algorithms involving twodimensional tables algorithm robotcoincollectioncn m applies dynamic programming to compute the largest number of coins a robot can collect on an n m board by starting at and moving right and down from upper left to down right corner input matrix cn m whose elements are equal to and for cells with and without a coin respectively output largest number of coins the robot can bring to cell n m f c for j to m do f j f j c j for i to n do f i f i ci for j to m do f i j maxf i j f i j ci j return f n m the algorithm is illustrated in figure b for the coin setup in figure a since computing the value of f i j by formula for each cell of the table takes constant time the time efficiency of the algorithm is nm its space efficiency is obviously also nm tracing the computations backward makes it possible to get an optimal path if f i j f i j an optimal path to cell i j must come down from the adjacent cell above it if f i j f i j an optimal path to cell i j must come from the adjacent cell on the left and if f i j f i j it can reach cell i j from either direction this yields two optimal paths for the instance in figure a which are shown in figure c if ties are ignored one optimal path can be obtained in n m time a b c figure a coins to collect b dynamic programming algorithm results c two paths to collect coins the maximum number of coins possible exercises what does dynamic programming have in common with divideandconquer what is a principal difference between them solve the instance of the coinrow problem a show that the time efficiency of solving the coinrow problem by straightforward application of recurrence is exponential b show that the time efficiency of solving the coinrow problem by exhaustive search is at least exponential apply the dynamic programming algorithm to find all the solutions to the changemaking problem for the denominations and the amount n how would you modify the dynamic programming algorithm for the coincollecting problem if some cells on the board are inaccessible for the robot apply your algorithm to the board below where the inaccessible cells are shown by xs how many optimal paths are there for this board rodcutting problem design a dynamic programming algorithm for the following problem find the maximum total sale price that can be obtained by cutting a rod of n units long into integerlength pieces if the sale price of a piece i units long is pi for i n what are the time and space efficiencies of your algorithm shortestpath counting a chess rook can move horizontally or vertically to any square in the same row or in the same column of a chessboard find the number of shortest paths by which a rook can move from one corner of a chessboard to the diagonally opposite corner the length of a path is measured by the number of squares it passes through including the first and the last squares solve the problem a by a dynamic programming algorithm b by using elementary combinatorics minimumsum descent some positive integers are arranged in an equilateral triangle with n numbers in its base like the one shown in the figure below for n the problem is to find the smallest sum in a descent from the triangle apex to its base through a sequence of adjacent numbers shown in the figure by the circles design a dynamic programming algorithm for this problem and indicate its time efficiency binomial coefficient design an efficient algorithm for computing the binomial coefficient cn k that uses no multiplications what are the time and space efficiencies of your algorithm longest path in a dag a design an efficient algorithm for finding the length of the longest path in a dag this problem is important both as a prototype of many other dynamic programming applications and in its own right because it determines the minimal time needed for completing a project comprising precedenceconstrained tasks b show how to reduce the coinrow problem discussed in this section to the problem of finding a longest path in a dag maximum square submatrix given an m n boolean matrix b find its largest square submatrix whose elements are all zeros design a dynamic programming algorithm and indicate its time efficiency the algorithm may be useful for say finding the largest free square area on a computer screen or for selecting a construction site world series odds consider two teams a and b playing a series of games until one of the teams wins n games assume that the probability of a winning a game is the same for each game and equal to p and the probability of a losing a game is q p hence there are no ties let p i j be the probability of a winning the series if a needs i more games to win the series and b needs j more games to win the series a set up a recurrence relation for p i j that can be used by a dynamic programming algorithm b find the probability of team a winning a sevengame series if the probability of it winning a game is c write pseudocode of the dynamic programming algorithm for solving this problem and determine its time and space efficiencies the knapsack problem and memory functions we start this section with designing a dynamic programming algorithm for the knapsack problem given n items of known weights w wn and values v vn and a knapsack of capacity w find the most valuable subset of the items that fit into the knapsack this problem was introduced in section where we discussed solving it by exhaustive search we assume here that all the weights and the knapsack capacity are positive integers the item values do not have to be integers to design a dynamic programming algorithm we need to derive a recurrence relation that expresses a solution to an instance of the knapsack problem in terms of solutions to its smaller subinstances let us consider an instance defined by the first i items i n with weights w wi values v vi and knapsack capacity j j w let f i j be the value of an optimal solution to this instance ie the value of the most valuable subset of the first i items that fit into the knapsack of capacity j we can divide all the subsets of the first i items that fit the knapsack of capacity j into two categories those that do not include the ith item and those that do note the following among the subsets that do not include the ith item the value of an optimal subset is by definition f i j among the subsets that do include the ith item hence j wi an optimal subset is made up of this item and an optimal subset of the first i items that fits into the knapsack of capacity j wi the value of such an optimal subset is vi f i j wi thus the value of an optimal solution among all feasible subsets of the first i items is the maximum of these two values of course if the ith item does not fit into the knapsack the value of an optimal subset selected from the first i items is the same as the value of an optimal subset selected from the first i items these observations lead to the following recurrence f i j maxf i j vi f i j wi if j wi f i j if j wi it is convenient to define the initial conditions as follows f j for j and f i for i our goal is to find f n w the maximal value of a subset of the n given items that fit into the knapsack of capacity w and an optimal subset itself figure illustrates the values involved in equations and for i j to compute the entry in the ith row and the j th column f i j we compute the maximum of the entry in the previous row and the same column and the sum of vi and the entry in the previous row and wi columns to the left the table can be filled either row by row or column by column j wi j w i f i j wi f i j wi vi i f i j n goal figure table for solving the knapsack problem by dynamic programming capacity j i w v w v w v w v figure example of solving an instance of the knapsack problem by the dynamic programming algorithm example let us consider the instance given by the following data item weight value capacity w the dynamic programming table filled by applying formulas and is shown in figure thus the maximal value is f we can find the composition of an optimal subset by backtracing the computations of this entry in the table since f f item has to be included in an optimal solution along with an optimal subset for filling remaining units of the knapsack capacity the value of the latter is f since f f item need not be in an optimal subset since f f item is a part of an optimal selection which leaves element f to specify its remaining composition similarly since f f item is the final part of the optimal solution item item item the time efficiency and space efficiency of this algorithm are both in nw the time needed to find the composition of an optimal solution is in on you are asked to prove these assertions in the exercises memory functions as we discussed at the beginning of this chapter and illustrated in subsequent sections dynamic programming deals with problems whose solutions satisfy a recurrence relation with overlapping subproblems the direct topdown approach to finding a solution to such a recurrence leads to an algorithm that solves common subproblems more than once and hence is very inefficient typically exponential or worse the classic dynamic programming approach on the other hand works bottom up it fills a table with solutions to all smaller subproblems but each of them is solved only once an unsatisfying aspect of this approach is that solutions to some of these smaller subproblems are often not necessary for getting a solution to the problem given since this drawback is not present in the topdown approach it is natural to try to combine the strengths of the topdown and bottomup approaches the goal is to get a method that solves only subproblems that are necessary and does so only once such a method exists it is based on using memory functions this method solves a given problem in the topdown manner but in addition maintains a table of the kind that would have been used by a bottomup dynamic programming algorithm initially all the tables entries are initialized with a special null symbol to indicate that they have not yet been calculated thereafter whenever a new value needs to be calculated the method checks the corresponding entry in the table first if this entry is not null it is simply retrieved from the table otherwise it is computed by the recursive call whose result is then recorded in the table the following algorithm implements this idea for the knapsack problem after initializing the table the recursive function needs to be called with i n the number of items and j w the knapsack capacity algorithm mfknapsacki j implements the memory function method for the knapsack problem input a nonnegative integer i indicating the number of the first items being considered and a nonnegative integer j indicating the knapsack capacity output the value of an optimal feasible subset of the first i items note uses as global variables input arrays w eightsn v aluesn and table f n w whose entries are initialized with s except for row and column initialized with s if f i j if j weightsi value mfknapsacki j else value maxmfknapsacki j valuesi mfknapsacki j weightsi f i j value return f i j example let us apply the memory function method to the instance considered in example the table in figure gives the results only out of nontrivial values ie not those in row or in column have been computed capacity j i w v w v w v w v figure example of solving an instance of the knapsack problem by the memory function algorithm just one nontrivial entry v is retrieved rather than being recomputed for larger instances the proportion of such entries can be significantly larger in general we can not expect more than a constantfactor gain in using the memory function method for the knapsack problem because its time efficiency class is the same as that of the bottomup algorithm why a more significant improvement can be expected for dynamic programming algorithms in which a computation of one value takes more than constant time you should also keep in mind that a memory function algorithm may be less spaceefficient than a spaceefficient version of a bottomup algorithm exercises a apply the bottomup dynamic programming algorithm to the following instance of the knapsack problem item weight value capacity w b how many different optimal subsets does the instance of part a have c in general how can we use the table generated by the dynamic programming algorithm to tell whether there is more than one optimal subset for the knapsack problems instance a write pseudocode of the bottomup dynamic programming algorithm for the knapsack problem b write pseudocode of the algorithm that finds the composition of an optimal subset from the table generated by the bottomup dynamic programming algorithm for the knapsack problem for the bottomup dynamic programming algorithm for the knapsack problem prove that a its time efficiency is nw b its space efficiency is nw c the time needed to find the composition of an optimal subset from a filled dynamic programming table is on a true or false a sequence of values in a row of the dynamic programming table for the knapsack problem is always nondecreasing b true or false a sequence of values in a column of the dynamic programming table for the knapsack problem is always nondecreasing design a dynamic programming algorithm for the version of the knapsack problem in which there are unlimited quantities of copies for each of the n item kinds given indicate the time efficiency of the algorithm apply the memory function method to the instance of the knapsack problem given in problem indicate the entries of the dynamic programming table that are i never computed by the memory function method ii retrieved without a recomputation prove that the efficiency class of the memory function algorithm for the knapsack problem is the same as that of the bottomup algorithm see problem explain why the memory function approach is unattractive for the problem of computing a binomial coefficient by the formula cn k cn k cn k write a research report on one of the following wellknown applications of dynamic programming a finding the longest common subsequence in two sequences b optimal string editing c minimal triangulation of a polygon optimal binary search trees a binary search tree is one of the most important data structures in computer science one of its principal applications is to implement a dictionary a set of elements with the operations of searching insertion and deletion if probabilities a b b a c c d d figure two out of possible binary search trees with keys a b c and d of searching for elements of a set are known eg from accumulated data about past searches it is natural to pose a question about an optimal binary search tree for which the average number of comparisons in a search is the smallest possible for simplicity we limit our discussion to minimizing the average number of comparisons in a successful search the method can be extended to include unsuccessful searches as well as an example consider four keys a b c and d to be searched for with probabilities and respectively figure depicts two out of possible binary search trees containing these keys the average number of comparisons in a successful search in the first of these trees is and for the second one it is neither of these two trees is in fact optimal can you tell which binary tree is optimal for our tiny example we could find the optimal tree by generating all binary search trees with these keys as a general algorithm this exhaustivesearch approach is unrealistic the total number of binary search trees with n keys is equal to the nth catalan number cn n for n c n n which grows to infinity as fast as nn see problem in this sections exercises so let a an be distinct keys ordered from the smallest to the largest and let p pn be the probabilities of searching for them let ci j be the smallest average number of comparisons made in a successful search in a binary search tree tij made up of keys ai aj where i j are some integer indices i j n following the classic dynamic programming approach we will find values of ci j for all smaller instances of the problem although we are interested just in c n to derive a recurrence underlying a dynamic programming algorithm we will consider all possible ways to choose a root ak among the keys ai aj for such a binary search tree figure the root contains key ak the left subtree tik contains keys ai ak optimally arranged and the right subtree tkj ak optimal optimal bst for bst for ai ak ak aj figure binary search tree bst with root ak and two optimal binary search subtrees tik and tkj contains keys ak aj also optimally arranged note how we are taking advantage of the principle of optimality here if we count tree levels starting with to make the comparison numbers equal the keys levels the following recurrence relation is obtained k ci j min pk ps level of as in tik ikj si j ps level of as in tkj sk k j j min ps level of as in tik ps level of as in tkj ps ikj si sk si j min ci k ck j ps ikj si thus we have the recurrence j ci j min ci k ck j ps for i j n ikj si we assume in formula that ci i for i n which can be interpreted as the number of comparisons in the empty tree note that this formula implies that ci i pi for i n as it should be for a onenode binary search tree containing ai j n p goal p i c ij pn n figure table of the dynamic programming algorithm for constructing an optimal binary search tree the twodimensional table in figure shows the values needed for computing ci j by formula they are in row i and the columns to the left of column j and in column j and the rows below row i the arrows point to the pairs of entries whose sums are computed in order to find the smallest one to be recorded as the value of ci j this suggests filling the table along its diagonals starting with all zeros on the main diagonal and given probabilities pi i n right above it and moving toward the upper right corner the algorithm we just sketched computes c n the average number of comparisons for successful searches in the optimal binary tree if we also want to get the optimal tree itself we need to maintain another twodimensional table to record the value of k for which the minimum in is achieved the table has the same shape as the table in figure and is filled in the same manner starting with entries ri i i for i n when the table is filled its entries indicate indices of the roots of the optimal subtrees which makes it possible to reconstruct an optimal tree for the entire set given example let us illustrate the algorithm by applying it to the fourkey set we used at the beginning of this section key a b c d probability the initial tables look like this main table root table let us compute c k c c ps c min s k c c ps s thus out of two possible binary trees containing the first two keys a and b the root of the optimal tree has index ie it contains b and the average number of comparisons in a successful search in this tree is we will ask you to finish the computations in the exercises you should arrive at the following final tables main table root table thus the average number of key comparisons in the optimal tree is equal to since r the root of the optimal tree contains the third key ie c its left subtree is made up of keys a and b and its right subtree contains just key d why to find the specific structure of these subtrees we find first their roots by consulting the root table again as follows since r the root of the optimal tree containing a and b is b with a being its left child and the root of the onenode tree r since r the root of this onenode optimal tree is its only key d figure presents the optimal tree in its entirety c b d a figure optimal binary search tree for the example here is pseudocode of the dynamic programming algorithm algorithm optimalbstp n finds an optimal binary search tree by dynamic programming input an array p n of search probabilities for a sorted list of n keys output average number of comparisons in successful searches in the optimal bst and table r of subtrees roots in the optimal bst for i to n do ci i ci i p i ri i i cn n for d to n do diagonal count for i to n d do j id minval for k i to j do if ci k ck j minval minval ci k ck j kmin k ri j kmin sum p i for s i to j do sum sum p s ci j minval sum return c n r the algorithms space efficiency is clearly quadratic the time efficiency of this version of the algorithm is cubic why a more careful analysis shows that entries in the root table are always nondecreasing along each row and column this limits values for ri j to the range ri j ri j and makes it possible to reduce the running time of the algorithm to n exercises finish the computations started in the sections example of constructing an optimal binary search tree a why is the time efficiency of algorithm optimalbst cubic b why is the space efficiency of algorithm optimalbst quadratic write pseudocode for a lineartime algorithm that generates the optimal binary search tree from the root table devise a way to compute the sums j ps which are used in the dynamic si programming algorithm for constructing an optimal binary search tree in constant time per sum true or false the root of an optimal binary search tree always contains the key with the highest search probability how would you construct an optimal binary search tree for a set of n keys if all the keys are equally likely to be searched for what will be the average number of comparisons in a successful search in such a tree if n k a show that the number of distinct binary search trees bn that can be constructed for a set of n orderable keys satisfies the recurrence relation n bn bkbn k for n b k b it is known that the solution to this recurrence is given by the catalan numbers verify this assertion for n c find the order of growth of bn what implication does the answer to this question have for the exhaustivesearch algorithm for constructing an optimal binary search tree design a n algorithm for finding an optimal binary search tree generalize the optimal binary search algorithm by taking into account unsuccessful searches write pseudocode of a memory function for the optimal binary search tree problem you may limit your function to finding the smallest number of key comparisons in a successful search matrix chain multiplication consider the problem of minimizing the total number of multiplications made in computing the product of n matrices a a an whose dimensions are d d d d dn dn respectively assume that all intermediate products of two matrices are computed by the bruteforce definitionbased algorithm a give an example of three matrices for which the number of multiplications in a a a and a a a differ at least by a factor of b how many different ways are there to compute the product of n matrices c design a dynamic programming algorithm for finding an optimal order of multiplying n matrices what is an algorithm although there is no universally agreedon wording to describe this notion there is general agreement about what the concept means an algorithm is a sequence of unambiguous instructions for solving a problem ie for obtaining a required output for any legitimate input in a finite amount of time this definition can be illustrated by a simple diagram figure the reference to instructions in the definition implies that there is something or someone capable of understanding and following the instructions given we call this a computer keeping in mind that before the electronic computer was invented the word computer meant a human being involved in performing numeric calculations nowadays of course computers are those ubiquitous electronic devices that have become indispensable in almost everything we do note however that although the majority of algorithms are indeed intended for eventual computer implementation the notion of algorithm does not depend on such an assumption as examples illustrating the notion of the algorithm we consider in this section three methods for solving the same problem computing the greatest common divisor of two integers these examples will help us to illustrate several important points the nonambiguity requirement for each step of an algorithm can not be compromised the range of inputs for which an algorithm works has to be specified carefully the same algorithm can be represented in several different ways there may exist several algorithms for solving the same problem problem algorithm input computer output figure the notion of the algorithm algorithms for the same problem can be based on very different ideas and can solve the problem with dramatically different speeds recall that the greatest common divisor of two nonnegative notbothzero integers m and n denoted gcdm n is defined as the largest integer that divides both m and n evenly ie with a remainder of zero euclid of alexandria third century bc outlined an algorithm for solving this problem in one of the volumes of his elements most famous for its systematic exposition of geometry in modern terms euclids algorithm is based on applying repeatedly the equality gcdm n gcdn m mod n where m mod n is the remainder of the division of m by n until m mod n is equal to since gcdm m why the last value of m is also the greatest common divisor of the initial m and n for example gcd can be computed as follows gcd gcd gcd if you are not impressed by this algorithm try finding the greatest common divisor of larger numbers such as those in problem in this sections exercises here is a more structured description of this algorithm euclids algorithm for computing gcdm n step if n return the value of m as the answer and stop otherwise proceed to step step divide m by n and assign the value of the remainder to r step assign the value of n to m and the value of r to n go to step alternatively we can express the same algorithm in pseudocode algorithm euclidm n computes gcdm n by euclids algorithm input two nonnegative notbothzero integers m and n output greatest common divisor of m and n while n do r m mod n mn nr return m how do we know that euclids algorithm eventually comes to a stop this follows from the observation that the second integer of the pair gets smaller with each iteration and it can not become negative indeed the new value of n on the next iteration is m mod n which is always smaller than n why hence the value of the second integer eventually becomes and the algorithm stops just as with many other problems there are several algorithms for computing the greatest common divisor let us look at the other two methods for this problem the first is simply based on the definition of the greatest common divisor of m and n as the largest integer that divides both numbers evenly obviously such a common divisor can not be greater than the smaller of these numbers which we will denote by t minm n so we can start by checking whether t divides both m and n if it does t is the answer if it does not we simply decrease t by and try again how do we know that the process will eventually stop for example for numbers and the algorithm will try first then and so on until it reaches where it stops consecutive integer checking algorithm for computing gcdm n step assign the value of minm n to t step divide m by t if the remainder of this division is go to step otherwise go to step step divide n by t if the remainder of this division is return the value of t as the answer and stop otherwise proceed to step step decrease the value of t by go to step note that unlike euclids algorithm this algorithm in the form presented does not work correctly when one of its input numbers is zero this example illustrates why it is so important to specify the set of an algorithms inputs explicitly and carefully the third procedure for finding the greatest common divisor should be familiar to you from middle school middleschool procedure for computing gcdm n step find the prime factors of m step find the prime factors of n step identify all the common factors in the two prime expansions found in step and step if p is a common factor occurring pm and pn times in m and n respectively it should be repeated minpm pn times step compute the product of all the common factors and return it as the greatest common divisor of the numbers given thus for the numbers and we get gcd nostalgia for the days when we learned this method should not prevent us from noting that the last procedure is much more complex and slower than euclids algorithm we will discuss methods for finding and comparing running times of algorithms in the next chapter in addition to inferior efficiency the middleschool procedure does not qualify in the form presented as a legitimate algorithm why because the prime factorization steps are not defined unambiguously they require a list of prime numbers and i strongly suspect that your middleschool math teacher did not explain how to obtain such a list this is not a matter of unnecessary nitpicking unless this issue is resolved we can not say write a program implementing this procedure incidentally step is also not defined clearly enough its ambiguity is much easier to rectify than that of the factorization steps however how would you find common elements in two sorted lists so let us introduce a simple algorithm for generating consecutive primes not exceeding any given integer n it was probably invented in ancient greece and is known as the sieve of eratosthenes ca bc the algorithm starts by initializing a list of prime candidates with consecutive integers from to n then on its first iteration the algorithm eliminates from the list all multiples of ie and so on then it moves to the next item on the list which is and eliminates its multiples in this straightforward version there is an overhead because some numbers such as are eliminated more than once no pass for number is needed since itself and all its multiples are also multiples of they were already eliminated on a previous pass the next remaining number on the list which is used on the third pass is the algorithm continues in this fashion until no more numbers can be eliminated from the list the remaining integers of the list are the primes needed as an example consider the application of the algorithm to finding the list of primes not exceeding n for this example no more passes are needed because they would eliminate numbers already eliminated on previous iterations of the algorithm the remaining numbers on the list are the consecutive primes less than or equal to what is the largest number p whose multiples can still remain on the list to make further iterations of the algorithm necessary before we answer this question let us first note that if p is a number whose multiples are being eliminated on the current pass then the first multiple we should consider is p p because all its smaller multiples p p p have been eliminated on earlier passes through the list this observation helps to avoid eliminating the same number more than once obviously p p should not be greater than n and therefore p can not exceed n rounded down denoted n using the socalled floor function we assume in the following pseudocode that there is a function available for computing n alternatively we could check the inequality p p n as the loop continuation condition there algorithm sieven implements the sieve of eratosthenes input a positive integer n output array l of all prime numbers less than or equal to n for p to ndo ap p for p to n do see note before pseudocode if ap p hasnt been eliminated on previous passes j pp while j n do aj mark element as eliminated j j p copy the remaining elements of a to array l of the primes i for p to n do if ap li ap ii return l so now we can incorporate the sieve of eratosthenes into the middleschool procedure to get a legitimate algorithm for computing the greatest common divisor of two positive integers note that special care needs to be exercised if one or both input numbers are equal to because mathematicians do not consider to be a prime number strictly speaking the method does not work for such inputs before we leave this section one more comment is in order the examples considered in this section notwithstanding the majority of algorithms in use today even those that are implemented as computer programs do not deal with mathematical problems look around for algorithms helping us through our daily routines both professional and personal may this ubiquity of algorithms in todays world strengthen your resolve to learn more about these fascinating engines of the information age exercises do some research on alkhorezmi also alkhwarizmi the man from whose name the word algorithm is derived in particular you should learn what the origins of the words algorithm and algebra have in common given that the official purpose of the us patent system is the promotion of the useful arts do you think algorithms are patentable in this country should they be a write down driving directions for going from your school to your home with the precision required from an algorithms description b write down a recipe for cooking your favorite dish with the precision required by an algorithm design an algorithm for computing n for any positive integer n besides assignment and comparison your algorithm may only use the four basic arithmetical operations design an algorithm to find all the common elements in two sorted lists of numbers for example for the lists and the output should be what is the maximum number of comparisons your algorithm makes if the lengths of the two given lists are m and n respectively a find gcd by applying euclids algorithm b estimate how many times faster it will be to find gcd by euclids algorithm compared with the algorithm based on checking consecutive integers from minm n down to gcdm n prove the equality gcdm n gcdn m mod n for every pair of positive integers m and n what does euclids algorithm do for a pair of integers in which the first is smaller than the second what is the maximum number of times this can happen during the algorithms execution on such an input a what is the minimum number of divisions made by euclids algorithm among all inputs m n b what is the maximum number of divisions made by euclids algorithm among all inputs m n a euclids algorithm as presented in euclids treatise uses subtractions rather than integer divisions write pseudocode for this version of euclids algorithm b euclids game see bog starts with two unequal positive integers on the board two players move in turn on each move a player has to write on the board a positive number equal to the difference of two numbers already on the board this number must be new ie different from all the numbers already on the board the player who can not move loses the game should you choose to move first or second in this game the extended euclids algorithm determines not only the greatest common divisor d of two positive integers m and n but also integers not necessarily positive x and y such that mx ny d a look up a description of the extended euclids algorithm see eg knui p and implement it in the language of your choice b modify your program to find integer solutions to the diophantine equation ax by c with any set of integer coefficients a b and c locker doors there are n lockers in a hallway numbered sequentially from to n initially all the locker doors are closed you make n passes by the lockers each time starting with locker on the ith pass i n you toggle the door of every ith locker if the door is closed you open it if it is open you close it after the last pass which locker doors are open and which are closed how many of them are open warshalls and floyds algorithms in this section we look at two wellknown algorithms warshalls algorithm for computing the transitive closure of a directed graph and floyds algorithm for the allpairs shortestpaths problem these algorithms are based on essentially the same idea exploit a relationship between a problem and its simpler rather than smaller version warshall and floyd published their algorithms without mentioning dynamic programming nevertheless the algorithms certainly have a dynamic programming flavor and have come to be considered applications of this technique warshalls algorithm recall that the adjacency matrix a aij of a directed graph is the boolean matrix that has in its ith row and j th column if and only if there is a directed edge from the ith vertex to the j th vertex we may also be interested in a matrix containing the information about the existence of directed paths of arbitrary lengths between vertices of a given graph such a matrix called the transitive closure of the digraph would allow us to determine in constant time whether the j th vertex is reachable from the ith vertex here are a few application examples when a value in a spreadsheet cell is changed the spreadsheet software must know all the other cells affected by the change if the spreadsheet is modeled by a digraph whose vertices represent the spreadsheet cells and edges indicate cell dependencies the transitive closure will provide such information in software engineering transitive closure can be used for investigating data flow and control flow dependencies as well as for inheritance testing of objectoriented software in electronic engineering it is used for redundancy identification and test generation for digital circuits definition the transitive closure of a directed graph with n vertices can be defined as the n n boolean matrix t tij in which the element in the ith row and the j th column is if there exists a nontrivial path ie directed path of a positive length from the ith vertex to the j th vertex otherwise tij is an example of a digraph its adjacency matrix and its transitive closure is given in figure we can generate the transitive closure of a digraph with the help of depthfirst search or breadthfirst search performing either traversal starting at the ith a b a b c d a b c d a a a b t b c c c d d d a b c figure a digraph b its adjacency matrix c its transitive closure vertex gives the information about the vertices reachable from it and hence the columns that contain s in the ith row of the transitive closure thus doing such a traversal for every vertex as a starting point yields the transitive closure in its entirety since this method traverses the same digraph several times we should hope that a better algorithm can be found indeed such an algorithm exists it is called warshalls algorithm after stephen warshall who discovered it war it is convenient to assume that the digraphs vertices and hence the rows and columns of the adjacency matrix are numbered from to n warshalls algorithm constructs the transitive closure through a series of n n boolean matrices r rk rk rn each of these matrices provides certain information about directed paths in the digraph specifically the element rijk in the ith row and j th column of matrix rk i j n k n is equal to if and only if there exists a directed path of a positive length from the ith vertex to the j th vertex with each intermediate vertex if any numbered not higher than k thus the series starts with r which does not allow any intermediate vertices in its paths hence r is nothing other than the adjacency matrix of the digraph recall that the adjacency matrix contains the information about oneedge paths ie paths with no intermediate vertices r contains the information about paths that can use the first vertex as intermediate thus with more freedom so to speak it may contain more s than r in general each subsequent matrix in series has one more vertex to use as intermediate for its paths than its predecessor and hence may but does not have to contain more s the last matrix in the series rn reflects paths that can use all n vertices of the digraph as intermediate and hence is nothing other than the digraphs transitive closure the central point of the algorithm is that we can compute all the elements of each matrix rk from its immediate predecessor rk in series let rijk the element in the ith row and j th column of matrix rk be equal to this means that there exists a path from the ith vertex vi to the j th vertex vj with each intermediate vertex numbered not higher than k vi a list of intermediate vertices each numbered not higher than k vj j k j k r k k r k k i i figure rule for changing zeros in warshalls algorithm two situations regarding this path are possible in the first the list of its intermediate vertices does not contain the kth vertex then this path from vi to vj has intermediate vertices numbered not higher than k and therefore rijk is equal to as well the second possibility is that path does contain the kth vertex vk among the intermediate vertices without loss of generality we may assume that vk occurs only once in that list if it is not the case we can create a new path from vi to vj with this property by simply eliminating all the vertices between the first and last occurrences of vk in it with this caveat path can be rewritten as follows vi vertices numbered k vk vertices numbered k vj the first part of this representation means that there exists a path from vi to vk with each intermediate vertex numbered not higher than k hence rikk and the second part means that there exists a path from vk to vj with each intermediate vertex numbered not higher than k hence rkjk what we have just proved is that if rijk then either rijk or both rikk and rkjk it is easy to see that the converse of this assertion is also true thus we have the following formula for generating the elements of matrix rk from the elements of matrix rk rijk rijk or rikk and rkjk formula is at the heart of warshalls algorithm this formula implies the following rule for generating elements of matrix rk from elements of matrix rk which is particularly convenient for applying warshalls algorithm by hand if an element rij is in rk it remains in rk if an element rij is in rk it has to be changed to in rk if and only if the element in its row i and column k and the element in its column j and row k are both s in rk this rule is illustrated in figure as an example the application of warshalls algorithm to the digraph in figure is shown in figure a b c d s reflect the existence of paths a b a with no intermediate vertices r b r is just the adjacency matrix c boxed row and column are used for getting r c d d a b c d s reflect the existence of paths a with intermediate vertices numbered r b not higher than ie just vertex a c note a new path from d to b d boxed row and column are used for getting r a b c d s reflect the existence of paths a with intermediate vertices numbered r b not higher than ie a and b c note two new paths d boxed row and column are used for getting r a b c d s reflect the existence of paths a with intermediate vertices numbered r b not higher than ie a b and c c no new paths d boxed row and column are used for getting r a b c d a s reflect the existence of paths r b with intermediate vertices numbered c not higher than ie a b c and d d note five new paths figure application of warshalls algorithm to the digraph shown new s are in bold here is pseudocode of warshalls algorithm algorithm warshallan n implements warshalls algorithm for computing the transitive closure input the adjacency matrix a of a digraph with n vertices output the transitive closure of the digraph r a for k to n do for i to n do for j to n do rki j rki j or rki k and rkk j return rn several observations need to be made about warshalls algorithm first it is remarkably succinct is it not still its time efficiency is only n in fact for sparse graphs represented by their adjacency lists the traversalbased algorithm a b a b c d a b c d a a w b d b c c c d d d a b c figure a digraph b its weight matrix c its distance matrix mentioned at the beginning of this section has a better asymptotic efficiency than warshalls algorithm why we can speed up the above implementation of warshalls algorithm for some inputs by restructuring its innermost loop see problem in this sections exercises another way to make the algorithm run faster is to treat matrix rows as bit strings and employ the bitwise or operation available in most modern computer languages as to the space efficiency of warshalls algorithm the situation is similar to that of computing a fibonacci number and some other dynamic programming algorithms although we used separate matrices for recording intermediate results of the algorithm this is in fact unnecessary problem in this sections exercises asks you to find a way of avoiding this wasteful use of the computer memory finally we shall see below how the underlying idea of warshalls algorithm can be applied to the more general problem of finding lengths of shortest paths in weighted graphs floyds algorithm for the allpairs shortestpaths problem given a weighted connected graph undirected or directed the allpairs shortestpaths problem asks to find the distances ie the lengths of the shortest paths from each vertex to all other vertices this is one of several variations of the problem involving shortest paths in graphs because of its important applications to communications transportation networks and operations research it has been thoroughly studied over the years among recent applications of the allpairs shortestpath problem is precomputing distances for motion planning in computer games it is convenient to record the lengths of shortest paths in an n n matrix d called the distance matrix the element dij in the ith row and the j th column of this matrix indicates the length of the shortest path from the ith vertex to the j th vertex for an example see figure we can generate the distance matrix with an algorithm that is very similar to warshalls algorithm it is called floyds algorithm after its coinventor robert w floyd it is applicable to both undirected and directed weighted graphs provided floyd explicitly referenced warshalls paper in presenting his algorithm flo three years earlier bernard roy published essentially the same algorithm in the proceedings of the french academy of sciences roy that they do not contain a cycle of a negative length the distance between any two vertices in such a cycle can be made arbitrarily small by repeating the cycle enough times the algorithm can be enhanced to find not only the lengths of the shortest paths for all vertex pairs but also the shortest paths themselves problem in this sections exercises floyds algorithm computes the distance matrix of a weighted graph with n vertices through a series of n n matrices d dk dk dn each of these matrices contains the lengths of shortest paths with certain constraints on the paths considered for the matrix in question specifically the element dijk in the ith row and the j th column of matrix dk i j n k n is equal to the length of the shortest path among all paths from the ith vertex to the j th vertex with each intermediate vertex if any numbered not higher than k in particular the series starts with d which does not allow any intermediate vertices in its paths hence d is simply the weight matrix of the graph the last matrix in the series dn contains the lengths of the shortest paths among all paths that can use all n vertices as intermediate and hence is nothing other than the distance matrix being sought as in warshalls algorithm we can compute all the elements of each matrix dk from its immediate predecessor dk in series let dijk be the element in the ith row and the j th column of matrix dk this means that dijk is equal to the length of the shortest path among all paths from the ith vertex vi to the j th vertex vj with their intermediate vertices numbered not higher than k vi a list of intermediate vertices each numbered not higher than k vj we can partition all such paths into two disjoint subsets those that do not use the kth vertex vk as intermediate and those that do since the paths of the first subset have their intermediate vertices numbered not higher than k the shortest of them is by definition of our matrices of length dijk what is the length of the shortest path in the second subset if the graph does not contain a cycle of a negative length we can limit our attention only to the paths in the second subset that use vertex vk as their intermediate vertex exactly once because visiting vk more than once can only increase the paths length all such paths have the following form vi vertices numbered k vk vertices numbered k vj in other words each of the paths is made up of a path from vi to vk with each intermediate vertex numbered not higher than k and a path from vk to vj with each intermediate vertex numbered not higher than k the situation is depicted symbolically in figure since the length of the shortest path from vi to vk among the paths that use intermediate vertices numbered not higher than k is equal to dikk and the length of the shortest path from vk to vj among the paths that use intermediate d k ij vi vj dikk dkkj vk figure underlying idea of floyds algorithm vertices numbered not higher than k is equal to dkjk the length of the shortest path among the paths that use the kth vertex is equal to dikk dkjk taking into account the lengths of the shortest paths in both subsets leads to the following recurrence dijk mindijk dikk dkjk for k dij wij to put it another way the element in row i and column j of the current distance matrix dk is replaced by the sum of the elements in the same row i and the column k and in the same column j and the row k if and only if the latter sum is smaller than its current value the application of floyds algorithm to the graph in figure is illustrated in figure here is pseudocode of floyds algorithm it takes advantage of the fact that the next matrix in sequence can be written over its predecessor algorithm floydw n n implements floyds algorithm for the allpairs shortestpaths problem input the weight matrix w of a graph with no negativelength cycle output the distance matrix of the shortest paths lengths dw is not necessary if w can be overwritten for k to n do for i to n do for j to n do di j mindi j di k dk j return d obviously the time efficiency of floyds algorithm is cubic as is the time efficiency of warshalls algorithm in the next chapter we examine dijkstras algorithm another method for finding shortest paths a b a b c d a lengths of the shortest paths d b with no intermediate vertices c d is simply the weight matrix c d d a b c d a lengths of the shortest paths b with intermediate vertices numbered d c not higher than ie just a d note two new shortest paths from b to c and from d to c a b c d a lengths of the shortest paths d b with intermediate vertices numbered c not higher than ie a and b d note a new shortest path from c to a a b c d a lengths of the shortest paths b with intermediate vertices numbered d c not higher than ie a b and c d note four new shortest paths from a to b from a to d from b to d and from d to b a b c d a lengths of the shortest paths b with intermediate vertices numbered d c not higher than ie a b c and d d note a new shortest path from c to a figure application of floyds algorithm to the digraph shown updated elements are shown in bold exercises apply warshalls algorithm to find the transitive closure of the digraph defined by the following adjacency matrix a prove that the time efficiency of warshalls algorithm is cubic b explain why the time efficiency class of warshalls algorithm is inferior to that of the traversalbased algorithm for sparse graphs represented by their adjacency lists explain how to implement warshalls algorithm without using extra memory for storing elements of the algorithms intermediate matrices explain how to restructure the innermost loop of the algorithm warshall to make it run faster at least on some inputs rewrite pseudocode of warshalls algorithm assuming that the matrix rows are represented by bit strings on which the bitwise or operation can be performed a explain how warshalls algorithm can be used to determine whether a given digraph is a dag directed acyclic graph is it a good algorithm for this problem b is it a good idea to apply warshalls algorithm to find the transitive closure of an undirected graph solve the allpairs shortestpath problem for the digraph with the following weight matrix prove that the next matrix in sequence of floyds algorithm can be written over its predecessor give an example of a graph or a digraph with negative weights for which floyds algorithm does not yield the correct result enhance floyds algorithm so that shortest paths themselves not just their lengths can be found jack straws in the game of jack straws a number of plastic or wooden straws are dumped on the table and players try to remove them one by one without disturbing the other straws here we are only concerned with whether various pairs of straws are connected by a path of touching straws given a list of the endpoints for n straws as if they were dumped on a large piece of graph paper determine all the pairs of straws that are connected note that touching is connecting but also that two straws can be connected indirectly via other connected straws eastcentral regionals of the acm international collegiate programming contest summary dynamic programming is a technique for solving problems with overlapping subproblems typically these subproblems arise from a recurrence relating a solution to a given problem with solutions to its smaller subproblems of the same type dynamic programming suggests solving each smaller subproblem once and recording the results in a table from which a solution to the original problem can be then obtained applicability of dynamic programming to an optimization problem requires the problem to satisfy the principle of optimality an optimal solution to any of its instances must be made up of optimal solutions to its subinstances among many other problems the changemaking problem with arbitrary coin denominations can be solved by dynamic programming solving a knapsack problem by a dynamic programming algorithm exemplifies an application of this technique to difficult problems of combinatorial optimization the memory function technique seeks to combine the strengths of the topdown and bottomup approaches to solving problems with overlapping subproblems it does this by solving in the topdown fashion but only once just the necessary subproblems of a given problem and recording their solutions in a table dynamic programming can be used for constructing an optimal binary search tree for a given set of keys and known probabilities of searching for them warshalls algorithm for finding the transitive closure and floyds algorithm for the allpairs shortestpaths problem are based on the idea that can be interpreted as an application of the dynamic programming technique greedy technique greed for lack of a better word is good greed is right greed works michael douglas us actor in the role of gordon gecko in the film wall street let us revisit the changemaking problem faced at least subconsciously by millions of cashiers all over the world give change for a specific amount n with the least number of coins of the denominations d d dm used in that locale here unlike section we assume that the denominations are ordered in decreasing order for example the widely used coin denominations in the united states are d quarter d dime d nickel and d penny how would you give change with coins of these denominations of say cents if you came up with the answer quarter dimes and pennies you followed consciously or not a logical strategy of making a sequence of best choices among the currently available alternatives indeed in the first step you could have given one coin of any of the four denominations greedy thinking leads to giving one quarter because it reduces the remaining amount the most namely to cents in the second step you had the same coins at your disposal but you could not give a quarter because it would have violated the problems constraints so your best selection in this step was one dime reducing the remaining amount to cents giving one more dime left you with cents to be given with three pennies is this solution to the instance of the changemaking problem optimal yes it is in fact one can prove that the greedy algorithm yields an optimal solution for every positive integer amount with these coin denominations at the same time it is easy to give an example of coin denominations that do not yield an optimal solution for some amounts eg d d d and n the approach applied in the opening paragraph to the changemaking problem is called greedy computer scientists consider it a general design technique despite the fact that it is applicable to optimization problems only the greedy approach suggests constructing a solution through a sequence of steps each expanding a partially constructed solution obtained so far until a complete solution to the problem is reached on each step and this is the central point of this technique the choice made must be feasible ie it has to satisfy the problems constraints locally optimal ie it has to be the best local choice among all feasible choices available on that step irrevocable ie once made it can not be changed on subsequent steps of the algorithm these requirements explain the techniques name on each step it suggests a greedy grab of the best alternative available in the hope that a sequence of locally optimal choices will yield a globally optimal solution to the entire problem we refrain from a philosophical discussion of whether greed is good or bad if you have not seen the movie from which the chapters epigraph is taken its hero did not end up well from our algorithmic perspective the question is whether such a greedy strategy works or not as we shall see there are problems for which a sequence of locally optimal choices does yield an optimal solution for every instance of the problem in question however there are others for which this is not the case for such problems a greedy algorithm can still be of value if we are interested in or have to be satisfied with an approximate solution in the first two sections of the chapter we discuss two classic algorithms for the minimum spanning tree problem prims algorithm and kruskals algorithm what is remarkable about these algorithms is the fact that they solve the same problem by applying the greedy approach in two different ways and both of them always yield an optimal solution in section we introduce another classic algorithm dijkstras algorithm for the shortestpath problem in a weighted graph section is devoted to huffman trees and their principal application huffman codes an important data compression method that can be interpreted as an application of the greedy technique finally a few examples of approximation algorithms based on the greedy approach are discussed in section as a rule greedy algorithms are both intuitively appealing and simple given an optimization problem it is usually easy to figure out how to proceed in a greedy manner possibly after considering a few small instances of the problem what is usually more difficult is to prove that a greedy algorithm yields an optimal solution when it does one of the common ways to do this is illustrated by the proof given in section using mathematical induction we show that a partially constructed solution obtained by the greedy algorithm on each iteration can be extended to an optimal solution to the problem the second way to prove optimality of a greedy algorithm is to show that on each step it does at least as well as any other algorithm could in advancing toward the problems goal consider as an example the following problem find the minimum number of moves needed for a chess knight to go from one corner of a board to the diagonally opposite corner the knights moves are lshaped jumps two squares horizontally or vertically followed by one square in the perpendicular direction a greedy solution is clear here jump as close to the goal as possible on each move thus if its start and finish squares are and respectively a sequence of moves such as solves the problem the number k of twomove advances can be obtained from the equation k why is this a minimummove solution because if we measure the distance to the goal by the manhattan distance which is the sum of the difference between the row numbers and the difference between the column numbers of two squares in question the greedy algorithm decreases it by on each move the best the knight can do the third way is simply to show that the final result obtained by a greedy algorithm is optimal based on the algorithms output rather than the way it operates as an example consider the problem of placing the maximum number of chips on an board so that no two chips are placed on the same or adjacent vertically horizontally or diagonally squares to follow the prescription of the greedy strategy we should place each new chip so as to leave as many available squares as possible for next chips for example starting with the upper left corner of the board we will be able to place chips as shown in figure a why is this solution optimal to see why partition the board into sixteen squares as shown in figure b obviously it is impossible to place more than one chip in each of these squares which implies that the total number of nonadjacent chips on the board can not exceed as a final comment we should mention that a rather sophisticated theory has been developed behind the greedy technique which is based on the abstract combinatorial structure called matroid an interested reader can check such books as cor as well as a variety of internet resources on the subject figure a placement of chips on nonadjacent squares b partition of the board proving impossibility of placing more than chips a b a b a b a b c d c d c d c d graph wt wt wt figure graph and its spanning trees with t being the minimum spanning tree prims algorithm the following problem arises naturally in many practical situations given n points connect them in the cheapest possible way so that there will be a path between every pair of points it has direct applications to the design of all kinds of networks including communication computer transportation and electrical by providing the cheapest way to achieve connectivity it identifies clusters of points in data sets it has been used for classification purposes in archeology biology sociology and other sciences it is also helpful for constructing approximate solutions to more difficult problems such the traveling salesman problem see section we can represent the points given by vertices of a graph possible connections by the graphs edges and the connection costs by the edge weights then the question can be posed as the minimum spanning tree problem defined formally as follows definition a spanning tree of an undirected connected graph is its connected acyclic subgraph ie a tree that contains all the vertices of the graph if such a graph has weights assigned to its edges a minimum spanning tree is its spanning tree of the smallest weight where the weight of a tree is defined as the sum of the weights on all its edges the minimum spanning tree problem is the problem of finding a minimum spanning tree for a given weighted connected graph figure presents a simple example illustrating these notions if we were to try constructing a minimum spanning tree by exhaustive search we would face two serious obstacles first the number of spanning trees grows exponentially with the graph size at least for dense graphs second generating all spanning trees for a given graph is not easy in fact it is more difficult than finding a minimum spanning tree for a weighted graph by using one of several efficient algorithms available for this problem in this section we outline prims algorithm which goes back to at least pri robert prim rediscovered the algorithm published years earlier by the czech mathematician vojte ch jarnik in a czech journal prims algorithm constructs a minimum spanning tree through a sequence of expanding subtrees the initial subtree in such a sequence consists of a single vertex selected arbitrarily from the set v of the graphs vertices on each iteration the algorithm expands the current tree in the greedy manner by simply attaching to it the nearest vertex not in that tree by the nearest vertex we mean a vertex not in the tree connected to a vertex in the tree by an edge of the smallest weight ties can be broken arbitrarily the algorithm stops after all the graphs vertices have been included in the tree being constructed since the algorithm expands a tree by exactly one vertex on each of its iterations the total number of such iterations is n where n is the number of vertices in the graph the tree generated by the algorithm is obtained as the set of edges used for the tree expansions here is pseudocode of this algorithm algorithm primg prims algorithm for constructing a minimum spanning tree input a weighted connected graph g v e output et the set of edges composing a minimum spanning tree of g vt v the set of tree vertices can be initialized with any vertex et for i to v do find a minimumweight edge e v u among all the edges v u such that v is in vt and u is in v vt vt vt u et et e return et the nature of prims algorithm makes it necessary to provide each vertex not in the current tree with the information about the shortest edge connecting the vertex to a tree vertex we can provide such information by attaching two labels to a vertex the name of the nearest tree vertex and the length the weight of the corresponding edge vertices that are not adjacent to any of the tree vertices can be given the label indicating their infinite distance to the tree vertices and a null label for the name of the nearest tree vertex alternatively we can split the vertices that are not in the tree into two sets the fringe and the unseen the fringe contains only the vertices that are not in the tree but are adjacent to at least one tree vertex these are the candidates from which the next tree vertex is selected the unseen vertices are all the other vertices of the graph called unseen because they are yet to be affected by the algorithm with such labels finding the next vertex to be added to the current tree t vt et becomes a simple task of finding a vertex with the smallest distance label in the set v vt ties can be broken arbitrarily after we have identified a vertex u to be added to the tree we need to perform two operations move u from the set v vt to the set of tree vertices vt for each remaining vertex u in v vt that is connected to u by a shorter edge than the us current distance label update its labels by u and the weight of the edge between u and u respectively figure demonstrates the application of prims algorithm to a specific graph does prims algorithm always yield a minimum spanning tree the answer to this question is yes let us prove by induction that each of the subtrees ti i n generated by prims algorithm is a part ie a subgraph of some minimum spanning tree this immediately implies of course that the last tree in the sequence tn is a minimum spanning tree itself because it contains all n vertices of the graph the basis of the induction is trivial since t consists of a single vertex and hence must be a part of any minimum spanning tree for the inductive step let us assume that ti is part of some minimum spanning tree t we need to prove that ti generated from ti by prims algorithm is also a part of a minimum spanning tree we prove this by contradiction by assuming that no minimum spanning tree of the graph can contain ti let ei v u be the minimum weight edge from a vertex in ti to a vertex not in ti used by prims algorithm to expand ti to ti by our assumption ei can not belong to any minimum spanning tree including t therefore if we add ei to t a cycle must be formed figure in addition to edge ei v u this cycle must contain another edge v u connecting a vertex v ti to a vertex u that is not in ti it is possible that v coincides with v or u coincides with u but not both if we now delete the edge v u from this cycle we will obtain another spanning tree of the entire graph whose weight is less than or equal to the weight of t since the weight of ei is less than or equal to the weight of v u hence this spanning tree is a minimum spanning tree which contradicts the assumption that no minimum spanning tree contains ti this completes the correctness proof of prims algorithm how efficient is prims algorithm the answer depends on the data structures chosen for the graph itself and for the priority queue of the set v vt whose vertex priorities are the distances to the nearest tree vertices you may want to take another look at the example in figure to see that the set v vt indeed operates as a priority queue in particular if a graph is represented by its weight matrix and the priority queue is implemented as an unordered array the algorithms running time will be in v indeed on each of the v iterations the array implementing the priority queue is traversed to find and delete the minimum and then to update if necessary the priorities of the remaining vertices we can also implement the priority queue as a minheap a minheap is a mirror image of the heap structure discussed in section in fact it can be implemented by constructing a heap after negating all the key values given namely a minheap is a complete binary tree in which every element is less than or equal if the implementation with the fringeunseen split is pursued all the unseen vertices adjacent to u must also be moved to the fringe a f d e tree vertices remaining vertices illustration a ba c d b c ea fa a f d e ba cb d ea b c fb a f d e cb dc ea fb b c a f d e fb df ef b c a f d e ef df b c a f d e df figure application of prims algorithm the parenthesized labels of a vertex in the middle column indicate the nearest tree vertex and edge weight selected vertices and edges are shown in bold v u v ei u ti figure correctness proof of prims algorithm to its children all the principal properties of heaps remain valid for minheaps with some obvious modifications for example the root of a minheap contains the smallest rather than the largest element deletion of the smallest element from and insertion of a new element into a minheap of size n are olog n operations and so is the operation of changing an elements priority see problem in this sections exercises if a graph is represented by its adjacency lists and the priority queue is implemented as a minheap the running time of the algorithm is in oe log v this is because the algorithm performs v deletions of the smallest element and makes e verifications and possibly changes of an elements priority in a minheap of size not exceeding v each of these operations as noted earlier is a olog v operation hence the running time of this implementation of prims algorithm is in v eolog v oe log v because in a connected graph v e in the next section you will find another greedy algorithm for the minimum spanning tree problem which is greedy in a manner different from that of prims algorithm exercises write pseudocode of the greedy algorithm for the changemaking problem with an amount n and coin denominations d d dm as its input what is the time efficiency class of your algorithm design a greedy algorithm for the assignment problem see section does your greedy algorithm always yield an optimal solution job scheduling consider the problem of scheduling n jobs of known durations t t tn for execution by a single processor the jobs can be executed in any order one job at a time you want to find a schedule that minimizes the total time spent by all the jobs in the system the time spent by one job in the system is the sum of the time spent by this job in waiting plus the time spent on its execution design a greedy algorithm for this problem does the greedy algorithm always yield an optimal solution compatible intervals given n open intervals a b a b an bn on the real line each representing start and end times of some activity requiring the same resource the task is to find the largest number of these intervals so that no two of them overlap investigate the three greedy algorithms based on a earliest start first b shortest duration first c earliest finish first for each of the three algorithms either prove that the algorithm always yields an optimal solution or give a counterexample showing this not to be the case bridge crossing revisited consider the generalization of the bridge crossing puzzle problem in exercises in which we have n people whose bridge crossing times are t t tn all the other conditions of the problem remain the same at most two people at a time can cross the bridge and they move with the speed of the slower of the two and they must carry with them the only flashlight the group has design a greedy algorithm for this problem and find how long it will take to cross the bridge by using this algorithm does your algorithm yield a minimum crossing time for every instance of the problem if it does prove it if it does not find an instance with the smallest number of people for which this happens averaging down there are n identical vessels one of them with w pints of water and the others empty you are allowed to perform the following operation take two of the vessels and split the total amount of water in them equally between them the object is to achieve a minimum amount of water in the vessel containing all the water in the initial set up by a sequence of such operations what is the best way to do this rumor spreading there are n people each in possession of a different rumor they want to share all the rumors with each other by sending electronic messages assume that a sender includes all the rumors he or she knows at the time the message is sent and that a message may only have one addressee design a greedy algorithm that always yields the minimum number of messages they need to send to guarantee that every one of them gets all the rumors bachets problem of weights find an optimal set of n weights w w wn so that it would be possible to weigh on a balance scale any integer load in the largest possible range from to w provided a weights can be put only on the free cup of the scale b weights can be put on both cups of the scale a apply prims algorithm to the following graph include in the priority queue all the vertices not already in the tree a b e c d b apply prims algorithm to the following graph include in the priority queue only the fringe vertices the vertices not in the current tree which are adjacent to at least one tree vertex a b c d e f g h i j k l the notion of a minimum spanning tree is applicable to a connected weighted graph do we have to check a graphs connectivity before applying prims algorithm or can the algorithm do it by itself does prims algorithm always work correctly on graphs with negative edge weights let t be a minimum spanning tree of graph g obtained by prims algorithm let gnew be a graph obtained by adding to g a new vertex and some edges with weights connecting the new vertex to some vertices in g can we construct a minimum spanning tree of gnew by adding one of the new edges to t if you answer yes explain how if you answer no explain why not how can one use prims algorithm to find a spanning tree of a connected graph with no weights on its edges is it a good algorithm for this problem prove that any weighted connected graph with distinct weights has exactly one minimum spanning tree outline an efficient algorithm for changing an elements value in a minheap what is the time efficiency of your algorithm kruskals algorithm in the previous section we considered the greedy algorithm that grows a minimum spanning tree through a greedy inclusion of the nearest vertex to the vertices already in the tree remarkably there is another greedy algorithm for the minimum spanning tree problem that also always yields an optimal solution it is named kruskals algorithm after joseph kruskal who discovered this algorithm when he was a secondyear graduate student kru kruskals algorithm looks at a minimum spanning tree of a weighted connected graph g v e as an acyclic subgraph with v edges for which the sum of the edge weights is the smallest it is not difficult to prove that such a subgraph must be a tree consequently the algorithm constructs a minimum spanning tree as an expanding sequence of subgraphs that are always acyclic but are not necessarily connected on the intermediate stages of the algorithm the algorithm begins by sorting the graphs edges in nondecreasing order of their weights then starting with the empty subgraph it scans this sorted list adding the next edge on the list to the current subgraph if such an inclusion does not create a cycle and simply skipping the edge otherwise algorithm kruskalg kruskals algorithm for constructing a minimum spanning tree input a weighted connected graph g v e output et the set of edges composing a minimum spanning tree of g sort e in nondecreasing order of the edge weights wei weie et ecounter initialize the set of tree edges and its size k initialize the number of processed edges while ecounter v do kk if et eik is acyclic et et eik ecounter ecounter return et the correctness of kruskals algorithm can be proved by repeating the essential steps of the proof of prims algorithm given in the previous section the fact that et is actually a tree in prims algorithm but generally just an acyclic subgraph in kruskals algorithm turns out to be an obstacle that can be overcome figure demonstrates the application of kruskals algorithm to the same graph we used for illustrating prims algorithm in section as you trace the algorithms operations note the disconnectedness of some of the intermediate subgraphs applying prims and kruskals algorithms to the same small graph by hand may create the impression that the latter is simpler than the former this impression is wrong because on each of its iterations kruskals algorithm has to check whether the addition of the next edge to the edges already selected would create a a f d e tree edges sorted list of edges illustration bc ef ab bf cf af df ae cd de b c a f d e bc bc ef ab bf cf af df ae cd de b c a f d e ef bc ef ab bf cf af df ae cd de b c a f d e ab bc ef ab bf cf af df ae cd de b c a f d e bf bc ef ab bf cf af df ae cd de b c a f d e df figure application of kruskals algorithm selected edges are shown in bold v v u u a b figure new edge connecting two vertices may a or may not b create a cycle cycle it is not difficult to see that a new cycle is created if and only if the new edge connects two vertices already connected by a path ie if and only if the two vertices belong to the same connected component figure note also that each connected component of a subgraph generated by kruskals algorithm is a tree because it has no cycles in view of these observations it is convenient to use a slightly different interpretation of kruskals algorithm we can consider the algorithms operations as a progression through a series of forests containing all the vertices of a given graph and some of its edges the initial forest consists of v trivial trees each comprising a single vertex of the graph the final forest consists of a single tree which is a minimum spanning tree of the graph on each iteration the algorithm takes the next edge u v from the sorted list of the graphs edges finds the trees containing the vertices u and v and if these trees are not the same unites them in a larger tree by adding the edge u v fortunately there are efficient algorithms for doing so including the crucial check for whether two vertices belong to the same tree they are called unionfind algorithms we discuss them in the following subsection with an efficient unionfind algorithm the running time of kruskals algorithm will be dominated by the time needed for sorting the edge weights of a given graph hence with an efficient sorting algorithm the time efficiency of kruskals algorithm will be in oe log e disjoint subsets and unionfind algorithms kruskals algorithm is one of a number of applications that require a dynamic partition of some n element set s into a collection of disjoint subsets s s sk after being initialized as a collection of n oneelement subsets each containing a different element of s the collection is subjected to a sequence of intermixed union and find operations note that the number of union operations in any such sequence must be bounded above by n because each union increases a subsets size at least by and there are only n elements in the entire set s thus we are dealing here with an abstract data type of a collection of disjoint subsets of a finite set with the following operations makesetx creates a oneelement set x it is assumed that this operation can be applied to each of the elements of set s only once findx returns a subset containing x unionx y constructs the union of the disjoint subsets sx and sy containing x and y respectively and adds it to the collection to replace sx and sy which are deleted from it for example let s then makeseti creates the set i and applying this operation six times initializes the structure to the collection of six singleton sets performing union and union yields and if followed by union and then by union we end up with the disjoint subsets most implementations of this abstract data type use one element from each of the disjoint subsets in a collection as that subsets representative some implementations do not impose any specific constraints on such a representative others do so by requiring say the smallest element of each subset to be used as the subsets representative also it is usually assumed that set elements are or can be mapped into integers there are two principal alternatives for implementing this data structure the first one called the quick find optimizes the time efficiency of the find operation the second one called the quick union optimizes the union operation the quick find uses an array indexed by the elements of the underlying set s the arrays values indicate the representatives of the subsets containing those elements each subset is implemented as a linked list whose header contains the pointers to the first and last elements of the list along with the number of elements in the list see figure for an example under this scheme the implementation of makesetx requires assigning the corresponding element in the representative array to x and initializing the corresponding linked list to a single node with the x value the time efficiency of this operation is obviously in and hence the initialization of n singleton subsets is in n the efficiency of findx is also in all we need to do is to retrieve the xs representative in the representative array executing unionx y takes longer a straightforward solution would simply append the ys list to the end of the xs list update the information about their representative for all the elements in the subset representatives element index representative size last first list null list null null list null list null null list null null list null null figure linkedlist representation of subsets and obtained by quick find after performing union union union and union the lists of size are considered deleted from the collection y list and then delete the ys list from the collection it is easy to verify however that with this algorithm the sequence of union operations union union unioni i unionn n runs in n time which is slow compared with several known alternatives a simple way to improve the overall efficiency of a sequence of union operations is to always append the shorter of the two lists to the longer one with ties broken arbitrarily of course the size of each list is assumed to be available by say storing the number of elements in the lists header this modification is called the a b figure a forest representation of subsets and used by quick union b result of union union by size though it does not improve the worstcase efficiency of a single application of the union operation it is still in n the worstcase running time of any legitimate sequence of unionbysize operations turns out to be in on log n here is a proof of this assertion let ai be an element of set s whose disjoint subsets we manipulate and let ai be the number of times ais representative is updated in a sequence of unionbysize operations how large can ai get if set s has n elements each time ais representative is updated ai must be in a smaller subset involved in computing the union whose size will be at least twice as large as the size of the subset containing ai hence when ais representative is updated for the first time the resulting set will have at least two elements when it is updated for the second time the resulting set will have at least four elements and in general if it is updated ai times the resulting set will have at least ai elements since the entire set s has n elements ai n and hence ai log n therefore the total number of possible updates of the representatives for all n elements in s will not exceed n log n thus for union by size the time efficiency of a sequence of at most n unions and m finds is in on log n m the quick union the second principal alternative for implementing disjoint subsets represents each subset by a rooted tree the nodes of the tree contain the subsets elements one per node with the roots element considered the subsets representative the trees edges are directed from children to their parents figure in addition a mapping of the set elements to their tree nodes implemented say as an array of pointers is maintained this mapping is not shown in figure for the sake of simplicity for this implementation makesetx requires the creation of a singlenode tree which is a operation hence the initialization of n singleton subsets is in n a unionx y is implemented by attaching the root of the ys tree to the root of the xs tree and deleting the ys tree from the collection by making the pointer to its root null the time efficiency of this operation is clearly a findx is this is a specific example of the usefulness of the amortized efficiency we mentioned back in chapter x t t t x t t t t t figure path compression performed by following the pointer chain from the node containing x to the trees root whose element is returned as the subsets representative accordingly the time efficiency of a single find operation is in on because a tree representing a subset can degenerate into a linked list with n nodes this time bound can be improved the straightforward way for doing so is to always perform a union operation by attaching a smaller tree to the root of a larger one with ties broken arbitrarily the size of a tree can be measured either by the number of nodes this version is called union by size or by its height this version is called union by rank of course these options require storing for each node of the tree either the number of node descendants or the height of the subtree rooted at that node respectively one can easily prove that in either case the height of the tree will be logarithmic making it possible to execute each find in olog n time thus for quick union the time efficiency of a sequence of at most n unions and m finds is in on m log n in fact an even better efficiency can be obtained by combining either variety of quick union with path compression this modification makes every node encountered during the execution of a find operation point to the trees root figure according to a quite sophisticated analysis that goes beyond the level of this book see tar this and similar techniques improve the efficiency of a sequence of at most n unions and m finds to only slightly worse than linear exercises apply kruskals algorithm to find a minimum spanning tree of the following graphs a b c a d e b a b c d e f g h i j k l indicate whether the following statements are true or false a if e is a minimumweight edge in a connected weighted graph it must be among edges of at least one minimum spanning tree of the graph b if e is a minimumweight edge in a connected weighted graph it must be among edges of each minimum spanning tree of the graph c if edge weights of a connected weighted graph are all distinct the graph must have exactly one minimum spanning tree d if edge weights of a connected weighted graph are not all distinct the graph must have more than one minimum spanning tree what changes if any need to be made in algorithm kruskal to make it find a minimum spanning forest for an arbitrary graph a minimum spanning forest is a forest whose trees are minimum spanning trees of the graphs connected components does kruskals algorithm work correctly on graphs that have negative edge weights design an algorithm for finding a maximum spanning tree a spanning tree with the largest possible edge weight of a weighted connected graph rewrite pseudocode of kruskals algorithm in terms of the operations of the disjoint subsets adt prove the correctness of kruskals algorithm prove that the time efficiency of findx is in olog n for the unionbysize version of quick union find at least two web sites with animations of kruskals and prims algorithms discuss their merits and demerits design and conduct an experiment to empirically compare the efficiencies of prims and kruskals algorithms on random graphs of different sizes and densities steiner tree four villages are located at the vertices of a unit square in the euclidean plane you are asked to connect them by the shortest network of roads so that there is a path between every pair of the villages along those roads find such a network write a program generating a random maze based on a prims algorithm b kruskals algorithm dijkstras algorithm in this section we consider the singlesource shortestpaths problem for a given vertex called the source in a weighted connected graph find shortest paths to all its other vertices it is important to stress that we are not interested here in a single shortest path that starts at the source and visits all the other vertices this would have been a much more difficult problem actually a version of the traveling salesman problem introduced in section and discussed again later in the book the singlesource shortestpaths problem asks for a family of paths each leading from the source to a different vertex in the graph though some paths may of course have edges in common a variety of practical applications of the shortestpaths problem have made the problem a very popular object of study the obvious but probably most widely used applications are transportation planning and packet routing in communication networks including the internet multitudes of less obvious applications include finding shortest paths in social networks speech recognition document formatting robotics compilers and airline crew scheduling in the world of entertainment one can mention pathfinding in video games and finding best solutions to puzzles using their statespace graphs see section for a very simple example of the latter there are several wellknown algorithms for finding shortest paths including floyds algorithm for the more general allpairs shortestpaths problem discussed in chapter here we consider the bestknown algorithm for the singlesource shortestpaths problem called dijkstras algorithm this algorithm is applicable to undirected and directed graphs with nonnegative weights only since in most applications this condition is satisfied the limitation has not impaired the popularity of dijkstras algorithm dijkstras algorithm finds the shortest paths to a graphs vertices in order of their distance from a given source first it finds the shortest path from the source edsger w dijkstra a noted dutch pioneer of the science and industry of computing discovered this algorithm in the mids dijkstra said about his algorithm this was the first graph problem i ever posed myself and solved the amazing thing was that i didnt publish it it was not amazing at the time at the time algorithms were hardly considered a scientific topic u v v figure idea of dijkstras algorithm the subtree of the shortest paths already found is shown in bold the next nearest to the source v vertex u is selected by comparing the lengths of the subtrees paths increased by the distances to vertices adjacent to the subtrees vertices to a vertex nearest to it then to a second nearest and so on in general before its ith iteration commences the algorithm has already identified the shortest paths to i other vertices nearest to the source these vertices the source and the edges of the shortest paths leading to them from the source form a subtree ti of the given graph figure since all the edge weights are nonnegative the next vertex nearest to the source can be found among the vertices adjacent to the vertices of ti the set of vertices adjacent to the vertices in ti can be referred to as fringe vertices they are the candidates from which dijkstras algorithm selects the next vertex nearest to the source actually all the other vertices can be treated as fringe vertices connected to tree vertices by edges of infinitely large weights to identify the ith nearest vertex the algorithm computes for every fringe vertex u the sum of the distance to the nearest tree vertex v given by the weight of the edge v u and the length dv of the shortest path from the source to v previously determined by the algorithm and then selects the vertex with the smallest such sum the fact that it suffices to compare the lengths of such special paths is the central insight of dijkstras algorithm to facilitate the algorithms operations we label each vertex with two labels the numeric label d indicates the length of the shortest path from the source to this vertex found by the algorithm so far when a vertex is added to the tree d indicates the length of the shortest path from the source to that vertex the other label indicates the name of the nexttolast vertex on such a path ie the parent of the vertex in the tree being constructed it can be left unspecified for the source s and vertices that are adjacent to none of the current tree vertices with such labeling finding the next nearest vertex u becomes a simple task of finding a fringe vertex with the smallest d value ties can be broken arbitrarily after we have identified a vertex u to be added to the tree we need to perform two operations move u from the fringe to the set of tree vertices for each remaining fringe vertex u that is connected to u by an edge of weight wu u such that du wu u du update the labels of u by u and du wu u respectively figure demonstrates the application of dijkstras algorithm to a specific graph the labeling and mechanics of dijkstras algorithm are quite similar to those used by prims algorithm see section both of them construct an expanding subtree of vertices by selecting the next vertex from the priority queue of the remaining vertices it is important not to mix them up however they solve different problems and therefore operate with priorities computed in a different manner dijkstras algorithm compares path lengths and therefore must add edge weights while prims algorithm compares the edge weights as given now we can give pseudocode of dijkstras algorithm it is spelled out in more detail than prims algorithm was in section in terms of explicit operations on two sets of labeled vertices the set vt of vertices for which a shortest path has already been found and the priority queue q of the fringe vertices note that in the following pseudocode vt contains a given source vertex and the fringe contains the vertices adjacent to it after iteration is completed algorithm dijkstrag s dijkstras algorithm for singlesource shortest paths input a weighted connected graph g v e with nonnegative weights and its vertex s output the length dv of a shortest path from s to v and its penultimate vertex pv for every vertex v in v initializeq initialize priority queue to empty for every vertex v in v dv pv null insertq v dv initialize vertex priority in the priority queue ds decreaseq s ds update priority of s with ds vt for i to v do u deleteminq delete the minimum priority element vt vt u for every vertex u in v vt that is adjacent to u do if du wu u du du du wu u pu u decreaseq u du the time efficiency of dijkstras algorithm depends on the data structures used for implementing the priority queue and for representing an input graph itself for the reasons explained in the analysis of prims algorithm in section it is b c a d e tree vertices remaining vertices illustration a ba c da e b c a d e ba cb db e b c a d e db cb ed b c a d e cb ed b c a d e ed the shortest paths identified by following nonnumeric labels backward from a destination vertex in the left column to the source and their lengths given by numeric labels of the tree vertices are as follows from a to b ab of length from a to d abd of length from a to c abc of length from a to e a bd e of length figure application of dijkstras algorithm the next closest vertex is shown in bold in v for graphs represented by their weight matrix and the priority queue implemented as an unordered array for graphs represented by their adjacency lists and the priority queue implemented as a minheap it is in oe log v a still better upper bound can be achieved for both prims and dijkstras algorithms if the priority queue is implemented using a sophisticated data structure called the fibonacci heap eg cor however its complexity and a considerable overhead make such an improvement primarily of theoretical value exercises explain what adjustments if any need to be made in dijkstras algorithm andor in an underlying graph to solve the following problems a solve the singlesource shortestpaths problem for directed weighted graphs b find a shortest path between two given vertices of a weighted graph or digraph this variation is called the singlepair shortestpath problem c find the shortest paths to a given vertex from each other vertex of a weighted graph or digraph this variation is called the singledestination shortestpaths problem d solve the singlesource shortestpaths problem in a graph with nonnegative numbers assigned to its vertices and the length of a path defined as the sum of the vertex numbers on the path solve the following instances of the singlesource shortestpaths problem with vertex a as the source a b c a d e b a b c d e f g h i j k l give a counterexample that shows that dijkstras algorithm may not work for a weighted connected graph with negative weights let t be a tree constructed by dijkstras algorithm in the process of solving the singlesource shortestpaths problem for a weighted connected graph g a true or false t is a spanning tree of g b true or false t is a minimum spanning tree of g write pseudocode for a simpler version of dijkstras algorithm that finds only the distances ie the lengths of shortest paths but not shortest paths themselves from a given vertex to all other vertices of a graph represented by its weight matrix prove the correctness of dijkstras algorithm for graphs with positive weights design a lineartime algorithm for solving the singlesource shortestpaths problem for dags directed acyclic graphs represented by their adjacency lists explain how the minimumsum descent problem problem in exercises can be solved by dijkstras algorithm shortestpath modeling assume you have a model of a weighted connected graph made of balls representing the vertices connected by strings of appropriate lengths representing the edges a describe how you can solve the singlepair shortestpath problem with this model b describe how you can solve the singlesource shortestpaths problem with this model revisit the exercise from section about determining the best route for a subway passenger to take from one designated station to another in a welldeveloped subway system like those in washington dc or london uk write a program for this task huffman trees and codes suppose we have to encode a text that comprises symbols from some nsymbol alphabet by assigning to each of the texts symbols some sequence of bits called the codeword for example we can use a fixedlength encoding that assigns to each symbol a bit string of the same length m m log n this is exactly what the standard ascii code does one way of getting a coding scheme that yields a shorter bit string on the average is based on the old idea of assigning shorter codewords to more frequent symbols and longer codewords to less frequent symbols this idea was used in particular in the telegraph code invented in the midth century by samuel morse in that code frequent letters such as e and a are assigned short sequences of dots and dashes while infrequent letters such as q and z have longer ones variablelength encoding which assigns codewords of different lengths to different symbols introduces a problem that fixedlength encoding does not have namely how can we tell how many bits of an encoded text represent the first or more generally the ith symbol to avoid this complication we can limit ourselves to the socalled prefixfree or simply prefix codes in a prefix code no codeword is a prefix of a codeword of another symbol hence with such an encoding we can simply scan a bit string until we get the first group of bits that is a codeword for some symbol replace these bits by this symbol and repeat this operation until the bit strings end is reached if we want to create a binary prefix code for some alphabet it is natural to associate the alphabets symbols with leaves of a binary tree in which all the left edges are labeled by and all the right edges are labeled by the codeword of a symbol can then be obtained by recording the labels on the simple path from the root to the symbols leaf since there is no simple path to a leaf that continues to another leaf no codeword can be a prefix of another codeword hence any such tree yields a prefix code among the many trees that can be constructed in this manner for a given alphabet with known frequencies of the symbol occurrences how can we construct a tree that would assign shorter bit strings to highfrequency symbols and longer ones to lowfrequency symbols it can be done by the following greedy algorithm invented by david huffman while he was a graduate student at mit huf huffmans algorithm step initialize n onenode trees and label them with the symbols of the alphabet given record the frequency of each symbol in its trees root to indicate the trees weight more generally the weight of a tree will be equal to the sum of the frequencies in the trees leaves step repeat the following operation until a single tree is obtained find two trees with the smallest weight ties can be broken arbitrarily but see problem in this sections exercises make them the left and right subtree of a new tree and record the sum of their weights in the root of the new tree as its weight a tree constructed by the above algorithm is called a huffman tree it defines in the manner described above a huffman code example consider the fivesymbol alphabet a b c d with the following occurrence frequencies in a text made up of these symbols symbol a b c d frequency the huffman tree construction for this input is shown in figure b c d a c d a b a b c d c d a b c d a b figure example of constructing a huffman coding tree the resulting codewords are as follows symbol a b c d frequency codeword hence dad is encoded as and is decoded as badad with the occurrence frequencies given and the codeword lengths obtained the average number of bits per symbol in this code is had we used a fixedlength encoding for the same alphabet we would have to use at least bits per each symbol thus for this toy example huffmans code achieves the compression ratio a standard measure of a compression algorithms effectiveness of in other words huffmans encoding of the text will use less memory than its fixedlength encoding extensive experiments with huffman codes have shown that the compression ratio for this scheme typically falls between and depending on the characteristics of the text being compressed huffmans encoding is one of the most important filecompression methods in addition to its simplicity and versatility it yields an optimal ie minimallength encoding provided the frequencies of symbol occurrences are independent and known in advance the simplest version of huffman compression calls in fact for a preliminary scanning of a given text to count the frequencies of symbol occurrences in it then these frequencies are used to construct a huffman coding tree and encode the text as described above this scheme makes it necessary however to include the coding table into the encoded text to make its decoding possible this drawback can be overcome by using dynamic huffman encoding in which the coding tree is updated each time a new symbol is read from the source text further modern alternatives such as lempelziv algorithms eg say assign codewords not to individual symbols but to strings of symbols allowing them to achieve better and more robust compressions in many applications it is important to note that applications of huffmans algorithm are not limited to data compression suppose we have n positive numbers w w wn that have to be assigned to n leaves of a binary tree one per node if we define the weighted path length as the sum n li wi where li is the length of the simple i path from the root to the ith leaf how can we construct a binary tree with minimum weighted path length it is this more general problem that huffmans algorithm actually solves for the coding application li and wi are the length of the codeword and the frequency of the ith symbol respectively this problem arises in many situations involving decision making consider for example the game of guessing a chosen object from n possibilities say an integer between and n by asking questions answerable by yes or no different strategies for playing this game can be modeled by decision trees such as those depicted in figure for n the length of the simple path from the root to a leaf in such a tree is equal to the number of questions needed to get to the chosen number represented by the leaf if number i is chosen with probability pi the sum decision trees are discussed in more detail in section no n no n yes yes n n n n no yes no yes no yes n n n n n n no yes n n figure two decision trees for guessing an integer between and n li pi where li is the length of the path from the root to the ith leaf indicates i the average number of questions needed to guess the chosen number with a game strategy represented by its decision tree if each of the numbers is chosen with the same probability of n the best strategy is to successively eliminate half or almost half the candidates as binary search does this may not be the case for arbitrary pis however for example if n and p p p and p the minimum weighted path tree is the rightmost one in figure thus we need huffmans algorithm to solve this problem in its general case note that this is the second time we are encountering the problem of constructing an optimal binary tree in section we discussed the problem of constructing an optimal binary search tree with positive numbers the search probabilities assigned to every node of the tree in this section given numbers are assigned just to leaves the latter problem turns out to be easier it can be solved by the greedy algorithm whereas the former is solved by the more complicated dynamic programming algorithm exercises a construct a huffman code for the following data symbol a b c d frequency b encode abacabad using the code of question a c decode using the code of question a for data transmission purposes it is often desirable to have a code with a minimum variance of the codeword lengths among codes of the same average length compute the average and variance of the codeword length in two huffman codes that result from a different tie breaking during a huffman code construction for the following data symbol a b c d e probability indicate whether each of the following properties is true for every huffman code a the codewords of the two least frequent symbols have the same length b the codewords length of a more frequent symbol is always smaller than or equal to the codewords length of a less frequent one what is the maximal length of a codeword possible in a huffman encoding of an alphabet of n symbols a write pseudocode of the huffmantree construction algorithm b what is the time efficiency class of the algorithm for constructing a huffman tree as a function of the alphabet size show that a huffman tree can be constructed in linear time if the alphabet symbols are given in a sorted order of their frequencies given a huffman coding tree which algorithm would you use to get the codewords for all the symbols what is its timeefficiency class as a function of the alphabet size explain how one can generate a huffman code without an explicit generation of a huffman coding tree a write a program that constructs a huffman code for a given english text and encode it b write a program for decoding of an english text which has been encoded with a huffman code c experiment with your encoding program to find a range of typical compression ratios for huffmans encoding of english texts of say words d experiment with your encoding program to find out how sensitive the compression ratios are to using standard estimates of frequencies instead of actual frequencies of symbol occurrences in english texts card guessing design a strategy that minimizes the expected number of questions asked in the following game gar you have a deck of cards that consists of one ace of spades two deuces of spades three threes and on up to nine nines making cards in all someone draws a card from the shuffled deck which you have to identify by asking questions answerable with yes or no summary the greedy technique suggests constructing a solution to an optimization problem through a sequence of steps each expanding a partially constructed solution obtained so far until a complete solution to the problem is reached on each step the choice made must be feasible locally optimal and irrevocable prims algorithm is a greedy algorithm for constructing a minimum spanning tree of a weighted connected graph it works by attaching to a previously constructed subtree a vertex closest to the vertices already in the tree kruskals algorithm is another greedy algorithm for the minimum spanning tree problem it constructs a minimum spanning tree by selecting edges in nondecreasing order of their weights provided that the inclusion does not create a cycle checking the latter condition efficiently requires an application of one of the socalled unionfind algorithms dijkstras algorithm solves the singlesource shortestpath problem of finding shortest paths from a given vertex the source to all the other vertices of a weighted graph or digraph it works as prims algorithm but compares path lengths rather than edge lengths dijkstras algorithm always yields a correct solution for a graph with nonnegative weights a huffman tree is a binary tree that minimizes the weighted path length from the root to the leaves of predefined weights the most important application of huffman trees is huffman codes a huffman code is an optimal prefixfree variablelength encoding scheme that assigns bit strings to symbols based on their frequencies in a given text this is accomplished by a greedy construction of a binary tree whose leaves represent the alphabet symbols and whose edges are labeled with s and s iterative improvement the most successful men in the end are those whose success is the result of steady accretion alexander graham bell the greedy strategy considered in the preceding chapter constructs a solution to an optimization problem piece by piece always adding a locally optimal piece to a partially constructed solution in this chapter we discuss a different approach to designing algorithms for optimization problems it starts with some feasible solution a solution that satisfies all the constraints of the problem and proceeds to improve it by repeated applications of some simple step this step typically involves a small localized change yielding a feasible solution with an improved value of the objective function when no such change improves the value of the objective function the algorithm returns the last feasible solution as optimal and stops there can be several obstacles to the successful implementation of this idea first we need an initial feasible solution for some problems we can always start with a trivial solution or use an approximate solution obtained by some other eg greedy algorithm but for others finding an initial solution may require as much effort as solving the problem after a feasible solution has been identified second it is not always clear what changes should be allowed in a feasible solution so that we can check efficiently whether the current solution is locally optimal and if not replace it with a better one third and this is the most fundamental difficulty is an issue of local versus global extremum maximum or minimum think about the problem of finding the highest point in a hilly area with no map on a foggy day a logical thing to do would be to start walking up the hill from the point you are at until it becomes impossible to do so because no direction would lead up you will have reached a local highest point but because of a limited feasibility there will be no simple way to tell whether the point is the highest global maximum you are after in the entire area fortunately there are important problems that can be solved by iterativeimprovement algorithms the most important of them is linear programming we have already encountered this topic in section here in section we introduce the simplex method the classic algorithm for linear programming discovered by the us mathematician george b dantzig in this algorithm has proved to be one of the most consequential achievements in the history of algorithmics in section we consider the important problem of maximizing the amount of flow that can be sent through a network with links of limited capacities this problem is a special case of linear programming however its special structure makes it possible to solve the problem by algorithms that are more efficient than the simplex method we outline the classic iterativeimprovement algorithm for this problem discovered by the american mathematicians l r ford jr and d r fulkerson in the s the last two sections of the chapter deal with bipartite matching this is the problem of finding an optimal pairing of elements taken from two disjoint sets examples include matching workers and jobs high school graduates and colleges and men and women for marriage section deals with the problem of maximizing the number of matched pairs section is concerned with the matching stability we also discuss several iterativeimprovement algorithms in section where we consider approximation algorithms for the traveling salesman and knapsack problems other examples of iterativeimprovement algorithms can be found in the algorithms textbook by moret and shapiro mor books on continuous and discrete optimization eg nem and the literature on heuristic search eg mic the simplex method we have already encountered linear programming see section the general problem of optimizing a linear function of several variables subject to a set of linear constraints maximize or minimize cx cnxn subject to aix ainxn or or bi for i m x xn we mentioned there that many important practical problems can be modeled as instances of linear programming two researchers l v kantorovich of the former soviet union and the dutchamerican t c koopmans were even awarded the nobel prize in for their contributions to linear programming theory and its applications to economics apparently because there is no nobel prize in mathematics the royal swedish academy of sciences failed to honor the us mathematician g b dantzig who is universally recognized as the father of linear programming in its modern form and the inventor of the simplex method the classic algorithm for solving such problems geometric interpretation of linear programming before we introduce a general method for solving linear programming problems let us consider a small example which will help us to see the fundamental properties of such problems example consider the following linear programming problem in two variables maximize x y subject to x y x y x y by definition a feasible solution to this problem is any point x y that satisfies all the constraints of the problem the problems feasible region is the set of all its feasible points it is instructive to sketch the feasible region in the cartesian plane recall that any equation ax by c where coefficients a and b are not both equal to zero defines a straight line such a line divides the plane into two halfplanes for all the points in one of them ax by c while for all the points in the other ax by c it is easy to determine which of the two halfplanes is which take any point x y not on the line ax by c and check which of the two inequalities hold ax by c or ax by c in particular the set of points defined by inequality x y comprises the points on and below the line x y and the set of points defined by inequality x y comprises the points on and below the line x y since the points of the feasible region must satisfy all the constraints of the problem the feasible region is obtained by the intersection of these two halfplanes and the first quadrant of the cartesian plane defined by the nonnegativity constraints x y see figure thus the feasible region for problem is the convex polygon with the vertices and the last point which is the point of intersection of the lines x y and x y is obtained by solving the system of these two linear equations our task is to find an optimal solution a point in the feasible region with the largest value of the objective function z x y are there feasible solutions for which the value of the objective function equals say the points x y for which the objective function z x y is equal to form the line x y since this line does not have common points george b dantzig has received many honors including the national medal of science presented by the president of the united states in the citation states that the national medal was awarded for inventing linear programming and discovering methods that led to widescale scientific and technical applications to important problems in logistics scheduling and network optimization and to the use of computers in making efficient use of the mathematical theory y x y x xy figure feasible region of problem with the feasible region see figure the answer to the posed question is no on the other hand there are infinitely many feasible points for which the objective function is equal to say they are the intersection points of the line x y with the feasible region note that the lines x y and x y have the same slope as would any line defined by equation x y z where z is some constant such lines are called level lines of the objective function thus our problem can be restated as finding the largest value of the parameter z for which the level line x y z has a common point with the feasible region we can find this line either by shifting say the line x y southwest without changing its slope toward the feasible region until it hits the region for the first time or by shifting say the line x y northeast until it hits the feasible region for the last time either way it will happen at the point with the corresponding z value this means that the optimal solution to the linear programming problem in question is x y with the maximal value of the objective function equal to note that if we had to maximize z x y as the objective function in problem the level line x y z for the largest value of z would coincide with the boundary line segment that has the same slope as the level lines draw this line in figure consequently all the points of the line segment between vertices and including the vertices themselves would be optimal solutions yielding of course the same maximal value of the objective function y x x y x y x y figure solving a twodimensional linear programming problem geometrically does every linear programming problem have an optimal solution that can be found at a vertex of its feasible region without appropriate qualifications the answer to this question is no to begin with the feasible region of a linear programming problem can be empty for example if the constraints include two contradictory requirements such as x y and x y there can be no points in the problems feasible region linear programming problems with the empty feasible region are called infeasible obviously infeasible problems do not have optimal solutions another complication may arise if the problems feasible region is unbounded as the following example demonstrates example if we reverse the inequalities in problem to x y and x y the feasible region of the new problem will become unbounded see figure if the feasible region of a linear programming problem is unbounded its objective function may or may not attain a finite optimal value on it for example the problem of maximizing z x y subject to the constraints x y x y x y has no optimal solution because there are points in the feasible region making x y as large as we wish such problems are called unbounded on the other hand the problem of minimizing z x y subject to the same constraints has an optimal solution which y x x y x y x y figure unbounded feasible region of a linear programming problem with constraints x y x y x y and three level lines of the function x y fortunately the most important features of the examples we considered above hold for problems with more than two variables in particular a feasible region of a typical linear programming problem is in many ways similar to convex polygons in the twodimensional cartesian plane specifically it always has a finite number of vertices which mathematicians prefer to call extreme points see section furthermore an optimal solution to a linear programming problem can be found at one of the extreme points of its feasible region we reiterate these properties in the following theorem theorem extreme point theorem any linear programming problem with a nonempty bounded feasible region has an optimal solution moreover an optimal solution can always be found at an extreme point of the problems feasible region this theorem implies that to solve a linear programming problem at least in the case of a bounded feasible region we can ignore all but a finite number of except for some degenerate instances such as maximizing z x y subject to x y if a linear programming problem with an unbounded feasible region has an optimal solution it can also be found at an extreme point of the feasible region points in its feasible region in principle we can solve such a problem by computing the value of the objective function at each extreme point and selecting the one with the best value there are two major obstacles to implementing this plan however the first lies in the need for a mechanism for generating the extreme points of the feasible region as we are going to see below a rather straightforward algebraic procedure for this task has been discovered the second obstacle lies in the number of extreme points a typical feasible region has here the news is bad the number of extreme points is known to grow exponentially with the size of the problem this makes the exhaustive inspection of extreme points unrealistic for most linear programming problems of nontrivial sizes fortunately it turns out that there exists an algorithm that typically inspects only a small fraction of the extreme points of the feasible region before reaching an optimal one this famous algorithm is called the simplex method the idea of this algorithm can be described in geometric terms as follows start by identifying an extreme point of the feasible region then check whether one can get an improved value of the objective function by going to an adjacent extreme point if it is not the case the current point is optimal stop if it is the case proceed to an adjacent extreme point with an improved value of the objective function after a finite number of steps the algorithm will either reach an extreme point where an optimal solution occurs or determine that no optimal solution exists an outline of the simplex method our task now is to translate the geometric description of the simplex method into the more algorithmically precise language of algebra to begin with before we can apply the simplex method to a linear programming problem it has to be represented in a special form called the standard form the standard form has the following requirements it must be a maximization problem all the constraints except the nonnegativity constraints must be in the form of linear equations with nonnegative righthand sides all the variables must be required to be nonnegative thus the general linear programming problem in standard form with m constraints and n unknowns n m is maximize cx cnxn subject to aix ainxn bi where bi for i m x xn it can also be written in compact matrix notations maximize cx subject to ax b x where x b x x a a a an b b c c c cn xn am am amn bm any linear programming problem can be transformed into an equivalent problem in standard form if an objective function needs to be minimized it can be replaced by the equivalent problem of maximizing the same objective function with all its coefficients cj replaced by cj j n see section for a more general discussion of such transformations if a constraint is given as an inequality it can be replaced by an equivalent equation by adding a slack variable representing the difference between the two sides of the original inequality for example the two inequalities of problem can be transformed respectively into the following equations x y u where u and x y v where v finally in most linear programming problems the variables are required to be nonnegative to begin with because they represent some physical quantities if this is not the case in an initial statement of a problem an unconstrained variable xj can be replaced by the difference between two new nonnegative variables xj xj xj xj xj thus problem in standard form is the following linear programming problem in four variables maximize x y u v subject to x y u x y v x y u v it is easy to see that if we find an optimal solution x y u v to problem we can obtain an optimal solution to problem by simply ignoring its last two coordinates the principal advantage of the standard form lies in the simple mechanism it provides for identifying extreme points of the feasible region to do this for problem for example we need to set two of the four variables in the constraint equations to zero to get a system of two linear equations in two unknowns and solve this system for the general case of a problem with m equations in n unknowns n m n m variables need to be set to zero to get a system of m equations in m unknowns if the system obtained has a unique solution as any nondegenerate system of linear equations with the number of equations equal to the number of unknowns does we have a basic solution its coordinates set to zero before solving the system are called nonbasic and its coordinates obtained by solving the system are called basic this terminology comes from linear algebra specifically we can rewrite the system of constraint equations of as x y u v a basis in the twodimensional vector space is composed of any two vectors that are not proportional to each other once a basis is chosen any vector can be uniquely expressed as a sum of multiples of the basis vectors basic and nonbasic variables indicate which of the given vectors are respectively included and excluded in a particular basis choice if all the coordinates of a basic solution are nonnegative the basic solution is called a basic feasible solution for example if we set to zero variables x and y and solve the resulting system for u and v we obtain the basic feasible solution if we set to zero variables x and u and solve the resulting system for y and v we obtain the basic solution which is not feasible the importance of basic feasible solutions lies in the onetoone correspondence between them and the extreme points of the feasible region for example is an extreme point of the feasible region of problem with the point in figure being its projection on the x y plane incidentally is a natural starting point for the simplex methods application to this problem as mentioned above the simplex method progresses through a series of adjacent extreme points basic feasible solutions with increasing values of the objective function each such point can be represented by a simplex tableau a table storing the information about the basic feasible solution corresponding to the extreme point for example the simplex tableau for of problem is presented below x y u v u v in general a simplex tableau for a linear programming problem in standard form with n unknowns and m linear equality constraints n m has m rows and n columns each of the first m rows of the table contains the coefficients of a corresponding constraint equation with the last columns entry containing the equations righthand side the columns except the last one are labeled by the names of the variables the rows are labeled by the basic variables of the basic feasible solution the tableau represents the values of the basic variables of this solution are in the last column also note that the columns labeled by the basic variables form the m m identity matrix the last row of a simplex tableau is called the objective row it is initialized by the coefficients of the objective function with their signs reversed in the first n columns and the value of the objective function at the initial point in the last column on subsequent iterations the objective row is transformed the same way as all the other rows the objective row is used by the simplex method to check whether the current tableau represents an optimal solution it does if all the entries in the objective row except possibly the one in the last column are nonnegative if this is not the case any of the negative entries indicates a nonbasic variable that can become basic in the next tableau for example according to this criterion the basic feasible solution represented by tableau is not optimal the negative value in the xcolumn signals the fact that we can increase the value of the objective function z x y u v by increasing the value of the xcoordinate in the current basic feasible solution indeed since the coefficient for x in the objective function is positive the larger the x value the larger the value of this function of course we will need to compensate an increase in x by adjusting the values of the basic variables u and v so that the new point is still feasible for this to be the case both conditions xu where u xv where v must be satisfied which means that x min note that if we increase the value of x from to the largest amount possible we will find ourselves at the point an adjacent to extreme point of the feasible region with z similarly the negative value in the ycolumn of the objective row signals the fact that we can also increase the value of the objective function by increasing the value of the ycoordinate in the initial basic feasible solution this requires yu where u y v where v which means that y min if we increase the value of y from to the largest amount possible we will find ourselves at the point another adjacent to extreme point with z if there are several negative entries in the objective row a commonly used rule is to select the most negative one ie the negative number with the largest absolute value this rule is motivated by the observation that such a choice yields the largest increase in the objective functions value per unit of change in a variables value in our example an increase in the xvalue from to at changes the value of z x y u v from to while an increase in the yvalue from to at changes z from to note however that the feasibility constraints impose different limits on how much each of the variables may increase in our example in particular the choice of the yvariable over the xvariable leads to a smaller increase in the value of the objective function still we will employ this commonly used rule and select variable y as we continue with our example a new basic variable is called the entering variable while its column is referred to as the pivot column we mark the pivot column by now we will explain how to choose a departing variable ie a basic variable to become nonbasic in the next tableau the total number of basic variables in any basic solution must be equal to m the number of the equality constraints as we saw above to get to an adjacent extreme point with a larger value of the objective function we need to increase the entering variable by the largest amount possible to make one of the old basic variables zero while preserving the nonnegativity of all the others we can translate this observation into the following rule for choosing a departing variable in a simplex tableau for each positive entry in the pivot column compute the ratio by dividing the rows last entry by the entry in the pivot column for the example of tableau these ratios are u v the row with the smallest ratio determines the departing variable ie the variable to become nonbasic ties may be broken arbitrarily for our example it is variable v we mark the row of the departing variable called the pivot row by and denote it ro w note that if there are no positive entries in the pivot column no ratio can be computed which indicates that the problem is unbounded and the algorithm stops finally the following steps need to be taken to transform a current tableau into the next one this transformation called pivoting is similar to the principal step of the gaussjordan elimination algorithm for solving systems of linear equations see problem in exercises first divide all the entries of the pivot row by the pivot its entry in the pivot column to obtain ro wnew for tableau we obtain ro wnew then replace each of the other rows including the objective row by the difference row c ro wnew where c is the rows entry in the pivot column for tableau this yields row ro wnew row ro wnew thus the simplex method transforms tableau into the following tableau x y u v u y tableau represents the basic feasible solution with an increased value of the objective function which is equal to it is not optimal however why the next iteration do it yourself as a good exercise yields tableau x y u v x y this tableau represents the basic feasible solution it is optimal because all the entries in the objective row of tableau are nonnegative the maximal value of the objective function is equal to the last entry in the objective row let us summarize the steps of the simplex method summary of the simplex method step initialization present a given linear programming problem in standard form and set up an initial tableau with nonnegative entries in the rightmost column and m other columns composing the m m identity matrix entries in the objective row are to be disregarded in verifying these requirements these m columns define the basic variables of the initial basic feasible solution used as the labels of the tableaus rows step optimality test if all the entries in the objective row except possibly the one in the rightmost column which represents the value of the objective function are nonnegative stop the tableau represents an optimal solution whose basic variables values are in the rightmost column and the remaining nonbasic variables values are zeros step finding the entering variable select a negative entry from among the first n elements of the objective row a commonly used rule is to select the negative entry with the largest absolute value with ties broken arbitrarily mark its column to indicate the entering variable and the pivot column step finding the departing variable for each positive entry in the pivot column calculate the ratio by dividing that rows entry in the rightmost column by its entry in the pivot column if all the entries in the pivot column are negative or zero the problem is unbounded stop find the row with the smallest ratio ties may be broken arbitrarily and mark this row to indicate the departing variable and the pivot row step forming the next tableau divide all the entries in the pivot row by its entry in the pivot column subtract from each of the other rows including the objective row the new pivot row multiplied by the entry in the pivot column of the row in question this will make all the entries in the pivot column s except for in the pivot row replace the label of the pivot row by the variables name of the pivot column and go back to step further notes on the simplex method formal proofs of validity of the simplex method steps can be found in books devoted to a detailed discussion of linear programming eg dan a few important remarks about the method still need to be made however generally speaking an iteration of the simplex method leads to an extreme point of the problems feasible region with a greater value of the objective function in degenerate cases which arise when one or more basic variables are equal to zero the simplex method can only guarantee that the value of the objective function at the new extreme point is greater than or equal to its value at the previous point in turn this opens the door to the possibility not only that the objective functions values stall for several iterations in a row but that the algorithm might cycle back to a previously considered point and hence never terminate the latter phenomenon is called cycling although it rarely if ever happens in practice specific examples of problems where cycling does occur have been constructed a simple modification of steps and of the simplex method called blands rule eliminates even the theoretical possibility of cycling assuming that the variables are denoted by a subscripted letter eg x x xn this rule can be stated as follows step modified among the columns with a negative entry in the objective row select the column with the smallest subscript step modified resolve a tie among the smallest ratios by selecting the row labeled by the basic variable with the smallest subscript another caveat deals with the assumptions made in step they are automatically satisfied if a problem is given in the form where all the constraints imposed on nonnegative variables are inequalities aix ainxn bi with bi for i m indeed by adding a nonnegative slack variable xni into the ith constraint we obtain the equality aix ainxn xni bi and all the requirements imposed on an initial tableau of the simplex method are satisfied for the obvious basic feasible solution x xn xn xnm but if a problem is not given in such a form finding an initial basic feasible solution may present a nontrivial obstacle moreover for problems with an empty feasible region no initial basic feasible solution exists and we need an algorithmic way to identify such problems one of the ways to address these issues is to use an extension to the classic simplex method called the twophase simplex method see eg kol in a nutshell this method adds a set of artificial variables to the equality constraints of a given problem so that the new problem has an obvious basic feasible solution it then solves the linear programming problem of minimizing the sum of the artificial variables by the simplex method the optimal solution to this problem either yields an initial tableau for the original problem or indicates that the feasible region of the original problem is empty how efficient is the simplex method since the algorithm progresses through a sequence of adjacent points of a feasible region one should probably expect bad news because the number of extreme points is known to grow exponentially with the problem size indeed the worstcase efficiency of the simplex method has been shown to be exponential as well fortunately more than half a century of practical experience with the algorithm has shown that the number of iterations in a typical application ranges between m and m with the number of operations per iteration proportional to mn where m and n are the numbers of equality constraints and variables respectively since its discovery in the simplex method has been a subject of intensive study by many researchers some of them have worked on improvements to the original algorithm and details of its efficient implementation as a result of these efforts programs implementing the simplex method have been polished to the point that very large problems with hundreds of thousands of constraints and variables can be solved in a routine manner in fact such programs have evolved into sophisticated software packages these packages enable the user to enter a problems constraints and obtain a solution in a userfriendly form they also provide tools for investigating important properties of the solution such as its sensitivity to changes in the input data such investigations are very important for many applications including those in economics at the other end of the spectrum linear programming problems of a moderate size can nowadays be solved on a desktop using a standard spreadsheet facility or by taking advantage of specialized software available on the internet researchers have also tried to find algorithms for solving linear programming problems with polynomialtime efficiency in the worst case an important milestone in the history of such algorithms was the proof by l g khachian kha showing that the ellipsoid method can solve any linear programming problem in polynomial time although the ellipsoid method was much slower than the simplex method in practice its better worstcase efficiency encouraged a search for alternatives to the simplex method in narendra karmarkar published an algorithm that not only had a polynomial worstcase efficiency but also was competitive with the simplex method in empirical tests as well although we are not going to discuss karmarkars algorithm kar here it is worth pointing out that it is also based on the iterativeimprovement idea however karmarkars algorithm generates a sequence of feasible solutions that lie within the feasible region rather than going through a sequence of adjacent extreme points as the simplex method does such algorithms are called interiorpoint methods see eg arb exercises consider the following version of the post office location problem problem in exercises given n integers x x xn representing coordinates of n villages located along a straight road find a location for a post office that minimizes the average distance between the villages the post office may be but is not required to be located at one of the villages devise an iterativeimprovement algorithm for this problem is this an efficient way to solve this problem solve the following linear programming problems geometrically a maximize x y subject to x y x y x y b maximize x y subject to x y y x x y consider the linear programming problem minimize cx cy subject to x y x y x y where c and c are some real numbers not both equal to zero a give an example of the coefficient values c and c for which the problem has a unique optimal solution b give an example of the coefficient values c and c for which the problem has infinitely many optimal solutions c give an example of the coefficient values c and c for which the problem does not have an optimal solution would the solution to problem be different if its inequality constraints were strict ie x y and x y respectively trace the simplex method on a the problem of exercise a b the problem of exercise b trace the simplex method on the problem of example in section a by hand b by using one of the implementations available on the internet determine how many iterations the simplex method needs to solve the problem n maximize xj j subject to xj bj where bj for j n can we apply the simplex method to solve the knapsack problem see example in section if you answer yes indicate whether it is a good algorithm for the problem in question if you answer no explain why not prove that no linear programming problem can have exactly k optimal solutions unless k if a linear programming problem n maximize cj xj j n subject to aij xj bi for i m j x x xn is considered as primal then its dual is defined as the linear programming problem m minimize bi yi i m subject to aij yi cj for j n i y y ym a express the primal and dual problems in matrix notations b find the dual of the linear programming problem maximize x x x subject to x x x x x x x x x c solve the primal and dual problems and compare the optimal values of their objective functions the maximumflow problem in this section we consider the important problem of maximizing the flow of a material through a transportation network pipeline system communication system electrical distribution system and so on we will assume that the transportation network in question can be represented by a connected weighted digraph with n vertices numbered from to n and a set of edges e with the following properties it contains exactly one vertex with no entering edges this vertex is called the source and assumed to be numbered it contains exactly one vertex with no leaving edges this vertex is called the sink and assumed to be numbered n the weight uij of each directed edge i j is a positive integer called the edge capacity this number represents the upper bound on the amount of the material that can be sent from i to j through a link represented by this edge a digraph satisfying these properties is called a flow network or simply a network a small instance of a network is given in figure it is assumed that the source and the sink are the only source and destination of the material respectively all the other vertices can serve only as points where a flow can be redirected without consuming or adding any amount of the material in other words the total amount of the material entering an intermediate vertex must be equal to the total amount of the material leaving the vertex this condition is called the flowconservation requirement if we denote the amount sent through edge i j by xij then for any intermediate vertex i the flowconservation requirement can be expressed by the following equality constraint xji xij for i n j jie j ij e in a slightly more general model one can consider a network with several sources and sinks and allow capacities uij to be infinitely large figure example of a network graph the vertex numbers are vertex names the edge numbers are edge capacities where the sums in the leftand righthand sides express the total inflow and outflow entering and leaving vertex i respectively since no amount of the material can change by going through intermediate vertices of the network it stands to reason that the total amount of the material leaving the source must end up at the sink this observation can also be derived formally from equalities a task you will be asked to do in the exercises thus we have the following equality xj xj n j j e j jne this quantity the total outflow from the source or equivalently the total inflow into the sink is called the value of the flow we denote it by v it is this quantity that we will want to maximize over all possible flows in a network thus a feasible flow is an assignment of real numbers xij to edges i j of a given network that satisfy flowconservation constraints and the capacity constraints xij uij for every edge i j e the maximumflow problem can be stated formally as the following optimization problem maximize v xj j j e subject to xji xij for i n j jie j ij e xij uij for every edge i j e we can solve linear programming problem by the simplex method or by another algorithm for general linear programming problems see section however the special structure of problem can be exploited to design faster algorithms in particular it is quite natural to employ the iterativeimprovement idea as follows we can always start with the zero flow ie set xij for every edge i j in the network then on each iteration we can try to find a path from source to sink along which some additional flow can be sent such a path is called flow augmenting if a flowaugmenting path is found we adjust the flow along the edges of this path to get a flow of an increased value and try to find an augmenting path for the new flow if no flowaugmenting path can be found we conclude that the current flow is optimal this general template for solving the maximumflow problem is called the augmentingpath method also known as the fordfulkerson method after l r ford jr and d r fulkerson who discovered it see for an actual implementation of the augmenting path idea is however not quite straightforward to see this let us consider the network in figure we start with the zero flow shown in figure a in that figure the zero amounts sent through each edge are separated from the edge capacities by the slashes we will use this notation in the other examples as well it is natural to search for a flowaugmenting path from source to sink by following directed edges i j for which the current flow xij is less than the edge capacity uij among several possibilities let us assume that we identify the augmenting path first we can increase the flow along this path by a maximum of units which is the smallest unused capacity of its edges the new flow is shown in figure b this is as far as our simpleminded idea about flowaugmenting paths will be able to take us unfortunately the flow shown in figure b is not optimal its value can still be increased along the path by increasing the flow by on edges and and decreasing it by on edge the flow obtained as the result of this augmentation is shown in figure c it is indeed maximal can you tell why thus to find a flowaugmenting path for a flow x we need to consider paths from source to sink in the underlying undirected graph in which any two consecutive vertices i j are either i connected by a directed edge from i to j with some positive unused capacity rij uij xij so that we can increase the flow through that edge by up to rij units or ii connected by a directed edge from j to i with some positive flow xji so that we can decrease the flow through that edge by up to xji units edges of the first kind are called forward edges because their tail is listed before their head in the vertex list i j n defining the path edges of the second kind are called backward edges because their tail is listed after their head in the path list i j n to illustrate for the path of the last example and are the forward edges and is the backward edge for a given flowaugmenting path let r be the minimum of all the unused capacities rij of its forward edges and all the flows xji of its backward edges it is easy to see that if we increase the current flow by r on each forward edge and decrease it by this amount on each backward edge we will obtain a feasible a b c figure illustration of the augmentingpath method flowaugmenting paths are shown in bold the flow amounts and edge capacities are indicated by the numbers before and after the slash respectively flow whose value is r units greater than the value of its predecessor indeed let i be an intermediate vertex on a flowaugmenting path there are four possible combinations of forward and backward edges incident to vertex i r i r r i r r i r r i r for each of them the flowconservation requirement for vertex i will still hold after the flow adjustments indicated above the edge arrows further since r is the minimum among all the positive unused capacities on the forward edges and all the positive flows on the backward edges of the flowaugmenting path the new flow will satisfy the capacity constraints as well finally adding r to the flow on the first edge of the augmenting path will increase the value of the flow by r under the assumption that all the edge capacities are integers r will be a positive integer too hence the flow value increases at least by on each iteration of the augmentingpath method since the value of a maximum flow is bounded above eg by the sum of the capacities of the source edges the augmentingpath method has to stop after a finite number of iterations surprisingly the final flow always turns out to be maximal irrespective of a sequence of augmenting paths this remarkable result stems from the proof of the maxflow mincut theorem see eg for which we replicate later in this section the augmentingpath method as described above in its general form does not indicate a specific way for generating flowaugmenting paths a bad sequence of such paths may however have a dramatic impact on the methods efficiency consider for example the network in figure a in which u stands for some large positive integer if we augment the zero flow along the path we shall obtain the flow of value shown in figure b augmenting that flow along the path will increase the flow value to figure c if we continue selecting this pair of flowaugmenting paths we will need a total of u iterations to reach the maximum flow of value u figure d of course we can obtain the maximum flow in just two iterations by augmenting the initial zero flow along the path followed by augmenting the new flow along the path the dramatic difference between u and iterations makes the point fortunately there are several ways to generate flowaugmenting paths efficiently and avoid the degradation in performance illustrated by the previous example the simplest of them uses breadthfirst search to generate augmenting paths with the least number of edges see section this version of the augmentingpath method called shortestaugmentingpath or firstlabeledfirstscanned algorithm was suggested by j edmonds and r m karp edm the labeling refers to marking a new unlabeled vertex with two labels the first label indicates the amount of additional flow that can be brought from the source to the vertex being labeled the second label is the name of the vertex from which the vertex being labeled was reached it can be left undefined for the source it is also convenient to add the or sign to the second label to indicate whether the vertex was reached via a forward or backward edge respectively the source can be always labeled with for the other vertices the labels are computed as follows if capacity upper bounds are irrational numbers the augmentingpath method may not terminate see eg chv pp for a cleverly devised example demonstrating such a situation this limitation is only of theoretical interest because we can not store irrational numbers in a computer and rational numbers can be transformed into integers by changing the capacity measurement unit u u u u u u u u a b u u uu uu u u uu uu c d figure efficiency degradation of the augmentingpath method if unlabeled vertex j is connected to the front vertex i of the traversal queue by a directed edge from i to j with positive unused capacity rij uij xij then vertex j is labeled with lj i where lj minli rij if unlabeled vertex j is connected to the front vertex i of the traversal queue by a directed edge from j to i with positive flow xji then vertex j is labeled with lj i where lj minli xji if this labelingenhanced traversal ends up labeling the sink the current flow can be augmented by the amount indicated by the sinks first label the augmentation is performed along the augmenting path traced by following the vertex second labels from sink to source the current flow quantities are increased on the forward edges and decreased on the backward edges of this path if on the other hand the sink remains unlabeled after the traversal queue becomes empty the algorithm returns the current flow as maximum and stops algorithm shortestaugmentingpathg implements the shortestaugmentingpath algorithm input a network with single source single sink n and positive integer capacities uij on its edges i j output a maximum flow x assign xij to every edge i j in the network label the source with and add the source to the empty queue q while not emptyq do i frontq dequeueq for every edge from i to j do forward edges if j is unlabeled rij uij xij if rij lj minli rij label j with lj i enqueueq j for every edge from j to i do backward edges if j is unlabeled if xji lj minli xji label j with lj ienqueueq j if the sink has been labeled augment along the augmenting path found j n start at the sink and move backwards using second labels while j the source hasnt been reached if the second label of vertex j is i xij xij ln else the second label of vertex j is ixji xji ln j i i the vertex indicated by is second label erase all vertex labels except the ones of the source reinitialize q with the source return x the current flow is maximum an application of this algorithm to the network in figure is illustrated in figure the optimality of a final flow obtained by the augmentingpath method stems from a theorem that relates network flows to network cuts a cut induced by partitioning vertices of a network into some subset x containing the source and x the complement of x containing the sink is the set of all the edges with a tail in x and a head in x we denote a cut cx x or simply c for example for the network in figure if x and hence x cx x if x and hence x cx x if x and hence x cx x the name cut stems from the following property if all the edges of a cut were deleted from the network there would be no directed path from source to sink indeed let cx x be a cut consider a directed path from source to sink if vi is the first vertex of that path which belongs to x the set of such vertices is not queue augment the flow by the sinks first label along the path queue augment the flow by the sinks first label along the path queue no augmenting path the sink is unlabeled the current flow is maximal figure illustration of the shortestaugmentingpath algorithm the diagrams on the left show the current flow before the next iteration begins the diagrams on the right show the results of the vertex labeling on that iteration the augmenting path found in bold and the flow before its augmentation vertices deleted from the queue are indicated by the symbol empty because it contains the sink then vi is not the source and its immediate predecessor vi on that path belongs to x hence the edge from vi to vi must be an element of the cut cx x this proves the property in question the capacity of a cut cx x denoted cx x is defined as the sum of capacities of the edges that compose the cut for the three examples of cuts given above the capacities are equal to and respectively since the number of different cuts in a network is nonempty and finite why there always exists a minimum cut ie a cut with the smallest capacity what is a minimum cut in the network of figure the following theorem establishes an important relationship between the notions of maximum flow and minimum cut theorem maxflow mincut theorem the value of a maximum flow in a network is equal to the capacity of its minimum cut proof first let x be a feasible flow of value v and let cx x be a cut of capacity c in the same network consider the flow across this cut defined as the difference between the sum of the flows on the edges from x to x and the sum of the flows on the edges from x to x it is intuitively clear and can be formally derived from the equations expressing the flowconservation requirement and the definition of the flow value problem b in this sections exercises that the flow across the cut cx x is equal to v the value of the flow v xij xj i ix j x j x ix since the second sum is nonnegative and the flow xij on any edge i j can not exceed the edge capacity uij equality implies that v xij uij ix j x ix j x ie v c thus the value of any feasible flow in a network can not exceed the capacity of any cut in that network let v be the value of a final flow x obtained by the augmentingpath method if we now find a cut whose capacity is equal to v we will have to conclude in view of inequality that i the value v of the final flow is maximal among all feasible flows ii the cuts capacity is minimal among all cuts in the network and iii the maximumflow value is equal to the minimumcut capacity to find such a cut consider the set of vertices x that can be reached from the source by following an undirected path composed of forward edges with positive unused capacities with respect to the final flow x and backward edges with positive flows on them this set contains the source but does not contain the sink if it did we would have an augmenting path for the flow x which would contradict the assumption that the flow x is final consider the cut cx x by the definition of set x each edge i j from x to x has zero unused capacity ie xij uij and each edge j i from x to x has the zero flow on it otherwise j would be in x applying equality to the final flow x and the set x defined above we obtain v xij xji uij cx x ix j x j x ix ix j x which proves the theorem the proof outlined above accomplishes more than proving the equality of the maximumflow value and the minimumcut capacity it also implies that when the augmentingpath method terminates it yields both a maximum flow and a minimum cut if labeling of the kind utilized in the shortestaugmentingpath algorithm is used a minimum cut is formed by the edges from the labeled to unlabeled vertices on the last iteration of the method finally the proof implies that all such edges must be full ie the flows must be equal to the edge capacities and all the edges from unlabeled vertices to labeled if any must be empty ie have zero flows on them in particular for the network in figure the algorithm finds the cut of minimum capacity both edges of which are full as required edmonds and karp proved in their paper edm that the number of augmenting paths needed by the shortestaugmentingpath algorithm never exceeds nm where n and m are the number of vertices and edges respectively since the time required to find a shortest augmenting path by breadthfirst search is in on m om for networks represented by their adjacency lists the time efficiency of the shortestaugmentingpath algorithm is in onm more efficient algorithms for the maximumflow problem are known see the monograph ahu as well as appropriate chapters in such books as cor and kle some of them implement the augmentingpath idea in a more efficient manner others are based on the concept of preflows a preflow is a flow that satisfies the capacity constraints but not the flowconservation requirement any vertex is allowed to have more flow entering the vertex than leaving it a preflowpush algorithm moves the excess flow toward the sink until the flowconservation requirement is reestablished for all intermediate vertices of the network faster algorithms of this kind have worstcase efficiency close to onm note that preflowpush algorithms fall outside the iterativeimprovement paradigm because they do not generate a sequence of improving solutions that satisfy all the constraints of the problem to conclude this section it is worth pointing out that although the initial interest in studying network flows was caused by transportation applications this model has also proved to be useful for many other areas we discuss one of them in the next section exercises since maximumflow algorithms require processing edges in both directions it is convenient to modify the adjacency matrix representation of a network as follows if there is a directed edge from vertex i to vertex j of capacity uij then the element in the ith row and the j th column is set to uij and the element in the j th row and the ith column is set to uij if there is no edge between vertices i and j both these elements are set to zero outline a simple algorithm for identifying a source and a sink in a network presented by such a matrix and indicate its time efficiency apply the shortestaugmenting path algorithm to find a maximum flow and a minimum cut in the following networks a b a does the maximumflow problem always have a unique solution would your answer be different for networks with different capacities on all their edges b answer the same questions for the minimumcut problem of finding a cut of the smallest capacity in a given network a explain how the maximumflow problem for a network with several sources and sinks can be transformed into the same problem for a network with a single source and a single sink b some networks have capacity constraints on the flow amounts that can flow through their intermediate vertices explain how the maximumflow problem for such a network can be transformed to the maximumflow problem for a network with edge capacity constraints only consider a network that is a rooted tree with the root as its source the leaves as its sinks and all the edges directed along the paths from the root to the leaves design an efficient algorithm for finding a maximum flow in such a network what is the time efficiency of your algorithm a prove equality b prove that for any flow in a network and any cut in it the value of the flow is equal to the flow across the cut see equality explain the relationship between this property and equality a express the maximumflow problem for the network in figure as a linear programming problem b solve this linear programming problem by the simplex method as an alternative to the shortestaugmentingpath algorithm edmonds and karp edm suggested the maximumcapacityaugmentingpath algorithm in which a flow is augmented along the path that increases the flow by the largest amount implement both these algorithms in the language of your choice and perform an empirical investigation of their relative efficiency write a report on a more advanced maximumflow algorithm such as i dinitzs algorithm ii karzanovs algorithm iii malhotrakamarmaheshwari algorithm or iv goldbergtarjan algorithm dining problem several families go out to dinner together to increase their social interaction they would like to sit at tables so that no two members of the same family are at the same table show how to find a seating arrangement that meets this objective or prove that no such arrangement exists by using a maximumflow problem assume that the dinner contingent has p families and that the ith family has ai members also assume that q tables are available and the j th table has a seating capacity of bj ahu maximum matching in bipartite graphs in many situations we are faced with a problem of pairing elements of two sets the traditional example is boys and girls for a dance but you can easily think of more serious applications it is convenient to represent elements of two given sets by vertices of a graph with edges between vertices that can be paired a matching in a graph is a subset of its edges with the property that no two edges share a vertex a maximum matching more precisely a maximum cardinality matching is a matching with the largest number of edges what is it for the graph in figure is it unique the maximummatching problem is the problem of finding a maximum matching in a given graph for an arbitrary graph this is a rather difficult problem it was solved in by jack edmonds edm see gal for a good survey and more recent references we limit our discussion in this section to the simpler case of bipartite graphs in a bipartite graph all the vertices can be partitioned into two disjoint sets v and u not necessarily of the same size so that every edge connects a vertex in one of these sets to a vertex in the other set in other words a graph is bipartite if its vertices can be colored in two colors so that every edge has its vertices colored in different colors such graphs are also said to be colorable the graph in figure is bipartite it is not difficult to prove that a graph is bipartite if and only if it does not have a cycle of an odd length we will assume for the rest of this section that v u figure example of a bipartite graph the vertex set of a given bipartite graph has been already partitioned into sets v and u as required by the definition see problem in exercises let us apply the iterativeimprovement technique to the maximumcardinalitymatching problem let m be a matching in a bipartite graph g v u e how can we improve it ie find a new matching with more edges obviously if every vertex in either v or u is matched has a mate ie serves as an endpoint of an edge in m this can not be done and m is a maximum matching therefore to have a chance at improving the current matching both v and u must contain unmatched also called free vertices ie vertices that are not incident to any edge in m for example for the matching ma in the graph in figure a vertices and are free and vertices and are matched another obvious observation is that we can immediately increase a current matching by adding an edge between two free vertices for example adding to the matching ma in the graph in figure a yields a larger matching mb figure b let us now try to find a matching larger than mb by matching vertex the only way to do this would be to include the edge in a new matching this inclusion requires removal of which can be compensated by inclusion of in the new matching this new matching mc is shown in figure c in general we increase the size of a current matching m by constructing a simple path from a free vertex in v to a free vertex in u whose edges are alternately in e m and in m that is the first edge of the path does not belong to m the second one does and so on until the last edge that does not belong to m such a path is called augmenting with respect to the matching m for example the path is an augmenting path with respect to the matching mb in figure b since the length of an augmenting path is always odd adding to the matching m the paths edges in the oddnumbered positions and deleting from it the paths edges in the evennumbered positions yields a matching with one more edge than in m such a matching adjustment is called augmentation thus in figure the matching mb was obtained by augmentation of the matching ma along the augmenting path and the matching mc was obtained by augmentation of the matching mb along the augmenting path moving further is an augmenting path for the matching mc figure c after adding to mc the edges and and deleting and we obtain the matching md shown in figure d the v u a augmenting path b augmenting path c augmenting path d maximum matching figure augmenting paths and matching augmentations matching md is not only a maximum matching but also perfect ie a matching that matches all the vertices of the graph before we discuss an algorithm for finding an augmenting path let us settle the issue of what nonexistence of such a path means according to the theorem discovered by the french mathematician claude berge it means the current matching is maximal theorem a matching m is a maximum matching if and only if there exists no augmenting path with respect to m proof if an augmenting path with respect to a matching m exists then the size of the matching can be increased by augmentation let us prove the more difficult part if no augmenting path with respect to a matching m exists then the matching is a maximum matching assume that on the contrary this is not the case for a certain matching m in a graph g let m be a maximum matching in g by our assumption the number of edges in m is at least one more than the number of edges in m ie m m consider the edges in the symmetric difference m m m m m m the set of all the edges that are either in m or in m but not in both note that m m m m because m m by assumption let g be the subgraph of g made up of all the edges in m m and their endpoints by definition of a matching any vertex in g g can be incident to no more than one edge in m and no more than one edge in m hence each of the vertices in g has degree or less and therefore every connected component of g is either a path or an evenlength cycle of alternating edges from m m and m m since m m m m and the number of edges from m m and m m is the same for any evenlength cycle of alternating edges in g there must exist at least one path of alternating edges that starts and ends with an edge from m m hence this is an augmenting path for the matching m which contradicts the assumption that no such path exists our discussion of augmenting paths leads to the following general method for constructing a maximum matching in a bipartite graph start with some initial matching eg the empty set find an augmenting path and augment the current matching along this path when no augmenting path can be found terminate the algorithm and return the last matching which is maximum we now give a specific algorithm implementing this general template we will search for an augmenting path for a matching m by a bfslike traversal of the graph that starts simultaneously at all the free vertices in one of the sets v and u say v it would be logical to select the smaller of the two vertex sets but we will ignore this observation in the pseudocode below recall that an augmenting path if it exists is an oddlength path that connects a free vertex in v with a free vertex in u and which unless it consists of a single edge zigs from a vertex in v to another vertex mate in u then zags back to v along the uniquely defined edge from m and so on until a free vertex in u is reached draw augmenting paths for the matchings in figure for example hence any candidate to be such a path must have its edges alternate in the pattern just described this motivates the following rules for labeling vertices during the bfslike traversal of the graph case the queues front vertex w is in v if u is a free vertex adjacent to w it is used as the other endpoint of an augmenting path so the labeling stops and augmentation of the matching commences the augmenting path in question is obtained by moving backward along the vertex labels see below to alternately add and delete its edges to and from the current matching if u is not free and connected to w by an edge not in m label u with w unless it has been already labeled case the front vertex w is in u in this case w must be matched and we label its mate in v with w here is pseudocode of the algorithm in its entirety algorithm maximumbipartitematchingg finds a maximum matching in a bipartite graph by a bfslike traversal input a bipartite graph g v u e output a maximumcardinality matching m in the input graph initialize set m of edges with some valid matching eg the empty set initialize queue q with all the free vertices in v in any order while not emptyq do w frontq dequeueq if w v for every vertex u adjacent to w do if u is free augment m m w u vw while v is labeled do u vertex indicated by vs label m m v u v vertex indicated by us label m m v u remove all vertex labels reinitialize q with all free vertices in v break exit the for loop else u is matched if w u m and u is unlabeled label u with w enqueueq u else w u and matched label the mate v of w with w enqueueq v return m current matching is maximum an application of this algorithm to the matching in figure a is shown in figure note that the algorithm finds a maximum matching that differs from the one in figure d v u queue queue augment from queue queue augment from queue queue augment from queue empty maximum matching figure application of the maximumcardinalitymatching algorithm the left column shows a current matching and initialized queue at the next iterations start the right column shows the vertex labeling generated by the algorithm before augmentation is performed matching edges are shown in bold vertex labels indicate the vertices from which the labeling is done the discovered endpoint of an augmenting path is shaded and labeled for clarity vertices deleted from the queue are indicated by how efficient is the maximummatching algorithm each iteration except the last one matches two previously free vertices one from each of the sets v and u therefore the total number of iterations can not exceed n where n v u is the number of vertices in the graph the time spent on each iteration is in on m where m e is the number of edges in the graph this assumes that the information about the status of each vertex free or matched and the vertex mate if the latter can be retrieved in constant time eg by storing it in an array hence the time efficiency of the algorithm is in onn m hopcroft and karp hop showed how the efficiency can be improved to o nn m by combining several iterations into a single stage to maximize the number of edges added to the matching with one search we were concerned in this section with matching the largest possible number of vertex pairs in a bipartite graph some applications may require taking into account the quality or cost of matching different pairs for example workers may execute jobs with different efficiencies or girls may have different preferences for their potential dance partners it is natural to model such situations by bipartite graphs with weights assigned to their edges this leads to the problem of maximizing the sum of the weights on edges connecting matched pairs of vertices this problem is called maximumweight matching we encountered it under a different name the assignment problem in section there are several sophisticated algorithms for this problem which are much more efficient than exhaustive search see eg pap gal ahu we have to leave them outside of our discussion however because of their complexity especially for general graphs exercises for each matching shown below in bold find an augmentation or explain why no augmentation exists a b apply the maximummatching algorithm to the following bipartite graph a what is the largest and what is the smallest possible cardinality of a matching in a bipartite graph g v u e with n vertices in each vertex set v and u and at least n edges b what is the largest and what is the smallest number of distinct solutions the maximumcardinalitymatching problem can have for a bipartite graph g v u e with n vertices in each vertex set v and u and at least n edges a halls marriage theorem asserts that a bipartite graph g v u e has a matching that matches all vertices of the set v if and only if for each subset s v rs s where rs is the set of all vertices adjacent to a vertex in s check this property for the following graph with i v and ii v b you have to devise an algorithm that returns yes if there is a matching in a bipartite graph g v u e that matches all vertices in v and returns no otherwise would you base your algorithm on checking the condition of halls marriage theorem suppose there are five committees a b c d and e composed of six persons a b c d e and f as follows committee as members are b and e committee bs members are b d and e committee cs members are a c d e and f committee ds members are b d and e committee es members are b and e is there a system of distinct representatives ie is it possible to select a representative from each committee so that all the selected persons are distinct show how the maximumcardinalitymatching problem for a bipartite graph can be reduced to the maximumflow problem discussed in section consider the following greedy algorithm for finding a maximum matching in a bipartite graph g v u e sort all the vertices in nondecreasing order of their degrees scan this sorted list to add to the current matching initially empty the edge from the lists free vertex to an adjacent free vertex of the lowest degree if the lists vertex is matched or if there are no adjacent free vertices for it the vertex is simply skipped does this algorithm always produce a maximum matching in a bipartite graph design a lineartime algorithm for finding a maximum matching in a tree implement the maximummatching algorithm of this section in the language of your choice experiment with its performance on bipartite graphs with n vertices in each of the vertex sets and randomly generated edges in both dense and sparse modes to compare the observed running time with the algorithms theoretical efficiency domino puzzle a domino is a tile that can be oriented either horizontally or vertically a tiling of a given board composed of squares is covering it with dominoes exactly and without overlap is it possible to tile with dominoes an board without two unit squares at its diagonally opposite corners the stable marriage problem in this section we consider an interesting version of bipartite matching called the stable marriage problem consider a set y m m mn of n men and a set x w w wn of n women each man has a preference list ordering the women as potential marriage partners with no ties allowed similarly each woman has a preference list of the men also with no ties examples of these two sets of lists are given in figures a and b the same information can also be presented by an n n ranking matrix see figure c the rows and columns of the matrix represent the men and women of the two sets respectively a cell in row m and column w contains two rankings the first is the position ranking of w in the ms preference list the second is the position ranking of m in the ws preference list for example the pair in jims row and anns column in the matrix in figure c indicates that ann is jims third choice while jim is anns first which of these two ways to represent such information is better depends on the task at hand for example it is easier to specify a match of the sets elements by using the ranking matrix whereas the preference lists might be a more efficient data structure for implementing a matching algorithm a marriage matching m is a set of n m w pairs whose members are selected from disjoint nelement sets y and x in a oneone fashion ie each man m from y is paired with exactly one woman w from x and vice versa if we represent y and x as vertices of a complete bipartite graph with edges connecting possible marriage partners then a marriage matching is a perfect matching in such a graph mens preferences womens preferences ranking matrix st nd rd st nd rd ann lea sue bob lea ann sue ann jim tom bob bob jim lea sue ann lea tom bob jim jim tom sue lea ann sue jim tom bob tom a b c figure data for an instance of the stable marriage problem a mens preference lists b womens preference lists c ranking matrix with the boxed cells composing an unstable matching a pair m w where m y w x is said to be a blocking pair for a marriage matching m if man m and woman w are not matched in m but they prefer each other to their mates in m for example bob lea is a blocking pair for the marriage matching m bob ann jim lea tom sue figure c because they are not matched in m while bob prefers lea to ann and lea prefers bob to jim a marriage matching m is called stable if there is no blocking pair for it otherwise m is called unstable according to this definition the marriage matching in figure c is unstable because bob and lea can drop their designated mates to join in a union they both prefer the stable marriage problem is to find a stable marriage matching for mens and womens given preferences surprisingly this problem always has a solution can you find it for the instance in figure it can be found by the following algorithm stable marriage algorithm input a set of n men and a set of n women along with rankings of the women by each man and rankings of the men by each woman with no ties allowed in the rankings output a stable marriage matching step start with all the men and women being free step while there are free men arbitrarily select one of them and do the following proposal the selected free man m proposes to w the next woman on his preference list who is the highestranked woman who has not rejected him before response if w is free she accepts the proposal to be matched with m if she is not free she compares m with her current mate if she prefers m to him she accepts ms proposal making her former mate free otherwise she simply rejects ms proposal leaving m free step return the set of n matched pairs before we analyze this algorithm it is useful to trace it on some input such an example is presented in figure let us discuss properties of the stable marriage algorithm theorem the stable marriage algorithm terminates after no more than n iterations with a stable marriage output proof the algorithm starts with n men having the total of n women on their ranking lists on each iteration one man makes a proposal to a woman this reduces the total number of women to whom the men can still propose in the future because no man proposes to the same woman more than once hence the algorithm must stop after no more than n iterations ann lea sue free men bob bob proposed to lea bob jim tom jim lea accepted tom ann lea sue free men bob jim proposed to lea jim tom jim lea rejected tom ann lea sue free men bob jim proposed to sue jim tom jim sue accepted tom ann lea sue free men bob tom proposed to sue tom jim sue rejected tom ann lea sue free men bob tom proposed to lea tom jim lea replaced bob with tom tom ann lea sue free men bob bob proposed to ann bob jim ann accepted tom figure application of the stable marriage algorithm an accepted proposal is indicated by a boxed cell a rejected proposal is shown by an underlined cell let us now prove that the final matching m is a stable marriage matching since the algorithm stops after all the n men are oneone matched to the n women the only thing that needs to be proved is the stability of m suppose on the contrary that m is unstable then there exists a blocking pair of a man m and a woman w who are unmatched in m and such that both m and w prefer each other to the persons they are matched with in m since m proposes to every woman on his ranking list in decreasing order of preference and w precedes ms match in m m must have proposed to w on some iteration whether w refused ms proposal or accepted it but replaced him on a subsequent iteration with a higherranked match ws mate in m must be higher on ws preference list than m because the rankings of the men matched to a given woman may only improve on each iteration of the algorithm this contradicts the assumption that w prefers m to her final match in m the stable marriage algorithm has a notable shortcoming it is not gender neutral in the form presented above it favors mens preferences over womens preferences we can easily see this by tracing the algorithm on the following instance of the problem woman woman man man the algorithm obviously yields the stable matching m man woman man woman in this matching both men are matched to their first choices which is not the case for the women one can prove that the algorithm always yields a stable matching that is manoptimal it assigns to each man the highestranked woman possible under any stable marriage of course this gender bias can be reversed but not eliminated by reversing the roles played by men and women in the algorithm ie by making women propose and men accept or reject their proposals there is another important corollary to the fact that the stable marriage algorithm always yields a genderoptimal stable matching it is easy to prove that a man womanoptimal matching is unique for a given set of participant preferences therefore the algorithms output does not depend on the order in which the free men women make their proposals consequently we can use any data structure we might prefer eg a queue or a stack for representing this set with no impact on the algorithms outcome the notion of the stable matching as well as the algorithm discussed above was introduced by d gale and l s shapley in the paper titled college admissions and the stability of marriage gal i do not know which of the two applications mentioned in the title you would consider more important the point is that stability is a matching property that can be desirable in a variety of applications for example it has been used for many years in the united states for matching medicalschool graduates with hospitals for residency training for a brief history of this application and an indepth discussion of the stable marriage problem and its extensions see the monograph by gusfield and irwing gus exercises consider an instance of the stable marriage problem given by the following ranking matrix a b c for each of its marriage matchings indicate whether it is stable or not for the unstable matchings specify a blocking pair for the stable matchings indicate whether they are manoptimal womanoptimal or neither assume that the greek and roman letters denote the men and women respectively design a simple algorithm for checking whether a given marriage matching is stable and determine its time efficiency class find a stable marriage matching for the instance given in problem by applying the stable marriage algorithm a in its menproposing version b in its womenproposing version find a stable marriage matching for the instance defined by the following ranking matrix a b c d determine the timeefficiency class of the stable marriage algorithm a in the worst case b in the best case prove that a manoptimal stable marriage set is always unique is it also true for a womanoptimal stable marriage matching prove that in the manoptimal stable matching each woman has the worst partner that she can have in any stable marriage matching implement the stablemarriage algorithm given in section so that its running time is in on run an experiment to ascertain its averagecase efficiency write a report on the college admission problem residentshospitals assignment that generalizes the stable marriage problem in that a college can accept proposals from more than one applicant consider the problem of the roommates which is related to but more difficult than the stable marriage problem an even number of boys wish to divide up into pairs of roommates a set of pairings is called stable if under it there are no two boys who are not roommates and who prefer each other to their actual roommates gal give an instance of this problem that does not have a stable pairing summary the iterativeimprovement technique involves finding a solution to an optimization problem by generating a sequence of feasible solutions with improving values of the problems objective function each subsequent solution in such a sequence typically involves a small localized change in the previous feasible solution when no such change improves the value of the objective function the algorithm returns the last feasible solution as optimal and stops important problems that can be solved exactly by iterativeimprovement algorithms include linear programming maximizing the flow in a network and matching the maximum possible number of vertices in a graph the simplex method is the classic method for solving the general linear programming problem it works by generating a sequence of adjacent extreme points of the problems feasible region with improving values of the objective function the maximumflow problem asks to find the maximum flow possible in a network a weighted directed graph with a source and a sink the fordfulkerson method is a classic template for solving the maximumflow problem by the iterativeimprovement approach the shortestaugmentingpath method implements this idea by labeling network vertices in the breadthfirst search manner the fordfulkerson method also finds a minimum cut in a given network a maximum cardinality matching is the largest subset of edges in a graph such that no two edges share the same vertex for a bipartite graph it can be found by a sequence of augmentations of previously obtained matchings the stable marriage problem is to find a stable matching for elements of two nelement sets based on given matching preferences this problem always has a solution that can be found by the galeshapley algorithm limitations of algorithm power intellect distinguishes between the possible and the impossible reason distinguishes between the sensible and the senseless even the possible can be senseless max born my life and my views in the preceding chapters of this book we encountered dozens of algorithms for solving a variety of different problems a fair assessment of algorithms as problemsolving tools is inescapable they are very powerful instruments especially when they are executed by modern computers but the power of algorithms is not unlimited and its limits are the subject of this chapter as we shall see some problems can not be solved by any algorithm other problems can be solved algorithmically but not in polynomial time and even when a problem can be solved in polynomial time by some algorithms there are usually lower bounds on their efficiency we start in section with methods for obtaining lower bounds which are estimates on a minimum amount of work needed to solve a problem in general obtaining a nontrivial lower bound even for a simplesounding problem is a very difficult task as opposed to ascertaining the efficiency of a particular algorithm the task here is to establish a limit on the efficiency of any algorithm known or unknown this also necessitates a careful description of the operations such algorithms are allowed to perform if we fail to define carefully the rules of the game so to speak our claims may end up in the large dustbin of impossibilityrelated statements as for example the one made by the celebrated british physicist lord kelvin in heavierthanair flying machines are impossible section discusses decision trees this technique allows us among other applications to establish lower bounds on the efficiency of comparisonbased algorithms for sorting and for searching in sorted arrays as a result we will be able to answer such questions as whether it is possible to invent a faster sorting algorithm than mergesort and whether binary search is the fastest algorithm for searching in a sorted array what does your intuition tell you the answers to these questions will turn out to be incidentally decision trees are also a great vehicle for directing us to a solution of some puzzles such as the coinweighing problem discussed in section section deals with the question of intractability which problems can and can not be solved in polynomial time this welldeveloped area of theoretical computer science is called computational complexity theory we present the basic elements of this theory and discuss informally such fundamental notions as p np and npcomplete problems including the most important unresolved question of theoretical computer science about the relationship between p and np problems the last section of this chapter deals with numerical analysis this branch of computer science concerns algorithms for solving problems of continuous mathematics solving equations and systems of equations evaluating such functions as sin x and ln x computing integrals and so on the nature of such problems imposes two types of limitations first most can not be solved exactly second solving them even approximately requires dealing with numbers that can be represented in a digital computer with only a limited level of precision manipulating approximate numbers without proper care can lead to very inaccurate results we will see that even solving a basic quadratic equation on a computer poses significant difficulties that require a modification of the canonical formula for the equations roots lowerbound arguments we can look at the efficiency of an algorithm two ways we can establish its asymptotic efficiency class say for the worst case and see where this class stands with respect to the hierarchy of efficiency classes outlined in section for example selection sort whose efficiency is quadratic is a reasonably fast algorithm whereas the algorithm for the tower of hanoi problem is very slow because its efficiency is exponential we can argue however that this comparison is akin to the proverbial comparison of apples to oranges because these two algorithms solve different problems the alternative and possibly fairer approach is to ask how efficient a particular algorithm is with respect to other algorithms for the same problem seen in this light selection sort has to be considered slow because there are on log n sorting algorithms the tower of hanoi algorithm on the other hand turns out to be the fastest possible for the problem it solves when we want to ascertain the efficiency of an algorithm with respect to other algorithms for the same problem it is desirable to know the best possible efficiency any algorithm solving the problem may have knowing such a lower bound can tell us how much improvement we can hope to achieve in our quest for a better algorithm for the problem in question if such a bound is tight ie we already know an algorithm in the same efficiency class as the lower bound we can hope for a constantfactor improvement at best if there is a gap between the efficiency of the fastest algorithm and the best lower bound known the door for possible improvement remains open either a faster algorithm matching the lower bound could exist or a better lower bound could be proved in this section we present several methods for establishing lower bounds and illustrate them with specific examples as we did in analyzing the efficiency of specific algorithms in the preceding chapters we should distinguish between a lowerbound class and a minimum number of times a particular operation needs to be executed as a rule the second problem is more difficult than the first for example we can immediately conclude that any algorithm for finding the median of n numbers must be in n why but it is not simple at all to prove that any comparisonbased algorithm for this problem must do at least n comparisons in the worst case for odd n trivial lower bounds the simplest method of obtaining a lowerbound class is based on counting the number of items in the problems input that must be processed and the number of output items that need to be produced since any algorithm must at least read all the items it needs to process and write all its outputs such a count yields a trivial lower bound for example any algorithm for generating all permutations of n distinct items must be in n because the size of the output is n and this bound is tight because good algorithms for generating permutations spend a constant time on each of them except the initial one see section as another example consider the problem of evaluating a polynomial of degree n px anxn anxn a at a given point x given its coefficients an an a it is easy to see that all the coefficients have to be processed by any polynomialevaluation algorithm indeed if it were not the case we could change the value of an unprocessed coefficient which would change the value of the polynomial at a nonzero point x this means that any such algorithm must be in n this lower bound is tight because both the righttoleft evaluation algorithm problem in exercises and horners rule section are both linear in a similar vein a trivial lower bound for computing the product of two n n matrices is n because any such algorithm has to process n elements in the input matrices and generate n elements of the product it is still unknown however whether this bound is tight trivial lower bounds are often too low to be useful for example the trivial bound for the traveling salesman problem is n because its input is nn intercity distances and its output is a list of n cities making up an optimal tour but this bound is all but useless because there is no known algorithm with the running time being a polynomial function of any degree there is another obstacle to deriving a meaningful lower bound by this method it lies in determining which part of an input must be processed by any algorithm solving the problem in question for example searching for an element of a given value in a sorted array does not require processing all its elements why as another example consider the problem of determining connectivity of an undirected graph defined by its adjacency matrix it is plausible to expect that any such algorithm would have to check the existence of each of the nn potential edges but the proof of this fact is not trivial informationtheoretic arguments while the approach outlined above takes into account the size of a problems output the informationtheoretical approach seeks to establish a lower bound based on the amount of information it has to produce consider as an example the wellknown game of deducing a positive integer between and n selected by somebody by asking that person questions with yesno answers the amount of uncertainty that any algorithm solving this problem has to resolve can be measured by log n the number of bits needed to specify a particular number among the n possibilities we can think of each question or to be more accurate an answer to each question as yielding at most bit of information about the algorithms output ie the selected number consequently any such algorithm will need at least log n such steps before it can determine its output in the worst case the approach we just exploited is called the informationtheoretic argument because of its connection to information theory it has proved to be quite useful for finding the socalled informationtheoretic lower bounds for many problems involving comparisons including sorting and searching its underlying idea can be realized much more precisely through the mechanism of decision trees because of the importance of this technique we discuss it separately and in more detail in section adversary arguments let us revisit the same game of guessing a number used to introduce the idea of an informationtheoretic argument we can prove that any algorithm that solves this problem must ask at least log n questions in its worst case by playing the role of a hostile adversary who wants to make an algorithm ask as many questions as possible the adversary starts by considering each of the numbers between and n as being potentially selected this is cheating of course as far as the game is concerned but not as a way to prove our assertion after each question the adversary gives an answer that leaves him with the largest set of numbers consistent with this and all the previously given answers this strategy leaves him with at least onehalf of the numbers he had before his last answer if an algorithm stops before the size of the set is reduced to the adversary can exhibit a number that could be a legitimate input the algorithm failed to identify it is a simple technical matter now to show that one needs log n iterations to shrink an nelement set to a oneelement set by halving and rounding up the size of the remaining set hence at least log n questions need to be asked by any algorithm in the worst case this example illustrates the adversary method for establishing lower bounds it is based on following the logic of a malevolent but honest adversary the malevolence makes him push the algorithm down the most timeconsuming path and his honesty forces him to stay consistent with the choices already made a lower bound is then obtained by measuring the amount of work needed to shrink a set of potential inputs to a single input along the most timeconsuming path as another example consider the problem of merging two sorted lists of size n a a an and b b bn into a single sorted list of size n for simplicity we assume that all the as and bs are distinct which gives the problem a unique solution we encountered this problem when discussing mergesort in section recall that we did merging by repeatedly comparing the first elements in the remaining lists and outputting the smaller among them the number of key comparisons in the worst case for this algorithm for merging is n is there an algorithm that can do merging faster the answer turns out to be no knuth knuiii p quotes the following adversary method for proving that n is a lower bound on the number of key comparisons made by any comparisonbased algorithm for this problem the adversary will employ the following rule reply true to the comparison ai bj if and only if i j this will force any correct merging algorithm to produce the only combined list consistent with this rule b a b a bn an to produce this combined list any correct algorithm will have to explicitly compare n adjacent pairs of its elements ie b to a a to b and so on if one of these comparisons has not been made eg a has not been compared to b we can transpose these keys to get b b a a bn an which is consistent with all the comparisons made but can not be distinguished from the correct configuration given above hence n is indeed a lower bound for the number of key comparisons needed for any merging algorithm problem reduction we have already encountered the problemreduction approach in section there we discussed getting an algorithm for problem p by reducing it to another problem q solvable with a known algorithm a similar reduction idea can be used for finding a lower bound to show that problem p is at least as hard as another problem q with a known lower bound we need to reduce q to p not p to q in other words we should show that an arbitrary instance of problem q can be transformed in a reasonably efficient fashion to an instance of problem p so any algorithm solving p would solve q as well then a lower bound for q will be a lower bound for p table lists several important problems that are often used for this purpose table problems often used for establishing lower bounds by problem reduction problem lower bound tightness sorting n log n yes searching in a sorted array log n yes element uniqueness problem n log n yes multiplication of ndigit integers n unknown multiplication of n n matrices n unknown we will establish the lower bounds for sorting and searching in the next section the element uniqueness problem asks whether there are duplicates among n given numbers we encountered this problem in sections and the proof of the lower bound for this seemingly simple problem is based on a very sophisticated mathematical analysis that is well beyond the scope of this book see eg pre for a rather elementary exposition as to the last two algebraic problems in table the lower bounds quoted are trivial but whether they can be improved remains unknown as an example of establishing a lower bound by reduction let us consider the euclidean minimum spanning tree problem given n points in the cartesian plane construct a tree of minimum total length whose vertices are the given points as a problem with a known lower bound we use the element uniqueness problem we can transform any set x x xn of n real numbers into a set of n points in the cartesian plane by simply adding as the points y coordinate x x xn let t be a minimum spanning tree found for this set of points since t must contain a shortest edge checking whether t contains a zerolength edge will answer the question about uniqueness of the given numbers this reduction implies that n log n is a lower bound for the euclidean minimum spanning tree problem too since the final results about the complexity of many problems are not known the reduction technique is often used to compare the relative complexity of problems for example the formulas x y x y x y and x x x show that the problems of computing the product of two ndigit integers and squaring an ndigit integer belong to the same complexity class despite the latter being seemingly simpler than the former there are several similar results for matrix operations for example multiplying two symmetric matrices turns out to be in the same complexity class as multiplying two arbitrary square matrices this result is based on the observation that not only is the former problem a special case of the latter one but also that we can reduce the problem of multiplying two arbitrary square matrices of order n say a and b to the problem of multiplying two symmetric matrices x a and y bt at b where at and bt are the transpose matrices of a and b ie at i j aj i and bt i j bj i respectively and stands for the n n matrix whose elements are all zeros indeed xy a bt ab at b at bt from which the needed product ab can be easily extracted true we will have to multiply matrices twice the original size but this is just a minor technical complication with no impact on the complexity classes though such results are interesting we will encounter even more important applications of the reduction approach to comparing problem complexity in section exercises prove that any algorithm solving the alternatingdisk puzzle problem in exercises must make at least nn moves to solve it is this lower bound tight prove that the classic recursive algorithm for the tower of hanoi puzzle section makes the minimum number of disk moves needed to solve the problem find a trivial lowerbound class for each of the following problems and indicate if you can whether this bound is tight a finding the largest element in an array b checking completeness of a graph represented by its adjacency matrix c generating all the subsets of an nelement set d determining whether n given real numbers are all distinct consider the problem of identifying a lighter fake coin among n identicallooking coins with the help of a balance scale can we use the same informationtheoretic argument as the one in the text for the number of questions in the guessing game to conclude that any algorithm for identifying the fake will need at least log n weighings in the worst case prove that any comparisonbased algorithm for finding the largest element of an nelement set of real numbers must make n comparisons in the worst case find a tight lower bound for sorting an array by exchanging its adjacent elements give an adversaryargument proof that the time efficiency of any algorithm that checks connectivity of a graph with n vertices is in n provided the only operation allowed for an algorithm is to inquire about the presence of an edge between two vertices of the graph is this lower bound tight what is the minimum number of comparisons needed for a comparisonbased sorting algorithm to merge any two sorted lists of sizes n and n elements respectively prove the validity of your answer find the product of matrices a and b through a transformation to a product of two symmetric matrices if a and b a can one use this sections formulas that indicate the complexity equivalence of multiplication and squaring of integers to show the complexity equivalence of multiplication and squaring of square matrices b show that multiplication of two matrices of order n can be reduced to squaring a matrix of order n find a tight lowerbound class for the problem of finding two closest numbers among n real numbers x x xn find a tight lowerbound class for the number placement problem problem in exercises decision trees many important algorithms especially those for sorting and searching work by comparing items of their inputs we can study the performance of such algorithms with a device called a decision tree as an example figure presents a decision tree of an algorithm for finding a minimum of three numbers each internal node of a binary decision tree represents a key comparison indicated in the node eg k k the nodes left subtree contains the information about subsequent comparisons made if k k and its right subtree does the same for the case of k k for the sake of simplicity we assume throughout this section that all input items are distinct each leaf represents a possible outcome of the algorithms run on some input of size n note that the number of leaves can be greater than the number of outcomes because for some algorithms the same outcome can be arrived at through a different chain of comparisons this happens to be the case for the decision tree in figure an important point is that the number of leaves must be at least as large as the number of possible outcomes the algorithms work on a particular input of size n can be traced by a path from the root to a leaf in its decision tree and the number of comparisons made by the algorithm on such yes a b no yes a c no yes b c no a c b c figure decision tree for finding a minimum of three numbers a run is equal to the length of this path hence the number of comparisons in the worst case is equal to the height of the algorithms decision tree the central idea behind this model lies in the observation that a tree with a given number of leaves which is dictated by the number of possible outcomes has to be tall enough to have that many leaves specifically it is not difficult to prove that for any binary tree with l leaves and height h h log l indeed a binary tree of height h with the largest number of leaves has all its leaves on the last level why hence the largest number of leaves in such a tree is h in other words h l which immediately implies inequality puts a lower bound on the heights of binary decision trees and hence the worstcase number of comparisons made by any comparisonbased algorithm for the problem in question such a bound is called the informationtheoretic lower bound see section we illustrate this technique below on two important problems sorting and searching in a sorted array decision trees for sorting most sorting algorithms are comparison based ie they work by comparing elements in a list to be sorted by studying properties of decision trees for such algorithms we can derive important lower bounds on their time efficiencies we can interpret an outcome of a sorting algorithm as finding a permutation of the element indices of an input list that puts the lists elements in ascending order consider as an example a threeelement list a b c of orderable items such as real numbers or strings for the outcome a c b obtained by sorting this list see figure the permutation in question is in general the number of possible outcomes for sorting an arbitrary nelement list is equal to n abc yes a b no abc abc yes a c no yes b c no abc cba bac cba yes b c no b a no yes a c no yes b a a bc a cb c ab b ac b ca c ba figure decision tree for the treeelement selection sort a triple above a node indicates the state of the array being sorted note two redundant comparisons b a with a single possible outcome because of the results of some previously made comparisons inequality implies that the height of a binary decision tree for any comparisonbased sorting algorithm and hence the worstcase number of comparisons made by such an algorithm can not be less than log n cworst n log n using stirlings formula for n we get log n log nnen log log log n log n log n n n n e in other words about n log n comparisons are necessary in the worst case to sort an arbitrary nelement list by any comparisonbased sorting algorithm note that mergesort makes about this number of comparisons in its worst case and hence is asymptotically optimal this also implies that the asymptotic lower bound n log n is tight and therefore can not be substantially improved we should point out however that the lower bound of log n can be improved for some values of n for example log but it has been proved that comparisons are necessary and sufficient to sort an array of elements in the worst case we can also use decision trees for analyzing the averagecase efficiencies of comparisonbased sorting algorithms we can compute the average number of comparisons for a particular algorithm as the average depth of its decision trees leaves ie as the average path length from the root to the leaves for example for abc yes a b no abc bac yes b c no yes a c no a bc acb b a c bca yes a c no yes b c no a cb c ab b ca c ba figure decision tree for the threeelement insertion sort the threeelement insertion sort whose decision tree is given in figure this number is under the standard assumption that all n outcomes of sorting are equally likely the following lower bound on the average number of comparisons cavg made by any comparisonbased algorithm in sorting an nelement list has been proved cavgn log n as we saw earlier this lower bound is about n log n you might be surprised that the lower bounds for the average and worst cases are almost identical remember however that these bounds are obtained by maximizing the number of comparisons made in the average and worst cases respectively for a particular sorting algorithm the averagecase efficiency can of course be significantly better than their worstcase efficiency decision trees for searching a sorted array in this section we shall see how decision trees can be used for establishing lower bounds on the number of key comparisons in searching a sorted array of n keys a a an the principal algorithm for this problem is binary search as we saw in section the number of comparisons made by binary search in the worst case cwbsorstn is given by the formula cwbsorst n log n logn a a a a a a a a a a a a a a a a figure ternary decision tree for binary search in a fourelement array we will use decision trees to determine whether this is the smallest possible number of comparisons since we are dealing here with threeway comparisons in which search key k is compared with some element ai to see whether k ai k ai or k ai it is natural to try using ternary decision trees figure presents such a tree for the case of n the internal nodes of that tree indicate the arrays elements being compared with the search key the leaves indicate either a matching element in the case of a successful search or a found interval that the search key belongs to in the case of an unsuccessful search we can represent any algorithm for searching a sorted array by threeway comparisons with a ternary decision tree similar to that in figure for an array of n elements all such decision trees will have n leaves n for successful searches and n for unsuccessful ones since the minimum height h of a ternary tree with l leaves is log l we get the following lower bound on the number of worstcase comparisons cworst n logn this lower bound is smaller than logn the number of worstcase comparisons for binary search at least for large values of n and smaller than or equal to logn for every positive integer n see problem in this sections exercises can we prove a better lower bound or is binary search far from being optimal the answer turns out to be the former to obtain a better lower bound we should consider binary rather than ternary decision trees such as the one in figure internal nodes in such a tree correspond to the same threeway comparisons as before but they also serve as terminal nodes for successful searches leaves therefore represent only unsuccessful searches and there are n of them for searching an nelement array a a a a a a a a a a a a figure binary decision tree for binary search in a fourelement array as comparison of the decision trees in figures and illustrates the binary decision tree is simply the ternary decision tree with all the middle subtrees eliminated applying inequality to such binary decision trees immediately yields cworst n logn this inequality closes the gap between the lower bound and the number of worstcase comparisons made by binary search which is also logn a much more sophisticated analysis see eg knuiii section shows that under the standard assumptions about searches binary search makes the smallest number of comparisons on the average as well the average number of comparisons made by this algorithm turns out to be about log n and logn for successful and unsuccessful searches respectively exercises prove by mathematical induction that a h log l for any binary tree with height h and the number of leaves l b h log l for any ternary tree with height h and the number of leaves l consider the problem of finding the median of a threeelement set a b c of orderable items a what is the informationtheoretic lower bound for comparisonbased algorithms solving this problem b draw a decision tree for an algorithm solving this problem c if the worstcase number of comparisons in your algorithm is greater than the informationtheoretic lower bound do you think an algorithm matching the lower bound exists either find such an algorithm or prove its impossibility draw a decision tree and find the number of key comparisons in the worst and average cases for a the threeelement basic bubble sort b the threeelement enhanced bubble sort which stops if no swaps have been made on its last pass design a comparisonbased algorithm for sorting a fourelement array with the smallest number of element comparisons possible design a comparisonbased algorithm for sorting a fiveelement array with seven comparisons in the worst case draw a binary decision tree for searching a fourelement sorted list by sequential search compare the two lower bounds for searching a sorted array logn and logn to show that a logn logn for every positive integer n b logn logn for every positive integer n n what is the informationtheoretic lower bound for finding the maximum of n numbers by comparisonbased algorithms is this bound tight a tournament tree is a complete binary tree reflecting results of a knockout tournament its leaves represent n players entering the tournament and each internal node represents a winner of a match played by the players represented by the nodes children hence the winner of the tournament is represented by the root of the tree a what is the total number of games played in such a tournament b how many rounds are there in such a tournament c design an efficient algorithm to determine the secondbest player using the information produced by the tournament how many extra games does your algorithm require advanced fakecoin problem there are n coins identical in appearance either all are genuine or exactly one of them is fake it is unknown whether the fake coin is lighter or heavier than the genuine one you have a balance scale with which you can compare any two sets of coins that is by tipping to the left to the right or staying even the balance scale will tell whether the sets weigh the same or which of the sets is heavier than the other but not by how much the problem is to find whether all the coins are genuine and if not to find the fake coin and establish whether it is lighter or heavier than the genuine ones a prove that any algorithm for this problem must make at least logn weighings in the worst case b draw a decision tree for an algorithm that solves the problem for n coins in two weighings c prove that there exists no algorithm that solves the problem for n coins in two weighings d draw a decision tree for an algorithm that solves the problem for n coins in two weighings by using an extra coin known to be genuine e draw a decision tree for an algorithm that solves the classic version of the problem that for n coins in three weighings with no extra coins being used jigsaw puzzle a jigsaw puzzle contains n pieces a section of the puzzle is a set of one or more pieces that have been connected to each other a move consists of connecting two sections what algorithm will minimize the number of moves required to complete the puzzle p np and npcomplete problems in the study of the computational complexity of problems the first concern of both computer scientists and computing professionals is whether a given problem can be solved in polynomial time by some algorithm definition we say that an algorithm solves a problem in polynomial time if its worstcase time efficiency belongs to opn where pn is a polynomial of the problems input size n note that since we are using bigoh notation here problems solvable in say logarithmic time are solvable in polynomial time as well problems that can be solved in polynomial time are called tractable and problems that can not be solved in polynomial time are called intractable there are several reasons for drawing the intractability line in this way first the entries of table and their discussion in section imply that we can not solve arbitrary instances of intractable problems in a reasonable amount of time unless such instances are very small second although there might be a huge difference between the running times in opn for polynomials of drastically different degrees there are very few useful polynomialtime algorithms with the degree of a polynomial higher than three in addition polynomials that bound running times of algorithms do not usually have extremely large coefficients third polynomial functions possess many convenient properties in particular both the sum and composition of two polynomials are always polynomials too fourth the choice of this class has led to a development of an extensive theory called computational complexity which seeks to classify problems according to their inherent difficulty and according to this theory a problems intractability remains the same for all principal models of computations and all reasonable inputencoding schemes for the problem under consideration we just touch on some basic notions and ideas of complexity theory in this section if you are interested in a more formal treatment of this theory you will have no trouble finding a wealth of textbooks devoted to the subject eg sip aro p and np problems most problems discussed in this book can be solved in polynomial time by some algorithm they include computing the product and the greatest common divisor of two integers sorting a list searching for a key in a list or for a pattern in a text string checking connectivity and acyclicity of a graph and finding a minimum spanning tree and shortest paths in a weighted graph you are invited to add more examples to this list informally we can think about problems that can be solved in polynomial time as the set that computer science theoreticians call p a more formal definition includes in p only decision problems which are problems with yesno answers definition class p is a class of decision problems that can be solved in polynomial time by deterministic algorithms this class of problems is called polynomial the restriction of p to decision problems can be justified by the following reasons first it is sensible to exclude problems not solvable in polynomial time because of their exponentially large output such problems do arise naturally eg generating subsets of a given set or all the permutations of n distinct items but it is apparent from the outset that they can not be solved in polynomial time second many important problems that are not decision problems in their most natural formulation can be reduced to a series of decision problems that are easier to study for example instead of asking about the minimum number of colors needed to color the vertices of a graph so that no two adjacent vertices are colored the same color we can ask whether there exists such a coloring of the graphs vertices with no more than m colors for m the latter is called the mcoloring problem the first value of m in this series for which the decision problem of mcoloring has a solution solves the optimization version of the graphcoloring problem as well it is natural to wonder whether every decision problem can be solved in polynomial time the answer to this question turns out to be no in fact some decision problems can not be solved at all by any algorithm such problems are called undecidable as opposed to decidable problems that can be solved by an algorithm a famous example of an undecidable problem was given by alan turing in the problem in question is called the halting problem given a computer program and an input to it determine whether the program will halt on that input or continue working indefinitely on it here is a surprisingly short proof of this remarkable fact by way of contradiction assume that a is an algorithm that solves the halting problem that is for any program p and input i ap i if program p halts on input i if program p does not halt on input i we can consider program p as an input to itself and use the output of algorithm a for pair p p to construct a program q as follows qp halts if ap p ie if program p does not halt on input p does not halt if ap p ie if program p halts on input p then on substituting q for p we obtain qq halts if aq q ie if program q does not halt on input q does not halt if aq q ie if program q halts on input q this is a contradiction because neither of the two outcomes for program q is possible which completes the proof are there decidable but intractable problems yes there are but the number of known examples is surprisingly small especially of those that arise naturally rather than being constructed for the sake of a theoretical argument there are many important problems however for which no polynomialtime algorithm has been found nor has the impossibility of such an algorithm been proved the classic monograph by m garey and d johnson gar contains a list of several hundred such problems from different areas of computer science mathematics and operations research here is just a small sample of some of the bestknown problems that fall into this category hamiltonian circuit problem determine whether a given graph has a hamiltonian circuit a path that starts and ends at the same vertex and passes through all the other vertices exactly once traveling salesman problem find the shortest tour through n cities with known positive integer distances between them find the shortest hamiltonian circuit in a complete graph with positive integer weights this was just one of many breakthrough contributions to theoretical computer science made by the english mathematician and computer science pioneer alan turing in recognition of this the acm the principal society of computing professionals and researchers has named after him an award given for outstanding contributions to theoretical computer science a lecture given on such an occasion by richard karp kar provides an interesting historical account of the development of complexity theory knapsack problem find the most valuable subset of n items of given positive integer weights and values that fit into a knapsack of a given positive integer capacity partition problem given n positive integers determine whether it is possible to partition them into two disjoint subsets with the same sum binpacking problem given n items whose sizes are positive rational numbers not larger than put them into the smallest number of bins of size graphcoloring problem for a given graph find its chromatic number which is the smallest number of colors that need to be assigned to the graphs vertices so that no two adjacent vertices are assigned the same color integer linear programming problem find the maximum or minimum value of a linear function of several integervalued variables subject to a finite set of constraints in the form of linear equalities and inequalities some of these problems are decision problems those that are not have decisionversion counterparts eg the mcoloring problem for the graphcoloring problem what all these problems have in common is an exponential or worse growth of choices as a function of input size from which a solution needs to be found note however that some problems that also fall under this umbrella can be solved in polynomial time for example the eulerian circuit problem the problem of the existence of a cycle that traverses all the edges of a given graph exactly once can be solved in on time by checking in addition to the graphs connectivity whether all the graphs vertices have even degrees this example is particularly striking it is quite counterintuitive to expect that the problem about cycles traversing all the edges exactly once eulerian circuits can be so much easier than the seemingly similar problem about cycles visiting all the vertices exactly once hamiltonian circuits another common feature of a vast majority of decision problems is the fact that although solving such problems can be computationally difficult checking whether a proposed solution actually solves the problem is computationally easy ie it can be done in polynomial time we can think of such a proposed solution as being randomly generated by somebody leaving us with the task of verifying its validity for example it is easy to check whether a proposed list of vertices is a hamiltonian circuit for a given graph with n vertices all we need to check is that the list contains n vertices of the graph in question that the first n vertices are distinct whereas the last one is the same as the first and that every consecutive pair of the lists vertices is connected by an edge this general observation about decision problems has led computer scientists to the notion of a nondeterministic algorithm definition a nondeterministic algorithm is a twostage procedure that takes as its input an instance i of a decision problem and does the following nondeterministic guessing stage an arbitrary string s is generated that can be thought of as a candidate solution to the given instance i but may be complete gibberish as well deterministic verification stage a deterministic algorithm takes both i and s as its input and outputs yes if s represents a solution to instance i if s is not a solution to instance i the algorithm either returns no or is allowed not to halt at all we say that a nondeterministic algorithm solves a decision problem if and only if for every yes instance of the problem it returns yes on some execution in other words we require a nondeterministic algorithm to be capable of guessing a solution at least once and to be able to verify its validity and of course we do not want it to ever output a yes answer on an instance for which the answer should be no finally a nondeterministic algorithm is said to be nondeterministic polynomial if the time efficiency of its verification stage is polynomial now we can define the class of np problems definition class np is the class of decision problems that can be solved by nondeterministic polynomial algorithms this class of problems is called nondeterministic polynomial most decision problems are in np first of all this class includes all the problems in p p np this is true because if a problem is in p we can use the deterministic polynomialtime algorithm that solves it in the verificationstage of a nondeterministic algorithm that simply ignores string s generated in its nondeterministic guessing stage but np also contains the hamiltonian circuit problem the partition problem decision versions of the traveling salesman the knapsack graph coloring and many hundreds of other difficult combinatorial optimization problems cataloged in gar the halting problem on the other hand is among the rare examples of decision problems that are known not to be in np this leads to the most important open question of theoretical computer science is p a proper subset of np or are these two classes in fact the same we can put this symbolically as p np note that p np would imply that each of many hundreds of difficult combinatorial decision problems can be solved by a polynomialtime algorithm although computer scientists have failed to find such algorithms despite their persistent efforts over many years moreover many wellknown decision problems are known to be npcomplete see below which seems to cast more doubts on the possibility that p np np complete problems informally an npcomplete problem is a problem in np that is as difficult as any other problem in this class because by definition any other problem in np can be reduced to it in polynomial time shown symbolically in figure here are more formal definitions of these concepts definition a decision problem d is said to be polynomially reducible to a decision problem d if there exists a function t that transforms instances of d to instances of d such that t maps all yes instances of d to yes instances of d and all no instances of d to no instances of d t is computable by a polynomial time algorithm this definition immediately implies that if a problem d is polynomially reducible to some problem d that can be solved in polynomial time then problem d can also be solved in polynomial time why definition a decision problem d is said to be npcomplete if it belongs to class np every problem in np is polynomially reducible to d the fact that closely related decision problems are polynomially reducible to each other is not very surprising for example let us prove that the hamiltonian circuit problem is polynomially reducible to the decision version of the traveling np problems np complete problem figure notion of an npcomplete problem polynomialtime reductions of np problems to an npcomplete problem are shown by arrows salesman problem the latter can be stated as the existence problem of a hamiltonian circuit not longer than a given positive integer m in a given complete graph with positive integer weights we can map a graph g of a given instance of the hamiltonian circuit problem to a complete weighted graph g representing an instance of the traveling salesman problem by assigning as the weight to each edge in g and adding an edge of weight between any pair of nonadjacent vertices in g as the upper bound m on the hamiltonian circuit length we take m n where n is the number of vertices in g and g obviously this transformation can be done in polynomial time let g be a yes instance of the hamiltonian circuit problem then g has a hamiltonian circuit and its image in g will have length n making the image a yes instance of the decision traveling salesman problem conversely if we have a hamiltonian circuit of the length not larger than n in g then its length must be exactly n why and hence the circuit must be made up of edges present in g making the inverse image of the yes instance of the decision traveling salesman problem be a yes instance of the hamiltonian circuit problem this completes the proof the notion of npcompleteness requires however polynomial reducibility of all problems in np both known and unknown to the problem in question given the bewildering variety of decision problems it is nothing short of amazing that specific examples of npcomplete problems have been actually found nevertheless this mathematical feat was accomplished independently by stephen cook in the united states and leonid levin in the former soviet union in his paper cook coo showed that the socalled cnfsatisfiability problem is npcomplete the cnfsatisfiability problem deals with boolean expressions each boolean expression can be represented in conjunctive normal form such as the following expression involving three boolean variables x x and x and their negations denoted x x and x respectively x x xx xx x x the cnfsatisfiability problem asks whether or not one can assign values true and false to variables of a given boolean expression in its cnf form to make the entire expression true it is easy to see that this can be done for the above formula if x true x true and x false the entire expression is true since the cooklevin discovery of the first known npcomplete problems computer scientists have found many hundreds if not thousands of other examples in particular the wellknown problems or their decision versions mentioned above hamiltonian circuit traveling salesman partition bin packing and graph coloring are all npcomplete it is known however that if p np there must exist np problems that neither are in p nor are npcomplete as it often happens in the history of science breakthrough discoveries are made independently and almost simultaneously by several scientists in fact levin introduced a more general notion than npcompleteness which was not limited to decision problems but his paper lev was published two years after cooks for a while the leading candidate to be such an example was the problem of determining whether a given integer is prime or composite but in an important theoretical breakthrough professor manindra agrawal and his students neeraj kayal and nitin saxena of the indian institute of technology in kanpur announced in a discovery of a deterministic polynomialtime algorithm for primality testing agr their algorithm does not solve however the related problem of factoring large composite integers which lies at the heart of the widely used encryption method called the rsa algorithm riv showing that a decision problem is npcomplete can be done in two steps first one needs to show that the problem in question is in np ie a randomly generated string can be checked in polynomial time to determine whether or not it represents a solution to the problem typically this step is easy the second step is to show that every problem in np is reducible to the problem in question in polynomial time because of the transitivity of polynomial reduction this step can be done by showing that a known npcomplete problem can be transformed to the problem in question in polynomial time see figure although such a transformation may need to be quite ingenious it is incomparably simpler than proving the existence of a transformation for every problem in np for example if we already know that the hamiltonian circuit problem is npcomplete its polynomial reducibility to the decision traveling salesman problem implies that the latter is also npcomplete after an easy check that the decision traveling salesman problem is in class np the definition of npcompleteness immediately implies that if there exists a deterministic polynomialtime algorithm for just one npcomplete problem then every problem in np can be solved in polynomial time by a deterministic algorithm and hence p np in other words finding a polynomialtime algorithm np problems known np complete problem candidate for np completeness figure proving npcompleteness by reduction for one npcomplete problem would mean that there is no qualitative difference between the complexity of checking a proposed solution and finding it in polynomial time for the vast majority of decision problems of all kinds such implications make most computer scientists believe that p np although nobody has been successful so far in finding a mathematical proof of this intriguing conjecture surprisingly in interviews with the authors of a book about the lives and discoveries of prominent computer scientists sha cook seemed to be uncertain about the eventual resolution of this dilemma whereas levin contended that we should expect the p np outcome whatever the eventual answer to the p np question proves to be knowing that a problem is npcomplete has important practical implications for today it means that faced with a problem known to be npcomplete we should probably not aim at gaining fame and fortune by designing a polynomialtime algorithm for solving all its instances rather we should concentrate on several approaches that seek to alleviate the intractability of such problems these approaches are outlined in the next chapter of the book exercises a game of chess can be posed as the following decision problem given a legal positioning of chess pieces and information about which side is to move determine whether that side can win is this decision problem decidable a certain problem can be solved by an algorithm whose running time is in onlog n which of the following assertions is true a the problem is tractable b the problem is intractable c impossible to tell give examples of the following graphs or explain why such examples can not exist a graph with a hamiltonian circuit but without an eulerian circuit b graph with an eulerian circuit but without a hamiltonian circuit c graph with both a hamiltonian circuit and an eulerian circuit d graph with a cycle that includes all the vertices but with neither a hamiltonian circuit nor an eulerian circuit in the clay mathematics institute cmi of cambridge massachusetts designated a million prize for the solution to this problem for each of the following graphs find its chromatic number a a e b a c a f e b d b f b g d c e c g c h d h design a polynomialtime algorithm for the graph coloring problem determine whether vertices of a given graph can be colored in no more than two colors so that no two adjacent vertices are colored the same color consider the following bruteforce algorithm for solving the composite number problem check successive integers from to n as possible divisors of n if one of them divides n evenly return yes ie the number is composite if none of them does return no why does this algorithm not put the problem in class p state the decision version for each of the following problems and outline a polynomialtime algorithm that verifies whether or not a proposed solution solves the problem you may assume that a proposed solution represents a legitimate input to your verification algorithm a knapsack problem b bin packing problem show that the partition problem is polynomially reducible to the decision version of the knapsack problem show that the following three problems are polynomially reducible to each other i determine for a given graph g v e and a positive integer m v whether g contains a clique of size m or more a clique of size k in a graph is its complete subgraph of k vertices ii determine for a given graph g v e and a positive integer m v whether there is a vertex cover of size m or less for g a vertex cover of size k for a graph g v e is a subset v v such that v k and for each edge u v e at least one of u and v belongs to v iii determine for a given graph g v e and a positive integer m v whether g contains an independent set of size m or more an independent set of size k for a graph g v e is a subset v v such that v k and for all u v v vertices u and v are not adjacent in g determine whether the following problem is npcomplete given several sequences of uppercase and lowercase letters is it possible to select a letter from each sequence without selecting both the upperand lowercase versions of any letter for example if the sequences are abc bc ab and ac it is possible to choose a from the first sequence b from the second and third and c from the fourth an example where there is no way to make the required selections is given by the four sequences ab ab ab and ab kar which of the following diagrams do not contradict the current state of our knowledge about the complexity classes p np and npc npcomplete problems a b p np p np npc npc c np d np p npc p npc e np p npc king arthur expects knights for an annual dinner at camelot unfortunately some of the knights quarrel with each other and arthur knows who quarrels with whom arthur wants to seat his guests around a table so that no two quarreling knights sit next to each other a which standard problem can be used to model king arthurs task b as a research project find a proof that arthurs problem has a solution if each knight does not quarrel with at least other knights fundamentals of the analysis of algorithm efficiency i often say that when you can measure what you are speaking about and express it in numbers you know something about it but when you can not express it in numbers your knowledge is a meagre and unsatisfactory kind it may be the beginning of knowledge but you have scarcely in your thoughts advanced to the stage of science whatever the matter may be lord kelvin not everything that can be counted counts and not everything that counts can be counted albert einstein this chapter is devoted to analysis of algorithms the american heritage dictionary defines analysis as the separation of an intellectual or substantial whole into its constituent parts for individual study accordingly each of the principal dimensions of an algorithm pointed out in section is both a legitimate and desirable subject of study but the term analysis of algorithms is usually used in a narrower technical sense to mean an investigation of an algorithms efficiency with respect to two resources running time and memory space this emphasis on efficiency is easy to explain first unlike such dimensions as simplicity and generality efficiency can be studied in precise quantitative terms second one can argue although this is hardly always the case given the speed and memory of todays computers that the efficiency considerations are of primary importance from a practical point of view in this chapter we too will limit the discussion to an algorithms efficiency we start with a general framework for analyzing algorithm efficiency in section this section is arguably the most important in the chapter the fundamental nature of the topic makes it also one of the most important sections in the entire book in section we introduce three notations o big oh big omega and big theta borrowed from mathematics these notations have become the language for discussing the efficiency of algorithms in section we show how the general framework outlined in section can be systematically applied to analyzing the efficiency of nonrecursive algorithms the main tool of such an analysis is setting up a sum representing the algorithms running time and then simplifying the sum by using standard sum manipulation techniques in section we show how the general framework outlined in section can be systematically applied to analyzing the efficiency of recursive algorithms here the main tool is not a summation but a special kind of equation called a recurrence relation we explain how such recurrence relations can be set up and then introduce a method for solving them although we illustrate the analysis framework and the methods of its applications by a variety of examples in the first four sections of this chapter section is devoted to yet another example that of the fibonacci numbers discovered years ago this remarkable sequence appears in a variety of applications both within and outside computer science a discussion of the fibonacci sequence serves as a natural vehicle for introducing an important class of recurrence relations not solvable by the method of section we also discuss several algorithms for computing the fibonacci numbers mostly for the sake of a few general observations about the efficiency of algorithms and methods of analyzing them the methods of sections and provide a powerful technique for analyzing the efficiency of many algorithms with mathematical clarity and precision but these methods are far from being foolproof the last two sections of the chapter deal with two approaches empirical analysis and algorithm visualization that complement the pure mathematical techniques of sections and much newer and hence less developed than their mathematical counterparts these approaches promise to play an important role among the tools available for analysis of algorithm efficiency challenges of numerical algorithms numerical analysis is usually described as the branch of computer science concerned with algorithms for solving mathematical problems this description needs an important clarification the problems in question are problems of continuous mathematics solving equations and systems of equations evaluating such functions as sin x and ln x computing integrals and so on as opposed to problems of discrete mathematics dealing with such structures as graphs trees permutations and combinations our interest in efficient algorithms for mathematical problems stems from the fact that these problems arise as models of many reallife phenomena both in the natural world and in the social sciences in fact numerical analysis used to be the main area of research study and application of computer science with the rapid proliferation of computers in business and everydaylife applications which deal primarily with storage and retrieval of information the relative importance of numerical analysis has shrunk in the last years however its applications enhanced by the power of modern computers continue to expand in all areas of fundamental research and technology thus wherever ones interests lie in the wide world of modern computing it is important to have at least some understanding of the special challenges posed by continuous mathematical problems we are not going to discuss the variety of difficulties posed by modeling the task of describing a reallife phenomenon in mathematical terms assuming that this has already been done what principal obstacles to solving a mathematical problem do we face the first major obstacle is the fact that most numerical analysis problems can not be solved exactly they have to be solved approximately and this is usually done by replacing an infinite object by a finite approximation for example the value of ex at a given point x can be computed by approximating its infinite taylors series about x by a finite sum of its first terms called the nthdegree taylor polynomial ex x x xn n to give another example the definite integral of a function can be approximated by a finite weighted sum of its values as in the composite trapezoidal rule that you might remember from your calculus class b h f n f x d x a f xi f b a i where h b an xi a ih for i n figure the errors of such approximations are called truncation errors one of the major tasks in numerical analysis is to estimate the magnitudes of truncation solving a system of linear equations and polynomial evaluation discussed in sections and respectively are rare exceptions to this rule h h h h x a x xi xi xi xn b figure composite trapezoidal rule errors this is typically done by using calculus tools from elementary to quite advanced for example for approximation we have ex x x xn m xn n n where m max e on the segment with the endpoints at and x this formula makes it possible to determine the degree of taylors polynomial needed to guarantee a predefined accuracy level of approximation for example if we want to compute e by formula and guarantee the truncation error to be smaller than we can proceed as follows first we estimate m of formula m max e e using this bound and the desired accuracy level of we obtain from m n n n n to solve the last inequality we can compute the first few values of n n n n to see that the smallest value of n for which this inequality holds is similarly for approximation the standard bound of the truncation error is given by the inequality b h n b ah f xdx f a f xi f b m a i where m max f x on the interval a x b you are asked to use this inequality in the exercises for this section problems and the other type of errors called roundoff errors are caused by the limited accuracy with which we can represent real numbers in a digital computer these errors arise not only for all irrational numbers which by definition require an infinite number of digits for their exact representation but for many rational numbers as well in the overwhelming majority of situations real numbers are represented as floatingpoint numbers dd dp be where b is the number base usually or or for unsophisticated calculators d d dp are digits di b for i p and d unless the number is representing together the fractional part of the number and called its mantissa and e is an integer exponent with the range of values approximately symmetric about the accuracy of the floatingpoint representation depends on the number of significant digits p in representation most computers permit two or even three levels of precision single precision typically equivalent to between and significant decimal digits double precision to significant decimal digits and extended precision to significant decimal digits using higherprecision arithmetic slows computations but may help to overcome some of the problems caused by roundoff errors higher precision may need to be used only for a particular step of the algorithm in question as with an approximation of any kind it is important to distinguish between the absolute error and the relative error of representing a number by its approximation absolute error relative error the relative error is undefined if very large and very small numbers can not be represented in floatingpoint arithmetic because of the phenomena called overflow and underflow respectively an overflow happens when an arithmetic operation yields a result outside the range of the computers floatingpoint numbers typical examples of overflow arise from the multiplication of large numbers or division by a very small number sometimes we can eliminate this problem by making a simple change in the order in which an expression is evaluated eg by replacing an expression with an equal one eg computing not as but as or by computing a logarithm of an expression instead of the expression itself underflow occurs when the result of an operation is a nonzero fraction of such a small magnitude that it can not be represented as a nonzero floatingpoint number usually underflow numbers are replaced by zero but a special signal is generated by hardware to indicate such an event has occurred it is important to remember that in addition to inaccurate representation of numbers the arithmetic operations performed in a computer are not always exact either in particular subtracting two nearly equal floatingpoint numbers may cause a large increase in relative error this phenomenon is called subtractive cancellation example consider two irrational numbers and represented by floatingpoint numbers and respectively the relative errors of these approximations are small and respectively the relative error of representing the difference by the difference of the floatingpoint representations is which is very large for a relative error despite quite accurate approximations for both and note that we may get a significant magnification of roundoff error if a lowaccuracy difference is used as a divisor we already encountered this problem in discussing gaussian elimination in section our solution there was to use partial pivoting many numerical algorithms involve thousands or even millions of arithmetic operations for typical inputs for such algorithms the propagation of roundoff errors becomes a major concern from both the practical and theoretical standpoints for some algorithms roundoff errors can propagate through the algorithms operations with increasing effect this highly undesirable property of a numerical algorithm is called instability some problems exhibit such a high level of sensitivity to changes in their input that it is all but impossible to design a stable algorithm to solve them such problems are called illconditioned example consider the following system of two linear equations in two unknowns x y x y its only solution is x y to see how sensitive this system is to small changes to its righthand side consider the system with the same coefficient matrix but slightly different righthand side values x y x y the only solution to this system is x y which is quite far from the solution to the previous system note that the coefficient matrix of this system is close to being singular why hence a minor change in its coefficients may yield a system with either no solutions or infinitely many solutions depending on its righthandside values you can find a more formal and detailed discussion of how we can measure the degree of illcondition of the coefficient matrix in numerical analysis textbooks eg ger we conclude with a wellknown problem of finding real roots of the quadratic equation ax bx c for any real coefficients a b and c a according to secondaryschool algebra equation has real roots if and only if its discriminant d b ac is nonnegative and these roots can be found by the following formula x b b ac a although formula provides a complete solution to the posed problem as far as a mathematician is concerned it is far from being a complete solution for an algorithm designer the first major obstacle is evaluating the square root even for most positive integers d d is an irrational number that can be computed only approximately there is a method of computing square roots that is much better than the one commonly taught in secondary school it follows from newtons method a very important algorithm for solving equations which we discuss in section this method generates the sequence xn of approximations to d where d is a given nonnegative number according to the formula xn d for n xn xn where the initial approximation x can be chosen among other possibilities as x d it is not difficultto prove that sequence is decreasing if d and always converges to d we can stop generating its elements either when the difference between its two consecutive elements is less than a predefined error tolerance xn xn or when xnis sufficiently close to d approximation sequence converges very fast to d for most values of d in particular one can prove that if d then no more than four iterations are needed to guarantee that xn d and we can always scale a given value of d to one in the interval by the formula d dp where p is an even integer example let us apply newtons algorithm to compute for simplicity we ignore scaling we will round off the numbers to six decimal places and use the standard numerical analysis notation to indicate the roundoffs x x x x x x x x x x x x x at this point we have to stop because x x and hence all other approximations will be the same the exact value of is with the issue of computing square roots squared away i do not know whether or not the pun was intended are we home free to write a program based on formula the answer is no because of the possible impact of roundoff errors among other obstacles we are faced here with the menace of subtractive cancellation if b is much larger than ac b ac will be very close to b and a root computed by formula might have a large relative error example let us follow a paper by george forsythe for and consider the equation x x its true roots to significant digits are x george e forsythe a noted numerical analyst played a leading role in establishing computer science as a separate academic discipline in the united states it is his words that are used as the epigraph to this books preface and x if we use formula and perform all the computations in decimal floatingpoint arithmetic with say seven significant digits we obtain b ac d d b d x a x b d a and although the relative error of approximating x by x is very small for the second root it is very large x x ie x to avoid the possibility of subtractive cancellation in formula we can use instead another formula obtained as follows x b b ac a b b acb b ac ab b ac c b b ac with no danger of subtractive cancellation in the denominator if b as to x it can be computed by the standard formula x b b ac a with no danger of cancellation either for a positive value of b the case of b is symmetric we can use the formulas x b b ac a and x c b b ac the case of b can be considered with either of the other two cases there are several other obstacles to applying formula which are related to limitations of floatingpoint arithmetic if a is very small division by a can cause an overflow there seems to be no way to fight the danger of subtractive cancellation in computing b ac other than calculating it with double precision and so on these problems have been overcome by william kahan of the university of toronto see for and his algorithm is considered to be a significant achievement in the history of numerical analysis hopefully this brief overview has piqued your interest enough for you to seek more information in the many books devoted exclusively to numerical algorithms in this book we discuss one more topic in the next chapter three classic methods for solving equations in one unknown exercises some textbooks define the number of significant digits in the approximation of number by number as the largest nonnegative integer k for which k according to this definition how many significant digits are there in the approximation of by a b if is known to approximate some number with the absolute error not exceeding find a the range of possible values of b the range of the relative errors of these approximations find the approximate value of e obtained by the fifthdegree taylors polynomial about and compute the truncation error of this approximation does the result agree with the theoretical prediction made in the section derive formula of the composite trapezoidal rule use the composite trapezoidal rule with n to approximate the following definite integrals find the truncation error of each approximation and compare it with the one given by formula a xd x b xd x if esin xdx is to be computed by the composite trapezoidal rule how large should the number of subintervals be to guarantee a truncation error smaller than smaller than solve the two systems of linear equations and indicate whether they are illconditioned a x y b x y x y x y write a computer program to solve the equation ax bx c a prove that for any nonnegative number d the sequence of newtons method for computing d is strictly decreasing and converges to d for any value of the initial approximation x d b prove that if d and x d no more than four iterations of newtons method are needed to guarantee that xn d apply four iterations of newtons method to compute and estimate the absolute and relative errors of this approximation summary given a class of algorithms for solving a particular problem a lower bound indicates the best possible efficiency any algorithm from this class can have a trivial lower bound is based on counting the number of items in the problems input that must be processed and the number of output items that need to be produced an informationtheoretic lower bound is usually obtained through a mechanism of decision trees this technique is particularly useful for comparisonbased algorithms for sorting and searching specifically any general comparisonbased sorting algorithm must perform at least log n n log n key comparisons in the worst case any general comparisonbased algorithm for searching a sorted array must perform at least logn key comparisons in the worst case the adversary method for establishing lower bounds is based on following the logic of a malevolent adversary who forces the algorithm into the most timeconsuming path a lower bound can also be established by reduction ie by reducing a problem with a known lower bound to the problem in question complexity theory seeks to classify problems according to their computational complexity the principal split is between tractable and intractable problems problems that can and can not be solved in polynomial time respectively for purely technical reasons complexity theory concentrates on decision problems which are problems with yesno answers the halting problem is an example of an undecidable decision problem ie it can not be solved by any algorithm p is the class of all decision problems that can be solved in polynomial time np is the class of all decision problems whose randomly guessed solutions can be verified in polynomial time many important problems in np such as the hamiltonian circuit problem are known to be npcomplete all other problems in np are reducible to such a problem in polynomial time the first proof of a problems npcompleteness was published by s cook for the cnfsatisfiability problem it is not known whether p np or p is just a proper subset of np this question is the most important unresolved issue in theoretical computer science a discovery of a polynomialtime algorithm for any of the thousands of known npcomplete problems would imply that p np numerical analysis is a branch of computer science dealing with solving continuous mathematical problems two types of errors occur in solving a majority of such problems truncation error and roundoff error truncation errors stem from replacing infinite objects by their finite approximations roundoff errors are due to inaccuracies of representing numbers in a digital computer subtractive cancellation happens as a result of subtracting two nearequal floatingpoint numbers it may lead to a sharp increase in the relative roundoff error and therefore should be avoided by either changing the expressions form or by using a higher precision in computing such a difference writing a general computer program for solving quadratic equations ax bx c is a difficult task the problem of computing square roots can be solved by utilizing newtons method the problem of subtractive cancellation can be dealt with by using different formulas depending on whether coefficient b is positive or negative and by computing the discriminant b ac with double precision the analysis framework in this section we outline a general framework for analyzing the efficiency of algorithms we already mentioned in section that there are two kinds of efficiency time efficiency and space efficiency time efficiency also called time complexity indicates how fast an algorithm in question runs space efficiency also called space complexity refers to the amount of memory units required by the algorithm in addition to the space needed for its input and output in the early days of electronic computing both resources time and space were at a premium half a century of relentless technological innovations have improved the computers speed and memory size by many orders of magnitude now the amount of extra space required by an algorithm is typically not of as much concern with the caveat that there is still of course a difference between the fast main memory the slower secondary memory and the cache the time issue has not diminished quite to the same extent however in addition the research experience has shown that for most problems we can achieve much more spectacular progress in speed than in space therefore following a wellestablished tradition of algorithm textbooks we primarily concentrate on time efficiency but the analytical framework introduced here is applicable to analyzing space efficiency as well measuring an inputs size lets start with the obvious observation that almost all algorithms run longer on larger inputs for example it takes longer to sort larger arrays multiply larger matrices and so on therefore it is logical to investigate an algorithms efficiency as a function of some parameter n indicating the algorithms input size in most cases selecting such a parameter is quite straightforward for example it will be the size of the list for problems of sorting searching finding the lists smallest element and most other problems dealing with lists for the problem of evaluating a polynomial px anxn a of degree n it will be the polynomials degree or the number of its coefficients which is larger by than its degree youll see from the discussion that such a minor difference is inconsequential for the efficiency analysis there are situations of course where the choice of a parameter indicating an input size does matter one such example is computing the product of two n n matrices there are two natural measures of size for this problem the first and more frequently used is the matrix order n but the other natural contender is the total number of elements n in the matrices being multiplied the latter is also more general since it is applicable to matrices that are not necessarily square since there is a simple formula relating these two measures we can easily switch from one to the other but the answer about an algorithms efficiency will be qualitatively different depending on which of these two measures we use see problem in this sections exercises the choice of an appropriate size metric can be influenced by operations of the algorithm in question for example how should we measure an inputs size for a spellchecking algorithm if the algorithm examines individual characters of its input we should measure the size by the number of characters if it works by processing words we should count their number in the input we should make a special note about measuring input size for algorithms solving problems such as checking primality of a positive integer n here the input is just one number and it is this numbers magnitude that determines the input some algorithms require more than one parameter to indicate the size of their inputs eg the number of vertices and the number of edges for algorithms on graphs represented by their adjacency lists size in such situations it is preferable to measure size by the number b of bits in the ns binary representation b log n this metric usually gives a better idea about the efficiency of algorithms in question units for measuring running time the next issue concerns units for measuring an algorithms running time of course we can simply use some standard unit of time measurement a second or millisecond and so on to measure the running time of a program implementing the algorithm there are obvious drawbacks to such an approach however dependence on the speed of a particular computer dependence on the quality of a program implementing the algorithm and of the compiler used in generating the machine code and the difficulty of clocking the actual running time of the program since we are after a measure of an algorithms efficiency we would like to have a metric that does not depend on these extraneous factors one possible approach is to count the number of times each of the algorithms operations is executed this approach is both excessively difficult and as we shall see usually unnecessary the thing to do is to identify the most important operation of the algorithm called the basic operation the operation contributing the most to the total running time and compute the number of times the basic operation is executed as a rule it is not difficult to identify the basic operation of an algorithm it is usually the most timeconsuming operation in the algorithms innermost loop for example most sorting algorithms work by comparing elements keys of a list being sorted with each other for such algorithms the basic operation is a key comparison as another example algorithms for mathematical problems typically involve some or all of the four arithmetical operations addition subtraction multiplication and division of the four the most timeconsuming operation is division followed by multiplication and then addition and subtraction with the last two usually considered together thus the established framework for the analysis of an algorithms time efficiency suggests measuring it by counting the number of times the algorithms basic operation is executed on inputs of size n we will find out how to compute such a count for nonrecursive and recursive algorithms in sections and respectively here is an important application let cop be the execution time of an algorithms basic operation on a particular computer and let cn be the number of times this operation needs to be executed for this algorithm then we can estimate on some computers multiplication does not take longer than additionsubtraction see for example the timing data provided by kernighan and pike in ker pp the running time t n of a program implementing this algorithm on that computer by the formula t n copcn of course this formula should be used with caution the count cn does not contain any information about operations that are not basic and in fact the count itself is often computed only approximately further the constant cop is also an approximation whose reliability is not always easy to assess still unless n is extremely large or very small the formula can give a reasonable estimate of the algorithms running time it also makes it possible to answer such questions as how much faster would this algorithm run on a machine that is times faster than the one we have the answer is obviously times or assuming that cn nn how much longer will the algorithm run if we double its input size the answer is about four times longer indeed for all but very small values of n cn nn n n n and therefore t n copcn n t n copcn n note that we were able to answer the last question without actually knowing the value of cop it was neatly cancelled out in the ratio also note that the multiplicative constant in the formula for the count cn was also cancelled out it is for these reasons that the efficiency analysis framework ignores multiplicative constants and concentrates on the counts order of growth to within a constant multiple for largesize inputs orders of growth why this emphasis on the counts order of growth for large input sizes a difference in running times on small inputs is not what really distinguishes efficient algorithms from inefficient ones when we have to compute for example the greatest common divisor of two small numbers it is not immediately clear how much more efficient euclids algorithm is compared to the other two algorithms discussed in section or even why we should care which of them is faster and by how much it is only when we have to find the greatest common divisor of two large numbers that the difference in algorithm efficiencies becomes both clear and important for large values of n it is the functions order of growth that counts just look at table which contains values of a few functions particularly important for analysis of algorithms the magnitude of the numbers in table has a profound significance for the analysis of algorithms the function growing the slowest among these is the logarithmic function it grows so slowly in fact that we should expect a program table values some approximate of several functions important for analysis of algorithms n log n n n log n n n n n implementing an algorithm with a logarithmic basicoperation count to run practically instantaneously on inputs of all realistic sizes also note that although specific values of such a count depend of course on the logarithms base the formula loga n loga b logb n makes it possible to switch from one base to another leaving the count logarithmic but with a new multiplicative constant this is why we omit a logarithms base and write simply log n in situations where we are interested just in a functions order of growth to within a multiplicative constant on the other end of the spectrum are the exponential function n and the factorial function n both these functions grow so fast that their values become astronomically large even for rather small values of n this is the reason why we did not include their values for n in table for example it would take about years for a computer making a trillion operations per second to execute operations though this is incomparably faster than it would have taken to execute operations it is still longer than billion years the estimated age of the planet earth there is a tremendous difference between the orders of growth of the functions n and n yet both are often referred to as exponentialgrowth functions or simply exponential despite the fact that strictly speaking only the former should be referred to as such the bottom line which is important to remember is this algorithms that require an exponential number of operations are practical for solving only problems of very small sizes another way to appreciate the qualitative difference among the orders of growth of the functions in table is to consider how they react to say a twofold increase in the value of their argument n the function log n increases in value by just because log n log log n log n the linear function increases twofold the linearithmic function n log n increases slightly more than twofold the quadratic function n and cubic function n increase fourfold and eightfold respectively because n n and n n the value of n gets squared because n n and n increases much more than that yes even mathematics refuses to cooperate to give a neat answer for n worstcase bestcase and averagecase efficiencies in the beginning of this section we established that it is reasonable to measure an algorithms efficiency as a function of a parameter indicating the size of the algorithms input but there are many algorithms for which running time depends not only on an input size but also on the specifics of a particular input consider as an example sequential search this is a straightforward algorithm that searches for a given item some search key k in a list of n elements by checking successive elements of the list until either a match with the search key is found or the list is exhausted here is the algorithms pseudocode in which for simplicity a list is implemented as an array it also assumes that the second condition ai k will not be checked if the first one which checks that the arrays index does not exceed its upper bound fails algorithm sequentialsearchan k searches for a given value in a given array by sequential search input an array an and a search key k output the index of the first element in a that matches k or if there are no matching elements i while i n and ai k do i i if i n return i else return clearly the running time of this algorithm can be quite different for the same list size n in the worst case when there are no matching elements or the first matching element happens to be the last one on the list the algorithm makes the largest number of key comparisons among all possible inputs of size n cworst n n the worstcase efficiency of an algorithm is its efficiency for the worstcase input of size n which is an input or inputs of size n for which the algorithm runs the longest among all possible inputs of that size the way to determine the worstcase efficiency of an algorithm is in principle quite straightforward analyze the algorithm to see what kind of inputs yield the largest value of the basic operations count cn among all possible inputs of size n and then compute this worstcase value cworstn for sequential search the answer was obvious the methods for handling less trivial situations are explained in subsequent sections of this chapter clearly the worstcase analysis provides very important information about an algorithms efficiency by bounding its running time from above in other words it guarantees that for any instance of size n the running time will not exceed cworstn its running time on the worstcase inputs the bestcase efficiency of an algorithm is its efficiency for the bestcase input of size n which is an input or inputs of size n for which the algorithm runs the fastest among all possible inputs of that size accordingly we can analyze the bestcase efficiency as follows first we determine the kind of inputs for which the count cn will be the smallest among all possible inputs of size n note that the best case does not mean the smallest input it means the input of size n for which the algorithm runs the fastest then we ascertain the value of cn on these most convenient inputs for example the bestcase inputs for sequential search are lists of size n with their first element equal to a search key accordingly cbestn for this algorithm the analysis of the bestcase efficiency is not nearly as important as that of the worstcase efficiency but it is not completely useless either though we should not expect to get bestcase inputs we might be able to take advantage of the fact that for some algorithms a good bestcase performance extends to some useful types of inputs close to being the bestcase ones for example there is a sorting algorithm insertion sort for which the bestcase inputs are already sorted arrays on which the algorithm works very fast moreover the bestcase efficiency deteriorates only slightly for almostsorted arrays therefore such an algorithm might well be the method of choice for applications dealing with almostsorted arrays and of course if the bestcase efficiency of an algorithm is unsatisfactory we can immediately discard it without further analysis it should be clear from our discussion however that neither the worstcase analysis nor its bestcase counterpart yields the necessary information about an algorithms behavior on a typical or random input this is the information that the averagecase efficiency seeks to provide to analyze the algorithms averagecase efficiency we must make some assumptions about possible inputs of size n lets consider again sequential search the standard assumptions are that a the probability of a successful search is equal to p p and b the probability of the first match occurring in the ith position of the list is the same for every i under these assumptions the validity of which is usually difficult to verify their reasonableness notwithstanding we can find the average number of key comparisons cavgn as follows in the case of a successful search the probability of the first match occurring in the ith position of the list is pn for every i and the number of comparisons made by the algorithm in such a situation is obviously i in the case of an unsuccessful search the number of comparisons will be n with the probability of such a search being p therefore cavgn p p i p n p n p n n n n p i n n p n p nn n p pn n p n this general formula yields some quite reasonable answers for example if p the search must be successful the average number of key comparisons made by sequential search is n that is the algorithm will inspect on average about half of the lists elements if p the search must be unsuccessful the average number of key comparisons will be n because the algorithm will inspect all n elements on all such inputs as you can see from this very elementary example investigation of the averagecase efficiency is considerably more difficult than investigation of the worstcase and bestcase efficiencies the direct approach for doing this involves dividing all instances of size n into several classes so that for each instance of the class the number of times the algorithms basic operation is executed is the same what were these classes for sequential search then a probability distribution of inputs is obtained or assumed so that the expected value of the basic operations count can be found the technical implementation of this plan is rarely easy however and probabilistic assumptions underlying it in each particular case are usually difficult to verify given our quest for simplicity we will mostly quote known results about the averagecase efficiency of algorithms under discussion if you are interested in derivations of these results consult such books as baa sed knui knuii and knuiii it should be clear from the preceding discussion that the averagecase efficiency can not be obtained by taking the average of the worstcase and the bestcase efficiencies even though this average does occasionally coincide with the averagecase cost it is not a legitimate way of performing the averagecase analysis does one really need the averagecase efficiency information the answer is unequivocally yes there are many important algorithms for which the averagecase efficiency is much better than the overly pessimistic worstcase efficiency would lead us to believe so without the averagecase analysis computer scientists could have missed many important algorithms yet another type of efficiency is called amortized efficiency it applies not to a single run of an algorithm but rather to a sequence of operations performed on the same data structure it turns out that in some situations a single operation can be expensive but the total time for an entire sequence of n such operations is always significantly better than the worstcase efficiency of that single operation multiplied by n so we can amortize the high cost of such a worstcase occurrence over the entire sequence in a manner similar to the way a business would amortize the cost of an expensive item over the years of the items productive life this sophisticated approach was discovered by the american computer scientist robert tarjan who used it among other applications in developing an interesting variation of the classic binary search tree see tar for a quite readable nontechnical discussion and tar for a technical account we will see an example of the usefulness of amortized efficiency in section when we consider algorithms for finding unions of disjoint sets recapitulation of the analysis framework before we leave this section let us summarize the main points of the framework outlined above both time and space efficiencies are measured as functions of the algorithms input size time efficiency is measured by counting the number of times the algorithms basic operation is executed space efficiency is measured by counting the number of extra memory units consumed by the algorithm the efficiencies of some algorithms may differ significantly for inputs of the same size for such algorithms we need to distinguish between the worstcase averagecase and bestcase efficiencies the frameworks primary interest lies in the order of growth of the algorithms running time extra memory units consumed as its input size goes to infinity in the next section we look at formal means to investigate orders of growth in sections and we discuss particular methods for investigating nonrecursive and recursive algorithms respectively it is there that you will see how the analysis framework outlined here can be applied to investigating the efficiency of specific algorithms you will encounter many more examples throughout the rest of the book exercises for each of the following algorithms indicate i a natural size metric for its inputs ii its basic operation and iii whether the basic operation count can be different for inputs of the same size a computing the sum of n numbers b computing n c finding the largest element in a list of n numbers d euclids algorithm e sieve of eratosthenes f penandpencil algorithm for multiplying two ndigit decimal integers a consider the definitionbased algorithm for adding two n n matrices what is its basic operation how many times is it performed as a function of the matrix order n as a function of the total number of elements in the input matrices b answer the same questions for the definitionbased algorithm for matrix multiplication consider a variation of sequential search that scans a list to return the number of occurrences of a given search key in the list does its efficiency differ from the efficiency of classic sequential search a glove selection there are gloves in a drawer pairs of red gloves pairs of yellow and pairs of green you select the gloves in the dark and can check them only after a selection has been made what is the smallest number of gloves you need to select to have at least one matching pair in the best case in the worst case b missing socks imagine that after washing distinct pairs of socks you discover that two socks are missing of course you would like to have the largest number of complete pairs remaining thus you are left with complete pairs in the bestcase scenario and with complete pairs in the worst case assuming that the probability of disappearance for each of the socks is the same find the probability of the bestcase scenario the probability of the worstcase scenario the number of pairs you should expect in the average case a prove formula for the number of bits in the binary representation of a positive decimal integer b prove the alternative formula for the number of bits in the binary representation of a positive integer n b logn c what would be the analogous formulas for the number of decimal digits d explain why within the accepted analysis framework it does not matter whether we use binary or decimal digits in measuring ns size suggest how any sorting algorithm can be augmented in a way to make the bestcase count of its key comparisons equal to just n n is a lists size of course do you think it would be a worthwhile addition to any sorting algorithm gaussian elimination the classic algorithm for solving systems of n linear equations in n unknowns requires about n multiplications which is the algorithms basic operation a how much longer should you expect gaussian elimination to work on a system of equations versus a system of equations b you are considering buying a computer that is times faster than the one you currently have by what factor will the faster computer increase the sizes of systems solvable in the same amount of time as on the old computer for each of the following functions indicate how much the functions value will change if its argument is increased fourfold a log n b n c n d n e n f n for each of the following pairs of functions indicate whether the first function of each of the following pairs has a lower same or higher order of growth to within a constant multiple than the second function a nn and n b n and n c log n and ln n d log n and log n e n and n f n and n invention of chess a according to a wellknown legend the game of chess was invented many centuries ago in northwestern india by a certain sage when he took his invention to his king the king liked the game so much that he offered the inventor any reward he wanted the inventor asked for some grain to be obtained as follows just a single grain of wheat was to be placed on the first square of the chessboard two on the second four on the third eight on the fourth and so on until all squares had been filled if it took just second to count each grain how long would it take to count all the grain due to him b how long would it take if instead of doubling the number of grains for each square of the chessboard the inventor asked for adding two grains coping with the limitations of algorithm power keep on the lookout for novel ideas that others have used successfully your idea has to be original only in its adaptation to the problem youre working on thomas edison as we saw in the previous chapter there are problems that are difficult to solve algorithmically at the same time some of them are so important that we can not just sigh in resignation and do nothing this chapter outlines several ways of dealing with such difficult problems sections and introduce two algorithm design techniques backtracking and branchandbound that often make it possible to solve at least some large instances of difficult combinatorial problems both strategies can be considered an improvement over exhaustive search discussed in section unlike exhaustive search they construct candidate solutions one component at a time and evaluate the partially constructed solutions if no potential values of the remaining components can lead to a solution the remaining components are not generated at all this approach makes it possible to solve some large instances of difficult combinatorial problems though in the worst case we still face the same curse of exponential explosion encountered in exhaustive search both backtracking and branchandbound are based on the construction of a statespace tree whose nodes reflect specific choices made for a solutions components both techniques terminate a node as soon as it can be guaranteed that no solution to the problem can be obtained by considering choices that correspond to the nodes descendants the techniques differ in the nature of problems they can be applied to branchandbound is applicable only to optimization problems because it is based on computing a bound on possible values of the problems objective function backtracking throughout the book see in particular sections and we have encountered problems that require finding an element with a special property in a domain that grows exponentially fast or faster with the size of the problems input a hamiltonian circuit among all permutations of a graphs vertices the most valuable subset of items for an instance of the knapsack problem and the like we addressed in section the reasons for believing that many such problems might not be solvable in polynomial time also recall that we discussed in section how such problems can be solved at least in principle by exhaustive search the exhaustivesearch technique suggests generating all candidate solutions and then identifying the one or the ones with a desired property backtracking is a more intelligent variation of this approach the principal idea is to construct solutions one component at a time and evaluate such partially constructed candidates as follows if a partially constructed solution can be developed further without violating the problems constraints it is done by taking the first remaining legitimate option for the next component if there is no legitimate option for the next component no alternatives for any remaining component need to be considered in this case the algorithm backtracks to replace the last component of the partially constructed solution with its next option it is convenient to implement this kind of processing by constructing a tree of choices being made called the statespace tree its root represents an initial state before the search for a solution begins the nodes of the first level in the tree represent the choices made for the first component of a solution the nodes of the second level represent the choices for the second component and so on a node in a statespace tree is said to be promising if it corresponds to a partially constructed solution that may still lead to a complete solution otherwise it is called nonpromising leaves represent either nonpromising dead ends or complete solutions found by the algorithm in the majority of cases a statespace tree for a backtracking algorithm is constructed in the manner of depthfirst search if the current node is promising its child is generated by adding the first remaining legitimate option for the next component of a solution and the processing moves to this child if the current node turns out to be nonpromising the algorithm backtracks to the nodes parent to consider the next possible option for its last component if there is no such option it backtracks one more level up the tree and so on finally if the algorithm reaches a complete solution to the problem it either stops if just one solution is required or continues searching for other possible solutions nqueens problem as our first example we use a perennial favorite of textbook writers the nqueens problem the problem is to place n queens on an n n chessboard so that no two queens attack each other by being in the same row or in the same column or on the same diagonal for n the problem has a trivial solution and it is easy to see that there is no solution for n and n so let us consider the fourqueens problem and solve it by the backtracking technique since each of the four queens has to be placed in its own row all we need to do is to assign a column for each queen on the board presented in figure we start with the empty board and then place queen in the first possible position of its row which is in column of row then we place queen after trying unsuccessfully columns and in the first acceptable position for it which is square the square in row and column this proves to be a dead end because there is no acceptable position for queen so the algorithm backtracks and puts queen in the next possible position at then queen is placed at which proves to be another dead end the algorithm then backtracks all the way to queen and moves it to queen then goes to queen to and queen to which is a solution to the problem the statespace tree of this search is shown in figure if other solutions need to be found how many of them are there for the fourqueens problem the algorithm can simply resume its operations at the leaf at which it stopped alternatively we can use the boards symmetry for this purpose queen queen queen queen figure board for the fourqueens problem q q q q q q q q q q q q q q q q q q solution figure statespace tree of solving the fourqueens problem by backtracking denotes an unsuccessful attempt to place a queen in the indicated column the numbers above the nodes indicate the order in which the nodes are generated finally it should be pointed out that a single solution to the nqueens problem for any n can be found in linear time in fact over the last years mathematicians have discovered several alternative formulas for nonattacking positions of n queens bel such positions can also be found by applying some general algorithm design strategies problem in this sections exercises hamiltonian circuit problem as our next example let us consider the problem of finding a hamiltonian circuit in the graph in figure a without loss of generality we can assume that if a hamiltonian circuit exists it starts at vertex a accordingly we make vertex a the root of the statespace a b a c f b d e e c f d e e d f c dead end dead end f d dead end a solution a b figure a graph b statespace tree for finding a hamiltonian circuit the numbers above the nodes of the tree indicate the order in which the nodes are generated tree figure b the first component of our future solution if it exists is a first intermediate vertex of a hamiltonian circuit to be constructed using the alphabet order to break the threeway tie among the vertices adjacent to a we select vertex b from b the algorithm proceeds to c then to d then to e and finally to f which proves to be a dead end so the algorithm backtracks from f to e then to d and then to c which provides the first alternative for the algorithm to pursue going from c to e eventually proves useless and the algorithm has to backtrack from e to c and then to b from there it goes to the vertices f e c and d from which it can legitimately return to a yielding the hamiltonian circuit a b f e c d a if we wanted to find another hamiltonian circuit we could continue this process by backtracking from the leaf of the solution found subsetsum problem as our last example we consider the subsetsum problem find a subset of a given set a a an of n positive integers whose sum is equal to a given positive integer d for example for a and d there are two solutions and of course some instances of this problem may have no solutions it is convenient to sort the sets elements in increasing order so we will assume that a a an with wo with wo with wo with wo with wo with wo with wo solution figure complete statespace tree of the backtracking algorithm applied to the instance a and d of the subsetsum problem the number inside a node is the sum of the elements already included in the subsets represented by the node the inequality below a leaf indicates the reason for its termination the statespace tree can be constructed as a binary tree like that in figure for the instance a and d the root of the tree represents the starting point with no decisions about the given elements made as yet its left and right children represent respectively inclusion and exclusion of a in a set being sought similarly going to the left from a node of the first level corresponds to inclusion of a while going to the right corresponds to its exclusion and so on thus a path from the root to a node on the ith level of the tree indicates which of the first i numbers have been included in the subsets represented by that node we record the value of s the sum of these numbers in the node if s is equal to d we have a solution to the problem we can either report this result and stop or if all the solutions need to be found continue by backtracking to the nodes parent if s is not equal to d we can terminate the node as nonpromising if either of the following two inequalities holds s ai d the sum s is too large n s aj d the sum s is too small j i general remarks from a more general perspective most backtracking algorithms fit the following description an output of a backtracking algorithm can be thought of as an ntuple x x xn where each coordinate xi is an element of some finite linearly ordered set si for example for the nqueens problem each si is the set of integers column numbers through n the tuple may need to satisfy some additional constraints eg the nonattacking requirements in the nqueens problem depending on the problem all solution tuples can be of the same length the nqueens and the hamiltonian circuit problem and of different lengths the subsetsum problem a backtracking algorithm generates explicitly or implicitly a statespace tree its nodes represent partially constructed tuples with the first i coordinates defined by the earlier actions of the algorithm if such a tuple x x xi is not a solution the algorithm finds the next element in si that is consistent with the values of x x xi and the problems constraints and adds it to the tuple as its i st coordinate if such an element does not exist the algorithm backtracks to consider the next value of xi and so on to start a backtracking algorithm the following pseudocode can be called for i x represents the empty tuple algorithm backtrackxi gives a template of a generic backtracking algorithm input xi specifies first i promising components of a solution output all the tuples representing the problems solutions if xi is a solution write xi else see problem in this sections exercises for each element x si consistent with xi and the constraints do xi x backtrackxi our success in solving small instances of three difficult problems earlier in this section should not lead you to the false conclusion that backtracking is a very efficient technique in the worst case it may have to generate all possible candidates in an exponentially or faster growing state space of the problem at hand the hope of course is that a backtracking algorithm will be able to prune enough branches of its statespace tree before running out of time or memory or both the success of this strategy is known to vary widely not only from problem to problem but also from one instance to another of the same problem there are several tricks that might help reduce the size of a statespace tree one is to exploit the symmetry often present in combinatorial problems for example the board of the nqueens problem has several symmetries so that some solutions can be obtained from others by reflection or rotation this implies in particular that we need not consider placements of the first queen in the last n columns because any solution with the first queen in square i n i n can be obtained by reflection which from a solution with the first queen in square n i this observation cuts the size of the tree by about half another trick is to preassign values to one or more components of a solution as we did in the hamiltonian circuit example data presorting in the subsetsum example demonstrates potential benefits of yet another opportunity rearrange data of an instance given it would be highly desirable to be able to estimate the size of the statespace tree of a backtracking algorithm as a rule this is too difficult to do analytically however knuth knu suggested generating a random path from the root to a leaf and using the information about the number of choices available during the path generation for estimating the size of the tree specifically let c be the number of values of the first component x that are consistent with the problems constraints we randomly select one of these values with equal probability c to move to one of the roots c children repeating this operation for c possible values for x that are consistent with x and the other constraints we move to one of the c children of that node we continue this process until a leaf is reached after randomly selecting values for x x xn by assuming that the nodes on level i have ci children on average we estimate the number of nodes in the tree as c cc cc cn generating several such estimates and computing their average yields a useful estimation of the actual size of the tree although the standard deviation of this random variable can be large in conclusion three things on behalf of backtracking need to be said first it is typically applied to difficult combinatorial problems for which no efficient algorithms for finding exact solutions possibly exist second unlike the exhaustivesearch approach which is doomed to be extremely slow for all instances of a problem backtracking at least holds a hope for solving some instances of nontrivial sizes in an acceptable amount of time this is especially true for optimization problems for which the idea of backtracking can be further enhanced by evaluating the quality of partially constructed solutions how this can be done is explained in the next section third even if backtracking does not eliminate any elements of a problems state space and ends up generating all its elements it provides a specific technique for doing so which can be of value in its own right exercises a continue the backtracking search for a solution to the fourqueens problem which was started in this section to find the second solution to the problem b explain how the boards symmetry can be used to find the second solution to the fourqueens problem a which is the last solution to the fivequeens problem found by the backtracking algorithm b use the boards symmetry to find at least four other solutions to the problem a implement the backtracking algorithm for the nqueens problem in the language of your choice run your program for a sample of n values to get the numbers of nodes in the algorithms statespace trees compare these numbers with the numbers of candidate solutions generated by the exhaustivesearch algorithm for this problem see problem in exercises b for each value of n for which you run your program in part a estimate the size of the statespace tree by the method described in section and compare the estimate with the actual number of nodes you obtained design a lineartime algorithm that finds a solution to the nqueens problem for any n apply backtracking to the problem of finding a hamiltonian circuit in the following graph a b c d e f g apply backtracking to solve the coloring problem for the graph in figure a generate all permutations of by backtracking a apply backtracking to solve the following instance of the subset sum problem a and d b will the backtracking algorithm work correctly if we use just one of the two inequalities to terminate a node as nonpromising the general template for backtracking algorithms which is given in the section works correctly only if no solution is a prefix to another solution to the problem change the templates pseudocode to work correctly without this restriction write a program implementing a backtracking algorithm for a the hamiltonian circuit problem b the mcoloring problem puzzle pegs this puzzlelike game is played on a board with small holes arranged in an equilateral triangle in an initial position all but one of the holes are occupied by pegs as in the example shown below a legal move is a jump of a peg over its immediate neighbor into an empty square opposite the jump removes the jumpedover neighbor from the board design and implement a backtracking algorithm for solving the following versions of this puzzle a starting with a given location of the empty hole find a shortest sequence of moves that eliminates pegs with no limitations on the final position of the remaining peg b starting with a given location of the empty hole find a shortest sequence of moves that eliminates pegs with the remaining peg at the empty hole of the initial board branchandbound recall that the central idea of backtracking discussed in the previous section is to cut off a branch of the problems statespace tree as soon as we can deduce that it can not lead to a solution this idea can be strengthened further if we deal with an optimization problem an optimization problem seeks to minimize or maximize some objective function a tour length the value of items selected the cost of an assignment and the like usually subject to some constraints note that in the standard terminology of optimization problems a feasible solution is a point in the problems search space that satisfies all the problems constraints eg a hamiltonian circuit in the traveling salesman problem or a subset of items whose total weight does not exceed the knapsacks capacity in the knapsack problem whereas an optimal solution is a feasible solution with the best value of the objective function eg the shortest hamiltonian circuit or the most valuable subset of items that fit the knapsack compared to backtracking branchandbound requires two additional items a way to provide for every node of a statespace tree a bound on the best value of the objective function on any solution that can be obtained by adding further components to the partially constructed solution represented by the node the value of the best solution seen so far if this information is available we can compare a nodes bound value with the value of the best solution seen so far if the bound value is not better than the value of the best solution seen so far ie not smaller for a minimization problem this bound should be a lower bound for a minimization problem and an upper bound for a maximization problem and not larger for a maximization problem the node is nonpromising and can be terminated some people say the branch is pruned indeed no solution obtained from it can yield a better solution than the one already available this is the principal idea of the branchandbound technique in general we terminate a search path at the current node in a statespace tree of a branchandbound algorithm for any one of the following three reasons the value of the nodes bound is not better than the value of the best solution seen so far the node represents no feasible solutions because the constraints of the problem are already violated the subset of feasible solutions represented by the node consists of a single point and hence no further choices can be made in this case we compare the value of the objective function for this feasible solution with that of the best solution seen so far and update the latter with the former if the new solution is better assignment problem let us illustrate the branchandbound approach by applying it to the problem of assigning n people to n jobs so that the total cost of the assignment is as small as possible we introduced this problem in section where we solved it by exhaustive search recall that an instance of the assignment problem is specified by an n n cost matrix c so that we can state the problem as follows select one element in each row of the matrix so that no two selected elements are in the same column and their sum is the smallest possible we will demonstrate how this problem can be solved using the branchandbound technique by considering the same small instance of the problem that we investigated in section job job job job person a c person b person c person d how can we find a lower bound on the cost of an optimal selection without actually solving the problem we can do this by several methods for example it is clear that the cost of any solution including an optimal one can not be smaller than the sum of the smallest elements in each of the matrixs rows for the instance here this sum is it is important to stress that this is not the cost of any legitimate selection and came from the same column of the matrix it is just a lower bound on the cost of any legitimate selection we can and will apply the same thinking to partially constructed solutions for example for any legitimate selection that selects from the first row the lower bound will be one more comment is in order before we embark on constructing the problems statespace tree it deals with the order in which the tree nodes will be generated rather than generating a single child of the last promising node as we did in backtracking we will generate all the children of the most promising node among nonterminated leaves in the current tree nonterminated ie still promising leaves are also called live how can we tell which of the nodes is most promising we can do this by comparing the lower bounds of the live nodes it is sensible to consider a node with the best bound as most promising although this does not of course preclude the possibility that an optimal solution will ultimately belong to a different branch of the statespace tree this variation of the strategy is called the bestfirst branchandbound so returning to the instance of the assignment problem given earlier we start with the root that corresponds to no elements selected from the cost matrix as we already discussed the lowerbound value for the root denoted lb is the nodes on the first level of the tree correspond to selections of an element in the first row of the matrix ie a job for person a figure so we have four live leaves nodes through that may contain an optimal solution the most promising of them is node because it has the smallest lowerbound value following our bestfirst search strategy we branch out from that node first by considering the three different ways of selecting an element from the second row and not in the second column the three different jobs that can be assigned to person b figure of the six live leaves nodes and that may contain an optimal solution we again choose the one with the smallest lower bound node first we consider selecting the third columns element from cs row ie assigning person c to job this leaves us with no choice but to select the element from the fourth column of ds row assigning person d to job this yields leaf figure which corresponds to the feasible solution a b c d with the total cost of its sibling node corresponds to the feasible solution a b c d with the total cost of since its cost is larger than the cost of the solution represented by leaf node is simply terminated of course if start lb a a a a lb lb lb lb figure levels and of the statespace tree for the instance of the assignment problem being solved with the bestfirst branchandbound algorithm the number above a node shows the order in which the node was generated a nodes fields indicate the job number assigned to person a and the lower bound value lb for this node start lb a a a a lb lb lb lb b b b lb lb lb figure levels and of the statespace tree for the instance of the assignment problem being solved with the bestfirst branchandbound algorithm start lb a a a a lb lb lb lb x x x b b b lb lb lb x x c c d d cost cost solution inferior solution figure complete statespace tree for the instance of the assignment problem solved with the bestfirst branchandbound algorithm its cost were smaller than we would have to replace the information about the best solution seen so far with the data provided by this node now as we inspect each of the live leaves of the last statespace tree nodes and in figure we discover that their lowerbound values are not smaller than the value of the best selection seen so far leaf hence we terminate all of them and recognize the solution represented by leaf as the optimal solution to the problem before we leave the assignment problem we have to remind ourselves again that unlike for our next examples there is a polynomialtime algorithm for this problem called the hungarian method eg pap in the light of this efficient algorithm solving the assignment problem by branchandbound should be considered a convenient educational device rather than a practical recommendation knapsack problem let us now discuss how we can apply the branchandbound technique to solving the knapsack problem this problem was introduced in section given n items of known weights wi and values vi i n and a knapsack of capacity w find the most valuable subset of the items that fit in the knapsack it is convenient to order the items of a given instance in descending order by their valuetoweight ratios then the first item gives the best payoff per weight unit and the last one gives the worst payoff per weight unit with ties resolved arbitrarily vw vw vnwn it is natural to structure the statespace tree for this problem as a binary tree constructed as follows see figure for an example each node on the ith level of this tree i n represents all the subsets of n items that include a particular selection made from the first i ordered items this particular selection is uniquely determined by the path from the root to the node a branch going to the left indicates the inclusion of the next item and a branch going to the right indicates its exclusion we record the total weight w and the total value v of this selection in the node along with some upper bound ub on the value of any subset that can be obtained by adding zero or more items to this selection a simple way to compute the upper bound ub is to add to v the total value of the items already selected the product of the remaining capacity of the knapsack w w and the best per unit payoff among the remaining items which is viwi ub v w wviwi as a specific example let us apply the branchandbound algorithm to the same instance of the knapsack problem we solved in section by exhaustive search we reorder the items in descending order of their valuetoweight ratios though item weight value value weight the knapsacks capacity w is w v ub with wo w v w v ub ub with wo x inferior to node w w v ub x with wo not feasible w v w v ub ub x with wo inferior to node w w v value x not feasible optimal solution figure statespace tree of the bestfirst branchandbound algorithm for the instance of the knapsack problem at the root of the statespace tree see figure no items have been selected as yet hence both the total weight of the items already selected w and their total value v are equal to the value of the upper bound computed by formula is node the left child of the root represents the subsets that include item the total weight and value of the items already included are and respectively the value of the upper bound is node represents the subsets that do not include item accordingly w v and ub since node has a larger upper bound than the upper bound of node it is more promising for this maximization problem and we branch from node first its children nodes and represent subsets with item and with and without item respectively since the total weight w of every subset represented by node exceeds the knapsacks capacity node can be terminated immediately node has the same values of w and v as its parent the upper bound ub is equal to selecting node over node for the next branching why we get nodes and by respectively including and excluding item the total weights and values as well as the upper bounds for these nodes are computed in the same way as for the preceding nodes branching from node yields node which represents no feasible solutions and node which represents just a single subset of value the remaining live nodes and have smaller upperbound values than the value of the solution represented by node hence both can be terminated making the subset of node the optimal solution to the problem solving the knapsack problem by a branchandbound algorithm has a rather unusual characteristic typically internal nodes of a statespace tree do not define a point of the problems search space because some of the solutions components remain undefined see for example the branchandbound tree for the assignment problem discussed in the preceding subsection for the knapsack problem however every node of the tree represents a subset of the items given we can use this fact to update the information about the best subset seen so far after generating each new node in the tree if we had done this for the instance investigated above we could have terminated nodes and before node was generated because they both are inferior to the subset of value of node traveling salesman problem we will be able to apply the branchandbound technique to instances of the traveling salesman problem if we come up with a reasonable lower bound on tour lengths one very simple lower bound can be obtained by finding the smallest element in the intercity distance matrix d and multiplying it by the number of cities n but there is a less obvious and more informative lower bound for instances with symmetric matrix d which does not require a lot of work to compute it is not difficult to show problem in this sections exercises that we can compute a lower bound on the length l of any tour as follows for each city i i n find the sum si of the distances from city i to the two nearest cities compute the sum s of these n numbers divide the result by and if all the distances are integers round up the result to the nearest integer lb s for example for the instance in figure a formula yields lb moreover for any subset of tours that must include particular edges of a given graph we can modify lower bound accordingly for example for all the hamiltonian circuits of the graph in figure a that must include edge a d we get the following lower bound by summing up the lengths of the two shortest edges incident with each of the vertices with the required inclusion of edges a d and d a we now apply the branchandbound algorithm with the bounding function given by formula to find the shortest hamiltonian circuit for the graph in a b a lb a b a c a d a e c d lb lb lb x x x b is not lb l lb l before c e of node of node a a b c a b d a b e lb lb lb x lb l of node a b c d a b c e a b d c a b d e e a d a e a c a l l l l first tour better tour inferior tour optimal tour b figure a weighted graph b statespace tree of the branchandbound algorithm to find a shortest hamiltonian circuit in this graph the list of vertices in a node specifies a beginning part of the hamiltonian circuits represented by the node figure a to reduce the amount of potential work we take advantage of two observations made in section first without loss of generality we can consider only tours that start at a second because our graph is undirected we can generate only tours in which b is visited before c in addition after visiting n cities a tour has no choice but to visit the remaining unvisited city and return to the starting one the statespace tree tracing the algorithms application is given in figure b the comments we made at the end of the preceding section about the strengths and weaknesses of backtracking are applicable to branchandbound as well to reiterate the main point these statespace tree techniques enable us to solve many large instances of difficult combinatorial problems as a rule however it is virtually impossible to predict which instances will be solvable in a realistic amount of time and which will not incorporation of additional information such as a symmetry of a games board can widen the range of solvable instances along this line a branchandbound algorithm can be sometimes accelerated by a knowledge of the objective functions value of some nontrivial feasible solution the information might be obtainable say by exploiting specifics of the data or even for some problems generated randomly before we start developing a statespace tree then we can use such a solution immediately as the best one seen so far rather than waiting for the branchandbound processing to lead us to the first feasible solution in contrast to backtracking solving a problem by branchandbound has both the challenge and opportunity of choosing the order of node generation and finding a good bounding function though the bestfirst rule we used above is a sensible approach it may or may not lead to a solution faster than other strategies artificial intelligence researchers are particularly interested in different strategies for developing statespace trees finding a good bounding function is usually not a simple task on the one hand we want this function to be easy to compute on the other hand it can not be too simplistic otherwise it would fail in its principal task to prune as many branches of a statespace tree as soon as possible striking a proper balance between these two competing requirements may require intensive experimentation with a wide variety of instances of the problem in question exercises what data structure would you use to keep track of live nodes in a bestfirst branchandbound algorithm solve the same instance of the assignment problem as the one solved in the section by the bestfirst branchandbound algorithm with the bounding function based on matrix columns rather than rows a give an example of the bestcase input for the branchandbound algorithm for the assignment problem b in the best case how many nodes will be in the statespace tree of the branchandbound algorithm for the assignment problem write a program for solving the assignment problem by the branchandbound algorithm experiment with your program to determine the average size of the cost matrices for which the problem is solved in a given amount of time say minute on your computer solve the following instance of the knapsack problem by the branchandbound algorithm item weight value w a suggest a more sophisticated bounding function for solving the knapsack problem than the one used in the section b use your bounding function in the branchandbound algorithm applied to the instance of problem write a program to solve the knapsack problem with the branchandbound algorithm a prove the validity of the lower bound given by formula for instances of the traveling salesman problem with symmetric matrices of integer intercity distances b how would you modify lower bound for nonsymmetric distance matrices apply the branchandbound algorithm to solve the traveling salesman problem for the following graph a b c d we solved this problem by exhaustive search in section as a research project write a report on how statespace trees are used for programming such games as chess checkers and tictactoe the two principal algorithms you should read about are the minimax algorithm and alphabeta pruning approximation algorithms for np hard problems in this section we discuss a different approach to handling difficult problems of combinatorial optimization such as the traveling salesman problem and the knapsack problem as we pointed out in section the decision versions of these problems are npcomplete their optimization versions fall in the class of nphard problems problems that are at least as hard as npcomplete problems hence there are no known polynomialtime algorithms for these problems and there are serious theoretical reasons to believe that such algorithms do not exist what then are our options for handling such problems many of which are of significant practical importance the notion of an nphard problem can be defined more formally by extending the notion of polynomial reducibility to problems that are not necessarily in class np including optimization problems of the type discussed in this section see gar chapter if an instance of the problem in question is very small we might be able to solve it by an exhaustivesearch algorithm section some such problems can be solved by the dynamic programming technique we demonstrated in section but even when this approach works in principle its practicality is limited by dependence on the instance parameters being relatively small the discovery of the branchandbound technique has proved to be an important breakthrough because this technique makes it possible to solve many large instances of difficult optimization problems in an acceptable amount of time however such good performance can not usually be guaranteed there is a radically different way of dealing with difficult optimization problems solve them approximately by a fast algorithm this approach is particularly appealing for applications where a good but not necessarily optimal solution will suffice besides in reallife applications we often have to operate with inaccurate data to begin with under such circumstances going for an approximate solution can be a particularly sensible choice although approximation algorithms run a gamut in level of sophistication most of them are based on some problemspecific heuristic a heuristic is a commonsense rule drawn from experience rather than from a mathematically proved assertion for example going to the nearest unvisited city in the traveling salesman problem is a good illustration of this notion we discuss an algorithm based on this heuristic later in this section of course if we use an algorithm whose output is just an approximation of the actual optimal solution we would like to know how accurate this approximation is we can quantify the accuracy of an approximate solution sa to a problem of minimizing some function f by the size of the relative error of this approximation r esa f sa f s f s where s is an exact solution to the problem alternatively since resa f sa f s we can simply use the accuracy ratio r sa f sa f s as a measure of accuracy of sa note that for the sake of scale uniformity the accuracy ratio of approximate solutions to maximization problems is usually computed as r sa f s f sa to make this ratio greater than or equal to as it is for minimization problems obviously the closer rsa is to the better the approximate solution is for most instances however we can not compute the accuracy ratio because we typically do not know f s the true optimal value of the objective function therefore our hope should lie in obtaining a good upper bound on the values of rsa this leads to the following definitions definition a polynomialtime approximation algorithm is said to be a capproximation algorithm where c if the accuracy ratio of the approximation it produces does not exceed c for any instance of the problem in question rsa c the best ie the smallest value of c for which inequality holds for all instances of the problem is called the performance ratio of the algorithm and denoted ra the performance ratio serves as the principal metric indicating the quality of the approximation algorithm we would like to have approximation algorithms with ra as close to as possible unfortunately as we shall see some approximation algorithms have infinitely large performance ratios ra this does not necessarily rule out using such algorithms but it does call for a cautious treatment of their outputs there are two important facts about difficult combinatorial optimization problems worth keeping in mind first although the difficulty level of solving most such problems exactly is the same to within a polynomialtime transformation of one problem to another this equivalence does not translate into the realm of approximation algorithms finding good approximate solutions is much easier for some of these problems than for others second some of the problems have special classes of instances that are both particularly important for reallife applications and easier to solve than their general counterparts the traveling salesman problem is a prime example of this situation approximation algorithms for the traveling salesman problem we solved the traveling salesman problem by exhaustive search in section mentioned its decision version as one of the most wellknown npcomplete problems in section and saw how its instances can be solved by a branchandbound algorithm in section here we consider several approximation algorithms a small sample of dozens of such algorithms suggested over the years for this famous problem for a much more detailed discussion of the topic see law hoc app and gut but first let us answer the question of whether we should hope to find a polynomialtime approximation algorithm with a finite performance ratio on all instances of the traveling salesman problem as the following theorem sah shows the answer turns out to be no unless p n p theorem if p np there exists no capproximation algorithm for the traveling salesman problem ie there exists no polynomialtime approximation algorithm for this problem so that for all instances f sa cf s for some constant c proof by way of contradiction suppose that such an approximation algorithm a and a constant c exist without loss of generality we can assume that c is a positive integer we will show that this algorithm could then be used for solving the hamiltonian circuit problem in polynomial time we will take advantage of a variation of the transformation used in section to reduce the hamiltonian circuit problem to the traveling salesman problem let g be an arbitrary graph with n vertices we map g to a complete weighted graph g by assigning weight to each edge in g and adding an edge of weight cn between each pair of vertices not adjacent in g if g has a hamiltonian circuit its length in g is n hence it is the exact solution s to the traveling salesman problem for g note that if sa is an approximate solution obtained for g by algorithm a then f sa cn by the assumption if g does not have a hamiltonian circuit in g the shortest tour in g will contain at least one edge of weight cn and hence f sa f s cn taking into account the two derived inequalities we could solve the hamiltonian circuit problem for graph g in polynomial time by mapping g to g applying algorithm a to get tour sa in g and comparing its length with cn since the hamiltonian circuit problem is npcomplete we have a contradiction unless p np greedy algorithms for the tsp the simplest approximation algorithms for the traveling salesman problem are based on the greedy technique we will discuss here two such algorithms nearestneighbor algorithm the following wellknown greedy algorithm is based on the nearestneighbor heuristic always go next to the nearest unvisited city step choose an arbitrary city as the start step repeat the following operation until all the cities have been visited go to the unvisited city nearest the one visited last ties can be broken arbitrarily step return to the starting city example for the instance represented by the graph in figure with a as the starting vertex the nearestneighbor algorithm yields the tour hamiltonian circuit sa a b c d a of length a b d c figure instance of the traveling salesman problem the optimal solution as can be easily checked by exhaustive search is the tour s a b d c a of length thus the accuracy ratio of this approximation is r sa f sa f s ie tour sa is longer than the optimal tour s unfortunately except for its simplicity not many good things can be said about the nearestneighbor algorithm in particular nothing can be said in general about the accuracy of solutions obtained by this algorithm because it can force us to traverse a very long edge on the last leg of the tour indeed if we change the weight of edge a d from to an arbitrary large number w in example the algorithm will still yield the tour a b c d a of length w and the optimal solution will still be a b d c a of length hence r sa f sa w f s which can be made as large as we wish by choosing an appropriately large value of w hence ra for this algorithm as it should be according to theorem multifragmentheuristic algorithm another natural greedy algorithm for the traveling salesman problem considers it as the problem of finding a minimumweight collection of edges in a given complete weighted graph so that all the vertices have degree with this emphasis on edges rather than vertices what other greedy algorithm does it remind you of an application of the greedy technique to this problem leads to the following algorithm ben step sort the edges in increasing order of their weights ties can be broken arbitrarily initialize the set of tour edges to be constructed to the empty set step repeat this step n times where n is the number of cities in the instance being solved add the next edge on the sorted edge list to the set of tour edges provided this addition does not create a vertex of degree or a cycle of length less than n otherwise skip the edge step return the set of tour edges as an example applying the algorithm to the graph in figure yields a b c d b c a d this set of edges forms the same tour as the one produced by the nearestneighbor algorithm in general the multifragmentheuristic algorithm tends to produce significantly better tours than the nearestneighbor algorithm as we are going to see from the experimental data quoted at the end of this section but the performance ratio of the multifragmentheuristic algorithm is also unbounded of course there is however a very important subset of instances called euclidean for which we can make a nontrivial assertion about the accuracy of both the nearestneighbor and multifragmentheuristic algorithms these are the instances in which intercity distances satisfy the following natural conditions triangle inequality di j di k dk j for any triple of cities i j and k the distance between cities i and j can not exceed the length of a twoleg path from i to some intermediate city k to j symmetry di j dj i for any pair of cities i and j the distance from i to j is the same as the distance from j to i a substantial majority of practical applications of the traveling salesman problem are its euclidean instances they include in particular geometric ones where cities correspond to points in the plane and distances are computed by the standard euclidean formula although the performance ratios of the nearestneighbor and multifragmentheuristic algorithms remain unbounded for euclidean instances their accuracy ratios satisfy the following inequality for any such instance with n cities f sa log n f s where f sa and f s are the lengths of the heuristic tour and shortest tour respectively see ros and ong minimumspanningtreebased algorithms there are approximation algorithms for the traveling salesman problem that exploit a connection between hamiltonian circuits and spanning trees of the same graph since removing an edge from a hamiltonian circuit yields a spanning tree we can expect that the structure of a minimum spanning tree provides a good basis for constructing a shortest tour approximation here is an algorithm that implements this idea in a rather straightforward fashion twicearoundthetree algorithm step construct a minimum spanning tree of the graph corresponding to a given instance of the traveling salesman problem step starting at an arbitrary vertex perform a walk around the minimum spanning tree recording all the vertices passed by this can be done by a dfs traversal step scan the vertex list obtained in step and eliminate from it all repeated occurrences of the same vertex except the starting one at the end of the list this step is equivalent to making shortcuts in the walk the vertices remaining on the list will form a hamiltonian circuit which is the output of the algorithm example let us apply this algorithm to the graph in figure a the minimum spanning tree of this graph is made up of edges a b b c b d and d e figure b a twicearoundthetree walk that starts and ends at a is a e a e b d b d c c a b figure illustration of the twicearoundthetree algorithm a graph b walk around the minimum spanning tree with the shortcuts a b c b d e d b a eliminating the second b a shortcut from c to d the second d and the third b a shortcut from e to a yields the hamiltonian circuit a b c d e a of length the tour obtained in example is not optimal although that instance is small enough to find an optimal solution by either exhaustive search or branchandbound we refrained from doing so to reiterate a general point as a rule we do not know what the length of an optimal tour actually is and therefore we can not compute the accuracy ratio f saf s for the twicearoundthetree algorithm we can at least estimate it above provided the graph is euclidean theorem the twicearoundthetree algorithm is a approximation algorithm for the traveling salesman problem with euclidean distances proof obviously the twicearoundthetree algorithm is polynomial time if we use a reasonable algorithm such as prims or kruskals in step we need to show that for any euclidean instance of the traveling salesman problem the length of a tour sa obtained by the twicearoundthetree algorithm is at most twice the length of the optimal tour s ie f sa f s since removing any edge from s yields a spanning tree t of weight wt which must be greater than or equal to the weight of the graphs minimum spanning tree wt we get the inequality f s wt wt this inequality implies that f s wt the length of the walk obtained in step of the algorithm the possible shortcuts outlined in step of the algorithm to obtain sa can not increase the total length of the walk in a euclidean graph ie the length of the walk obtained in step the length of the tour sa combining the last two inequalities we get the inequality f s f sa which is in fact a slightly stronger assertion than the one we needed to prove christofides algorithm there is an approximation algorithm with a better performance ratio for the euclidean traveling salesman problem the wellknown christofides algorithm chr it also uses a minimum spanning tree but does this in a more sophisticated way than the twicearoundthetree algorithm note that a twicearoundthetree walk generated by the latter algorithm is an eulerian circuit in the multigraph obtained by doubling every edge in the graph given recall that an eulerian circuit exists in a connected multigraph if and only if all its vertices have even degrees the christofides algorithm obtains such a multigraph by adding to the graph the edges of a minimumweight matching of all the odddegree vertices in its minimum spanning tree the number of such vertices is always even and hence this can always be done then the algorithm finds an eulerian circuit in the multigraph and transforms it into a hamiltonian circuit by shortcuts exactly the same way it is done in the last step of the twicearoundthetree algorithm example let us trace the christofides algorithm in figure on the same instance figure a used for tracing the twicearoundthetree algorithm in figure the graphs minimum spanning tree is shown in figure b it has four odddegree vertices a b c and e the minimumweight matching of these four vertices consists of edges a b and c e for this tiny instance it can be found easily by comparing the total weights of just three alternatives a b and c e a c and b e a e and b c the traversal of the multigraph starting at vertex a produces the eulerian circuit a b c e d b a which after one shortcut yields the tour a b c e d a of length the performance ratio of the christofides algorithm on euclidean instances is see eg pap it tends to produce significantly better approximations to optimal tours than the twicearoundthetree algorithm does in empirical tests we quote some results of such tests at the end of this subsection the quality of a tour obtained by this heuristic can be further improved by optimizing shortcuts made on the last step of the algorithm as follows examine the multiplyvisited cities in some arbitrary order and for each make the best possible shortcut this a e b d c a a e a e b d b d c c b c figure application of the christofides algorithm a graph b minimum spanning tree with added edges in dash of a minimumweight matching of all odddegree vertices c hamiltonian circuit obtained enhancement would have not improved the tour a b c e d a obtained in example from a b c e d b a because shortcutting the second occurrence of b happens to be better than shortcutting its first occurrence in general however this enhancement tends to decrease the gap between the heuristic and optimal tour lengths from about to about at least for randomly generated euclidean instances joha local search heuristics for euclidean instances surprisingly good approximations to optimal tours can be obtained by iterativeimprovement algorithms which are also called local search heuristics the bestknown of these are the opt opt and linkernighan algorithms these algorithms start with some initial tour eg constructed randomly or by some simpler approximation algorithm such as the nearestneighbor on each iteration the algorithm explores a neighborhood around the current tour by replacing a few edges in the current tour by other edges if the changes produce a shorter tour the algorithm makes it the current c c c c c c c c a b figure change a original tour b new tour tour and continues by exploring its neighborhood in the same manner otherwise the current tour is returned as the algorithms output and the algorithm stops the opt algorithm works by deleting a pair of nonadjacent edges in a tour and reconnecting their endpoints by the different pair of edges to obtain another tour see figure this operation is called the change note that there is only one way to reconnect the endpoints because the alternative produces two disjoint fragments example if we start with the nearestneighbor tour a b c d e a in the graph of figure whose length lnn is equal to the opt algorithm will move to the next tour as shown in figure to generalize the notion of the change one can consider the kchange for any k this operation replaces up to k edges in a current tour in addition to changes only the changes have proved to be of practical interest the two principal possibilities of changes are shown in figure there are several other local search algorithms for the traveling salesman problem the most prominent of them is the linkernighan algorithm lin which for two decades after its publication in was considered the best algorithm to obtain highquality approximations of optimal tours the linkernighan algorithm is a variableopt algorithm its move can be viewed as a opt move followed by a sequence of opt moves because of its complexity we have to refrain from discussing this algorithm here the excellent survey by johnson and mcgeoch joha contains an outline of the algorithm and its modern extensions as well as methods for its efficient implementation this survey also contain results from the important empirical studies about performance of many heuristics for the traveling salesman problem including of course the linkernighan algorithm we conclude our discussion by quoting some of these data empirical results the traveling salesman problem has been the subject of intense study for the last years this interest was driven by a combination of pure a e a e l lnn b d b d c c a e a e l lnn b d b d c c a e a e l lnn b d b d c c a e a e l lnn new tour b d b d c c figure changes from the nearestneighbor tour of the graph in figure c c c c c c c c c c c c c c c c c c a b c figure change a original tour b c new tours theoretical interest and serious practical needs stemming from such newer applications as circuitboard and vlsichip fabrication xray crystallography and genetic engineering progress in developing effective heuristics their efficient implementation by using sophisticated data structures and the everincreasing power of computers have led to a situation that differs drastically from a pessimistic picture painted by the worstcase theoretical results this is especially true for the most important applications class of instances of the traveling salesman problem points in the twodimensional plane with the standard euclidean distances between them nowadays euclidean instances with up to cities can be solved exactly in quite a reasonable amount of time typically in minutes or faster on a good workstation by such optimization packages as concord app in fact according to the information on the web site maintained by the authors of that package the largest instance of the traveling salesman problem solved exactly as of january was a tour through points in a vlsi application it significantly exceeded the previous record of the shortest tour through all cities in sweden there should be little doubt that the latest record will also be eventually superseded and our ability to solve ever larger instances exactly will continue to expand this remarkable progress does not eliminate the usefulness of approximation algorithms for such problems however first some applications lead to instances that are still too large to be solved exactly in a reasonable amount of time second one may well prefer spending seconds to find a tour that is within a few percent of optimum than to spend many hours or even days of computing time to find the shortest tour exactly but how can one tell how good or bad the approximate solution is if we do not know the length of an optimal tour a convenient way to overcome this difficulty is to solve the linear programming problem describing the instance in question by ignoring the integrality constraints this provides a lower bound called the heldkarp bound on the length of the shortest tour the heldkarp bound is typically very close less than to the length of an optimal tour and this bound can be computed in seconds or minutes unless the instance is truly huge thus for a tour table average tour quality and running times for various heuristics on the city random uniform euclidean instances joha excess over the running time heuristic heldkarp bound seconds nearest neighbor multifragment christofides opt opt linkernighan sa obtained by some heuristic we estimate the accuracy ratio rsa f saf s from above by the ratio f sah ks where f sa is the length of the heuristic tour sa and h ks is the heldkarp lower bound on the shortesttour length the results see table from a large empirical study joha indicate the average tour quality and running times for the discussed heuristics the instances in the reported sample have cities generated randomly and uniformly as integralcoordinate points in the plane with the euclidean distances rounded to the nearest integer the quality of tours generated by the heuristics remain about the same for much larger instances up to a million cities as long as they belong to the same type of instances the running times quoted are for expert implementations run on a compaq es with mhz alpha processors and gigabytes of main memory or its equivalents asymmetric instances of the traveling salesman problem ie those with a nonsymmetic matrix of intercity distances have proved to be significantly harder to solve both exactly and approximately than euclidean instances in particular exact optimal solutions for many city asymmetric instances remained unknown at the time of the stateoftheart survey by johnson et al johb approximation algorithms for the knapsack problem the knapsack problem another wellknown nphard problem was also introduced in section given n items of known weights w wn and values v vn and a knapsack of weight capacity w find the most valuable subset of the items that fits into the knapsack we saw how this problem can be solved by exhaustive search section dynamic programming section we did not include the results for the twicearoundthetree heuristic because of the inferior quality of its approximations with the average excess of about nor did we quote the results for the most sophisticated local search heuristics with the average excess over optimum of less than a fraction of and branchandbound section now we will solve this problem by approximation algorithms greedy algorithms for the knapsack problem we can think of several greedy approaches to this problem one is to select the items in decreasing order of their weights however heavier items may not be the most valuable in the set alternatively if we pick up the items in decreasing order of their value there is no guarantee that the knapsacks capacity will be used efficiently can we find a greedy strategy that takes into account both the weights and values yes we can by computing the valuetoweight ratios viwi i n and selecting the items in decreasing order of these ratios in fact we already used this approach in designing the branchandbound algorithm for the problem in section here is the algorithm based on this greedy heuristic greedy algorithm for the discrete knapsack problem step compute the valuetoweight ratios ri viwi i n for the items given step sort the items in nonincreasing order of the ratios computed in step ties can be broken arbitrarily step repeat the following operation until no item is left in the sorted list if the current item on the list fits into the knapsack place it in the knapsack and proceed to the next item otherwise just proceed to the next item example let us consider the instance of the knapsack problem with the knapsack capacity and the item information as follows item weight value computing the valuetoweight ratios and sorting the items in nonincreasing order of these efficiency ratios yields item weight value valueweight the greedy algorithm will select the first item of weight skip the next item of weight select the next item of weight and skip the last item of weight the solution obtained happens to be optimal for this instance see section where we solved the same instance by the branchandbound algorithm does this greedy algorithm always yield an optimal solution the answer of course is no if it did we would have a polynomialtime algorithm for the nphard problem in fact the following example shows that no finite upper bound on the accuracy of its approximate solutions can be given either example item weight value valueweight the knapsack capacity is w w w since the items are already ordered as required the algorithm takes the first item and skips the second one the value of this subset is the optimal selection consists of item whose value is w hence the accuracy ratio rsa of this approximate solution is w which is unbounded above it is surprisingly easy to tweak this greedy algorithm to get an approximation algorithm with a finite performance ratio all it takes is to choose the better of two alternatives the one obtained by the greedy algorithm or the one consisting of a single item of the largest value that fits into the knapsack note that for the instance of the preceding example the second alternative is better than the first one it is not difficult to prove that the performance ratio of this enhanced greedy algorithm is that is the value of an optimal subset s will never be more than twice as large as the value of the subset sa obtained by this enhanced greedy algorithm and is the smallest multiple for which such an assertion can be made it is instructive to consider the continuous version of the knapsack problem as well in this version we are permitted to take arbitrary fractions of the items given for this version of the problem it is natural to modify the greedy algorithm as follows greedy algorithm for the continuous knapsack problem step compute the valuetoweight ratios viwi i n for the items given step sort the items in nonincreasing order of the ratios computed in step ties can be broken arbitrarily step repeat the following operation until the knapsack is filled to its full capacity or no item is left in the sorted list if the current item on the list fits into the knapsack in its entirety take it and proceed to the next item otherwise take its largest fraction to fill the knapsack to its full capacity and stop for example for the fouritem instance used in example to illustrate the greedy algorithm for the discrete version the algorithm will take the first item of weight and then of the next item on the sorted list to fill the knapsack to its full capacity it should come as no surprise that this algorithm always yields an optimal solution to the continuous knapsack problem indeed the items are ordered according to their efficiency in using the knapsacks capacity if the first item on the sorted list has weight w and value v no solution can use w units of capacity with a higher payoff than v if we can not fill the knapsack with the first item or its fraction we should continue by taking as much as we can of the secondmost efficient item and so on a formal rendering of this proof idea is somewhat involved and we will leave it for the exercises note also that the optimal value of the solution to an instance of the continuous knapsack problem can serve as an upper bound on the optimal value of the discrete version of the same instance this observation provides a more sophisticated way of computing upper bounds for solving the discrete knapsack problem by the branchandbound method than the one used in section approximation schemes we now return to the discrete version of the knapsack problem for this problem unlike the traveling salesman problem there exist polynomialtime approximation schemes which are parametric families of algorithms that allow us to get approximations sak with any predefined accuracy level f s k for any instance of size n f sak where k is an integer parameter in the range k n the first approximation scheme was suggested by s sahni in sah this algorithm generates all subsets of k items or less and for each one that fits into the knapsack it adds the remaining items as the greedy algorithm would do ie in nonincreasing order of their valuetoweight ratios the subset of the highest value obtained in this fashion is returned as the algorithms output example a small example of an approximation scheme with k is provided in figure the algorithm yields which is the optimal solution for this instance you can be excused for not being overly impressed by this example and indeed the importance of this scheme is mostly theoretical rather than practical it lies in the fact that in addition to approximating the optimal solution with any predefined accuracy level the time efficiency of this algorithm is polynomial in n indeed the total number of subsets the algorithm generates before adding extra elements is k n k nn n j k k j j nj nk k nk j j j j item weight value valueweight subset added items value capacity w not feasible not feasible a b figure example of applying sahnis approximation scheme for k a instance b subsets generated by the algorithm for each of those subsets it needs on time to determine the subsets possible extension thus the algorithms efficiency is in oknk note that although it is polynomial in n the time efficiency of sahnis scheme is exponential in k more sophisticated approximation schemes called fully polynomial schemes do not have this shortcoming among several books that discuss such algorithms the monographs mar and kel are especially recommended for their wealth of other material about the knapsack problem exercises a apply the nearestneighbor algorithm to the instance defined by the intercity distance matrix below start the algorithm at the first city assuming that the cities are numbered from to b compute the accuracy ratio of this approximate solution a write pseudocode for the nearestneighbor algorithm assume that its input is given by an n n intercity distance matrix b what is the time efficiency of the nearestneighbor algorithm apply the twicearoundthetree algorithm to the graph in figure a with a walk around the minimum spanning tree that starts at the same vertex a but differs from the walk in figure b is the length of the obtained tour the same as the length of the tour in figure b prove that making a shortcut of the kind used by the twicearoundthetree algorithm can not increase the tours length in a euclidean graph what is the time efficiency class of the greedy algorithm for the knapsack problem prove that the performance ratio ra of the enhanced greedy algorithm for the knapsack problem is equal to consider the greedy algorithm for the binpacking problem which is called the firstfit ff algorithm place each of the items in the order given into the first bin the item fits in when there are no such bins place the item in a new bin and add this bin to the end of the bin list a apply ff to the instance s s s s s and determine whether the solution obtained is optimal b determine the worstcase time efficiency of ff c prove that ff is a approximation algorithm the firstfit decreasing ffd approximation algorithm for the binpacking problem starts by sorting the items in nonincreasing order of their sizes and then acts as the firstfit algorithm a apply ffd to the instance s s s s s and determine whether the solution obtained is optimal b does ffd always yield an optimal solution justify your answer c prove that ffd is a approximation algorithm d run an experiment to determine which of the two algorithms ff or ffd yields more accurate approximations on a random sample of the problems instances a design a simple approximation algorithm for finding a minimum vertex cover a vertex cover with the smallest number of vertices in a given graph b consider the following approximation algorithm for finding a maximum independent set an independent set with the largest number of vertices in a given graph apply the approximation algorithm of part a and output all the vertices that are not in the obtained vertex cover can we claim that this algorithm is a approximation algorithm too a design a polynomialtime greedy algorithm for the graphcoloring problem b show that the performance ratio of your approximation algorithm is infinitely large algorithms for solving nonlinear equations in this section we discuss several algorithms for solving nonlinear equations in one unknown f x there are several reasons for this choice among subareas of numerical analysis first of all this is an extremely important problem from both a practical and theoretical point of view it arises as a mathematical model of numerous phenomena in the sciences and engineering both directly and indirectly recall for example that the standard calculus technique for finding extremum points of a function f x is based on finding its critical points which are the roots of the equation f x second it represents the most accessible topic in numerical analysis and at the same time exhibits its typical tools and concerns third some methods for solving equations closely parallel algorithms for array searching and hence provide examples of applying general algorithm design techniques to problems of continuous mathematics let us start with dispelling a misconception you might have about solving equations your experience with equation solving from middle school to calculus courses might have led you to believe that we can solve equations by factoring or by applying a readily available formula sorry to break it to you but you have been deceived with the best of educational intentions of course you were able to solve all those equations only because they had been carefully selected to make it possible in general we can not solve equations exactly and need approximation algorithms to do so this is true even for solving the quadratic equation ax bx c because the standard formula for its roots x b b ac a requires computing the square root which can be done only approximately for most positive numbers in addition as we discussed in section this canonical formula needs to be modified to avoid the possibility of lowaccuracy solutions what about formulas for roots of polynomials of degrees higher than two such formulas for thirdand fourthdegree polynomials exist but they are too cumbersome to be of practical value for polynomials of degrees higher than four there can be no general formula for their roots that would involve only the polynomials coefficients arithmetical operations and radicals taking roots this remarkable result was published first by the italian mathematician and physician paolo ruffini in and rediscovered a quarter century later by the norwegian mathematician niels abel it was developed further by the french mathematician evariste galois the impossibility of such a formula can hardly be considered a great disappointment as the great german mathematician carl friedrich gauss put it in his thesis of the algebraic solution of an equation was no better than devising a symbol for the root of the equation and then saying that the equation had a root equal to the symbol oco we can interpret solutions to equation as points at which the graph of the function f x intersects with the xaxis the three algorithms we discuss in this section take advantage of this interpretation of course the graph of f x may intersect the xaxis at a single point eg x at multiple or even infinitely many points sin x or at no point ex equation would then have a single root several roots and no roots respectively it is a good idea to sketch a graph of the function before starting to approximate its roots it can help to determine the number of roots and their approximate locations in general it is a good idea to isolate roots ie to identify intervals containing a single root of the equation in question bisection method this algorithm is based on an observation that the graph of a continuous function must intersect with the xaxis between two points a and b at least once if the functions values have opposite signs at these two points figure the validity of this observation is proved as a theorem in calculus courses and we take it for granted here it serves as the basis of the following algorithm called the bisection method for solving equation starting with an interval a b at whose endpoints f x has opposite signs the algorithm computes the value of f x at the middle point xmid a b if f xmid a root was found and the algorithm stops otherwise it continues the search for a root either on a xmid or on xmid b depending on which of the two halves the values of f x have opposite signs at the endpoints of the new interval since we can not expect the bisection algorithm to stumble on the exact value of the equations root and stop we need a different criterion for stopping the algo ruffinis discovery was completely ignored by almost all prominent mathematicians of that time abel died young after a difficult life of poverty galois was killed in a duel when he was only years old their results on the solution of higherdegree equations are now considered to be among the crowning achievements in the history of mathematics f x a x b x figure first iteration of the bisection method x is the middle point of interval a b rithm we can stop the algorithm after the interval an bn bracketing some root x becomes so small that we can guarantee that the absolute error of approximating x by xn the middle point of this interval is smaller than some small preselected number since xn is the middle point of an bn and x lies within this interval as well we have xn x bn an hence we can stop the algorithm as soon as bn an or equivalently xn an it is not difficult to prove that xn x b a for n n this inequality implies that the sequence of approximations xn can be made as close to root x as we wish by choosing n large enough in other words we can say that xn converges to root x note however that because any digital computer represents extremely small values by zero section the convergence assertion is true in theory but not necessarily in practice in fact if we choose below a certain machinedependent threshold the algorithm may never stop another source of potential complications is roundoff errors in computing values of the function in question therefore it is a good practice to include in a program implementing the bisection method a limit on the number of iterations the algorithm is allowed to run here is pseudocode of the bisection method algorithm bisectionf x a b eps n implements the bisection method for finding a root of f x input two real numbers a and b a b a continuous function f x on a b f af b an upper bound on the absolute error eps an upper bound on the number of iterations n output an approximate or exact value x of a root in a b or an interval bracketing the root if the iteration number limit is reached n iteration count while n n do x a b if x a eps return x fval f x if fval return x if fval f a bx else a x nn return iteration limit a b note that we can use inequality to find in advance the number of iterations that should suffice at least in theory to achieve a preselected accuracy level indeed choosing the number of iterations n large enough to satisfy b an ie n log b a does the trick example let us consider equation x x it has one real root see figure for the graph of f x x x since f and f the root must lie within interval if we choose the error tolerance level as inequality would require n log or n iterations figure contains a trace of the first eight iterations of the bisection method applied to equation thus we obtained x as an approximate value for the root x of equation and we can guarantee that x moreover if we take into account the signs of the function f x at a b and x we can assert that the root lies between and the principal weakness of the bisection method as a general algorithm for solving equations is its slow rate of convergence compared with other known methods it is for this reason that the method is rarely used also it can not be extended to solving more general equations and systems of equations but it does have several strong points it always converges to a root whenever we start with an y fx x x x figure graph of function f x x x n an bn xn f xn figure trace of the bisection method for solving equation the signs after the numbers in the second and third columns indicate the sign of f x x x at the corresponding endpoints of the intervals interval whose properties are very easy to check and it does not use derivatives of the function f x as some faster methods do what important algorithm does the method of bisection remind you of if you have found it to closely resemble binary search you are correct both of them solve variations of the searching problem and they are both dividebyhalf algorithms the principal difference lies in the problems domain discrete for binary search and continuous for the bisection method also note that while binary search requires its input array to be sorted the bisection method does not require its function to be nondecreasing or nonincreasing finally whereas binary search is very fast the bisection method is relatively slow fx an xn bn x figure iteration of the method of false position method of false position the method of false position also known by its name in latin regula falsi is to interpolation search as the bisection method is to binary search like the bisection method it has on each iteration some interval an bn bracketing a root of a continuous function f x that has oppositesign values at an and bn unlike the bisection method however it computes the next root approximation not as the middle of an bn but as the xintercept of the straight line through the points an f an and bn f bn figure you are asked in the exercises to show that the formula for this xintercept can be written as xn anf bn bnf an f bn f an example figure contains the results of the first eight iterations of this method for solving equation although for this example the method of false position does not perform as well as the bisection method for many instances it yields a faster converging sequence newtons method newtons method also called the newtonraphson method is one of the most important general algorithms for solving equations when applied to equation in one unknown it can be illustrated by figure the next element xn of the methods approximation sequence is obtained as the xintercept of the tangent line to the graph of function f x at xn the analytical formula for the elements of the approximation sequence turns out to be xn xn f xn for n f xn an bn xn f xn figure trace of the method of false position for equation the signs after the numbers in the second and third columns indicate the sign of f x x x at the corresponding endpoints of the intervals fxn xn xn x figure iteration of newtons method in most cases newtons algorithm guarantees convergence of sequence if an initial approximation x is chosen close enough to the root precisely defined prescriptions for choosing x can be found in numerical analysis textbooks it may converge for initial approximations far from the root as well but this is not always true example computing a for a can be done by finding a nonnegative root of equation x a if we use formula for this case of f x x a and f x x we obtain xn xn f xn xn xn a xn a a xn f xn xn xn xn which is exactly the formula we used in section for computing approximate values of square roots example let us apply newtons method to equation which we previously solved with the bisection method and the method of false position formula for this case becomes xn xn xn xn xn as an initial element of the approximation sequence we take say x figure contains the results of the first five iterations of newtons method you can not fail to notice how much faster newtons approximation sequence converges to the root than the approximation sequences of both the bisection method and the method of false position this very fast convergence is typical of newtons method if an initial approximation is close to the equations root note however that on each iteration of this method we need to evaluate new values of the function and its derivative whereas the previous two methods require only one new value of the function itself also newtons method does not bracket a root as these two methods do moreover for an arbitrary function and arbitrarily chosen initial approximation its approximation sequence may diverge and because formula has the functions derivative in the denominator the method may break down if it is equal to zero in fact newtons method is most effective when f x is bounded away from zero near root x in particular if f x m on the interval between xn and x we can estimate the distance between xn and x by using the mean value theorem of calculus as follows f xn f x f cxn x where c is some point between xn and x since f x and f c m we obtain n xn xn f xn figure trace of newtons method for equation xn x f xn m formula can be used as a criterion for stopping newtons algorithm when its righthand side becomes smaller than a preselected accuracy level other possible stopping criteria are xn xn and f xn where is a small positive number since the last two criteria do not necessarily imply closeness of xn to root x they should be considered inferior to the one based on the shortcomings of newtons method should not overshadow its principal strengths fast convergence for an appropriately chosen initial approximation and applicability to much more general types of equations and systems of equations asymptotic notations and basic efficiency classes as pointed out in the previous section the efficiency analysis framework concentrates on the order of growth of an algorithms basic operation count as the principal indicator of the algorithms efficiency to compare and rank such orders of growth computer scientists use three notations o big oh big omega and big theta first we introduce these notations informally and then after several examples formal definitions are given in the following discussion t n and gn can be any nonnegative functions defined on the set of natural numbers in the context we are interested in t n will be an algorithms running time usually indicated by its basic operation count cn and gn will be some simple function to compare the count with informal introduction informally ogn is the set of all functions with a lower or same order of growth as gn to within a constant multiple as n goes to infinity thus to give a few examples the following assertions are all true n on n on nn on indeed the first two functions are linear and hence have a lower order of growth than gn n while the last one is quadratic and hence has the same order of growth as n on the other hand n on n on n n on indeed the functions n and n are both cubic and hence have a higher order of growth than n and so has the fourthdegree polynomial n n the second notation gn stands for the set of all functions with a higher or same order of growth as gn to within a constant multiple as n goes to infinity for example n n nn n but n n finally gn is the set of all functions that have the same order of growth as gn to within a constant multiple as n goes to infinity thus every quadratic function an bn c with a is in n but so are among infinitely many others n sin n and n log n can you explain why hopefully this informal introduction has made you comfortable with the idea behind the three asymptotic notations so now come the formal definitions o notation definition a function t n is said to be in ogn denoted t n ogn if t n is bounded above by some constant multiple of gn for all large n ie if there exist some positive constant c and some nonnegative integer n such that t n cgn for all n n the definition is illustrated in figure where for the sake of visual clarity n is extended to be a real number as an example let us formally prove one of the assertions made in the introduction n on indeed n n n for all n n n thus as values of the constants c and n required by the definition we can take and respectively note that the definition gives us a lot of freedom in choosing specific values for constants c and n for example we could also reason that n n n for all n n to complete the proof with c and n cg n t n doesnt matter n n figure bigoh notation t n ogn t n cg n doesnt matter n n figure bigomega notation t n gn notation definition a function t n is said to be in gn denoted t n gn if t n is bounded below by some positive constant multiple of gn for all large n ie if there exist some positive constant c and some nonnegative integer n such that t n cgn for all n n the definition is illustrated in figure here is an example of the formal proof that n n n n for all n ie we can select c and n cg n t n cg n doesnt matter n n figure bigtheta notation t n gn notation definition a function t n is said to be in gn denoted t n gn if t n is bounded both above and below by some positive constant multiples of gn for all large n ie if there exist some positive constants c and c and some nonnegative integer n such that cgn t n cgn for all n n the definition is illustrated in figure for example let us prove that nn n first we prove the right inequality the upper bound nn n n n for all n second we prove the left inequality the lower bound nn n n n n n for all n n hence we can select c c and n useful property involving the asymptotic notations using the formal definitions of the asymptotic notations we can prove their general properties see problem in this sections exercises for a few simple examples the following property in particular is useful in analyzing algorithms that comprise two consecutively executed parts theorem if tn ogn and tn ogn then tn tn omaxgn gn the analogous assertions are true for the and notations as well proof the proof extends to orders of growth the following simple fact about four arbitrary real numbers a b a b if a b and a b then a a maxb b since tn ogn there exist some positive constant c and some nonnegative integer n such that tn cgn for all n n similarly since tn ogn tn cgn for all n n let us denote c maxc c and consider n maxn n so that we can use both inequalities adding them yields the following tn tn cgn cgn cgn cgn cgn gn c maxgn gn hence tn tn omaxgn gn with the constants c and n required by the o definition being c maxc c and maxn n respectively so what does this property imply for an algorithm that comprises two consecutively executed parts it implies that the algorithms overall efficiency is determined by the part with a higher order of growth ie its least efficient part tn ogn tn tn omaxgn gn tn ogn for example we can check whether an array has equal elements by the following twopart algorithm first sort the array by applying some known sorting algorithm second scan the sorted array to check its consecutive elements for equality if for example a sorting algorithm used in the first part makes no more than nn comparisons and hence is in on while the second part makes no more than n comparisons and hence is in on the efficiency of the entire algorithm will be in omaxn n on using limits for comparing orders of growth though the formal definitions of o and are indispensable for proving their abstract properties they are rarely used for comparing the orders of growth of two specific functions a much more convenient method for doing so is based on computing the limit of the ratio of two functions in question three principal cases may arise implies that t n has a smaller order of growth than gn t n lim c implies that t n has the same order of growth as gn n gn implies that t n has a larger order of growth than gn note that the first two cases mean that t n ogn the last two mean that t n gn and the second case means that t n gn the limitbased approach is often more convenient than the one based on the definitions because it can take advantage of the powerful calculus techniques developed for computing limits such as lho pitals rule lim t n lim t n n gn n g n and stirlings formula n n n n e for large values of n here are three examples of using the limitbased approach to comparing orders of growth of two functions example compare the orders of growth of nn and n this is one of the examples we used at the beginning of this section to illustrate the definitions nn n n lim lim lim n n n n n n since the limit is equal to a positive constant the functions have the same order of growth or symbolically nn n example compare the orders of growth of log n and n unlike example the answer here is not immediately obvious log n log n log e lim lim lim n log e lim n n n n n n n n since the limit is equal to zero log n has a smaller order of growth than n since limn log n we can use the socalled littleoh notation log n o n n unlike the bigoh the littleoh notation is rarely used in analysis of algorithms the fourth case in which such a limit does not exist rarely happens in the actual practice of analyzing algorithms still this possibility makes the limitbased approach to comparing orders of growth less general than the one based on the definitions of o and example compare the orders of growth of n and n we discussed this informally in section taking advantage of stirlings formula we get n n n nn n lim n lim e lim n lim n n n n n n n nen n e thus though n grows very fast ngrows still faster we can write symbolically that n n note however that while the bigomega notation does not preclude the possibility that n and n have the same order of growth the limit computed here certainly does basic efficiency classes even though the efficiency analysis framework puts together all the functions whose orders of growth differ by a constant multiple there are still infinitely many such classes for example the exponential functions an have different orders of growth for different values of base a therefore it may come as a surprise that the time efficiencies of a large number of algorithms fall into only a few classes these classes are listed in table in increasing order of their orders of growth along with their names and a few comments you could raise a concern that classifying algorithms by their asymptotic efficiency would be of little practical use since the values of multiplicative constants are usually left unspecified this leaves open the possibility of an algorithm in a worse efficiency class running faster than an algorithm in a better efficiency class for inputs of realistic sizes for example if the running time of one algorithm is n while the running time of the other is n the cubic algorithm will outperform the quadratic algorithm unless n exceeds a few such anomalies are indeed known fortunately multiplicative constants usually do not differ that drastically as a rule you should expect an algorithm from a better asymptotic efficiency class to outperform an algorithm from a worse class even for moderately sized inputs this observation is especially true for an algorithm with a better than exponential running time versus an exponential or worse algorithm exercises use the most appropriate notation among o and to indicate the time efficiency class of sequential search see section a in the worst case b in the best case c in the average case use the informal definitions of o and to determine whether the following assertions are true or false table basic asymptotic efficiency classes class name comments constant short of bestcase efficiencies very few reasonable examples can be given since an algorithms running time typically goes to infinity when its input size grows infinitely large log n logarithmic typically a result of cutting a problems size by a constant factor on each iteration of the algorithm see section note that a logarithmic algorithm can not take into account all its input or even a fixed fraction of it any algorithm that does so will have at least linear running time n linear algorithms that scan a list of size n eg sequential search belong to this class n log n linearithmic many divideandconquer algorithms see chapter including mergesort and quicksort in the average case fall into this category n quadratic typically characterizes efficiency of algorithms with two embedded loops see the next section elementary sorting algorithms and certain operations on n n matrices are standard examples n cubic typically characterizes efficiency of algorithms with three embedded loops see the next section several nontrivial algorithms from linear algebra fall into this class n exponential typical for algorithms that generate all subsets of an nelement set often the term exponential is used in a broader sense to include this and larger orders of growth as well n factorial typical for algorithms that generate all permutations of an nelement set a nn on b nn on c nn n d nn n for each of the following functions indicate the class gn the function belongs to use the simplest gn possible in your answers prove your assertions a n b n n c n lgn n lg n d n n e log n a table contains values of several functions that often arise in the analysis of algorithms these values certainly suggest that the functions log n n n log n n n n n are listed in increasing order of their order of growth do these values prove this fact with mathematical certainty b prove that the functions are indeed listed in increasing order of their order of growth list the following functions according to their order of growth from the lowest to the highest n lgn n n n ln n n n a prove that every polynomial of degree k pn aknk aknk a with ak belongs to nk b prove that exponential functions an have different orders of growth for different values of base a prove the following assertions by using the definitions of the notations involved or disprove them by giving a specific counterexample a if t n ogn then gn t n b gn gn where c gn ogn gn d for any two nonnegative functions t n and gn defined on the set of nonnegative integers either t n ogn or t n gn or both prove the sections theorem for a notation b notation we mentioned in this section that one can check whether all elements of an array are distinct by a twopart algorithm based on the arrays presorting a if the presorting is done by an algorithm with a time efficiency in n log n what will be a timeefficiency class of the entire algorithm b if the sorting algorithm used for presorting needs an extra array of size n what will be the spaceefficiency class of the entire algorithm the range of a finite nonempty set of n real numbers s is defined as the difference between the largest and smallest elements of s for each representation of s given below describe in english an algorithm to compute the range indicate the time efficiency classes of these algorithms using the most appropriate notation o or a an unsorted array b a sorted array c a sorted singly linked list d a binary search tree lighter or heavier you have n identicallooking coins and a twopan balance scale with no weights one of the coins is a fake but you do not know whether it is lighter or heavier than the genuine coins which all weigh the same design a algorithm to determine whether the fake coin is lighter or heavier than the others door in a wall you are facing a wall that stretches infinitely in both directions there is a door in the wall but you know neither how far away nor in which direction you can see the door only when you are right next to it design an algorithm that enables you to reach the door by walking at most on steps where n is the unknown to you number of steps between your initial position and the door par mathematical analysis of nonrecursive algorithms in this section we systematically apply the general framework outlined in section to analyzing the time efficiency of nonrecursive algorithms let us start with a very simple example that demonstrates all the principal steps typically taken in analyzing such algorithms example consider the problem of finding the value of the largest element in a list of n numbers for simplicity we assume that the list is implemented as an array the following is pseudocode of a standard algorithm for solving the problem algorithm maxelementan determines the value of the largest element in a given array input an array an of real numbers output the value of the largest element in a maxval a for i to n do if ai maxval maxval ai return maxval the obvious measure of an inputs size here is the number of elements in the array ie n the operations that are going to be executed most often are in the algorithms for loop there are two operations in the loops body the comparison ai maxval and the assignment maxval ai which of these two operations should we consider basic since the comparison is executed on each repetition of the loop and the assignment is not we should consider the comparison to be the algorithms basic operation note that the number of comparisons will be the same for all arrays of size n therefore in terms of this metric there is no need to distinguish among the worst average and best cases here let us denote cn the number of times this comparison is executed and try to find a formula expressing it as a function of size n the algorithm makes one comparison on each execution of the loop which is repeated for each value of the loops variable i within the bounds and n inclusive therefore we get the following sum for cn n cn i this is an easy sum to compute because it is nothing other than repeated n times thus n cn n n i here is a general plan to follow in analyzing nonrecursive algorithms general plan for analyzing the time efficiency of nonrecursive algorithms decide on a parameter or parameters indicating an inputs size identify the algorithms basic operation as a rule it is located in the innermost loop check whether the number of times the basic operation is executed depends only on the size of an input if it also depends on some additional property the worstcase averagecase and if necessary bestcase efficiencies have to be investigated separately set up a sum expressing the number of times the algorithms basic operation is executed using standard formulas and rules of sum manipulation either find a closedform formula for the count or at the very least establish its order of growth before proceeding with further examples you may want to review appendix a which contains a list of summation formulas and rules that are often useful in analysis of algorithms in particular we use especially frequently two basic rules of sum manipulation u u cai c ai r il il u u u ai bi ai bi r il il il sometimes an analysis of a nonrecursive algorithm requires setting up not a sum but a recurrence relation for the number of times its basic operation is executed using recurrence relations is much more typical for analyzing recursive algorithms see section and two summation formulas u ul where l u are some lower and upper integer limits s il n n i n nn n i n s i i note that the formula n n which we used in example is a special i case of formula s for l and u n example consider the element uniqueness problem check whether all the elements in a given array of n elements are distinct this problem can be solved by the following straightforward algorithm algorithm uniqueelementsan determines whether all the elements in a given array are distinct input an array an output returns true if all the elements in a are distinct and false otherwise for i to n do for j i to n do if ai aj return false return true the natural measure of the inputs size here is again n the number of elements in the array since the innermost loop contains a single operation the comparison of two elements we should consider it as the algorithms basic operation note however that the number of element comparisons depends not only on n but also on whether there are equal elements in the array and if there are which array positions they occupy we will limit our investigation to the worst case only by definition the worst case input is an array for which the number of element comparisons cworstn is the largest among all arrays of size n an inspection of the innermost loop reveals that there are two kinds of worstcase inputs inputs for which the algorithm does not exit the loop prematurely arrays with no equal elements and arrays in which the last two elements are the only pair of equal elements for such inputs one comparison is made for each repetition of the innermost loop ie for each value of the loop variable j between its limits i and n this is repeated for each value of the outer loop ie for each value of the loop variable i between its limits and n accordingly we get n n n n cworst n n i n i i j i i i n n n n n n i n i i i n n n n n n n we also could have computed the sum nin i faster as follows n n n n i n n i where the last equality is obtained by applying summation formula s note that this result was perfectly predictable in the worst case the algorithm needs to compare all nn distinct pairs of its n elements example given two n n matrices a and b find the time efficiency of the definitionbased algorithm for computing their product c ab by definition c is an n n matrix whose elements are computed as the scalar dot products of the rows of matrix a and the columns of matrix b a b c row i c i j col j where ci j ai b j ai kbk j ai n bn j for every pair of indices i j n algorithm matrixmultiplicationan n bn n multiplies two square matrices of order n by the definitionbased algorithm input two n n matrices a and b output matrix c ab for i to n do for j to n do ci j for k to n do ci j ci j ai k bk j return c we measure an inputs size by matrix order n there are two arithmetical operations in the innermost loop here multiplication and addition that in principle can compete for designation as the algorithms basic operation actually we do not have to choose between them because on each repetition of the innermost loop each of the two is executed exactly once so by counting one we automatically count the other still following a wellestablished tradition we consider multiplication as the basic operation see section let us set up a sum for the total number of multiplications mn executed by the algorithm since this count depends only on the size of the input matrices we do not have to investigate the worstcase averagecase and bestcase efficiencies separately obviously there is just one multiplication executed on each repetition of the algorithms innermost loop which is governed by the variable k ranging from the lower bound to the upper bound n therefore the number of multiplications made for every pair of specific values of variables i and j is n k and the total number of multiplications mn is expressed by the following triple sum n n n mn i j k now we can compute this sum by using formula s and rule r given above starting with the innermost sum n which is equal to n why we get k n n n n n n mn n n n i j k i j i this example is simple enough so that we could get this result without all the summation machinations how the algorithm computes n elements of the product matrix each of the products elements is computed as the scalar dot product of an nelement row of the first matrix and an nelement column of the second matrix which takes n multiplications so the total number of multiplications is n n n it is this kind of reasoning that we expected you to employ when answering this question in problem of exercises if we now want to estimate the running time of the algorithm on a particular machine we can do it by the product t n cmmn cmn where cm is the time of one multiplication on the machine in question we would get a more accurate estimate if we took into account the time spent on the additions too t n cmmn caan cmn can cm can where ca is the time of one addition note that the estimates differ only by their multiplicative constants and not by their order of growth you should not have the erroneous impression that the plan outlined above always succeeds in analyzing a nonrecursive algorithm an irregular change in a loop variable a sum too complicated to analyze and the difficulties intrinsic to the average case analysis are just some of the obstacles that can prove to be insurmountable these caveats notwithstanding the plan does work for many simple nonrecursive algorithms as you will see throughout the subsequent chapters of the book as a last example let us consider an algorithm in which the loops variable changes in a different manner from that of the previous examples example the following algorithm finds the number of binary digits in the binary representation of a positive decimal integer algorithm binaryn input a positive decimal integer n output the number of binary digits in ns binary representation count while n do count count n n return count first notice that the most frequently executed operation here is not inside the while loop but rather the comparison n that determines whether the loops body will be executed since the number of times the comparison will be executed is larger than the number of repetitions of the loops body by exactly the choice is not that important a more significant feature of this example is the fact that the loop variable takes on only a few values between its lower and upper limits therefore we have to use an alternative way of computing the number of times the loop is executed since the value of n is about halved on each repetition of the loop the answer should be about log n the exact formula for the number of times the comparison n will be executed is actually log n the number of bits in the binary representation of n according to formula we could also get this answer by applying the analysis technique based on recurrence relations we discuss this technique in the next section because it is more pertinent to the analysis of recursive algorithms exercises compute the following sums a b c n d n i e n ii i i i f n j g n n ij h n ii j i j i find the order of growth of the following sums use the gn notation with the simplest function gn possible a ni i b n lg i i iji j n c ini i d i the sample variance of n measurements x xn can be computed as either inxi x n xi where x i n n or n xi n xi n i i n find and compare the number of divisions multiplications and additions subtractions additions and subtractions are usually bunched together that are required for computing the variance according to each of these formulas consider the following algorithm algorithm mysteryn input a nonnegative integer n s for i to n do ssii return s a what does this algorithm compute b what is its basic operation c how many times is the basic operation executed d what is the efficiency class of this algorithm e suggest an improvement or a better algorithm altogether and indicate its efficiency class if you can not do it try to prove that in fact it can not be done consider the following algorithm algorithm secretan input an array an of n real numbers minval a maxval a for i to n do if ai minval minval ai if ai maxval maxval ai return maxval minval answer questions ae of problem about this algorithm consider the following algorithm algorithm enigmaan n input a matrix an n of real numbers for i to n do for j i to n do if ai j aj i return false return true answer questions ae of problem about this algorithm improve the implementation of the matrix multiplication algorithm see example by reducing the number of additions made by the algorithm what effect will this change have on the algorithms efficiency determine the asymptotic order of growth for the total number of times all the doors are toggled in the locker doors puzzle problem in exercises prove the formula n i n nn i either by mathematical induction or by following the insight of a yearold school boy named carl friedrich gauss who grew up to become one of the greatest mathematicians of all times mental arithmetic a table is filled with repeating numbers on its diagonals as shown below calculate the total sum of the tables numbers in your head after cra question consider the following version of an important algorithm that we will study later in the book algorithm gean n input an n n matrix an n of real numbers for i to n do for j i to n do for k i to n do aj k aj k ai k aj i ai i a find the time efficiency class of this algorithm b what glaring inefficiency does this pseudocode contain and how can it be eliminated to speed the algorithm up von neumanns neighborhood consider the algorithm that starts with a single square and on each of its n iterations adds new squares all around the outside how many onebyone squares are there after n iterations gar in the parlance of cellular automata theory the answer is the number of cells in the von neumann neighborhood of range n the results for n and are illustrated below n n n page numbering find the total number of decimal digits needed for numbering pages in a book of pages assume that the pages are numbered consecutively starting with mathematical analysis of recursive algorithms in this section we will see how to apply the general framework for analysis of algorithms to recursive algorithms we start with an example often used to introduce novices to the idea of a recursive algorithm example compute the factorial function f n n for an arbitrary nonnegative integer n since n n n n n for n and by definition we can compute f n f n n with the following recursive algorithm algorithm fn computes n recursively input a nonnegative integer n output the value of n if n return else return f n n for simplicity we consider n itself as an indicator of this algorithms input size rather than the number of bits in its binary expansion the basic operation of the algorithm is multiplication whose number of executions we denote mn since the function f n is computed according to the formula f n f n n for n alternatively we could count the number of times the comparison n is executed which is the same as counting the total number of calls made by the algorithm see problem in this sections exercises the number of multiplications mn needed to compute it must satisfy the equality mn mn for n to compute to multiply f n f n by n indeed mn multiplications are spent to compute f n and one more multiplication is needed to multiply the result by n the last equation defines the sequence mn that we need to find this equation defines mn not explicitly ie as a function of n but implicitly as a function of its value at another point namely n such equations are called recurrence relations or for brevity recurrences recurrence relations play an important role not only in analysis of algorithms but also in some areas of applied mathematics they are usually studied in detail in courses on discrete mathematics or discrete structures a very brief tutorial on them is provided in appendix b our goal now is to solve the recurrence relation mn mn ie to find an explicit formula for mn in terms of n only note however that there is not one but infinitely many sequences that satisfy this recurrence can you give examples of say two of them to determine a solution uniquely we need an initial condition that tells us the value with which the sequence starts we can obtain this value by inspecting the condition that makes the algorithm stop its recursive calls if n return this tells us two things first since the calls stop when n the smallest value of n for which this algorithm is executed and hence mn defined is second by inspecting the pseudocodes exiting line we can see that when n the algorithm performs no multiplications therefore the initial condition we are after is m the calls stop when n no multiplications when n thus we succeeded in setting up the recurrence relation and initial condition for the algorithms number of multiplications mn mn mn for n m before we embark on a discussion of how to solve this recurrence let us pause to reiterate an important point we are dealing here with two recursively defined functions the first is the factorial function f n itself it is defined by the recurrence f n f n n for every n f the second is the number of multiplications mn needed to compute f n by the recursive algorithm whose pseudocode was given at the beginning of the section as we just showed mn is defined by recurrence and it is recurrence that we need to solve now though it is not difficult to guess the solution here what sequence starts with when n and increases by on each step it will be more useful to arrive at it in a systematic fashion from the several techniques available for solving recurrence relations we use what can be called the method of backward substitutions the methods idea and the reason for the name is immediately clear from the way it applies to solving our particular recurrence mn mn substitute mn mn mn mn substitute mn mn mn mn after inspecting the first three lines we see an emerging pattern which makes it possible to predict not only the next line what would it be but also a general formula for the pattern mn mn i i strictly speaking the correctness of this formula should be proved by mathematical induction but it is easier to get to the solution as follows and then verify its correctness what remains to be done is to take advantage of the initial condition given since it is specified for n we have to substitute i n in the patterns formula to get the ultimate result of our backward substitutions mn mn mn i i mn n n n you should not be disappointed after exerting so much effort to get this obvious answer the benefits of the method illustrated in this simple example will become clear very soon when we have to solve more difficult recurrences also note that the simple iterative algorithm that accumulates the product of n consecutive integers requires the same number of multiplications and it does so without the overhead of time and space used for maintaining the recursions stack the issue of time efficiency is actually not that important for the problem of computing n however as we saw in section the functions values get so large so fast that we can realistically compute exact values of n only for very small ns again we use this example just as a simple and convenient vehicle to introduce the standard approach to analyzing recursive algorithms generalizing our experience with investigating the recursive algorithm for computing n we can now outline a general plan for investigating recursive algorithms general plan for analyzing the time efficiency of recursive algorithms decide on a parameter or parameters indicating an inputs size identify the algorithms basic operation check whether the number of times the basic operation is executed can vary on different inputs of the same size if it can the worstcase averagecase and bestcase efficiencies must be investigated separately set up a recurrence relation with an appropriate initial condition for the number of times the basic operation is executed solve the recurrence or at least ascertain the order of growth of its solution example as our next example we consider another educational workhorse of recursive algorithms the tower of hanoi puzzle in this puzzle we or mythical monks if you do not like to move disks have n disks of different sizes that can slide onto any of three pegs initially all the disks are on the first peg in order of size the largest on the bottom and the smallest on top the goal is to move all the disks to the third peg using the second one as an auxiliary if necessary we can move only one disk at a time and it is forbidden to place a larger disk on top of a smaller one the problem has an elegant recursive solution which is illustrated in figure to move n disks from peg to peg with peg as auxiliary we first move recursively n disks from peg to peg with peg as auxiliary then move the largest disk directly from peg to peg and finally move recursively n disks from peg to peg using peg as auxiliary of course if n we simply move the single disk directly from the source peg to the destination peg figure recursive solution to the tower of hanoi puzzle let us apply the general plan outlined above to the tower of hanoi problem the number of disks n is the obvious choice for the inputs size indicator and so is moving one disk as the algorithms basic operation clearly the number of moves mn depends on n only and we get the following recurrence equation for it mn mn mn for n with the obvious initial condition m we have the following recurrence relation for the number of moves mn mn mn for n m we solve this recurrence by the same method of backward substitutions mn mn sub mn mn mn mn sub mn mn mn mn the pattern of the first three sums on the left suggests that the next one will be mn and generally after i substitutions we get mn imn i i i imn i i since the initial condition is specified for n which is achieved for i n we get the following formula for the solution to recurrence mn nmn n n nm n n n n thus we have an exponential algorithm which will run for an unimaginably long time even for moderate values of n see problem in this sections exercises this is not due to the fact that this particular algorithm is poor in fact it is not difficult to prove that this is the most efficient algorithm possible for this problem it is the problems intrinsic difficulty that makes it so computationally hard still this example makes an important general point one should be careful with recursive algorithms because their succinctness may mask their inefficiency when a recursive algorithm makes more than a single call to itself it can be useful for analysis purposes to construct a tree of its recursive calls in this tree nodes correspond to recursive calls and we can label them with the value of the parameter or more generally parameters of the calls for the tower of hanoi example the tree is given in figure by counting the number of nodes in the tree we can get the total number of calls made by the tower of hanoi algorithm n cn l where l is the level in the tree in figure n l n n n n n n n figure tree of recursive calls made by the recursive algorithm for the tower of hanoi puzzle the number agrees as it should with the move count obtained earlier example as our next example we investigate a recursive version of the algorithm discussed at the end of section algorithm binrecn input a positive decimal integer n output the number of binary digits in ns binary representation if n return else return binrec n let us set up a recurrence and an initial condition for the number of additions an made by the algorithm the number of additions made in computing binrec n is a n plus one more addition is made by the algorithm to increase the returned value by this leads to the recurrence an a n for n since the recursive calls end when n is equal to and there are no additions made then the initial condition is a the presence of n in the functions argument makes the method of backward substitutions stumble on values of n that are not powers of therefore the standard approach to solving such a recurrence is to solve it only for n k and then take advantage of the theorem called the smoothness rule see appendix b which claims that under very broad assumptions the order of growth observed for n k gives a correct answer about the order of growth for all values of n alternatively after getting a solution for powers of we can sometimes finetune this solution to get a formula valid for an arbitrary n so let us apply this recipe to our recurrence which for n k takes the form ak ak for k a now backward substitutions encounter no problems ak ak substitute ak ak ak ak substitute ak ak ak ak aki i akk k thus we end up with ak a k k or after returning to the original variable n k and hence k log n an log n log n in fact one can prove problem in this sections exercises that the exact solution for an arbitrary value of n is given by just a slightly more refined formula an log n this section provides an introduction to the analysis of recursive algorithms these techniques will be used throughout the book and expanded further as necessary in the next section we discuss the fibonacci numbers their analysis involves more difficult recurrence relations to be solved by a method different from backward substitutions exercises solve the following recurrence relations a xn xn for n x b xn xn for n x c xn xn n for n x d xn xn n for n x solve for n k e xn xn for n x solve for n k set up and solve a recurrence relation for the number of calls made by f n the recursive algorithm for computing n consider the following recursive algorithm for computing the sum of the first n cubes sn n algorithm sn input a positive integer n output the sum of the first n cubes if n return else return sn n n n a set up and solve a recurrence relation for the number of times the algorithms basic operation is executed b how does this algorithm compare with the straightforward nonrecursive algorithm for computing this sum consider the following recursive algorithm algorithm qn input a positive integer n if n return else return qn n a set up a recurrence relation for this functions values and solve it to determine what this algorithm computes b set up a recurrence relation for the number of multiplications made by this algorithm and solve it c set up a recurrence relation for the number of additionssubtractions made by this algorithm and solve it tower of hanoi a in the original version of the tower of hanoi puzzle as it was published in the s by e douard lucas a french mathematician the world will end after disks have been moved from a mystical tower of brahma estimate the number of years it will take if monks could move one disk per minute assume that monks do not eat sleep or die b how many moves are made by the ith largest disk i n in this algorithm c find a nonrecursive algorithm for the tower of hanoi puzzle and implement it in the language of your choice restricted tower of hanoi consider the version of the tower of hanoi puzzle in which n disks have to be moved from peg a to peg c using peg b so that any move should either place a disk on peg b or move a disk from that peg of course the prohibition of placing a larger disk on top of a smaller one remains in place too design a recursive algorithm for this problem and find the number of moves made by it a prove that the exact number of additions made by the recursive algorithm binrecn for an arbitrary positive decimal integer n is log n b set up a recurrence relation for the number of additions made by the nonrecursive version of this algorithm see section example and solve it a design a recursive algorithm for computing n for any nonnegative integer n that is based on the formula n n n b set up a recurrence relation for the number of additions made by the algorithm and solve it c draw a tree of recursive calls for this algorithm and count the number of calls made by the algorithm d is it a good algorithm for solving this problem consider the following recursive algorithm algorithm riddlean input an array an of real numbers if n return a else temp riddlean if temp an return temp else return an a what does this algorithm compute b set up a recurrence relation for the algorithms basic operation count and solve it consider the following algorithm to check whether a graph defined by its adjacency matrix is complete algorithm graphcompletean n input adjacency matrix an n of an undirected graph g output true if g is complete and false otherwise if n return onevertex graph is complete by definition else if not graphcompletean n return else for j to n do if an j return return what is the algorithms efficiency class in the worst case the determinant of an n n matrix a a n a a a n an an n denoted det a can be defined as a for n and for n by the recursive formula n det a sj a j det aj j where sj is if j is even and if j is odd a j is the element in row and column j and aj is the n n matrix obtained from matrix a by deleting its row and column j a set up a recurrence relation for the number of multiplications made by the algorithm implementing this recursive definition b without solving the recurrence what can you say about the solutions order of growth as compared to n von neumanns neighborhood revisited find the number of cells in the von neumann neighborhood of range n problem in exercises by setting up and solving a recurrence relation frying hamburgers there are n hamburgers to be fried on a small grill that can hold only two hamburgers at a time each hamburger has to be fried on both sides frying one side of a hamburger takes minute regardless of whether one or two hamburgers are fried at the same time consider the following recursive algorithm for executing this task in the minimum amount of time if n fry the hamburger or the two hamburgers together on each side if n fry any two hamburgers together on each side and then apply the same procedure recursively to the remaining n hamburgers a set up and solve the recurrence for the amount of time this algorithm needs to fry n hamburgers b explain why this algorithm does not fry the hamburgers in the minimum amount of time for all n c give a correct recursive algorithm that executes the task in the minimum amount of time celebrity problem a celebrity among a group of n people is a person who knows nobody but is known by everybody else the task is to identify a celebrity by only asking questions to people of the form do you know himher design an efficient algorithm to identify a celebrity or determine that the group has no such person how many questions does your algorithm need in the worst case example computing the nth fibonacci number in this section we consider the fibonacci numbers a famous sequence that can be defined by the simple recurrence f n f n f n for n and two initial conditions f f the fibonacci numbers were introduced by leonardo fibonacci in as a solution to a problem about the size of a rabbit population problem in this sections exercises many more examples of fibonaccilike numbers have since been discovered in the natural world and they have even been used in predicting the prices of stocks and commodities there are some interesting applications of the fibonacci numbers in computer science as well for example worstcase inputs for euclids algorithm discussed in section happen to be consecutive elements of the fibonacci sequence in this section we briefly consider algorithms for computing the nth element of this sequence among other benefits the discussion will provide us with an opportunity to introduce another method for solving recurrence relations useful for analysis of recursive algorithms to start let us get an explicit formula for f n if we try to apply the method of backward substitutions to solve recurrence we will fail to get an easily discernible pattern instead we can take advantage of a theorem that describes solutions to a homogeneous secondorder linear recurrence with constant coefficients axn bxn cxn where a b and c are some fixed real numbers a called the coefficients of the recurrence and xn is the generic term of an unknown sequence to be found applying this theorem to our recurrence with the initial conditions given see appendix b we obtain the formula f n n n where and it is hard to believe that formula which includes arbitrary integer powers of irrational numbers yields nothing else but all the elements of fibonacci sequence but it does one of the benefits of formula is that it immediately implies that f n grows exponentially remember fibonaccis rabbits ie f n n this constant is known as the golden ratio since antiquity it has been considered the most pleasing ratio of a rectangles two sides to the human eye and might have been consciously used by ancient architects and sculptors follows from the observation that is a fraction between and and hence n gets infinitely small as n goes to infinity in fact one can prove that the impact of the second term n on the value of f n can be obtained by rounding off the value of the first term to the nearest integer in other words for every nonnegative integer n f n n rounded to the nearest integer in the algorithms that follow we consider for the sake of simplicity such operations as additions and multiplications at unit cost since the fibonacci numbers grow infinitely large and grow very rapidly a more detailed analysis than the one offered here is warranted in fact it is the size of the numbers rather than a timeefficient method for computing them that should be of primary concern here still these caveats notwithstanding the algorithms we outline and their analysis provide useful examples for a student of the design and analysis of algorithms to begin with we can use recurrence and initial conditions for the obvious recursive algorithm for computing f n algorithm f n computes the nth fibonacci number recursively by using its definition input a nonnegative integer n output the nth fibonacci number if n return n else return f n f n before embarking on its formal analysis can you tell whether this is an efficient algorithm well we need to do a formal analysis anyway the algorithms basic operation is clearly addition so let an be the number of additions performed by the algorithm in computing f n then the numbers of additions needed for computing f n and f n are an and an respectively and the algorithm needs one more addition to compute their sum thus we get the following recurrence for an an an an for n a a the recurrence an an an is quite similar to recurrence f n f n f n but its righthand side is not equal to zero such recurrences are called inhomogeneous there are general techniques for solving inhomogeneous recurrences see appendix b or any textbook on discrete mathematics but for this particular recurrence a special trick leads to a faster solution we can reduce our inhomogeneous recurrence to a homogeneous one by rewriting it as an an an and substituting bn an bn bn bn b b this homogeneous recurrence can be solved exactly in the same manner as recurrence was solved to find an explicit formula for f n but it can actually be avoided by noting that bn is in fact the same recurrence as f n except that it starts with two s and thus runs one step ahead of f n so bn f n and an bn f n n n hence an n and if we measure the size of n by the number of bits b log n in its binary representation the efficiency class will be even worse namely doubly exponential ab b the poor efficiency class of the algorithm could be anticipated by the nature of recurrence indeed it contains two recursive calls with the sizes of smaller instances only slightly smaller than size n have you encountered such a situation before we can also see the reason behind the algorithms inefficiency by looking at a recursive tree of calls tracing the algorithms execution an example of such a tree for n is given in figure note that the same values of the function are being evaluated here again and again which is clearly extremely inefficient we can obtain a much faster algorithm by simply computing the successive elements of the fibonacci sequence iteratively as is done in the following algorithm algorithm fibn computes the nth fibonacci number iteratively by using its definition input a nonnegative integer n output the nth fibonacci number f f for i to n do f i f i f i return f n f f f f f f f f f f f f f f f figure tree of recursive calls for computing the th fibonacci number by the definitionbased algorithm this algorithm clearly makes n additions hence it is linear as a function of n and only exponential as a function of the number of bits b in ns binary representation note that using an extra array for storing all the preceding elements of the fibonacci sequence can be avoided storing just two values is necessary to accomplish the task see problem in this sections exercises the third alternative for computing the nth fibonacci number lies in using formula the efficiency of the algorithm will obviously be determined by the efficiency of an exponentiation algorithm used for computing n if it is done by simply multiplying by itself n times the algorithm will be in n b there are faster algorithms for the exponentiation problem for example we will discuss log n b algorithms for this problem in chapters and note also that special care should be exercised in implementing this approach to computing the nth fibonacci number since all its intermediate results are irrational numbers we would have to make sure that their approximations in the computer are accurate enough so that the final roundoff yields a correct result finally there exists a log n algorithm for computing the nth fibonacci number that manipulates only integers it is based on the equality f n f n n f n f n for n and an efficient way of computing matrix powers exercises find a web site dedicated to applications of the fibonacci numbers and study it fibonaccis rabbits problem a man put a pair of rabbits in a place surrounded by a wall how many pairs of rabbits will be there in a year if the initial pair of rabbits male and female are newborn and all rabbit pairs are not fertile during their first month of life but thereafter give birth to one new malefemale pair at the end of every month climbing stairs find the number of different ways to climb an nstair staircase if each step is either one or two stairs for example a stair staircase can be climbed three ways and how many even numbers are there among the first n fibonacci numbers ie among the numbers f f f n give a closedform formula valid for every n check by direct substitutions that the function n n indeed satisfies recurrence and initial conditions the maximum values of the java primitive types int and long are and respectively find the smallest n for which the nth fibonacci number is not going to fit in a memory allocated for a the type int b the type long consider the recursive definitionbased algorithm for computing the nth fibonacci number f n let cn and zn be the number of times f and f are computed respectively prove that a cn f n b zn f n improve algorithm f ib of the text so that it requires only space prove the equality f n f n n f n f n for n how many modulo divisions are made by euclids algorithm on two consecutive fibonacci numbers f n and f n as the algorithms input dissecting a fibonacci rectangle given a rectangle whose sides are two consecutive fibonacci numbers design an algorithm to dissect it into squares with no more than two squares being the same size what is the time efficiency class of your algorithm in the language of your choice implement two algorithms for computing the last five digits of the nth fibonacci number that are based on a the recursive definitionbased algorithm fn b the iterative definitionbased algorithm fibn perform an experiment to find the largest value of n for which your programs run under minute on your computer empirical analysis of algorithms in sections and we saw how algorithms both nonrecursive and recursive can be analyzed mathematically though these techniques can be applied successfully to many simple algorithms the power of mathematics even when enhanced with more advanced techniques see sed pur gra and gre is far from limitless in fact even some seemingly simple algorithms have proved to be very difficult to analyze with mathematical precision and certainty as we pointed out in section this is especially true for the averagecase analysis the principal alternative to the mathematical analysis of an algorithms efficiency is its empirical analysis this approach implies steps spelled out in the following plan general plan for the empirical analysis of algorithm time efficiency understand the experiments purpose decide on the efficiency metric m to be measured and the measurement unit an operation count vs a time unit decide on characteristics of the input sample its range size and so on prepare a program implementing the algorithm or algorithms for the experimentation generate a sample of inputs run the algorithm or algorithms on the samples inputs and record the data observed analyze the data obtained let us discuss these steps one at a time there are several different goals one can pursue in analyzing algorithms empirically they include checking the accuracy of a theoretical assertion about the algorithms efficiency comparing the efficiency of several algorithms for solving the same problem or different implementations of the same algorithm developing a hypothesis about the algorithms efficiency class and ascertaining the efficiency of the program implementing the algorithm on a particular machine obviously an experiments design should depend on the question the experimenter seeks to answer in particular the goal of the experiment should influence if not dictate how the algorithms efficiency is to be measured the first alternative is to insert a counter or counters into a program implementing the algorithm to count the number of times the algorithms basic operation is executed this is usually a straightforward operation you should only be mindful of the possibility that the basic operation is executed in several places in the program and that all its executions need to be accounted for as straightforward as this task usually is you should always test the modified program to ensure that it works correctly in terms of both the problem it solves and the counts it yields the second alternative is to time the program implementing the algorithm in question the easiest way to do this is to use a systems command such as the time command in unix alternatively one can measure the running time of a code fragment by asking for the system time right before the fragments start tstart and just after its completion tfinish and then computing the difference between the two tfinishtstart in c and c you can use the function clock for this purpose in java the method currenttimemillis in the system class is available it is important to keep several facts in mind however first a systems time is typically not very accurate and you might get somewhat different results on repeated runs of the same program on the same inputs an obvious remedy is to make several such measurements and then take their average or the median as the samples observation point second given the high speed of modern computers the running time may fail to register at all and be reported as zero the standard trick to overcome this obstacle is to run the program in an extra loop many times measure the total running time and then divide it by the number of the loops repetitions third on a computer running under a timesharing system such as unix the reported time may include the time spent by the cpu on other programs which obviously defeats the purpose of the experiment therefore you should take care to ask the system for the time devoted specifically to execution of if the system time is given in units called ticks the difference should be divided by a constant indicating the number of ticks per time unit your program in unix this time is called the user time and it is automatically provided by the time command thus measuring the physical running time has several disadvantages both principal dependence on a particular machine being the most important of them and technical not shared by counting the executions of a basic operation on the other hand the physical running time provides very specific information about an algorithms performance in a particular computing environment which can be of more importance to the experimenter than say the algorithms asymptotic efficiency class in addition measuring time spent on different segments of a program can pinpoint a bottleneck in the programs performance that can be missed by an abstract deliberation about the algorithms basic operation getting such data called profiling is an important resource in the empirical analysis of an algorithms running time the data in question can usually be obtained from the system tools available in most computing environments whether you decide to measure the efficiency by basic operation counting or by time clocking you will need to decide on a sample of inputs for the experiment often the goal is to use a sample representing a typical input so the challenge is to understand what a typical input is for some classes of algorithms eg for algorithms for the traveling salesman problem that we are going to discuss later in the book researchers have developed a set of instances they use for benchmarking but much more often than not an input sample has to be developed by the experimenter typically you will have to make decisions about the sample size it is sensible to start with a relatively small sample and increase it later if necessary the range of instance sizes typically neither trivially small nor excessively large and a procedure for generating instances in the range chosen the instance sizes can either adhere to some pattern eg or or be generated randomly within the range chosen the principal advantage of size changing according to a pattern is that its impact is easier to analyze for example if a samples sizes are generated by doubling you can compute the ratios mnmn of the observed metric m the count or the time to see whether the ratios exhibit a behavior typical of algorithms in one of the basic efficiency classes discussed in section the major disadvantage of nonrandom sizes is the possibility that the algorithm under investigation exhibits atypical behavior on the sample chosen for example if all the sizes in a sample are even and your algorithm runs much more slowly on oddsize inputs the empirical results will be quite misleading another important issue concerning sizes in an experiments sample is whether several instances of the same size should be included if you expect the observed metric to vary considerably on instances of the same size it would be probably wise to include several instances for every size in the sample there are welldeveloped methods in statistics to help the experimenter make such decisions you will find no shortage of books on this subject of course if several instances of the same size are included in the sample the averages or medians of the observed values for each size should be computed and investigated instead of or in addition to individual sample points much more often than not an empirical analysis requires generating random numbers even if you decide to use a pattern for input sizes you will typically want instances themselves generated randomly generating random numbers on a digital computer is known to present a difficult problem because in principle the problem can be solved only approximately this is the reason computer scientists prefer to call such numbers pseudorandom as a practical matter the easiest and most natural way of getting such numbers is to take advantage of a random number generator available in computer language libraries typically its output will be a value of a pseudorandom variable uniformly distributed in the interval between and if a different pseudorandom variable is desired an appropriate transformation needs to be made for example if x is a continuous random variable uniformly distributed on the interval x the variable y l xr l will be uniformly distributed among the integer values between integers l and r l r alternatively you can implement one of several known algorithms for generating pseudorandom numbers the most widely used and thoroughly studied of such algorithms is the linear congruential method algorithm randomn m seed a b generates a sequence of n pseudorandom numbers according to the linear congruential method input a positive integer n and positive integer parameters m seed a b output a sequence r rn of n pseudorandom integers uniformly distributed among integer values between and m note pseudorandom numbers between and can be obtained by treating the integers generated as digits after the decimal point r seed for i to n do ri a ri b mod m the simplicity of this pseudocode is misleading because the devil lies in the details of choosing the algorithms parameters here is a partial list of recommendations based on the results of a sophisticated mathematical analysis see knuii pp for details seed may be chosen arbitrarily and is often set to the current date and time m should be large and may be conveniently taken as w where w is the computers word size a should be selected as an integer between m and m with no particular pattern in its digits but such that a mod and the value of b can be chosen as the empirical data obtained as the result of an experiment need to be recorded and then presented for an analysis data can be presented numerically in a table or graphically in a scatterplot ie by points in a cartesian coordinate system it is a good idea to use both these options whenever it is feasible because both methods have their unique strengths and weaknesses the principal advantage of tabulated data lies in the opportunity to manipulate it easily for example one can compute the ratios mngn where gn is a candidate to represent the efficiency class of the algorithm in question if the algorithm is indeed in gn most likely these ratios will converge to some positive constant as n gets large note that careless novices sometimes assume that this constant must be which is of course incorrect according to the definition of gn or one can compute the ratios mnmn and see how the running time reacts to doubling of its input size as we discussed in section such ratios should change only slightly for logarithmic algorithms and most likely converge to and for linear quadratic and cubic algorithms respectively to name the most obvious and convenient cases on the other hand the form of a scatterplot may also help in ascertaining the algorithms probable efficiency class for a logarithmic algorithm the scatterplot will have a concave shape figure a this fact distinguishes it from all the other basic efficiency classes for a linear algorithm the points will tend to aggregate around a straight line or more generally to be contained between two straight lines figure b scatterplots of functions in n lg n and n will have a convex shape figure c making them difficult to differentiate a scatterplot of a cubic algorithm will also have a convex shape but it will show a much more rapid increase in the metrics values an exponential algorithm will most probably require a logarithmic scale for the vertical axis in which the values of loga mn rather than those of mn are plotted the commonly used logarithm base is or in such a coordinate system a scatterplot of a truly exponential algorithm should resemble a linear function because mn can implies logb mn logb c n logb a and vice versa one of the possible applications of the empirical analysis is to predict the algorithms performance on an instance not included in the experiment sample for example if you observe that the ratios mngn are close to some constant c for the sample instances it could be sensible to approximate mn by the product cgn for other instances too this approach should be used with caution especially for values of n outside the sample range mathematicians call such predictions extrapolation as opposed to interpolation which deals with values within the sample range of course you can try unleashing the standard techniques of statistical data analysis and prediction note however that the majority of such techniques are based on specific probabilistic assumptions that may or may not be valid for the experimental data in question it seems appropriate to end this section by pointing out the basic differences between mathematical and empirical analyses of algorithms the principal strength of the mathematical analysis is its independence of specific inputs its principal weakness is its limited applicability especially for investigating the averagecase efficiency the principal strength of the empirical analysis lies in its applicability to any algorithm but its results can depend on the particular sample of instances and the computer used in the experiment count or time count or time n n a b count or time n c figure typical scatter plots a logarithmic b linear c one of the convex functions exercises consider the following wellknown sorting algorithm which is studied later in the book with a counter inserted to count the number of key comparisons algorithm sortanalysisan input an array an of n orderable elements output the total number of key comparisons made count for i to n do v ai j i while j and aj v do count count aj aj j j aj v return count is the comparison counter inserted in the right place if you believe it is prove it if you believe it is not make an appropriate correction a run the program of problem with a properly inserted counter or counters for the number of key comparisons on random arrays of sizes b analyze the data obtained to form a hypothesis about the algorithms averagecase efficiency c estimate the number of key comparisons we should expect for a randomly generated array of size sorted by the same algorithm repeat problem by measuring the programs running time in milliseconds hypothesize a likely efficiency class of an algorithm based on the following empirical observations of its basic operations count size count what scale transformation will make a logarithmic scatterplot look like a linear one how can one distinguish a scatterplot for an algorithm in lg lg n from a scatterplot for an algorithm in lg n a find empirically the largest number of divisions made by euclids algorithm for computing gcdm n for n m b for each positive integer k find empirically the smallest pair of integers n m for which euclids algorithm needs to make k divisions in order to find gcdm n the averagecase efficiency of euclids algorithm on inputs of size n can be measured by the average number of divisions davgn made by the algorithm in computing gcdn gcdn gcdn n for example davg produce a scatterplot of davgn and indicate the algorithms likely averagecase efficiency class run an experiment to ascertain the efficiency class of the sieve of eratosthenes see section run a timing experiment for the three algorithms for computing gcdm n presented in section fundamentals of algorithmic problem solving let us start by reiterating an important point made in the introduction to this chapter we can consider algorithms to be procedural solutions to problems these solutions are not answers but specific instructions for getting answers it is this emphasis on precisely defined constructive procedures that makes computer science distinct from other disciplines in particular this distinguishes it from theoretical mathematics whose practitioners are typically satisfied with just proving the existence of a solution to a problem and possibly investigating the solutions properties we now list and briefly discuss a sequence of steps one typically goes through in designing and analyzing an algorithm figure understanding the problem from a practical perspective the first thing you need to do before designing an algorithm is to understand completely the problem given read the problems description carefully and ask questions if you have any doubts about the problem do a few small examples by hand think about special cases and ask questions again if needed there are a few types of problems that arise in computing applications quite often we review them in the next section if the problem in question is one of them you might be able to use a known algorithm for solving it of course it helps to understand how such an algorithm works and to know its strengths and weaknesses especially if you have to choose among several available algorithms but often you will not find a readily available algorithm and will have to design your own the sequence of steps outlined in this section should help you in this exciting but not always easy task an input to an algorithm specifies an instance of the problem the algorithm solves it is very important to specify exactly the set of instances the algorithm needs to handle as an example recall the variations in the set of instances for the three greatest common divisor algorithms discussed in the previous section if you fail to do this your algorithm may work correctly for a majority of inputs but crash on some boundary value remember that a correct algorithm is not one that works most of the time but one that works correctly for all legitimate inputs do not skimp on this first step of the algorithmic problemsolving process otherwise you will run the risk of unnecessary rework ascertaining the capabilities of the computational device once you completely understand a problem you need to ascertain the capabilities of the computational device the algorithm is intended for the vast majority of understand the problem decide on computational means exact vs approximate solving algorithm design technique design an algorithm prove correctness analyze the algorithm code the algorithm figure algorithm design and analysis process algorithms in use today are still destined to be programmed for a computer closely resembling the von neumann machine a computer architecture outlined by the prominent hungarianamerican mathematician john von neumann in collaboration with a burks and h goldstine in the essence of this architecture is captured by the socalled randomaccess machine ram its central assumption is that instructions are executed one after another one operation at a time accordingly algorithms designed to be executed on such machines are called sequential algorithms the central assumption of the ram model does not hold for some newer computers that can execute operations concurrently ie in parallel algorithms that take advantage of this capability are called parallel algorithms still studying the classic techniques for design and analysis of algorithms under the ram model remains the cornerstone of algorithmics for the foreseeable future should you worry about the speed and amount of memory of a computer at your disposal if you are designing an algorithm as a scientific exercise the answer is a qualified no as you will see in section most computer scientists prefer to study algorithms in terms independent of specification parameters for a particular computer if you are designing an algorithm as a practical tool the answer may depend on a problem you need to solve even the slow computers of today are almost unimaginably fast consequently in many situations you need not worry about a computer being too slow for the task there are important problems however that are very complex by their nature or have to process huge volumes of data or deal with applications where the time is critical in such situations it is imperative to be aware of the speed and memory available on a particular computer system choosing between exact and approximate problem solving the next principal decision is to choose between solving the problem exactly or solving it approximately in the former case an algorithm is called an exact algorithm in the latter case an algorithm is called an approximation algorithm why would one opt for an approximation algorithm first there are important problems that simply can not be solved exactly for most of their instances examples include extracting square roots solving nonlinear equations and evaluating definite integrals second available algorithms for solving a problem exactly can be unacceptably slow because of the problems intrinsic complexity this happens in particular for many problems involving a very large number of choices you will see examples of such difficult problems in chapters and third an approximation algorithm can be a part of a more sophisticated algorithm that solves a problem exactly algorithm design techniques now with all the components of the algorithmic problem solving in place how do you design an algorithm to solve a given problem this is the main question this book seeks to answer by teaching you several general design techniques what is an algorithm design technique an algorithm design technique or strategy or paradigm is a general approach to solving problems algorithmically that is applicable to a variety of problems from different areas of computing check this books table of contents and you will see that a majority of its chapters are devoted to individual design techniques they distill a few key ideas that have proven to be useful in designing algorithms learning these techniques is of utmost importance for the following reasons first they provide guidance for designing algorithms for new problems ie problems for which there is no known satisfactory algorithm therefore to use the language of a famous proverb learning such techniques is akin to learning to fish as opposed to being given a fish caught by somebody else it is not true of course that each of these general techniques will be necessarily applicable to every problem you may encounter but taken together they do constitute a powerful collection of tools that you will find quite handy in your studies and work second algorithms are the cornerstone of computer science every science is interested in classifying its principal subject and computer science is no exception algorithm design techniques make it possible to classify algorithms according to an underlying design idea therefore they can serve as a natural way to both categorize and study algorithms designing an algorithm and data structures while the algorithm design techniques do provide a powerful set of general approaches to algorithmic problem solving designing an algorithm for a particular problem may still be a challenging task some design techniques can be simply inapplicable to the problem in question sometimes several techniques need to be combined and there are algorithms that are hard to pinpoint as applications of the known design techniques even when a particular design technique is applicable getting an algorithm often requires a nontrivial ingenuity on the part of the algorithm designer with practice both tasks choosing among the general techniques and applying them get easier but they are rarely easy of course one should pay close attention to choosing data structures appropriate for the operations performed by the algorithm for example the sieve of eratosthenes introduced in section would run longer if we used a linked list instead of an array in its implementation why also note that some of the algorithm design techniques discussed in chapters and depend intimately on structuring or restructuring data specifying a problems instance many years ago an influential textbook proclaimed the fundamental importance of both algorithms and data structures for computer programming by its very title algorithms data structures programs wir in the new world of objectoriented programming data structures remain crucially important for both design and analysis of algorithms we review basic data structures in section methods of specifying an algorithm once you have designed an algorithm you need to specify it in some fashion in section to give you an example euclids algorithm is described in words in a free and also a stepbystep form and in pseudocode these are the two options that are most widely used nowadays for specifying algorithms using a natural language has an obvious appeal however the inherent ambiguity of any natural language makes a succinct and clear description of algorithms surprisingly difficult nevertheless being able to do this is an important skill that you should strive to develop in the process of learning algorithms pseudocode is a mixture of a natural language and programming languagelike constructs pseudocode is usually more precise than natural language and its usage often yields more succinct algorithm descriptions surprisingly computer scientists have never agreed on a single form of pseudocode leaving textbook authors with a need to design their own dialects fortunately these dialects are so close to each other that anyone familiar with a modern programming language should be able to understand them all this books dialect was selected to cause minimal difficulty for a reader for the sake of simplicity we omit declarations of variables and use indentation to show the scope of such statements as for if and while as you saw in the previous section we use an arrow for the assignment operation and two slashes for comments in the earlier days of computing the dominant vehicle for specifying algorithms was a flowchart a method of expressing an algorithm by a collection of connected geometric shapes containing descriptions of the algorithms steps this representation technique has proved to be inconvenient for all but very simple algorithms nowadays it can be found only in old algorithm books the state of the art of computing has not yet reached a point where an algorithms description be it in a natural language or pseudocode can be fed into an electronic computer directly instead it needs to be converted into a computer program written in a particular computer language we can look at such a program as yet another way of specifying the algorithm although it is preferable to consider it as the algorithms implementation proving an algorithms correctness once an algorithm has been specified you have to prove its correctness that is you have to prove that the algorithm yields a required result for every legitimate input in a finite amount of time for example the correctness of euclids algorithm for computing the greatest common divisor stems from the correctness of the equality gcdm n gcdn m mod n which in turn needs a proof see problem in exercises the simple observation that the second integer gets smaller on every iteration of the algorithm and the fact that the algorithm stops when the second integer becomes for some algorithms a proof of correctness is quite easy for others it can be quite complex a common technique for proving correctness is to use mathematical induction because an algorithms iterations provide a natural sequence of steps needed for such proofs it might be worth mentioning that although tracing the algorithms performance for a few specific inputs can be a very worthwhile activity it can not prove the algorithms correctness conclusively but in order to show that an algorithm is incorrect you need just one instance of its input for which the algorithm fails the notion of correctness for approximation algorithms is less straightforward than it is for exact algorithms for an approximation algorithm we usually would like to be able to show that the error produced by the algorithm does not exceed a predefined limit you can find examples of such investigations in chapter analyzing an algorithm we usually want our algorithms to possess several qualities after correctness by far the most important is efficiency in fact there are two kinds of algorithm efficiency time efficiency indicating how fast the algorithm runs and space efficiency indicating how much extra memory it uses a general framework and specific techniques for analyzing an algorithms efficiency appear in chapter another desirable characteristic of an algorithm is simplicity unlike efficiency which can be precisely defined and investigated with mathematical rigor simplicity like beauty is to a considerable degree in the eye of the beholder for example most people would agree that euclids algorithm is simpler than the middleschool procedure for computing gcdm n but it is not clear whether euclids algorithm is simpler than the consecutive integer checking algorithm still simplicity is an important algorithm characteristic to strive for why because simpler algorithms are easier to understand and easier to program consequently the resulting programs usually contain fewer bugs there is also the undeniable aesthetic appeal of simplicity sometimes simpler algorithms are also more efficient than more complicated alternatives unfortunately it is not always true in which case a judicious compromise needs to be made yet another desirable characteristic of an algorithm is generality there are in fact two issues here generality of the problem the algorithm solves and the set of inputs it accepts on the first issue note that it is sometimes easier to design an algorithm for a problem posed in more general terms consider for example the problem of determining whether two integers are relatively prime ie whether their only common divisor is equal to it is easier to design an algorithm for a more general problem of computing the greatest common divisor of two integers and to solve the former problem check whether the gcd is or not there are situations however where designing a more general algorithm is unnecessary or difficult or even impossible for example it is unnecessary to sort a list of n numbers to find its median which is its n th smallest element to give another example the standard formula for roots of a quadratic equation can not be generalized to handle polynomials of arbitrary degrees as to the set of inputs your main concern should be designing an algorithm that can handle a set of inputs that is natural for the problem at hand for example excluding integers equal to as possible inputs for a greatest common divisor algorithm would be quite unnatural on the other hand although the standard formula for the roots of a quadratic equation holds for complex coefficients we would normally not implement it on this level of generality unless this capability is explicitly required if you are not satisfied with the algorithms efficiency simplicity or generality you must return to the drawing board and redesign the algorithm in fact even if your evaluation is positive it is still worth searching for other algorithmic solutions recall the three different algorithms in the previous section for computing the greatest common divisor generally you should not expect to get the best algorithm on the first try at the very least you should try to finetune the algorithm you already have for example we made several improvements in our implementation of the sieve of eratosthenes compared with its initial outline in section can you identify them you will do well if you keep in mind the following observation of antoine de saintexupe ry the french writer pilot and aircraft designer a designer knows he has arrived at perfection not when there is no longer anything to add but when there is no longer anything to take away coding an algorithm most algorithms are destined to be ultimately implemented as computer programs programming an algorithm presents both a peril and an opportunity the peril lies in the possibility of making the transition from an algorithm to a program either incorrectly or very inefficiently some influential computer scientists strongly believe that unless the correctness of a computer program is proven with full mathematical rigor the program can not be considered correct they have developed special techniques for doing such proofs see gri but the power of these techniques of formal verification is limited so far to very small programs as a practical matter the validity of programs is still established by testing testing of computer programs is an art rather than a science but that does not mean that there is nothing in it to learn look up books devoted to testing and debugging even more important test and debug your program thoroughly whenever you implement an algorithm also note that throughout the book we assume that inputs to algorithms belong to the specified sets and hence require no verification when implementing algorithms as programs to be used in actual applications you should provide such verifications of course implementing an algorithm correctly is necessary but not sufficient you would not like to diminish your algorithms power by an inefficient implementation modern compilers do provide a certain safety net in this regard especially when they are used in their code optimization mode still you need to be aware of such standard tricks as computing a loops invariant an expression that does not change its value outside the loop collecting common subexpressions replacing expensive operations by cheap ones and so on see ker and ben for a good discussion of code tuning and other issues related to algorithm programming typically such improvements can speed up a program only by a constant factor whereas a better algorithm can make a difference in running time by orders of magnitude but once an algorithm is selected a speedup may be worth an effort i found this call for design simplicity in an essay collection by jon bentley ben the essays deal with a variety of issues in algorithm design and implementation and are justifiably titled programming pearls i wholeheartedly recommend the writings of both jon bentley and antoine de saintexupe ry a working program provides an additional opportunity in allowing an empirical analysis of the underlying algorithm such an analysis is based on timing the program on several inputs and then analyzing the results obtained we discuss the advantages and disadvantages of this approach to analyzing algorithms in section in conclusion let us emphasize again the main lesson of the process depicted in figure as a rule a good algorithm is a result of repeated effort and rework even if you have been fortunate enough to get an algorithmic idea that seems perfect you should still try to see whether it can be improved actually this is good news since it makes the ultimate result so much more enjoyable yes i did think of naming this book the joy of algorithms on the other hand how does one know when to stop in the real world more often than not a projects schedule or the impatience of your boss will stop you and so it should be perfection is expensive and in fact not always called for designing an algorithm is an engineeringlike activity that calls for compromises among competing goals under the constraints of available resources with the designers time being one of the resources in the academic world the question leads to an interesting but usually difficult investigation of an algorithms optimality actually this question is not about the efficiency of an algorithm but about the complexity of the problem it solves what is the minimum amount of effort any algorithm will need to exert to solve the problem for some problems the answer to this question is known for example any algorithm that sorts an array by comparing values of its elements needs about n log n comparisons for some arrays of size n see section but for many seemingly easy problems such as integer multiplication computer scientists do not yet have a final answer another important issue of algorithmic problem solving is the question of whether or not every problem can be solved by an algorithm we are not talking here about problems that do not have a solution such as finding real roots of a quadratic equation with a negative discriminant for such cases an output indicating that the problem does not have a solution is all we can and should expect from an algorithm nor are we talking about ambiguously stated problems even some unambiguous problems that must have a simple yes or no answer are undecidable ie unsolvable by any algorithm an important example of such a problem appears in section fortunately a vast majority of problems in practical computing can be solved by an algorithm before leaving this section let us be sure that you do not have the misconception possibly caused by the somewhat mechanical nature of the diagram of figure that designing an algorithm is a dull activity there is nothing further from the truth inventing or discovering algorithms is a very creative and rewarding process this book is designed to convince you that this is the case exercises old world puzzle a peasant finds himself on a riverbank with a wolf a goat and a head of cabbage he needs to transport all three to the other side of the river in his boat however the boat has room for only the peasant himself and one other item either the wolf the goat or the cabbage in his absence the wolf would eat the goat and the goat would eat the cabbage solve this problem for the peasant or prove it has no solution note the peasant is a vegetarian but does not like cabbage and hence can eat neither the goat nor the cabbage to help him solve the problem and it goes without saying that the wolf is a protected species new world puzzle there are four people who want to cross a rickety bridge they all begin on the same side you have minutes to get them all across to the other side it is night and they have one flashlight a maximum of two people can cross the bridge at one time any party that crosses either one or two people must have the flashlight with them the flashlight must be walked back and forth it can not be thrown for example person takes minute to cross the bridge person takes minutes person takes minutes and person takes minutes a pair must walk together at the rate of the slower persons pace note according to a rumor on the internet interviewers at a wellknown software company located near seattle have given this problem to interviewees which of the following formulas can be considered an algorithm for computing the area of a triangle whose side lengths are given positive numbers a b and c a s pp ap bp c where p a b c b s bc sin a where a is the angle between sides b and c c s aha where ha is the height to base a write pseudocode for an algorithm for finding real roots of equation ax bx c for arbitrary real coefficients a b and c you may assume the availability of the square root function sqrt x describe the standard algorithm for finding the binary representation of a positive decimal integer a in english b in pseudocode describe the algorithm used by your favorite atm machine in dispensing cash you may give your description in either english or pseudocode whichever you find more convenient a can the problem of computing the number be solved exactly b how many instances does this problem have c look up an algorithm for this problem on the internet give an example of a problem other than computing the greatest common divisor for which you know more than one algorithm which of them is simpler which is more efficient consider the following algorithm for finding the distance between the two closest elements in an array of numbers algorithm mindistancean input array an of numbers output minimum distance between two of its elements dmin for i to n do for j to n do if i j and ai aj dmin dmin ai aj return dmin make as many improvements as you can in this algorithmic solution to the problem if you need to you may change the algorithm altogether if not improve the implementation given one of the most influential books on problem solving titled how to solve it pol was written by the hungarianamerican mathematician george po lya po lya summarized his ideas in a fourpoint summary find this summary on the internet or better yet in his book and compare it with the plan outlined in section what do they have in common how are they different algorithm visualization in addition to the mathematical and empirical analyses of algorithms there is yet a third way to study algorithms it is called algorithm visualization and can be defined as the use of images to convey some useful information about algorithms that information can be a visual illustration of an algorithms operation of its performance on different kinds of inputs or of its execution speed versus that of other algorithms for the same problem to accomplish this goal an algorithm visualization uses graphic elements points line segments twoor threedimensional bars and so on to represent some interesting events in the algorithms operation there are two principal variations of algorithm visualization static algorithm visualization dynamic algorithm visualization also called algorithm animation static algorithm visualization shows an algorithms progress through a series of still images algorithm animation on the other hand shows a continuous movielike presentation of an algorithms operations animation is an arguably more sophisticated option which of course is much more difficult to implement early efforts in the area of algorithm visualization go back to the s the watershed event happened in with the appearance of a minute color sound film titled sorting out sorting this algorithm visualization classic was produced at the university of toronto by ronald baecker with the assistance of d sherman bae bae it contained visualizations of nine wellknown sorting algorithms more than half of them are discussed later in the book and provided quite a convincing demonstration of their relative speeds the success of sorting out sorting made sorting algorithms a perennial favorite for algorithm animation indeed the sorting problem lends itself quite naturally to visual presentation via vertical or horizontal bars or sticks of different heights or lengths which need to be rearranged according to their sizes figure this presentation is convenient however only for illustrating actions of a typical sorting algorithm on small inputs for larger files sorting out sorting used the ingenious idea of presenting data by a scatterplot of points on a coordinate plane with the first coordinate representing an items position in the file and the second one representing the items value with such a representation the process of sorting looks like a transformation of a random scatterplot of points into the points along a frames diagonal figure in addition most sorting algorithms figure initial and final screens of a typical visualization of a sorting algorithm using the bar representation work by comparing and exchanging two given items at a time an event that can be animated relatively easily since the appearance of sorting out sorting a great number of algorithm animations have been created especially after the appearance of java and the figure initial and final screens of a typical visualization of a sorting algorithm using the scatterplot representation world wide web in the s they range in scope from one particular algorithm to a group of algorithms for the same problem eg sorting or the same application area eg geometric algorithms to generalpurpose animation systems at the end of a catalog of links to existing visualizations maintained under the nsfsupported algovizproject contained over links unfortunately a survey of existing visualizations found most of them to be of low quality with the content heavily skewed toward easier topics such as sorting sha there are two principal applications of algorithm visualization research and education potential benefits for researchers are based on expectations that algorithm visualization may help uncover some unknown features of algorithms for example one researcher used a visualization of the recursive tower of hanoi algorithm in which oddand evennumbered disks were colored in two different colors he noticed that two disks of the same color never came in direct contact during the algorithms execution this observation helped him in developing a better nonrecursive version of the classic algorithm to give another example bentley and mcilroy ben mentioned using an algorithm animation system in their work on improving a library implementation of a leading sorting algorithm the application of algorithm visualization to education seeks to help students learning algorithms the available evidence of its effectiveness is decisively mixed although some experiments did register positive learning outcomes others failed to do so the increasing body of evidence indicates that creating sophisticated software systems is not going to be enough in fact it appears that the level of student involvement with visualization might be more important than specific features of visualization software in some experiments lowtech visualizations prepared by students were more effective than passive exposure to sophisticated software systems to summarize although some successes in both research and education have been reported in the literature they are not as impressive as one might expect a deeper understanding of human perception of images will be required before the true potential of algorithm visualization is fulfilled summary there are two kinds of algorithm efficiency time efficiency and space efficiency time efficiency indicates how fast the algorithm runs space efficiency deals with the extra space it requires an algorithms time efficiency is principally measured as a function of its input size by counting the number of times its basic operation is executed a basic operation is the operation that contributes the most to running time typically it is the most timeconsuming operation in the algorithms innermost loop for some algorithms the running time can differ considerably for inputs of the same size leading to worstcase efficiency averagecase efficiency and bestcase efficiency the established framework for analyzing time efficiency is primarily grounded in the order of growth of the algorithms running time as its input size goes to infinity the notations o and are used to indicate and compare the asymptotic orders of growth of functions expressing algorithm efficiencies the efficiencies of a large number of algorithms fall into the following few classes constant logarithmic linear linearithmic quadratic cubic and exponential the main tool for analyzing the time efficiency of a nonrecursive algorithm is to set up a sum expressing the number of executions of its basic operation and ascertain the sums order of growth the main tool for analyzing the time efficiency of a recursive algorithm is to set up a recurrence relation expressing the number of executions of its basic operation and ascertain the solutions order of growth succinctness of a recursive algorithm may mask its inefficiency the fibonacci numbers are an important sequence of integers in which every element is equal to the sum of its two immediate predecessors there are several algorithms for computing the fibonacci numbers with drastically different efficiencies empirical analysis of an algorithm is performed by running a program implementing the algorithm on a sample of inputs and analyzing the data observed the basic operations count or physical running time this often involves generating pseudorandom numbers the applicability to any algorithm is the principal strength of this approach the dependence of results on the particular computer and instance sample is its main weakness algorithm visualization is the use of images to convey useful information about algorithms the two principal variations of algorithm visualization are static algorithm visualization and dynamic algorithm visualization also called algorithm animation brute force and exhaustive search science is as far removed from brute force as this sword from a crowbar edward lytton leila book ii chapter i doing a thing well is often a waste of time robert byrne a master pool and billiards player and a writer after introducing the framework and methods for algorithm analysis in the preceding chapter we are ready to embark on a discussion of algorithm design strategies each of the next eight chapters is devoted to a particular design strategy the subject of this chapter is brute force and its important special case exhaustive search brute force can be described as follows brute force is a straightforward approach to solving a problem usually directly based on the problem statement and definitions of the concepts involved the force implied by the strategys definition is that of a computer and not that of ones intellect just do it would be another way to describe the prescription of the bruteforce approach and often the bruteforce strategy is indeed the one that is easiest to apply as an example consider the exponentiation problem compute an for a nonzero number a and a nonnegative integer n although this problem might seem trivial it provides a useful vehicle for illustrating several algorithm design strategies including the brute force also note that computing an mod m for some large integers is a principal component of a leading encryption algorithm by the definition of exponentiation an a a n times this suggests simply computing an by multiplying by a n times we have already encountered at least two bruteforce algorithms in the book the consecutive integer checking algorithm for computing gcdm n in section and the definitionbased algorithm for matrix multiplication in section many other examples are given later in this chapter can you identify a few algorithms you already know as being based on the bruteforce approach though rarely a source of clever or efficient algorithms the bruteforce approach should not be overlooked as an important algorithm design strategy first unlike some of the other strategies brute force is applicable to a very wide variety of problems in fact it seems to be the only general approach for which it is more difficult to point out problems it can not tackle second for some important problems eg sorting searching matrix multiplication string matching the bruteforce approach yields reasonable algorithms of at least some practical value with no limitation on instance size third the expense of designing a more efficient algorithm may be unjustifiable if only a few instances of a problem need to be solved and a bruteforce algorithm can solve those instances with acceptable speed fourth even if too inefficient in general a bruteforce algorithm can still be useful for solving smallsize instances of a problem finally a bruteforce algorithm can serve an important theoretical or educational purpose as a yardstick with which to judge more efficient alternatives for solving a problem selection sort and bubble sort in this section we consider the application of the bruteforce approach to the problem of sorting given a list of n orderable items eg numbers characters from some alphabet character strings rearrange them in nondecreasing order as we mentioned in section dozens of algorithms have been developed for solving this very important problem you might have learned several of them in the past if you have try to forget them for the time being and look at the problem afresh now after your mind is unburdened of previous knowledge of sorting algorithms ask yourself a question what would be the most straightforward method for solving the sorting problem reasonable people may disagree on the answer to this question the two algorithms discussed here selection sort and bubble sort seem to be the two prime candidates selection sort we start selection sort by scanning the entire given list to find its smallest element and exchange it with the first element putting the smallest element in its final position in the sorted list then we scan the list starting with the second element to find the smallest among the last n elements and exchange it with the second element putting the second smallest element in its final position generally on the ith pass through the list which we number from to n the algorithm searches for the smallest item among the last n i elements and swaps it with ai a a ai ai amin an in their final positions the last n i elements after n passes the list is sorted here is pseudocode of this algorithm which for simplicity assumes that the list is implemented as an array algorithm selectionsortan sorts a given array by selection sort input an array an of orderable elements output array an sorted in nondecreasing order for i to n do min i for j i to n do if aj amin min j swap ai and amin as an example the action of the algorithm on the list is illustrated in figure the analysis of selection sort is straightforward the input size is given by the number of elements n the basic operation is the key comparison aj amin the number of times it is executed depends only on the array size and is given by the following sum n n n n cn n i n i i j i i i figure example of sorting with selection sort each line corresponds to one iteration of the algorithm ie a pass through the lists tail to the right of the vertical bar an element in bold indicates the smallest element found elements to the left of the vertical bar are in their final positions and are not considered in this and subsequent iterations since we have already encountered the last sum in analyzing the algorithm of example in section you should be able to compute it now on your own whether you compute this sum by distributing the summation symbol or by immediately getting the sum of decreasing integers the answer of course must be the same n n n n n cn n i i j i i thus selection sort is a n algorithm on all inputs note however that the number of key swaps is only n or more precisely n one for each repetition of the i loop this property distinguishes selection sort positively from many other sorting algorithms bubble sort another bruteforce application to the sorting problem is to compare adjacent elements of the list and exchange them if they are out of order by doing it repeatedly we end up bubbling up the largest element to the last position on the list the next pass bubbles up the second largest element and so on until after n passes the list is sorted pass i i n of bubble sort can be represented by the following diagram a aj aj ani ani an in their final positions here is pseudocode of this algorithm algorithm bubblesortan sorts a given array by bubble sort input an array an of orderable elements output array an sorted in nondecreasing order for i to n do for j to n i do if aj aj swap aj and aj the action of the algorithm on the list is illustrated as an example in figure the number of key comparisons for the bubblesort version given above is the same for all arrays of size n it is obtained by a sum that is almost identical to the sum for selection sort etc figure first two passes of bubble sort on the list a new line is shown after a swap of two elements is done the elements to the right of the vertical bar are in their final positions and are not considered in subsequent iterations of the algorithm n ni n cn n i i j i n n n n i n i the number of key swaps however depends on the input in the worst case of decreasing arrays it is the same as the number of key comparisons sworst n cn n n n as is often the case with an application of the bruteforce strategy the first version of an algorithm obtained can often be improved upon with a modest amount of effort specifically we can improve the crude version of bubble sort given above by exploiting the following observation if a pass through the list makes no exchanges the list has been sorted and we can stop the algorithm problem a in this sections exercises though the new version runs faster on some inputs it is still in n in the worst and average cases in fact even among elementary sorting methods bubble sort is an inferior choice and if it were not for its catchy name you would probably have never heard of it however the general lesson you just learned is important and worth repeating a first application of the bruteforce approach often results in an algorithm that can be improved with a modest amount of effort exercises a give an example of an algorithm that should not be considered an application of the bruteforce approach b give an example of a problem that can not be solved by a bruteforce algorithm a what is the time efficiency of the bruteforce algorithm for computing an as a function of n as a function of the number of bits in the binary representation of n b if you are to compute an mod m where a and n is a large positive integer how would you circumvent the problem of a very large magnitude of an for each of the algorithms in problems and of exercises tell whether or not the algorithm is based on the bruteforce approach a design a bruteforce algorithm for computing the value of a polynomial px anxn anxn ax a at a given point x and determine its worstcase efficiency class n design a linear algorithm for this b if the algorithm you designed is in problem c is it possible to design an algorithm with a betterthanlinear efficiency for this problem a network topology specifies how computers printers and other devices are connected over a network the figure below illustrates three common topologies of networks the ring the star and the fully connected mesh ring star fully connected mesh you are given a boolean matrix an n where n which is supposed to be the adjacency matrix of a graph modeling a network with one of these topologies your task is to determine which of these three topologies if any the matrix represents design a bruteforce algorithm for this task and indicate its time efficiency class tetromino tilings tetrominoes are tiles made of four squares there are five types of tetrominoes shown below straight tetromino square tetromino ltetromino ttetromino ztetromino is it possible to tile ie cover exactly without overlaps an chessboard with a straight tetrominoes b square tetrominoes c ltetrominoes d ttetrominoes e ztetrominoes a stack of fake coins there are n stacks of n identicallooking coins all of the coins in one of these stacks are counterfeit while all the coins in the other stacks are genuine every genuine coin weighs grams every fake weighs grams you have an analytical scale that can determine the exact weight of any number of coins a devise a bruteforce algorithm to identify the stack with the fake coins and determine its worstcase efficiency class b what is the minimum number of weighings needed to identify the stack with the fake coins sort the list e x a m p l e in alphabetical order by selection sort is selection sort stable the definition of a stable sorting algorithm was given in section is it possible to implement selection sort for linked lists with the same n efficiency as the array version sort the list e x a m p l e in alphabetical order by bubble sort a prove that if bubble sort makes no exchanges on its pass through a list the list is sorted and the algorithm can be stopped b write pseudocode of the method that incorporates this improvement c prove that the worstcase efficiency of the improved version is quadratic is bubble sort stable alternating disks you have a row of n disks of two colors n dark and n light they alternate dark light dark light and so on you want to get all the dark disks to the righthand end and all the light disks to the lefthand end the only moves you are allowed to make are those that interchange the positions of two neighboring disks design an algorithm for solving this puzzle and determine the number of moves it takes gar